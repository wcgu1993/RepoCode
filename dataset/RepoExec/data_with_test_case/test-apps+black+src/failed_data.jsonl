{"problem_ids": 234, "project": "test-apps/black/src", "focal_function": "def generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")", "module": "blib2to3.pgen2.tokenize", "code": "from pprint import pprint\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\nfrom . import token\n\nENDMARKER: Final = 0\n\nNAME: Final = 1\n\nNUMBER: Final = 2\n\nSTRING: Final = 3\n\nNEWLINE: Final = 4\n\nINDENT: Final = 5\n\nDEDENT: Final = 6\n\nOP: Final = 52\n\nCOMMENT: Final = 53\n\nNL: Final = 54\n\nAWAIT: Final = 56\n\nASYNC: Final = 57\n\nERRORTOKEN: Final = 58\n\nclass Grammar(object):\n    \"\"\"Pgen parsing tables conversion class.\n\n    Once initialized, this class supplies the grammar tables for the\n    parsing engine implemented by parse.py.  The parsing engine\n    accesses the instance variables directly.  The class here does not\n    provide initialization of the tables; several subclasses exist to\n    do this (see the conv and pgen modules).\n\n    The load() method reads the tables from a pickle file, which is\n    much faster than the other ways offered by subclasses.  The pickle\n    file is written by calling dump() (after loading the grammar\n    tables using a subclass).  The report() method prints a readable\n    representation of the tables to stdout, for debugging.\n\n    The instance variables are as follows:\n\n    symbol2number -- a dict mapping symbol names to numbers.  Symbol\n    numbers are always 256 or higher, to distinguish\n    them from token numbers, which are between 0 and\n    255 (inclusive).\n\n    number2symbol -- a dict mapping numbers to symbol names;\n    these two are each other's inverse.\n\n    states        -- a list of DFAs, where each DFA is a list of\n    states, each state is a list of arcs, and each\n    arc is a (i, j) pair where i is a label and j is\n    a state number.  The DFA number is the index into\n    this list.  (This name is slightly confusing.)\n    Final states are represented by a special arc of\n    the form (0, j) where j is its own state number.\n\n    dfas          -- a dict mapping symbol numbers to (DFA, first)\n    pairs, where DFA is an item from the states list\n    above, and first is a set of tokens that can\n    begin this grammar rule (represented by a dict\n    whose values are always 1).\n\n    labels        -- a list of (x, y) pairs where x is either a token\n    number or a symbol number, and y is either None\n    or a string; the strings are keywords.  The label\n    number is the index in this list; label numbers\n    are used to mark state transitions (arcs) in the\n    DFAs.\n\n    start         -- the number of the grammar's start symbol.\n\n    keywords      -- a dict mapping keyword strings to arc labels.\n\n    tokens        -- a dict mapping token numbers to arc labels.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n\n    def dump(self, filename: Path) -> None:\n        \"\"\"Dump the grammar tables to a pickle file.\"\"\"\n\n    def _update(self, attrs: Dict[str, Any]) -> None:\n\n    def load(self, filename: Path) -> None:\n        \"\"\"Load the grammar tables from a pickle file.\"\"\"\n\n    def loads(self, pkl: bytes) -> None:\n        \"\"\"Load the grammar tables from a pickle bytes object.\"\"\"\n\n    def copy(self: _P) -> _P:\n        \"\"\"\n        Copy the grammar.\n        \"\"\"\n\n    def report(self) -> None:\n        \"\"\"Dump the grammar tables to standard output, for debugging.\"\"\"\n\ndel token\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\n\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\nclass TokenError(Exception):\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")", "entry_point": "generate_tokens", "test": null, "test_list": ["@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_0():\n    assert 0 == (\n        len(\n            list(\n                generate_tokens(\n                    \"\"\"# comment\n    pass\"\"\"\n                )\n            )\n        )\n        - 5\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_1():\n    assert 5 == len(list(generate_tokens(\"def f(x):\\n\\tx=x+1\\n\\treturn x\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_2():\n    assert 41 == len([tok for tok in generate_tokens(\"if x == 1:\\n  x = x+1\")])", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_3():\n    assert 3 == len(list(generate_tokens(\"def foo():\\n  pass\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_4():\n    assert 0 == evalfile(\n        sys.argv[0], globals(), {\"generate_tokens\": generate_tokens}\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_5():\n    assert 3 == len(list(generate_tokens(\"a+1\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_6():\n    assert 0 == sum([x != y for x, y in enumerate(generate_tokens(\"\"\"\n    class A():\n        pass\n    class B(A):\n        pass\n    \"\"\"))])", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_7():\n    assert 10 == len(list(generate_tokens(\"def foo():\\n  a+1\\n  pass\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_8():\n    assert 0 == sum([x != y for x, y in enumerate(generate_tokens(\"\"\"\n    def f(x):\n        return 2 * x\n    def g(x):\n        return 3 * x\n    \"\"\"))])", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_9():\n    assert 0 == len(list(generate_tokens(\"\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_10():\n    assert 1 == len(list(generate_tokens(\"\\n\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_11():\n    assert 86 == len([i for i in generate_tokens(\"print('Hello')\\n\")])", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_12():\n    assert list(generate_tokens(\"def foo():\\n\\tif True:\\n\\t\\tx = 1\\n\")) == [\n        (NAME, 'def', (1, 0), (1, 3), 'def foo():'),\n        (OP, '(', (1, 3), (1, 4), '('),\n        (OP, ')', (1, 4), (1, 5), ')'),\n        (OP, ':', (1, 5), (1, 6), ':'),\n        (NL, '\\n', (1, 6), (2, 0), '\\n'),\n        (INDENT, '\\t', (2, 0), (2, 1), '\\t'),\n        (NAME, 'if', (2, 1), (2, 3), '\\tif True:'),\n        (NAME, 'True', (2, 4), (2, 8), '\\t\\tTrue'),\n        (OP, ':', (2, 8), (2, 9), ':'),\n        (NL, '\\n', (2, 9), (3, 0), '\\t\\n'),\n        (INDENT, '\\t\\t', (3, 0), (3, 2), '\\t\\tx = 1'),\n        (NAME, 'x', (3, 2), (3, 3), '\\t\\tx = 1'),\n        (OP, '=', (3, 4), (3, 5), '='),\n        (NUMBER, '1', (3, 6), (3, 7), '1'),\n        (NL, '', (3, 7), (3, 7), ''),\n        (DEDENT, '', (3, 7), (3, 7), ''),\n        (DEDENT, '', (3, 7), (3, 7), ''),\n        (ENDMARKER, '', (3, 7), (3, 7), ''),\n    ]", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_13():\n    assert 2 == len(list(generate_tokens(\"a\\n\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_14():\n    assert [\n        (ERRORTOKEN, '!', (1, 0), (1, 1), '!\\n'),\n        (ENDMARKER, '', (1, 0), (1, 0), '')] == list(generate_tokens(\"!\\n\"))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_15():\n    assert 12 == len(list(generate_tokens(\"def foo():\\n  a+1\\n  pass\\n\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_16():\n    assert 5 == len(list(generate_tokens(\"\"\"def k(a, b=2):\n        if a == 0:\n            b = a + 1\n            return b\n        elif a == 1:\n            return 0\n        return -1\"\"\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_17():\n    assert [\n        (NAME, 'pass', (1, 0), (1, 4), 'pass\\n'),\n        (ENDMARKER, '', (1, 0), (1, 0), '')] == list(generate_tokens(\"pass\\n\"))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_18():\n    assert 0 == sum([x != y for x, y in enumerate(generate_tokens(\"\"\"\n    def f(x):\n        return 2 * x\n    \"\"\"))])", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_19():\n    assert 8 == len(list(generate_tokens(\"def foo():\\n  pass\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_20():\n    assert [\n        (NAME, 'print', (1, 0), (1, 5), 'print\\n'),\n        (OP, '(', (1, 5), (1, 6), '(\\n'),\n        (NAME, 'x', (1, 6), (1, 7), 'x'),\n        (OP, ',', (1, 7), (1, 8), ','),\n        (NAME, 'y', (1, 8), (1, 9), 'y'),\n        (OP, ')', (1, 9), (1, 10), ')\\n'),\n        (ENDMARKER, '', (1, 0), (1, 0), '')] == list(generate_tokens(\"print(x, y)\\n\"))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_21():\n    assert 78 == len(\n        list(\n            generate_tokens(\n                \"\\n\".join(\n                    (\n                        \"a = 3\",\n                        \"assert True\",\n                        \"while True:\",\n                        \"    pass\",\n                        \"class object:\",\n                        \"    pass\",\n                        \"    def function(self, param):\",\n                        \"        pass\",\n                        \"    def function(self, param): # a comment\",\n                        \"        pass\",\n                    )\n                )\n            )\n        )\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_22():\n    assert 0 == len(list(generate_tokens(\"\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_23():\n    assert 0 == evalfile(\n        \"examples/tokenize.py\", {\"generate_tokens\": generate_tokens}, (\"__name__\", \"generate_tokens\")\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_24():\n    assert 23 == len(list(generate_tokens(\"def foo():\\n  a+1\\n  pass\\n\\nclass X:\\n  pass\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_25():\n    assert 3 == len(list(generate_tokens(\"a+1\\n\")))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_26():\n    assert [\n        (OP, '(', (1, 0), (1, 1), '(\\n'),\n        (ERRORTOKEN, ',', (1, 1), (1, 2), ',\\n'),\n        (ERRORTOKEN, ')', (1, 2), (1, 3), ')\\n'),\n        (ENDMARKER, '', (1, 0), (1, 0), '')] == list(generate_tokens(\",\\n)\"))", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_27():\n    assert 1 == len(list(generate_tokens(\"a\\n\")))"]}
{"problem_ids": 237, "project": "test-apps/black/src", "focal_function": "def load_packaged_grammar(\n    package: str, grammar_source: Text, cache_dir: Optional[Path] = None\n) -> grammar.Grammar:\n    \"\"\"Normally, loads a pickled grammar by doing\n        pkgutil.get_data(package, pickled_grammar)\n    where *pickled_grammar* is computed from *grammar_source* by adding the\n    Python version and using a ``.pickle`` extension.\n\n    However, if *grammar_source* is an extant file, load_grammar(grammar_source)\n    is called instead. This facilitates using a packaged grammar file when needed\n    but preserves load_grammar's automatic regeneration behavior when possible.\n\n    \"\"\"\n    if os.path.isfile(grammar_source):\n        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None\n        return load_grammar(grammar_source, gp=gp)\n    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)\n    data = pkgutil.get_data(package, pickled_name)\n    assert data is not None\n    g = grammar.Grammar()\n    g.loads(data)\n    return g", "module": "blib2to3.pgen2.driver", "code": "from pprint import pprint\nimport codecs\nimport io\nimport os\nimport logging\nimport pkgutil\nimport sys\nfrom typing import (\n    Any,\n    Callable,\n    IO,\n    Iterable,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Union,\n    Sequence,\n)\nfrom . import grammar, parse, token, tokenize, pgen\nfrom logging import Logger\nfrom blib2to3.pytree import _Convert, NL\nfrom blib2to3.pgen2.grammar import Grammar\n\nclass Grammar(object):\n    \"\"\"Pgen parsing tables conversion class.\n\n    Once initialized, this class supplies the grammar tables for the\n    parsing engine implemented by parse.py.  The parsing engine\n    accesses the instance variables directly.  The class here does not\n    provide initialization of the tables; several subclasses exist to\n    do this (see the conv and pgen modules).\n\n    The load() method reads the tables from a pickle file, which is\n    much faster than the other ways offered by subclasses.  The pickle\n    file is written by calling dump() (after loading the grammar\n    tables using a subclass).  The report() method prints a readable\n    representation of the tables to stdout, for debugging.\n\n    The instance variables are as follows:\n\n    symbol2number -- a dict mapping symbol names to numbers.  Symbol\n    numbers are always 256 or higher, to distinguish\n    them from token numbers, which are between 0 and\n    255 (inclusive).\n\n    number2symbol -- a dict mapping numbers to symbol names;\n    these two are each other's inverse.\n\n    states        -- a list of DFAs, where each DFA is a list of\n    states, each state is a list of arcs, and each\n    arc is a (i, j) pair where i is a label and j is\n    a state number.  The DFA number is the index into\n    this list.  (This name is slightly confusing.)\n    Final states are represented by a special arc of\n    the form (0, j) where j is its own state number.\n\n    dfas          -- a dict mapping symbol numbers to (DFA, first)\n    pairs, where DFA is an item from the states list\n    above, and first is a set of tokens that can\n    begin this grammar rule (represented by a dict\n    whose values are always 1).\n\n    labels        -- a list of (x, y) pairs where x is either a token\n    number or a symbol number, and y is either None\n    or a string; the strings are keywords.  The label\n    number is the index in this list; label numbers\n    are used to mark state transitions (arcs) in the\n    DFAs.\n\n    start         -- the number of the grammar's start symbol.\n\n    keywords      -- a dict mapping keyword strings to arc labels.\n\n    tokens        -- a dict mapping token numbers to arc labels.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n\n    def dump(self, filename: Path) -> None:\n        \"\"\"Dump the grammar tables to a pickle file.\"\"\"\n\n    def _update(self, attrs: Dict[str, Any]) -> None:\n\n    def load(self, filename: Path) -> None:\n        \"\"\"Load the grammar tables from a pickle file.\"\"\"\n\n    def loads(self, pkl: bytes) -> None:\n        \"\"\"Load the grammar tables from a pickle bytes object.\"\"\"\n\n    def copy(self: _P) -> _P:\n        \"\"\"\n        Copy the grammar.\n        \"\"\"\n\n    def report(self) -> None:\n        \"\"\"Dump the grammar tables to standard output, for debugging.\"\"\"\n\nPath = Union[str, \"os.PathLike[str]\"]\n\ndef _generate_pickle_name(gt: Path, cache_dir: Optional[Path] = None) -> Text:\n\ndef load_grammar(\n    gt: Text = \"Grammar.txt\",\n    gp: Optional[Text] = None,\n    save: bool = True,\n    force: bool = False,\n    logger: Optional[Logger] = None,\n) -> Grammar:\n    \"\"\"Load the grammar (maybe from a pickle).\"\"\"\n\ndef load_packaged_grammar(\n    package: str, grammar_source: Text, cache_dir: Optional[Path] = None\n) -> grammar.Grammar:\n    \"\"\"Normally, loads a pickled grammar by doing\n        pkgutil.get_data(package, pickled_grammar)\n    where *pickled_grammar* is computed from *grammar_source* by adding the\n    Python version and using a ``.pickle`` extension.\n\n    However, if *grammar_source* is an extant file, load_grammar(grammar_source)\n    is called instead. This facilitates using a packaged grammar file when needed\n    but preserves load_grammar's automatic regeneration behavior when possible.\n\n    \"\"\"\n    if os.path.isfile(grammar_source):\n        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None\n        return load_grammar(grammar_source, gp=gp)\n    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)\n    data = pkgutil.get_data(package, pickled_name)\n    assert data is not None\n    g = grammar.Grammar()\n    g.loads(data)\n    return g", "entry_point": "load_packaged_grammar", "test": null, "test_list": ["@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_0():\n    assert load_packaged_grammar(__package__, 'Grammar.txt') is not None", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_1():\n    assert load_packaged_grammar(\"astroid\", \"Grammar.txt\").version == '2.0.0'", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_2():\n    assert load_packaged_grammar(\n        __name__, \"Grammar.txt\"\n    ).load(\"Grammar.txt\", force=False) == load_grammar(\"Grammar.txt\", force=False)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_3():\n    assert \"plasTeX.TeX\" == load_packaged_grammar(\"plasTeX\", \"TeX.txt\").name", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_4():\n    assert load_packaged_grammar(__name__, \"Grammar.txt\").loadable is True", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_5():\n    assert 0 == load_packaged_grammar(\"tokenize_main\", \"Grammar.txt\").check(\"if a<b: print\").next()", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_6():\n    assert isinstance(load_packaged_grammar(__name__, \"Grammar.txt\", cache_dir=__file__), grammar.Grammar)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_7():\n    assert load_packaged_grammar(\"astroid\", \"Grammar.txt\") is not None", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_8():\n    assert load_packaged_grammar(__name__, \"Grammar.txt\").load(\"Grammar.txt\") == 0", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_9():\n    assert 10 == len(load_packaged_grammar(\"json.tool\", \"Grammar.txt\").dfas)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_10():\n    assert 0 == load_packaged_grammar(\n        package=\"z3\",\n        grammar_source=\"z3/z3.g\",\n        cache_dir=None,\n    ).load(\n        \"z3/z3.g\"\n    ).load(\n        \"z3/z3.g\"\n    ).start", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_11():\n    assert grammar.load_packaged_grammar('pyexpander', 'Grammar.txt')", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_12():\n    assert grammar.Grammar().load_packaged_grammar('pyparsing', 'Grammar')", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_13():\n    assert load_packaged_grammar(__name__, __file__, cache_dir=None) is not None", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_14():\n    assert load_packaged_grammar(\"pyexpander\", \"Grammar.txt\").load() == load_grammar(\"Grammar.txt\").load()", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_15():\n    assert 10 == len(load_packaged_grammar(\"json.tool\", \"Grammar_3_9.txt\", None).dfas)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_16():\n    assert load_packaged_grammar('pymatgen.io.vasp.outputs', 'Grammar.txt')", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_17():\n    assert 0 == len(\n        difflib.unified_diff(\n            [\n                line\n                for line in load_packaged_grammar(\"blib2to3\", \"Grammar.txt\").unsplit_lines()\n                if not line.startswith(\"#\")\n            ],\n            [\n                line\n                for line in load_grammar(\"Grammar.txt\").unsplit_lines()\n                if not line.startswith(\"#\")\n            ],\n        )\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_18():\n    assert load_packaged_grammar(__package__, 'Grammar.txt').checkVersion(Version)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_19():\n    assert isinstance(load_packaged_grammar(\"astroid\", \"Grammar.txt\"), grammar.Grammar)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_20():\n    assert (\n        load_packaged_grammar('parso.python.grammar', 'Grammar38.txt').__class__.__name__\n        == 'Grammar'\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_21():\n    assert \"Grammar\" == load_packaged_grammar('curtsies', 'Grammar.txt').__class__.__name__", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_22():\n    assert 10 == len(load_packaged_grammar(\"json.tool\", \"Grammar_3_9.txt\").dfas)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_23():\n    assert not load_packaged_grammar(__name__, \"not_existing_grammar\").loadable", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_24():\n    assert \"Grammar\" in load_packaged_grammar(\"tokenize\", \"Grammar\").__dict__", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_25():\n    assert load_packaged_grammar(\n        \"tokenize\", \"Grammar.txt\").loadable == True", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_26():\n    assert isinstance(load_packaged_grammar('ply.yacc', 'Grammar.txt'), grammar.Grammar)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_27():\n    assert load_packaged_grammar(\n            'sphinx.ext.autodoc', 'Grammar.txt') == load_packaged_grammar(\n            'sphinx.ext.autodoc', 'Grammar.txt')", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_28():\n    assert load_packaged_grammar('pymitblod', 'Grammar.txt') == load_packaged_grammar(\n        'pymitblod', os.path.join(os.path.dirname(__file__), 'Grammar.txt')\n    )", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_29():\n    assert 224 == grammar.Grammar().load(load_packaged_grammar(\"mwparserfromhell\", \"mwparserfromhell/parser.py\")).number_symbols()", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_30():\n    assert load_packaged_grammar(__package__, \"Grammar.txt\").load(\"Grammar.txt\") == load_grammar(\"Grammar.txt\")", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_31():\n    assert 10 == len(load_packaged_grammar(\"json.tool\", \"Grammar.txt\", None).dfas)", "@timeout_decorator.timeout(LOCAL_TIMEOUT)\ndef test_32():\n    assert isinstance(\n        load_packaged_grammar(\"reportlab.lib.utils\", \"Grammar.txt\"), grammar.Grammar\n    )"]}
