[{"file": "./dataset/RepoExec/test-apps/luigi/setup.py", "function": "get_static_files", "line_number": 21, "body": "def get_static_files(path):\n    return [os.path.join(dirpath.replace(\"luigi/\", \"\"), ext)\n            for (dirpath, dirnames, filenames) in os.walk(path)\n            for ext in [\"*.html\", \"*.js\", \"*.css\", \"*.png\",\n                        \"*.eot\", \"*.svg\", \"*.ttf\", \"*.woff\", \"*.woff2\"]]", "is_method": false, "function_description": "This function collects and returns paths of static files with specified extensions within a directory tree, omitting a specific prefix in their paths. It facilitates gathering web assets like HTML, JavaScript, CSS, and fonts for use or deployment."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "_warn_node", "line_number": 56, "body": "def _warn_node(self, msg, node, *args, **kwargs):\n    \"\"\"\n    Mute warnings that are like ``WARNING: nonlocal image URI found: https://img. ...``\n\n    Solution was found by googling, copied it from SO:\n\n    http://stackoverflow.com/questions/12772927/specifying-an-online-image-in-sphinx-restructuredtext-format\n    \"\"\"\n    if not msg.startswith('nonlocal image URI found:'):\n        self._warnfunc(msg, '%s:%s' % get_source_line(node), *args, **kwargs)", "is_method": false, "function_description": "This function selectively suppresses specific image URI warnings while allowing other warnings to be processed normally, helping control noise during document processing or building."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "parameter_repr", "line_number": 27, "body": "def parameter_repr(self):\n        \"\"\"\n        When building documentation, we want Parameter objects to show their\n        description in a nice way\n        \"\"\"\n        significance = 'Insignificant ' if not self.significant else ''\n        class_name = self.__class__.__name__\n        has_default = self._default != luigi.parameter._no_value\n        default = ' (defaults to {})'.format(self._default) if has_default else ''\n        description = (': ' + self.description if self.description else '')\n        return significance + class_name + default + description", "is_method": false, "function_description": "Provides a formatted string representation of a Parameter object, including significance, default value, and description, for improved readability in documentation and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "assertIn", "line_number": 41, "body": "def assertIn(needle, haystack):\n        \"\"\"\n        We test repr of Parameter objects, since it'll be used for readthedocs\n        \"\"\"\n        assert needle in haystack", "is_method": false, "function_description": "Function that asserts whether a specified value exists within another container, raising an error if not found. It can be used to validate membership conditions during testing or debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "requires", "line_number": 66, "body": "def requires(self):\n        return [ErrorTask1(), ErrorTask2(), SuccessTask1(), DynamicErrorTaskSubmitter()]", "is_method": true, "class_name": "PerTaskRetryPolicy", "function_description": "Returns a list of task instances that this retry policy depends on, indicating prerequisites or related tasks to be executed or monitored."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 69, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "PerTaskRetryPolicy", "function_description": "Returns a local file target for storing task-specific output data, enabling consistent access to task results within Luigi pipeline workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 82, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "ErrorTask1", "function_description": "Raises a test exception each time it's run, incrementing a retry counter to track the number of attempts for the task. Useful for simulating failure and retry behavior in task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 86, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "ErrorTask1", "function_description": "Returns a local file target path specific to the task, facilitating task output management in Luigi workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 99, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "ErrorTask2", "function_description": "Raises an exception with retry count and task family information each time it is called, simulating a failure for testing retry mechanisms."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 103, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "ErrorTask2", "function_description": "Provides a local file target path for output, uniquely identified by the task ID within the ErrorTask2 Luigi task. This enables consistent access to task-specific output data storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 110, "body": "def run(self):\n        target = yield DynamicErrorTask1()\n\n        if target.exists():\n            with self.output().open('w') as output:\n                output.write('SUCCESS DynamicErrorTaskSubmitter\\n')", "is_method": true, "class_name": "DynamicErrorTaskSubmitter", "function_description": "This method runs a dynamic error task and writes a success message if the task's target exists. It facilitates conditional task execution and result recording within a task submission workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 117, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "DynamicErrorTaskSubmitter", "function_description": "Returns a local file target path for storing output related to the task, facilitating file handling within Luigi workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 130, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "DynamicErrorTask1", "function_description": "Raises an exception with a retry count and task identifier each time it is run, primarily to simulate errors and track retry attempts within the DynamicErrorTask1."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 134, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "DynamicErrorTask1", "function_description": "Returns a local file target path for storing output data unique to the task instance, facilitating task output management in Luigi workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "requires", "line_number": 139, "body": "def requires(self):\n        return [SuccessSubTask1()]", "is_method": true, "class_name": "SuccessTask1", "function_description": "Returns a list of prerequisite subtasks needed before executing the main success task, enabling task dependency management within the SuccessTask1 workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 142, "body": "def run(self):\n        with self.output().open('w') as output:\n            output.write('SUCCESS Test Task 4\\n')", "is_method": true, "class_name": "SuccessTask1", "function_description": "This function writes a fixed success message to its designated output target, serving as a simple completion indicator for the SuccessTask1 class. It can be used to signal or log successful task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 146, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "SuccessTask1", "function_description": "Returns a LocalTarget pointing to a task-specific file path for storing output data, enabling downstream tasks to locate this task's result within the Luigi pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 155, "body": "def run(self):\n        with self.output().open('w') as output:\n            output.write('SUCCESS Test Task 4.1\\n')", "is_method": true, "class_name": "SuccessSubTask1", "function_description": "Method of SuccessSubTask1 that writes a fixed success message to its output, typically used to indicate task completion in a workflow or pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 159, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "SuccessSubTask1", "function_description": "Returns a local file target representing the output location for the task, enabling downstream tasks to access generated data by this task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "run", "line_number": 31, "body": "def run(self):\n        \"\"\"\n        Generates :py:attr:`~.UserItemMatrix.data_size` elements.\n        Writes this data in \\\\ separated value format into the target :py:func:`~/.UserItemMatrix.output`.\n\n        The data has the following elements:\n\n        * `user` is the default Elasticsearch id field,\n        * `track`: the text,\n        * `rating`: the day when the data was created.\n\n        \"\"\"\n        w = self.output().open('w')\n        for user in range(self.data_size):\n            track = int(random.random() * self.data_size)\n            w.write('%d\\\\%d\\\\%f' % (user, track, 1.0))\n        w.close()", "is_method": true, "class_name": "UserItemMatrix", "function_description": "Generates a user-item interaction dataset of specified size and writes it in a backslash-separated format to a target output for use in recommendation or data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "output", "line_number": 49, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('data-matrix', format=luigi.format.Gzip)", "is_method": true, "class_name": "UserItemMatrix", "function_description": "Provides the target output for a task, specifically indicating a compressed file in HDFS as the task's successful completion artifact. This facilitates integration with Luigi workflow management for tracking task outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "app_options", "line_number": 84, "body": "def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]", "is_method": true, "class_name": "SparkALS", "function_description": "Returns the input and output paths as a list for use as arguments in a Spark application. This function provides the necessary file locations for launching Spark jobs within the SparkALS context."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "requires", "line_number": 88, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.UserItemMatrix`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return UserItemMatrix(self.data_size)", "is_method": true, "class_name": "SparkALS", "function_description": "Returns the prerequisite task required before running the current task, specifying the UserItemMatrix with the given data size as a dependency. This enables task orchestration and ensures proper execution order in workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "output", "line_number": 98, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        # The corresponding Spark job outputs as GZip format.\n        return luigi.contrib.hdfs.HdfsTarget('als-output/', format=luigi.format.Gzip)", "is_method": true, "class_name": "SparkALS", "function_description": "Provides the target output location for the SparkALS task, specifying the expected HDFS file in GZip format created upon successful task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "output", "line_number": 33, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on a remote server using SSH.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return RemoteTarget(\n            \"/tmp/stuff\",\n            SSH_HOST\n        )", "is_method": true, "class_name": "CreateRemoteData", "function_description": "Provides the target output representing a file on a remote server, enabling tasks to interact with remote files over SSH for further processing or validation."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "run", "line_number": 46, "body": "def run(self):\n        remote = RemoteContext(SSH_HOST)\n        print(remote.check_output([\n            \"ps aux > {0}\".format(self.output().path)\n        ]))", "is_method": true, "class_name": "CreateRemoteData", "function_description": "Runs a remote SSH command to list all active processes and saves the output to a specified file. This method enables capturing remote system process information for monitoring or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "requires", "line_number": 60, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.CreateRemoteData`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return CreateRemoteData()", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "Indicates that this task depends on the completion of the CreateRemoteData task, ensuring prerequisite data preparation before execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "run", "line_number": 70, "body": "def run(self):\n        processes_per_user = defaultdict(int)\n        with self.input().open('r') as infile:\n            for line in infile:\n                username = line.split()[0]\n                processes_per_user[username] += 1\n\n        toplist = sorted(\n            processes_per_user.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n\n        with self.output().open('w') as outfile:\n            for user, n_processes in toplist:\n                print(n_processes, user, file=outfile)", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "Method of the ProcessRemoteData class that counts and ranks users by their number of running processes from input data, then writes this sorted summary to an output destination. It enables quick identification of users with the most active processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "output", "line_number": 87, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will simulate the creation of a file in a filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return MockTarget(\"output\", mirror_on_stderr=True)", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "Returns a simulated file target representing the expected output of the task, enabling workflow systems to track task completion without actual filesystem side effects."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "output", "line_number": 27, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in the local file system.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(self.date.strftime('/var/tmp/text/%Y-%m-%d.txt'))", "is_method": true, "class_name": "InputText", "function_description": "Provides the target output file path for the task based on the date, enabling task workflows to locate or create the expected local output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "requires", "line_number": 41, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.InputText`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [InputText(date) for date in self.date_interval.dates()]", "is_method": true, "class_name": "WordCount", "function_description": "Returns the list of dependent InputText tasks for the given date interval, specifying the prerequisites required before executing this WordCount task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "output", "line_number": 51, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/var/tmp/text-count/%s' % self.date_interval)", "is_method": true, "class_name": "WordCount", "function_description": "Provides a filesystem target representing the output location for storing the word count result based on a date interval. This enables other tasks to access or verify the output file in pipeline workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "run", "line_number": 61, "body": "def run(self):\n        \"\"\"\n        1. count the words for each of the :py:meth:`~.InputText.output` targets created by :py:class:`~.InputText`\n        2. write the count into the :py:meth:`~.WordCount.output` target\n        \"\"\"\n        count = {}\n\n        # NOTE: self.input() actually returns an element for the InputText.output() target\n        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in count.items():\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close()", "is_method": true, "class_name": "WordCount", "function_description": "Utility method of the WordCount class that counts word occurrences across input text targets and writes the aggregated word counts to an output target for further processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "run", "line_number": 39, "body": "def run(self):\n        print(\"Running Foo\")", "is_method": true, "class_name": "Foo", "function_description": "Simple method of the Foo class that indicates the instance is running by printing a message. It serves as a basic action trigger or status notifier."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "requires", "line_number": 42, "body": "def requires(self):\n        global current_nodes\n        for i in range(30 // max_depth):\n            current_nodes += 1\n            yield Bar(i)", "is_method": true, "class_name": "Foo", "function_description": "This method generates a sequence of Bar instances based on a calculation involving max_depth and updates a global node counter. It provides an iterable of dependencies or related objects for the Foo class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "run", "line_number": 54, "body": "def run(self):\n        time.sleep(1)\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "This function pauses execution briefly, then creates or clears a designated output target, signaling task completion within the Bar class's workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "requires", "line_number": 58, "body": "def requires(self):\n        global current_nodes\n\n        if max_total_nodes > current_nodes:\n            valor = int(random.uniform(1, 30))\n            for i in range(valor // max_depth):\n                current_nodes += 1\n                yield Bar(current_nodes)", "is_method": true, "class_name": "Bar", "function_description": "Generates and yields a sequence of new Bar instances while ensuring the total number of nodes does not exceed a global maximum limit. It supports controlled, incremental creation of Bar objects based on current state and predefined depth constraints."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "output", "line_number": 67, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        time.sleep(1)\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "Provides the target output location for the Bar task, returning a local file target based on its identifier. Useful for defining where task results are stored in a Luigi workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 27, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Config_%d.txt' % self.seed)", "is_method": true, "class_name": "Configuration", "function_description": "Provides the target output of the Configuration task as a local file path, representing where the task\u2019s result will be stored on the filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 37, "body": "def run(self):\n        time.sleep(5)\n        rnd.seed(self.seed)\n\n        result = ','.join(\n            [str(x) for x in rnd.sample(list(range(300)), rnd.randint(7, 25))])\n        with self.output().open('w') as f:\n            f.write(result)", "is_method": true, "class_name": "Configuration", "function_description": "Generates and saves a randomized, comma-separated list of unique numbers within a specified range, seeded for reproducibility. Useful for producing consistent random samples for configurations or test data."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Data_%d.txt' % self.magic_number)", "is_method": true, "class_name": "Data", "function_description": "Provides the target output of the task as a local file path, representing the expected result upon successful task execution. This enables other functions to access or verify the generated file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 60, "body": "def run(self):\n        time.sleep(1)\n        with self.output().open('w') as f:\n            f.write('%s' % self.magic_number)", "is_method": true, "class_name": "Data", "function_description": "Writes the object's magic number to an output destination after a brief delay, enabling downstream processes to access this value as a task completion signal or data artifact."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 69, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Dynamic_%d.txt' % self.seed)", "is_method": true, "class_name": "Dynamic", "function_description": "Provides the target output file path for the Dynamic task, representing the expected result of successful execution stored locally. This enables workflow components to identify and verify task completion through the generated file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 79, "body": "def run(self):\n        # This could be done using regular requires method\n        config = self.clone(Configuration)\n        yield config\n\n        with config.output().open() as f:\n            data = [int(x) for x in f.read().split(',')]\n\n        # ... but not this\n        data_dependent_deps = [Data(magic_number=x) for x in data]\n        yield data_dependent_deps\n\n        with self.output().open('w') as f:\n            f.write('Tada!')", "is_method": true, "class_name": "Dynamic", "function_description": "Core method of the Dynamic class that manages a data processing workflow by yielding configuration and data-dependent objects, then writes a completion marker. It supports dynamic dependency resolution based on runtime data."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/hello_world.py", "function": "run", "line_number": 16, "body": "def run(self):\n        print(\"{task} says: Hello world!\".format(task=self.__class__.__name__))", "is_method": true, "class_name": "HelloWorldTask", "function_description": "Core method of HelloWorldTask that outputs a greeting message identifying the task by its class name, useful for simple demonstration or testing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "output", "line_number": 34, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file that will be created in a FTP server.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return RemoteTarget('/experiment/output1.txt', HOST, username=USER, password=PWD)", "is_method": true, "class_name": "ExperimentTask", "function_description": "Returns the target output location representing the file created on the FTP server upon successful completion of the task. This enables downstream tasks to access or verify the task's result in a pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "run", "line_number": 44, "body": "def run(self):\n        \"\"\"\n        The execution of this task will write 4 lines of data on this task's target output.\n        \"\"\"\n        with self.output().open('w') as outfile:\n            print(\"data 0 200 10 50 60\", file=outfile)\n            print(\"data 1 190 9 52 60\", file=outfile)\n            print(\"data 2 200 10 52 60\", file=outfile)\n            print(\"data 3 195 1 52 60\", file=outfile)", "is_method": true, "class_name": "ExperimentTask", "function_description": "Runs the ExperimentTask by writing four specific lines of data to its designated output file. This method provides a predefined data output useful for testing or demonstration purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "requires", "line_number": 61, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.ExperimentTask`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return ExperimentTask()", "is_method": true, "class_name": "ProcessingTask", "function_description": "Returns the dependency task required before this processing task can run, ensuring proper execution order in Luigi workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "output", "line_number": 71, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/processeddata.txt')", "is_method": true, "class_name": "ProcessingTask", "function_description": "Returns the designated output target representing the file created upon successful completion of this processing task, enabling downstream tasks to access or verify the result."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "run", "line_number": 81, "body": "def run(self):\n        avg = 0.0\n        elements = 0\n        sumval = 0.0\n\n        # Target objects are a file system/format abstraction and this will return a file stream object\n        # NOTE: self.input() actually returns the ExperimentTask.output() target\n        for line in self.input().open('r'):\n            values = line.split(\" \")\n            avg += float(values[2])\n            sumval += float(values[3])\n            elements = elements + 1\n\n        # average\n        avg = avg / elements\n\n        # save calculated values\n        with self.output().open('w') as outfile:\n            print(avg, sumval, file=outfile)", "is_method": true, "class_name": "ProcessingTask", "function_description": "Method of ProcessingTask class that reads input data line-by-line, calculates the average of the third and sum of the fourth values, and writes these summarized results to an output target."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "run", "line_number": 32, "body": "def run(self):\n        \"\"\"\n        Writes data in JSON format into the task's output target.\n\n        The data objects have the following attributes:\n\n        * `_id` is the default Elasticsearch id field,\n        * `text`: the text,\n        * `date`: the day when the data was created.\n\n        \"\"\"\n        today = datetime.date.today()\n        with self.output().open('w') as output:\n            for i in range(5):\n                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n                                         'date': str(today)}))\n                output.write('\\n')", "is_method": true, "class_name": "FakeDocuments", "function_description": "Generates and writes five JSON-formatted data entries with id, text, and creation date to the specified output, useful for creating sample or placeholder documents in testing or data pipeline tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.date)", "is_method": true, "class_name": "FakeDocuments", "function_description": "Returns a Luigi LocalTarget pointing to the file path where the task's output will be stored, representing the expected result of the task execution on the local filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "requires", "line_number": 103, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.FakeDocuments`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return FakeDocuments()", "is_method": true, "class_name": "IndexDocuments", "function_description": "Specifies the prerequisite task that must be completed before this task runs, ensuring task execution order within a Luigi pipeline. It returns the dependent FakeDocuments task required by IndexDocuments."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "complete", "line_number": 54, "body": "def complete(self):\n        return False", "is_method": true, "class_name": "MyExternal", "function_description": "Simple status-check method in the MyExternal class that indicates the completion state, always returning False. It can be used to signal ongoing or unfinished processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 62, "body": "def run(self):\n        print(\"Running Boom\")", "is_method": true, "class_name": "Boom", "function_description": "Simple method of the Boom class that outputs a confirmation message indicating it is running. It can be used for basic status signaling or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 65, "body": "def requires(self):\n        for i in range(5, 200):\n            yield Bar(i)", "is_method": true, "class_name": "Boom", "function_description": "Generator method in the Boom class that produces Bar instances with integer values from 5 up to 199, providing a sequence of dependent or required Bar objects for iteration or processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 75, "body": "def run(self):\n        print(\"Running Foo\")", "is_method": true, "class_name": "Foo", "function_description": "Simple method of the Foo class that outputs a fixed message indicating it is running. It primarily serves as a basic demonstration or placeholder function."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 78, "body": "def requires(self):\n        yield MyExternal()\n        yield Boom(0)", "is_method": true, "class_name": "Foo", "function_description": "Generates and yields required dependencies or components for the Foo class, specifying necessary external objects or configurations it relies on."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 87, "body": "def run(self):\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "This method clears or initializes the target output resource by opening it in write mode and then immediately closing it, effectively resetting its contents. It is useful for preparing the output destination before writing new data."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "output", "line_number": 90, "body": "def output(self):\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "Provides a Luigi task target pointing to a local file path based on the instance's number attribute, facilitating task output management and file handling within workflow pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 99, "body": "def run(self):\n        print(\"Running DateTask\")", "is_method": true, "class_name": "DateTask", "function_description": "Core method of the DateTask class that executes the task's main operation by signaling its start. It serves as a simple entry point to initiate the DateTask process."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 102, "body": "def requires(self):\n        yield MyExternal()\n        yield Boom(0)", "is_method": true, "class_name": "DateTask", "function_description": "This method provides an iterable sequence of task dependencies required by the DateTask, enabling integration with external and internal components for sequential task execution or setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 110, "body": "def run(self):\n        print(\"Running EntryPoint\")", "is_method": true, "class_name": "EntryPoint", "function_description": "Simple method of the EntryPoint class that starts execution by printing a fixed message. It serves primarily to indicate that the entry point process has begun."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 113, "body": "def requires(self):\n        for i in range(10):\n            yield Foo(100, 2 * i)\n        for i in range(10):\n            yield DateTask(datetime.date(1998, 3, 23) + datetime.timedelta(days=i), 5)", "is_method": true, "class_name": "EntryPoint", "function_description": "Generates a sequence of prerequisite tasks, alternating between parameterized Foo instances and DateTask instances spread over sequential dates. This method provides the EntryPoint class with iterable dependencies needed for subsequent processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "run", "line_number": 34, "body": "def run(self):\n        print(\"Running Foo\")", "is_method": true, "class_name": "Foo", "function_description": "Simple method in Foo that outputs a fixed message indicating the class is running; primarily serves as a basic demonstration or placeholder action."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "requires", "line_number": 37, "body": "def requires(self):\n        for i in range(10):\n            yield Bar(i)", "is_method": true, "class_name": "Foo", "function_description": "Generator method of the Foo class that produces a sequence of Bar instances initialized with integers from 0 to 9. It provides iterative access to these Bar objects for consumers needing a fixed range of such elements."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "run", "line_number": 46, "body": "def run(self):\n        time.sleep(1)\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "This method simulates a delay and then creates or resets a target file by opening it in write mode and immediately closing it, typically signaling task completion in workflow pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        time.sleep(1)\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "Provides the target output location for the task, returning a specific local file path wrapped as a Luigi Target object for task dependency and output management."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "input", "line_number": 41, "body": "def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "Returns the input data location in S3 for the word count job, specifying where the input files are stored for processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "output", "line_number": 44, "body": "def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "Returns the S3 target location where the word count output is stored. This method provides the output path for downstream tasks to access the job results."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "main", "line_number": 47, "body": "def main(self, sc, *args):\n        sc.textFile(self.input().path) \\\n          .flatMap(lambda line: line.split()) \\\n          .map(lambda word: (word, 1)) \\\n          .reduceByKey(lambda a, b: a + b) \\\n          .saveAsTextFile(self.output().path)", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "Core method of InlinePySparkWordCount that performs word count on text from an input file using Spark and saves the aggregated results to an output path for further analysis or processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "app_options", "line_number": 81, "body": "def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]", "is_method": true, "class_name": "PySparkWordCount", "function_description": "Returns the input and output file paths as ordered arguments for the Spark application. This method provides essential configuration options for running the word count job."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "input", "line_number": 85, "body": "def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")", "is_method": true, "class_name": "PySparkWordCount", "function_description": "Provides the input data location as an S3 target for the PySparkWordCount job to process the word count task. This enables seamless access to the input files stored on S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "output", "line_number": 88, "body": "def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')", "is_method": true, "class_name": "PySparkWordCount", "function_description": "Returns the S3 target location where the PySparkWordCount job's output data is stored. This facilitates defining or accessing the output destination in a cloud storage environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "hadoop_examples_jar", "line_number": 28, "body": "def hadoop_examples_jar():\n    config = luigi.configuration.get_config()\n    examples_jar = config.get('hadoop', 'examples-jar')\n    if not examples_jar:\n        logger.error(\"You must specify hadoop:examples-jar in luigi.cfg\")\n        raise\n    if not os.path.exists(examples_jar):\n        logger.error(\"Can't find example jar: \" + examples_jar)\n        raise\n    return examples_jar", "is_method": false, "function_description": "Function that retrieves and validates the path to the Hadoop examples JAR from configuration, ensuring availability for Hadoop-related tasks or demonstrations requiring this specific JAR file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "output", "line_number": 54, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.terasort_in)", "is_method": true, "class_name": "TeraGen", "function_description": "Provides the target output location of the task, representing a file in HDFS created upon successful completion in the TeraGen workflow. This enables downstream tasks to reference the generated HDFS file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "jar", "line_number": 64, "body": "def jar(self):\n        return hadoop_examples_jar()", "is_method": true, "class_name": "TeraGen", "function_description": "Returns the Hadoop examples JAR file path, providing access to the necessary resource for running Hadoop example jobs within the TeraGen class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "main", "line_number": 67, "body": "def main(self):\n        return \"teragen\"", "is_method": true, "class_name": "TeraGen", "function_description": "Returns the string \"teragen\", possibly as an identifier or status indicator."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "args", "line_number": 70, "body": "def args(self):\n        # First arg is 10B -- each record is 100bytes\n        return [self.records, self.output()]", "is_method": true, "class_name": "TeraGen", "function_description": "Returns a list containing the number of records and the output destination, providing key parameters for the data generation process within the TeraGen class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "requires", "line_number": 85, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.TeraGen`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return TeraGen(terasort_in=self.terasort_in)", "is_method": true, "class_name": "TeraSort", "function_description": "Returns the task dependencies required before this TeraSort task can run, specifically the TeraGen task with the given input parameter. This ensures proper task execution order in a pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "output", "line_number": 95, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.terasort_out)", "is_method": true, "class_name": "TeraSort", "function_description": "Provides the output target representing the HDFS file created upon successful execution of the TeraSort task, enabling integration with Luigi's workflow management for task completion tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "jar", "line_number": 105, "body": "def jar(self):\n        return hadoop_examples_jar()", "is_method": true, "class_name": "TeraSort", "function_description": "Returns the file path of the Hadoop examples JAR used by TeraSort for running sorting tasks on Hadoop clusters."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "main", "line_number": 108, "body": "def main(self):\n        return \"terasort\"", "is_method": true, "class_name": "TeraSort", "function_description": "Returns a fixed string indicating the name of the process or algorithm, likely serving as an identifier within the TeraSort class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "args", "line_number": 111, "body": "def args(self):\n        return [self.input(), self.output()]", "is_method": true, "class_name": "TeraSort", "function_description": "Returns a list containing the input and output parameters for the TeraSort process, facilitating access to the essential data sources the sort operation will handle."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "output", "line_number": 38, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('/tmp/text/%Y-%m-%d.txt'))", "is_method": true, "class_name": "InputText", "function_description": "Returns an HDFS file target representing the expected output for this task, enabling integration with Luigi workflows that manage file-based tasks linked to specific dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "requires", "line_number": 60, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.InputText`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [InputText(date) for date in self.date_interval.dates()]", "is_method": true, "class_name": "WordCount", "function_description": "Returns a list of prerequisite InputText tasks for each date in the date interval, defining this task's dependencies within a Luigi workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "output", "line_number": 70, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('/tmp/text-count/%s' % self.date_interval)", "is_method": true, "class_name": "WordCount", "function_description": "Returns the HDFS file target representing the output location for the word count task's results. This enables downstream tasks to access the generated output file by date interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "mapper", "line_number": 80, "body": "def mapper(self, line):\n        for word in line.strip().split():\n            yield word, 1", "is_method": true, "class_name": "WordCount", "function_description": "Core method of the WordCount class that processes a line of text and yields each word paired with the count one, enabling word frequency counting in text processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "reducer", "line_number": 84, "body": "def reducer(self, key, values):\n        yield key, sum(values)", "is_method": true, "class_name": "WordCount", "function_description": "Simple reducer function in the WordCount class that sums numeric values associated with a given key, typically used to aggregate counts in data processing or map-reduce workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 37, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('data/streams_%Y-%m-%d.tsv'))", "is_method": true, "class_name": "ExternalStreams", "function_description": "Provides the target output file in HDFS for the ExternalStreams task, specifying the expected file path based on the task's date attribute. This enables downstream tasks to locate and access the required output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 54, "body": "def run(self):\n        \"\"\"\n        Generates bogus data and writes it into the :py:meth:`~.Streams.output` target.\n        \"\"\"\n        with self.output().open('w') as output:\n            for _ in range(1000):\n                output.write('{} {} {}\\n'.format(\n                    random.randint(0, 999),\n                    random.randint(0, 999),\n                    random.randint(0, 999)))", "is_method": true, "class_name": "Streams", "function_description": "Generates and writes 1000 lines of random integer data to the Streams output target, simulating a data source for downstream processing or testing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 65, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in the local file system.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(self.date.strftime('data/streams_%Y_%m_%d_faked.tsv'))", "is_method": true, "class_name": "Streams", "function_description": "Provides the target output file path for the Streams task, specifying where the task's resulting file will be saved on the local filesystem. This enables workflow management systems to locate and verify task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 84, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('data/streams_%Y_%m_%d_faked.tsv'))", "is_method": true, "class_name": "StreamsHdfs", "function_description": "Returns the HDFS file target representing the output of the task, indicating where the task's result will be stored upon successful execution. This enables downstream tasks to locate and depend on this output."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 103, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(\"data/artist_streams_{}.tsv\".format(self.date_interval))", "is_method": true, "class_name": "AggregateArtists", "function_description": "Provides a Luigi task output target pointing to a local TSV file named with the specified date interval, representing the expected result location after task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 113, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.Streams`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [Streams(date) for date in self.date_interval]", "is_method": true, "class_name": "AggregateArtists", "function_description": "Declares the task dependencies by returning a list of Stream tasks for each date in the set interval, ensuring prerequisite data streams are processed before this task runs."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 123, "body": "def run(self):\n        artist_count = defaultdict(int)\n\n        for t in self.input():\n            with t.open('r') as in_file:\n                for line in in_file:\n                    _, artist, track = line.strip().split()\n                    artist_count[artist] += 1\n\n        with self.output().open('w') as out_file:\n            for artist, count in artist_count.items():\n                out_file.write('{}\\t{}\\n'.format(artist, count))", "is_method": true, "class_name": "AggregateArtists", "function_description": "Core method of AggregateArtists that counts occurrences of each artist from input files and writes the aggregated artist frequency data to an output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 159, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(\"data/artist_streams_%s.tsv\" % self.date_interval)", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "Returns the HDFS file target representing the output location for the aggregated artist streams task, enabling downstream processes to access the generated data."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 169, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.StreamsHdfs`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [StreamsHdfs(date) for date in self.date_interval]", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "Method defining the task dependencies by returning a list of prerequisite tasks for each date in its interval, ensuring required data streams are processed before this task runs."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "app_options", "line_number": 179, "body": "def app_options(self):\n        # :func:`~luigi.task.Task.input` returns the targets produced by the tasks in\n        # `~luigi.task.Task.requires`.\n        return [','.join([p.path for p in self.input()]),\n                self.output().path]", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "Returns a list containing a comma-separated string of input file paths and the output file path, facilitating task input-output path management within the AggregateArtistsSpark workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 196, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.AggregateArtists` or\n        * :py:class:`~.AggregateArtistsSpark` if :py:attr:`~/.Top10Artists.use_spark` is set.\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        if self.use_spark:\n            return AggregateArtistsSpark(self.date_interval)\n        else:\n            return AggregateArtists(self.date_interval)", "is_method": true, "class_name": "Top10Artists", "function_description": "Provides the prerequisite task for Top10Artists, selecting between AggregateArtists or AggregateArtistsSpark based on the use_spark flag to ensure proper data aggregation before processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 210, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(\"data/top_artists_%s.tsv\" % self.date_interval)", "is_method": true, "class_name": "Top10Artists", "function_description": "Returns a Luigi LocalTarget pointing to a TSV file named for a specific date interval, representing the output location of the Top10Artists task for downstream processing or tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 220, "body": "def run(self):\n        top_10 = nlargest(10, self._input_iterator())\n        with self.output().open('w') as out_file:\n            for streams, artist in top_10:\n                out_line = '\\t'.join([\n                    str(self.date_interval.date_a),\n                    str(self.date_interval.date_b),\n                    artist,\n                    str(streams)\n                ])\n                out_file.write((out_line + '\\n'))", "is_method": true, "class_name": "Top10Artists", "function_description": "Generates and saves the top 10 artists with the highest streams over a specified date interval, providing a ranked list output for further analysis or reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "_input_iterator", "line_number": 232, "body": "def _input_iterator(self):\n        with self.input().open('r') as in_file:\n            for line in in_file:\n                artist, streams = line.strip().split()\n                yield int(streams), artist", "is_method": true, "class_name": "Top10Artists", "function_description": "Utility method of the Top10Artists class that reads input data line-by-line, yielding pairs of stream counts and artist names for iterative processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 264, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.Top10Artists`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return Top10Artists(self.date_interval, self.use_spark)", "is_method": true, "class_name": "ArtistToplistToDatabase", "function_description": "Provides the Luigi task dependency for ArtistToplistToDatabase, specifying that it requires the Top10Artists task before execution. This ensures proper task sequencing in the data pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_partition_tasks", "line_number": 91, "body": "def _partition_tasks(worker):\n    \"\"\"\n    Takes a worker and sorts out tasks based on their status.\n    Still_pending_not_ext is only used to get upstream_failure, upstream_missing_dependency and run_by_other_worker\n    \"\"\"\n    task_history = worker._add_task_history\n    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}\n    set_tasks = {}\n    set_tasks[\"completed\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}\n    set_tasks[\"already_done\"] = {task for (task, status, ext) in task_history\n                                 if status == 'DONE' and task not in pending_tasks and task not in set_tasks[\"completed\"]}\n    set_tasks[\"ever_failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"failed\"] = set_tasks[\"ever_failed\"] - set_tasks[\"completed\"]\n    set_tasks[\"scheduling_error\"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}\n    set_tasks[\"still_pending_ext\"] = {task for (task, status, ext) in task_history\n                                      if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and not ext}\n    set_tasks[\"still_pending_not_ext\"] = {task for (task, status, ext) in task_history\n                                          if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and ext}\n    set_tasks[\"run_by_other_worker\"] = set()\n    set_tasks[\"upstream_failure\"] = set()\n    set_tasks[\"upstream_missing_dependency\"] = set()\n    set_tasks[\"upstream_run_by_other_worker\"] = set()\n    set_tasks[\"upstream_scheduling_error\"] = set()\n    set_tasks[\"not_run\"] = set()\n    return set_tasks", "is_method": false, "function_description": "Utility function that categorizes a worker's tasks by their execution status, enabling task management systems to easily track progress, failures, and scheduling states for efficient workflow monitoring and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_root_task", "line_number": 118, "body": "def _root_task(worker):\n    \"\"\"\n    Return the first task scheduled by the worker, corresponding to the root task\n    \"\"\"\n    return worker._add_task_history[0][0]", "is_method": false, "function_description": "Core utility function that retrieves the initial task scheduled by a worker, identifying the root task in the worker\u2019s task history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_populate_unknown_statuses", "line_number": 125, "body": "def _populate_unknown_statuses(set_tasks):\n    \"\"\"\n    Add the \"upstream_*\" and \"not_run\" statuses my mutating set_tasks.\n    \"\"\"\n    visited = set()\n    for task in set_tasks[\"still_pending_not_ext\"]:\n        _depth_first_search(set_tasks, task, visited)", "is_method": false, "function_description": "Internal helper function that updates a set of tasks by assigning \"upstream_*\" and \"not_run\" statuses. It supports task status propagation to track pending or unexecuted tasks within a workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_depth_first_search", "line_number": 134, "body": "def _depth_first_search(set_tasks, current_task, visited):\n    \"\"\"\n    This dfs checks why tasks are still pending.\n    \"\"\"\n    visited.add(current_task)\n    if current_task in set_tasks[\"still_pending_not_ext\"]:\n        upstream_failure = False\n        upstream_missing_dependency = False\n        upstream_run_by_other_worker = False\n        upstream_scheduling_error = False\n        for task in current_task._requires():\n            if task not in visited:\n                _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n                set_tasks[\"upstream_failure\"].add(current_task)\n                upstream_failure = True\n            if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:\n                set_tasks[\"upstream_missing_dependency\"].add(current_task)\n                upstream_missing_dependency = True\n            if task in set_tasks[\"run_by_other_worker\"] or task in set_tasks[\"upstream_run_by_other_worker\"]:\n                set_tasks[\"upstream_run_by_other_worker\"].add(current_task)\n                upstream_run_by_other_worker = True\n            if task in set_tasks[\"scheduling_error\"]:\n                set_tasks[\"upstream_scheduling_error\"].add(current_task)\n                upstream_scheduling_error = True\n        if not upstream_failure and not upstream_missing_dependency and \\\n                not upstream_run_by_other_worker and not upstream_scheduling_error and \\\n                current_task not in set_tasks[\"run_by_other_worker\"]:\n            set_tasks[\"not_run\"].add(current_task)", "is_method": false, "function_description": "Performs a depth-first search to diagnose why specific tasks remain pending by identifying upstream issues such as failures, missing dependencies, scheduling errors, or execution conflicts with other workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str", "line_number": 165, "body": "def _get_str(task_dict, extra_indent):\n    \"\"\"\n    This returns a string for each status\n    \"\"\"\n    summary_length = execution_summary().summary_length\n\n    lines = []\n    task_names = sorted(task_dict.keys())\n    for task_family in task_names:\n        tasks = task_dict[task_family]\n        tasks = sorted(tasks, key=lambda x: str(x))\n        prefix_size = 8 if extra_indent else 4\n        prefix = ' ' * prefix_size\n\n        line = None\n\n        if summary_length > 0 and len(lines) >= summary_length:\n            line = prefix + \"...\"\n            lines.append(line)\n            break\n        if len(tasks[0].get_params()) == 0:\n            line = prefix + '- {0} {1}()'.format(len(tasks), str(task_family))\n        elif _get_len_of_params(tasks[0]) > 60 or len(str(tasks[0])) > 200 or \\\n                (len(tasks) == 2 and len(tasks[0].get_params()) > 1 and (_get_len_of_params(tasks[0]) > 40 or len(str(tasks[0])) > 100)):\n            \"\"\"\n            This is to make sure that there is no really long task in the output\n            \"\"\"\n            line = prefix + '- {0} {1}(...)'.format(len(tasks), task_family)\n        elif len((tasks[0].get_params())) == 1:\n            attributes = {getattr(task, tasks[0].get_params()[0][0]) for task in tasks}\n            param_class = tasks[0].get_params()[0][1]\n            first, last = _ranging_attributes(attributes, param_class)\n            if first is not None and last is not None and len(attributes) > 3:\n                param_str = '{0}...{1}'.format(param_class.serialize(first), param_class.serialize(last))\n            else:\n                param_str = '{0}'.format(_get_str_one_parameter(tasks))\n            line = prefix + '- {0} {1}({2}={3})'.format(len(tasks), task_family, tasks[0].get_params()[0][0], param_str)\n        else:\n            ranging = False\n            params = _get_set_of_params(tasks)\n            unique_param_keys = list(_get_unique_param_keys(params))\n            if len(unique_param_keys) == 1:\n                unique_param, = unique_param_keys\n                attributes = params[unique_param]\n                param_class = unique_param[1]\n                first, last = _ranging_attributes(attributes, param_class)\n                if first is not None and last is not None and len(attributes) > 2:\n                    ranging = True\n                    line = prefix + '- {0} {1}({2}'.format(len(tasks), task_family, _get_str_ranging_multiple_parameters(first, last, tasks, unique_param))\n            if not ranging:\n                if len(tasks) == 1:\n                    line = prefix + '- {0} {1}'.format(len(tasks), tasks[0])\n                if len(tasks) == 2:\n                    line = prefix + '- {0} {1} and {2}'.format(len(tasks), tasks[0], tasks[1])\n                if len(tasks) > 2:\n                    line = prefix + '- {0} {1} ...'.format(len(tasks), tasks[0])\n        lines.append(line)\n    return '\\n'.join(lines)", "is_method": false, "function_description": "Utility function that generates a concise, formatted string summary of tasks grouped by status, optionally controlling detail level and indentation to facilitate readable task overviews."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_len_of_params", "line_number": 225, "body": "def _get_len_of_params(task):\n    return sum(len(param[0]) for param in task.get_params())", "is_method": false, "function_description": "Utility function that calculates the total length of the first elements in each parameter provided by the task, likely to quantify the size of task-related inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str_ranging_multiple_parameters", "line_number": 229, "body": "def _get_str_ranging_multiple_parameters(first, last, tasks, unique_param):\n    row = ''\n    str_unique_param = '{0}...{1}'.format(unique_param[1].serialize(first), unique_param[1].serialize(last))\n    for param in tasks[0].get_params():\n        row += '{0}='.format(param[0])\n        if param[0] == unique_param[0]:\n            row += '{0}'.format(str_unique_param)\n        else:\n            row += '{0}'.format(param[1].serialize(getattr(tasks[0], param[0])))\n        if param != tasks[0].get_params()[-1]:\n            row += \", \"\n    row += ')'\n    return row", "is_method": false, "function_description": "Constructs a formatted string representing parameter assignments for a set of tasks, showing a ranged serialization for a specified unique parameter while serializing others normally. Useful for generating concise parameter summaries across multiple related tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_set_of_params", "line_number": 244, "body": "def _get_set_of_params(tasks):\n    params = {}\n    for param in tasks[0].get_params():\n        params[param] = {getattr(task, param[0]) for task in tasks}\n    return params", "is_method": false, "function_description": "Utility function that collects unique values of each parameter from a list of task objects, producing a dictionary mapping parameter names to their distinct sets of values across the tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_unique_param_keys", "line_number": 251, "body": "def _get_unique_param_keys(params):\n    for param_key, param_values in params.items():\n        if len(param_values) > 1:\n            yield param_key", "is_method": false, "function_description": "Yield parameter keys from a dictionary whose associated value lists contain multiple items, enabling identification of parameters with multiple distinct values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_ranging_attributes", "line_number": 257, "body": "def _ranging_attributes(attributes, param_class):\n    \"\"\"\n    Checks if there is a continuous range\n    \"\"\"\n    next_attributes = {param_class.next_in_enumeration(attribute) for attribute in attributes}\n    in_first = attributes.difference(next_attributes)\n    in_second = next_attributes.difference(attributes)\n    if len(in_first) == 1 and len(in_second) == 1:\n        for x in attributes:\n            if {param_class.next_in_enumeration(x)} == in_second:\n                return next(iter(in_first)), x\n    return None, None", "is_method": false, "function_description": "Helper function that identifies if a given set of attributes forms a continuous range based on a sequencing defined by param_class. It returns the start and end attributes of that range if found, enabling range detection in enumerated parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str_one_parameter", "line_number": 271, "body": "def _get_str_one_parameter(tasks):\n    row = ''\n    count = 0\n    for task in tasks:\n        if (len(row) >= 30 and count > 2 and count != len(tasks) - 1) or len(row) > 200:\n            row += '...'\n            break\n        param = task.get_params()[0]\n        row += '{0}'.format(param[1].serialize(getattr(task, param[0])))\n        if count < len(tasks) - 1:\n            row += ','\n        count += 1\n    return row", "is_method": false, "function_description": "This function generates a concise, comma-separated string summarizing the first parameter of each task in a list, truncating output for readability. It helps create brief representations of task parameters for logging or display purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_serialize_first_param", "line_number": 286, "body": "def _serialize_first_param(task):\n    return task.get_params()[0][1].serialize(getattr(task, task.get_params()[0][0]))", "is_method": false, "function_description": "This function serializes the first parameter of a given task object using its associated serialization method. It enables converting task parameters into a standardized serialized format for further processing or storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_number_of_tasks_for", "line_number": 290, "body": "def _get_number_of_tasks_for(status, group_tasks):\n    if status == \"still_pending\":\n        return (_get_number_of_tasks(group_tasks[\"still_pending_ext\"]) +\n                _get_number_of_tasks(group_tasks[\"still_pending_not_ext\"]))\n    return _get_number_of_tasks(group_tasks[status])", "is_method": false, "function_description": "Calculates the total number of tasks for a given status, summing subtotals for multiple pending categories if needed. Useful for aggregating task counts across different grouped statuses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_number_of_tasks", "line_number": 297, "body": "def _get_number_of_tasks(task_dict):\n    return sum(len(tasks) for tasks in task_dict.values())", "is_method": false, "function_description": "Utility function that calculates the total number of tasks contained within a dictionary mapping keys to task lists. It provides a quick aggregate count of all tasks across categories or groups."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_comments", "line_number": 301, "body": "def _get_comments(group_tasks):\n    \"\"\"\n    Get the human readable comments and quantities for the task types.\n    \"\"\"\n    comments = {}\n    for status, human in _COMMENTS:\n        num_tasks = _get_number_of_tasks_for(status, group_tasks)\n        if num_tasks:\n            space = \"    \" if status in _PENDING_SUB_STATUSES else \"\"\n            comments[status] = '{space}* {num_tasks} {human}:\\n'.format(\n                space=space,\n                num_tasks=num_tasks,\n                human=human)\n    return comments", "is_method": false, "function_description": "Utility function that generates formatted comments with task counts for different task statuses, providing a human-readable summary useful for reporting or display purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_run_by_other_worker", "line_number": 350, "body": "def _get_run_by_other_worker(worker):\n    \"\"\"\n    This returns a set of the tasks that are being run by other worker\n    \"\"\"\n    task_sets = _get_external_workers(worker).values()\n    return functools.reduce(lambda a, b: a | b, task_sets, set())", "is_method": false, "function_description": "Utility function that aggregates and returns all tasks currently being executed by workers other than the specified one, facilitating task management or coordination across multiple workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_external_workers", "line_number": 358, "body": "def _get_external_workers(worker):\n    \"\"\"\n    This returns a dict with a set of tasks for all of the other workers\n    \"\"\"\n    worker_that_blocked_task = collections.defaultdict(set)\n    get_work_response_history = worker._get_work_response_history\n    for get_work_response in get_work_response_history:\n        if get_work_response['task_id'] is None:\n            for running_task in get_work_response['running_tasks']:\n                other_worker_id = running_task['worker']\n                other_task_id = running_task['task_id']\n                other_task = worker._scheduled_tasks.get(other_task_id)\n                if other_worker_id == worker._id or not other_task:\n                    continue\n                worker_that_blocked_task[other_worker_id].add(other_task)\n    return worker_that_blocked_task", "is_method": false, "function_description": "This function gathers tasks assigned to all workers other than the given one, mapping each external worker to the set of their currently running tasks. It helps track which tasks are being executed by peers, supporting task management and coordination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_group_tasks_by_name_and_status", "line_number": 376, "body": "def _group_tasks_by_name_and_status(task_dict):\n    \"\"\"\n    Takes a dictionary with sets of tasks grouped by their status and\n    returns a dictionary with dictionaries with an array of tasks grouped by\n    their status and task name\n    \"\"\"\n    group_status = {}\n    for task in task_dict:\n        if task.task_family not in group_status:\n            group_status[task.task_family] = []\n        group_status[task.task_family].append(task)\n    return group_status", "is_method": false, "function_description": "This function reorganizes a collection of tasks by grouping them according to their task family attribute. It provides a structured mapping from task families to their corresponding tasks for easier access and management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_summary_format", "line_number": 397, "body": "def _summary_format(set_tasks, worker):\n    group_tasks = {}\n    for status, task_dict in set_tasks.items():\n        group_tasks[status] = _group_tasks_by_name_and_status(task_dict)\n    comments = _get_comments(group_tasks)\n    num_all_tasks = sum([len(set_tasks[\"already_done\"]),\n                         len(set_tasks[\"completed\"]), len(set_tasks[\"failed\"]),\n                         len(set_tasks[\"scheduling_error\"]),\n                         len(set_tasks[\"still_pending_ext\"]),\n                         len(set_tasks[\"still_pending_not_ext\"])])\n    str_output = ''\n    str_output += 'Scheduled {0} tasks of which:\\n'.format(num_all_tasks)\n    for status in _ORDERED_STATUSES:\n        if status not in comments:\n            continue\n        str_output += '{0}'.format(comments[status])\n        if status != 'still_pending':\n            str_output += '{0}\\n'.format(_get_str(group_tasks[status], status in _PENDING_SUB_STATUSES))\n    ext_workers = _get_external_workers(worker)\n    group_tasks_ext_workers = {}\n    for ext_worker, task_dict in ext_workers.items():\n        group_tasks_ext_workers[ext_worker] = _group_tasks_by_name_and_status(task_dict)\n    if len(ext_workers) > 0:\n        str_output += \"\\nThe other workers were:\\n\"\n        count = 0\n        for ext_worker, task_dict in ext_workers.items():\n            if count > 3 and count < len(ext_workers) - 1:\n                str_output += \"    and {0} other workers\".format(len(ext_workers) - count)\n                break\n            str_output += \"    - {0} ran {1} tasks\\n\".format(ext_worker, len(task_dict))\n            count += 1\n        str_output += '\\n'\n    if num_all_tasks == sum([len(set_tasks[\"already_done\"]),\n                             len(set_tasks[\"scheduling_error\"]),\n                             len(set_tasks[\"still_pending_ext\"]),\n                             len(set_tasks[\"still_pending_not_ext\"])]):\n        if len(ext_workers) == 0:\n            str_output += '\\n'\n        str_output += 'Did not run any tasks'\n    one_line_summary = _create_one_line_summary(_tasks_status(set_tasks))\n    str_output += \"\\n{0}\".format(one_line_summary)\n    if num_all_tasks == 0:\n        str_output = 'Did not schedule any tasks'\n    return str_output", "is_method": false, "function_description": "Constructs a detailed textual summary of task statuses and worker activity, presenting counts and comments for scheduled tasks, including information about external workers when applicable. Useful for reporting task execution overview in workflow management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_create_one_line_summary", "line_number": 443, "body": "def _create_one_line_summary(status_code):\n    \"\"\"\n    Given a status_code of type LuigiStatusCode which has a tuple value, returns a one line summary\n    \"\"\"\n    return \"This progress looks {0} because {1}\".format(*status_code.value)", "is_method": false, "function_description": "Helper function that creates a concise one-line summary describing progress based on a given status code's tuple value. It formats a readable message reflecting the progress state and reason."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_tasks_status", "line_number": 450, "body": "def _tasks_status(set_tasks):\n    \"\"\"\n    Given a grouped set of tasks, returns a LuigiStatusCode\n    \"\"\"\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            return LuigiStatusCode.SUCCESS_WITH_RETRY\n        else:\n            if set_tasks[\"scheduling_error\"]:\n                return LuigiStatusCode.FAILED_AND_SCHEDULING_FAILED\n            return LuigiStatusCode.FAILED\n    elif set_tasks[\"scheduling_error\"]:\n        return LuigiStatusCode.SCHEDULING_FAILED\n    elif set_tasks[\"not_run\"]:\n        return LuigiStatusCode.NOT_RUN\n    elif set_tasks[\"still_pending_ext\"]:\n        return LuigiStatusCode.MISSING_EXT\n    else:\n        return LuigiStatusCode.SUCCESS", "is_method": false, "function_description": "Determines the overall Luigi task status based on grouped task conditions, providing a consolidated LuigiStatusCode for monitoring workflow execution outcomes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_summary_wrap", "line_number": 471, "body": "def _summary_wrap(str_output):\n    return textwrap.dedent(\"\"\"\n    ===== Luigi Execution Summary =====\n\n    {str_output}\n\n    ===== Luigi Execution Summary =====\n    \"\"\").format(str_output=str_output)", "is_method": false, "function_description": "Utility function that formats a given string into a standardized execution summary block, enclosing it between labeled header and footer lines for clear display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "summary", "line_number": 481, "body": "def summary(worker):\n    \"\"\"\n    Given a worker, return a human readable summary of what the worker have\n    done.\n    \"\"\"\n    return _summary_wrap(_summary_format(_summary_dict(worker), worker))", "is_method": false, "function_description": "Function that generates a human-readable summary of the tasks or actions performed by a given worker object. It provides a concise overview of the worker's completed activities or state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "__str__", "line_number": 84, "body": "def __str__(self):\n        return \"LuigiRunResult with status {0}\".format(self.status)", "is_method": true, "class_name": "LuigiRunResult", "function_description": "String representation method of LuigiRunResult that returns a brief summary showing its current status for easy identification or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "__repr__", "line_number": 87, "body": "def __repr__(self):\n        return \"LuigiRunResult(status={0!r},worker={1!r},scheduling_succeeded={2!r})\".format(self.status, self.worker, self.scheduling_succeeded)", "is_method": true, "class_name": "LuigiRunResult", "function_description": "Provides a concise string representation of a LuigiRunResult instance, summarizing its status, worker, and scheduling outcome for easier debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_is_external", "line_number": 80, "body": "def _is_external(task):\n    return task.run is None or task.run == NotImplemented", "is_method": false, "function_description": "This function determines whether a task is considered external by checking if its run method is unassigned or marked as not implemented. It helps identify tasks that rely on external execution logic or placeholders."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_retry_policy_dict", "line_number": 84, "body": "def _get_retry_policy_dict(task):\n    return RetryPolicy(task.retry_count, task.disable_hard_timeout, task.disable_window)._asdict()", "is_method": false, "function_description": "This function converts a task's retry configuration into a dictionary format, providing a standardized representation of retry policies for use in task execution or management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "check_complete", "line_number": 395, "body": "def check_complete(task, out_queue):\n    \"\"\"\n    Checks if task is complete, puts the result to out_queue.\n    \"\"\"\n    logger.debug(\"Checking if %s is complete\", task)\n    try:\n        is_complete = task.complete()\n    except Exception:\n        is_complete = TracebackWrapper(traceback.format_exc())\n    out_queue.put((task, is_complete))", "is_method": false, "function_description": "Utility function that checks a task's completion status and sends the result, including any errors, to a specified output queue for asynchronous processing or monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "rpc_message_callback", "line_number": 500, "body": "def rpc_message_callback(fn):\n    fn.is_rpc_message_callback = True\n    return fn", "is_method": false, "function_description": "Decorator function that marks a given function as an RPC message callback by setting a specific attribute, enabling identification of RPC handlers in the system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_run_get_new_deps", "line_number": 132, "body": "def _run_get_new_deps(self):\n        task_gen = self.task.run()\n\n        if not isinstance(task_gen, types.GeneratorType):\n            return None\n\n        next_send = None\n        while True:\n            try:\n                if next_send is None:\n                    requires = next(task_gen)\n                else:\n                    requires = task_gen.send(next_send)\n            except StopIteration:\n                return None\n\n            new_req = flatten(requires)\n            if all(t.complete() for t in new_req):\n                next_send = getpaths(requires)\n            else:\n                new_deps = [(t.task_module, t.task_family, t.to_str_params())\n                            for t in new_req]\n                return new_deps", "is_method": true, "class_name": "TaskProcess", "function_description": "Core method of TaskProcess that iterates a task's generator to identify and return new unresolved dependencies, enabling dynamic tracking and management of dependent tasks during task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 156, "body": "def run(self):\n        logger.info('[pid %s] Worker %s running   %s', os.getpid(), self.worker_id, self.task)\n\n        if self.use_multiprocessing:\n            # Need to have different random seeds if running in separate processes\n            random.seed((os.getpid(), time.time()))\n\n        status = FAILED\n        expl = ''\n        missing = []\n        new_deps = []\n        try:\n            # Verify that all the tasks are fulfilled! For external tasks we\n            # don't care about unfulfilled dependencies, because we are just\n            # checking completeness of self.task so outputs of dependencies are\n            # irrelevant.\n            if self.check_unfulfilled_deps and not _is_external(self.task):\n                missing = [dep.task_id for dep in self.task.deps() if not dep.complete()]\n                if missing:\n                    deps = 'dependency' if len(missing) == 1 else 'dependencies'\n                    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n            self.task.trigger_event(Event.START, self.task)\n            t0 = time.time()\n            status = None\n\n            if _is_external(self.task):\n                # External task\n                if self.task.complete():\n                    status = DONE\n                else:\n                    status = FAILED\n                    expl = 'Task is an external data dependency ' \\\n                        'and data does not exist (yet?).'\n            else:\n                with self._forward_attributes():\n                    new_deps = self._run_get_new_deps()\n                if not new_deps:\n                    if not self.check_complete_on_run or self.task.complete():\n                        status = DONE\n                    else:\n                        raise TaskException(\"Task finished running, but complete() is still returning false.\")\n                else:\n                    status = PENDING\n\n            if new_deps:\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task)\n            elif status == DONE:\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                expl = self.task.on_success()\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n\n        except KeyboardInterrupt:\n            raise\n        except BaseException as ex:\n            status = FAILED\n            expl = self._handle_run_exception(ex)\n\n        finally:\n            self.result_queue.put(\n                (self.task.task_id, status, expl, missing, new_deps))", "is_method": true, "class_name": "TaskProcess", "function_description": "Core method of TaskProcess that manages executing a task, checking dependencies, updating status, handling exceptions, and reporting results, supporting both internal and external tasks with event notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_run_exception", "line_number": 222, "body": "def _handle_run_exception(self, ex):\n        logger.exception(\"[pid %s] Worker %s failed    %s\", os.getpid(), self.worker_id, self.task)\n        self.task.trigger_event(Event.FAILURE, self.task, ex)\n        return self.task.on_failure(ex)", "is_method": true, "class_name": "TaskProcess", "function_description": "Handles exceptions during task execution by logging the failure, triggering a failure event, and invoking the task's failure callback to manage error response or recovery."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_recursive_terminate", "line_number": 227, "body": "def _recursive_terminate(self):\n        import psutil\n\n        try:\n            parent = psutil.Process(self.pid)\n            children = parent.children(recursive=True)\n\n            # terminate parent. Give it a chance to clean up\n            super(TaskProcess, self).terminate()\n            parent.wait()\n\n            # terminate children\n            for child in children:\n                try:\n                    child.terminate()\n                except psutil.NoSuchProcess:\n                    continue\n        except psutil.NoSuchProcess:\n            return", "is_method": true, "class_name": "TaskProcess", "function_description": "Private method of TaskProcess that forcefully terminates the process and all its descendant processes to ensure complete cleanup of a task's process tree."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "terminate", "line_number": 247, "body": "def terminate(self):\n        \"\"\"Terminate this process and its subprocesses.\"\"\"\n        # default terminate() doesn't cleanup child processes, it orphans them.\n        try:\n            return self._recursive_terminate()\n        except ImportError:\n            return super(TaskProcess, self).terminate()", "is_method": true, "class_name": "TaskProcess", "function_description": "Provides a robust termination method for TaskProcess that ensures this process and all its subprocesses are cleanly stopped, preventing orphaned child processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_forward_attributes", "line_number": 256, "body": "def _forward_attributes(self):\n        # forward configured attributes to the task\n        for reporter_attr, task_attr in self.forward_reporter_attributes.items():\n            setattr(self.task, task_attr, getattr(self.status_reporter, reporter_attr))\n        try:\n            yield self\n        finally:\n            # reset attributes again\n            for reporter_attr, task_attr in self.forward_reporter_attributes.items():\n                setattr(self.task, task_attr, None)", "is_method": true, "class_name": "TaskProcess", "function_description": "Provides a context manager that temporarily forwards specified attributes from a status reporter to a task, ensuring these attributes are reset after use. Useful for managing attribute propagation during task execution phases."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 275, "body": "def run(self):\n        if self.context:\n            logger.debug('Importing module and instantiating ' + self.context)\n            module_path, class_name = self.context.rsplit('.', 1)\n            module = importlib.import_module(module_path)\n            cls = getattr(module, class_name)\n\n            with cls(self):\n                super(ContextManagedTaskProcess, self).run()\n        else:\n            super(ContextManagedTaskProcess, self).run()", "is_method": true, "class_name": "ContextManagedTaskProcess", "function_description": "Runs the process within a specified context manager if provided, allowing setup and cleanup actions around the task execution; otherwise, it runs the task directly. This supports controlled execution environments for tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_tracking_url", "line_number": 301, "body": "def update_tracking_url(self, tracking_url):\n        self._scheduler.add_task(\n            task_id=self._task_id,\n            worker=self._worker_id,\n            status=RUNNING,\n            tracking_url=tracking_url\n        )", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Updates the task's tracking URL and marks its status as running within the scheduler, enabling monitoring of task progress and associated resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_status_message", "line_number": 309, "body": "def update_status_message(self, message):\n        self._scheduler.set_task_status_message(self._task_id, message)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Updates the status message of the current task within the scheduler, providing real-time task progress or information updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_progress_percentage", "line_number": 312, "body": "def update_progress_percentage(self, percentage):\n        self._scheduler.set_task_progress_percentage(self._task_id, percentage)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Updates the progress percentage of a specific task through the scheduler, enabling real-time tracking of task completion status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "decrease_running_resources", "line_number": 315, "body": "def decrease_running_resources(self, decrease_resources):\n        self._scheduler.decrease_running_task_resources(self._task_id, decrease_resources)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Utility method of TaskStatusReporter that reduces the allocated resources for the running task by delegating the update to the scheduler, helping manage resource usage dynamically during task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__str__", "line_number": 335, "body": "def __str__(self):\n        return str(self.content)", "is_method": true, "class_name": "SchedulerMessage", "function_description": "Returns a string representation of the SchedulerMessage by converting its content to a string. This provides a readable format for displaying or logging the message."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__eq__", "line_number": 338, "body": "def __eq__(self, other):\n        return self.content == other", "is_method": true, "class_name": "SchedulerMessage", "function_description": "Overrides equality to compare the SchedulerMessage's content directly with another object, enabling simplified content-based equality checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "respond", "line_number": 341, "body": "def respond(self, response):\n        self._scheduler.add_scheduler_message_response(self._task_id, self._message_id, response)", "is_method": true, "class_name": "SchedulerMessage", "function_description": "Adds a response to a specific scheduled message, associating it with a task and message identifier within the scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "apply_async", "line_number": 352, "body": "def apply_async(self, function, args):\n        return function(*args)", "is_method": true, "class_name": "SingleProcessPool", "function_description": "Simplified implementation of asynchronous execution that immediately calls a function with given arguments and returns the result, useful for compatibility where asynchronous behavior is not needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "put", "line_number": 367, "body": "def put(self, obj, block=None, timeout=None):\n        return self.append(obj)", "is_method": true, "class_name": "DequeQueue", "function_description": "Adds an object to the end of the queue. This method provides a simple way to enqueue elements in the DequeQueue structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "get", "line_number": 370, "body": "def get(self, block=None, timeout=None):\n        try:\n            return self.pop()\n        except IndexError:\n            raise Queue.Empty", "is_method": true, "class_name": "DequeQueue", "function_description": "Method of DequeQueue that retrieves and removes the next item from the queue, raising an exception if the queue is empty. It provides basic queue consumption functionality without blocking or timeout support."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "stop", "line_number": 478, "body": "def stop(self):\n        self._should_stop.set()", "is_method": true, "class_name": "KeepAliveThread", "function_description": "Simple control method in KeepAliveThread that signals the thread to stop running by setting an internal flag. It allows external code to cleanly request thread termination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 481, "body": "def run(self):\n        while True:\n            self._should_stop.wait(self._ping_interval)\n            if self._should_stop.is_set():\n                logger.info(\"Worker %s was stopped. Shutting down Keep-Alive thread\" % self._worker_id)\n                break\n            with fork_lock:\n                response = None\n                try:\n                    response = self._scheduler.ping(worker=self._worker_id)\n                except BaseException:  # httplib.BadStatusLine:\n                    logger.warning('Failed pinging scheduler')\n\n                # handle rpc messages\n                if response:\n                    for message in response[\"rpc_messages\"]:\n                        self._rpc_message_callback(message)", "is_method": true, "class_name": "KeepAliveThread", "function_description": "Continuously sends periodic ping messages to a scheduler to maintain a worker's active status and processes any remote procedure call messages received in response."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_task", "line_number": 565, "body": "def _add_task(self, *args, **kwargs):\n        \"\"\"\n        Call ``self._scheduler.add_task``, but store the values too so we can\n        implement :py:func:`luigi.execution_summary.summary`.\n        \"\"\"\n        task_id = kwargs['task_id']\n        status = kwargs['status']\n        runnable = kwargs['runnable']\n        task = self._scheduled_tasks.get(task_id)\n        if task:\n            self._add_task_history.append((task, status, runnable))\n            kwargs['owners'] = task._owner_list()\n\n        if task_id in self._batch_running_tasks:\n            for batch_task in self._batch_running_tasks.pop(task_id):\n                self._add_task_history.append((batch_task, status, True))\n\n        if task and kwargs.get('params'):\n            kwargs['param_visibilities'] = task._get_param_visibilities()\n\n        self._scheduler.add_task(*args, **kwargs)\n\n        logger.info('Informed scheduler that task   %s   has status   %s', task_id, status)", "is_method": true, "class_name": "Worker", "function_description": "Internal method of the Worker class that adds a task to the scheduler while tracking task details and history for execution summaries and logging progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__enter__", "line_number": 589, "body": "def __enter__(self):\n        \"\"\"\n        Start the KeepAliveThread.\n        \"\"\"\n        self._keep_alive_thread = KeepAliveThread(self._scheduler, self._id,\n                                                  self._config.ping_interval,\n                                                  self._handle_rpc_message)\n        self._keep_alive_thread.daemon = True\n        self._keep_alive_thread.start()\n        return self", "is_method": true, "class_name": "Worker", "function_description": "Context manager entry method that starts a background keep-alive thread to maintain the Worker's active state during its lifecycle."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__exit__", "line_number": 600, "body": "def __exit__(self, type, value, traceback):\n        \"\"\"\n        Stop the KeepAliveThread and kill still running tasks.\n        \"\"\"\n        self._keep_alive_thread.stop()\n        self._keep_alive_thread.join()\n        for task in self._running_tasks.values():\n            if task.is_alive():\n                task.terminate()\n        self._task_result_queue.close()\n        return False", "is_method": true, "class_name": "Worker", "function_description": "Cleans up the Worker by stopping its keep-alive thread and terminating any still-running tasks when exiting a context, ensuring proper resource release and shutdown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_generate_worker_info", "line_number": 612, "body": "def _generate_worker_info(self):\n        # Generate as much info as possible about the worker\n        # Some of these calls might not be available on all OS's\n        args = [('salt', '%09d' % random.randrange(0, 999999999)),\n                ('workers', self.worker_processes)]\n        try:\n            args += [('host', socket.gethostname())]\n        except BaseException:\n            pass\n        try:\n            args += [('username', getpass.getuser())]\n        except BaseException:\n            pass\n        try:\n            args += [('pid', os.getpid())]\n        except BaseException:\n            pass\n        try:\n            sudo_user = os.getenv(\"SUDO_USER\")\n            if sudo_user:\n                args.append(('sudo_user', sudo_user))\n        except BaseException:\n            pass\n        return args", "is_method": true, "class_name": "Worker", "function_description": "Generates a list of key identifying information about the worker, including hostname, username, process ID, and environment details, to aid in worker tracking or logging across different operating systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_generate_worker_id", "line_number": 637, "body": "def _generate_worker_id(self, worker_info):\n        worker_info_str = ', '.join(['{}={}'.format(k, v) for k, v in worker_info])\n        return 'Worker({})'.format(worker_info_str)", "is_method": true, "class_name": "Worker", "function_description": "Generates a unique identifier string for a worker by formatting its attribute key-value pairs. This helps distinguish workers based on their characteristics in the Worker context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_validate_task", "line_number": 641, "body": "def _validate_task(self, task):\n        if not isinstance(task, Task):\n            raise TaskException('Can not schedule non-task %s' % task)\n\n        if not task.initialized():\n            # we can't get the repr of it since it's not initialized...\n            raise TaskException('Task of class %s not initialized. Did you override __init__ and forget to call super(...).__init__?' % task.__class__.__name__)", "is_method": true, "class_name": "Worker", "function_description": "Private method in Worker that ensures a given input is a properly initialized Task instance before scheduling it, preventing invalid or incomplete tasks from being processed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_complete_error", "line_number": 649, "body": "def _log_complete_error(self, task, tb):\n        log_msg = \"Will not run {task} or any dependencies due to error in complete() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)", "is_method": true, "class_name": "Worker", "function_description": "Private method of Worker class that logs a warning indicating a task and its dependencies will not run due to an error in the complete() method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_dependency_error", "line_number": 653, "body": "def _log_dependency_error(self, task, tb):\n        log_msg = \"Will not run {task} or any dependencies due to error in deps() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)", "is_method": true, "class_name": "Worker", "function_description": "Private method of the Worker class that logs a warning when a task or its dependencies cannot run due to an error in the dependency checking process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_unexpected_error", "line_number": 657, "body": "def _log_unexpected_error(self, task):\n        logger.exception(\"Luigi unexpected framework error while scheduling %s\", task)", "is_method": true, "class_name": "Worker", "function_description": "Private method in the Worker class that logs unexpected framework errors encountered during task scheduling, aiding in debugging and monitoring task execution issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_announce_scheduling_failure", "line_number": 660, "body": "def _announce_scheduling_failure(self, task, expl):\n        try:\n            self._scheduler.announce_scheduling_failure(\n                worker=self._id,\n                task_name=str(task),\n                family=task.task_family,\n                params=task.to_str_params(only_significant=True),\n                expl=expl,\n                owners=task._owner_list(),\n            )\n        except Exception:\n            formatted_traceback = traceback.format_exc()\n            self._email_unexpected_error(task, formatted_traceback)\n            raise", "is_method": true, "class_name": "Worker", "function_description": "Private method in Worker that reports scheduling failures to the scheduler, including task details and an explanation, and emails errors if notification fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_complete_error", "line_number": 675, "body": "def _email_complete_error(self, task, formatted_traceback):\n        self._announce_scheduling_failure(task, formatted_traceback)\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} failed scheduling. Host: {host}\",\n                              headline=\"Will not run {task} or any dependencies due to error in complete() method\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "Private method of the Worker class that handles scheduling failure notifications by announcing the error and optionally sending an email alert if configured. It ensures stakeholders are informed about task completion errors and their impact on dependent tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_dependency_error", "line_number": 683, "body": "def _email_dependency_error(self, task, formatted_traceback):\n        self._announce_scheduling_failure(task, formatted_traceback)\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} failed scheduling. Host: {host}\",\n                              headline=\"Will not run {task} or any dependencies due to error in deps() method\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "Private method in Worker that notifies of a task scheduling failure and optionally emails error details when a task's dependency check fails, supporting error handling and alerting workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_unexpected_error", "line_number": 691, "body": "def _email_unexpected_error(self, task, formatted_traceback):\n        # this sends even if failure e-mails are disabled, as they may indicate\n        # a more severe failure that may not reach other alerting methods such\n        # as scheduler batch notification\n        self._email_error(task, formatted_traceback,\n                          subject=\"Luigi: Framework error while scheduling {task}. Host: {host}\",\n                          headline=\"Luigi framework error\",\n                          )", "is_method": true, "class_name": "Worker", "function_description": "Internal method of the Worker class that sends critical error notification emails for unexpected failures, ensuring important framework errors are reported even if general failure alerts are disabled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_task_failure", "line_number": 700, "body": "def _email_task_failure(self, task, formatted_traceback):\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} FAILED. Host: {host}\",\n                              headline=\"A task failed when running. Most likely run() raised an exception.\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "Sends an email notification when a task fails, including error details like the traceback and task information, if failure email alerts are enabled in the configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_error", "line_number": 707, "body": "def _email_error(self, task, formatted_traceback, subject, headline):\n        formatted_subject = subject.format(task=task, host=self.host)\n        formatted_headline = headline.format(task=task, host=self.host)\n        command = subprocess.list2cmdline(sys.argv)\n        message = notifications.format_task_error(\n            formatted_headline, task, command, formatted_traceback)\n        notifications.send_error_email(formatted_subject, message, task.owner_email)", "is_method": true, "class_name": "Worker", "function_description": "Internal method of the Worker class that formats and sends an error notification email about a task failure, including traceback details and host information, to the task owner's email address."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_task_load_error", "line_number": 715, "body": "def _handle_task_load_error(self, exception, task_ids):\n        msg = 'Cannot find task(s) sent by scheduler: {}'.format(','.join(task_ids))\n        logger.exception(msg)\n        subject = 'Luigi: {}'.format(msg)\n        error_message = notifications.wrap_traceback(exception)\n        for task_id in task_ids:\n            self._add_task(\n                worker=self._id,\n                task_id=task_id,\n                status=FAILED,\n                runnable=False,\n                expl=error_message,\n            )\n        notifications.send_error_email(subject, error_message)", "is_method": true, "class_name": "Worker", "function_description": "Private Worker method that logs task loading errors, marks affected tasks as failed, and sends an error notification email to inform about issues retrieving scheduled tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "add", "line_number": 730, "body": "def add(self, task, multiprocess=False, processes=0):\n        \"\"\"\n        Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.\n        \"\"\"\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool(processes=processes if processes > 0 else None)\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = {task.task_id}\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n            raise\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded", "is_method": true, "class_name": "Worker", "function_description": "Core method of the Worker class that schedules and executes a task along with its dependencies, optionally using multiprocessing, and returns whether all were successfully completed or scheduled without errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_task_batcher", "line_number": 776, "body": "def _add_task_batcher(self, task):\n        family = task.task_family\n        if family not in self._batch_families_sent:\n            task_class = type(task)\n            batch_param_names = task_class.batch_param_names()\n            if batch_param_names:\n                self._scheduler.add_task_batcher(\n                    worker=self._id,\n                    task_family=family,\n                    batched_args=batch_param_names,\n                    max_batch_size=task.max_batch_size,\n                )\n            self._batch_families_sent.add(family)", "is_method": true, "class_name": "Worker", "function_description": "Private method of the Worker class that registers a batch processing configuration for a task family with the scheduler, enabling efficient handling of tasks that support batching."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add", "line_number": 790, "body": "def _add(self, task, is_complete):\n        if self._config.task_limit is not None and len(self._scheduled_tasks) >= self._config.task_limit:\n            logger.warning('Will not run %s or any dependencies due to exceeded task-limit of %d', task, self._config.task_limit)\n            deps = None\n            status = UNKNOWN\n            runnable = False\n\n        else:\n            formatted_traceback = None\n            try:\n                self._check_complete_value(is_complete)\n            except KeyboardInterrupt:\n                raise\n            except AsyncCompletionException as ex:\n                formatted_traceback = ex.trace\n            except BaseException:\n                formatted_traceback = traceback.format_exc()\n\n            if formatted_traceback is not None:\n                self.add_succeeded = False\n                self._log_complete_error(task, formatted_traceback)\n                task.trigger_event(Event.DEPENDENCY_MISSING, task)\n                self._email_complete_error(task, formatted_traceback)\n                deps = None\n                status = UNKNOWN\n                runnable = False\n\n            elif is_complete:\n                deps = None\n                status = DONE\n                runnable = False\n                task.trigger_event(Event.DEPENDENCY_PRESENT, task)\n\n            elif _is_external(task):\n                deps = None\n                status = PENDING\n                runnable = self._config.retry_external_tasks\n                task.trigger_event(Event.DEPENDENCY_MISSING, task)\n                logger.warning('Data for %s does not exist (yet?). The task is an '\n                               'external data dependency, so it cannot be run from'\n                               ' this luigi process.', task)\n\n            else:\n                try:\n                    deps = task.deps()\n                    self._add_task_batcher(task)\n                except Exception as ex:\n                    formatted_traceback = traceback.format_exc()\n                    self.add_succeeded = False\n                    self._log_dependency_error(task, formatted_traceback)\n                    task.trigger_event(Event.BROKEN_TASK, task, ex)\n                    self._email_dependency_error(task, formatted_traceback)\n                    deps = None\n                    status = UNKNOWN\n                    runnable = False\n                else:\n                    status = PENDING\n                    runnable = True\n\n            if task.disabled:\n                status = DISABLED\n\n            if deps:\n                for d in deps:\n                    self._validate_dependency(d)\n                    task.trigger_event(Event.DEPENDENCY_DISCOVERED, task, d)\n                    yield d  # return additional tasks to add\n\n                deps = [d.task_id for d in deps]\n\n        self._scheduled_tasks[task.task_id] = task\n        self._add_task(\n            worker=self._id,\n            task_id=task.task_id,\n            status=status,\n            deps=deps,\n            runnable=runnable,\n            priority=task.priority,\n            resources=task.process_resources(),\n            params=task.to_str_params(),\n            family=task.task_family,\n            module=task.task_module,\n            batchable=task.batchable,\n            retry_policy_dict=_get_retry_policy_dict(task),\n            accepts_messages=task.accepts_messages,\n        )", "is_method": true, "class_name": "Worker", "function_description": "Adds a task to the worker's schedule while managing task dependencies, status, and execution eligibility. It handles error logging, event triggering, and supports yielding new dependent tasks for task management workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_validate_dependency", "line_number": 877, "body": "def _validate_dependency(self, dependency):\n        if isinstance(dependency, Target):\n            raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')\n        elif not isinstance(dependency, Task):\n            raise Exception('requires() must return Task objects but {} is a {}'.format(dependency, type(dependency)))", "is_method": true, "class_name": "Worker", "function_description": "Private method of the Worker class that verifies dependencies are valid Task instances and prevents Target objects from being used directly, ensuring correct dependency specification in task workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_check_complete_value", "line_number": 883, "body": "def _check_complete_value(self, is_complete):\n        if is_complete not in (True, False):\n            if isinstance(is_complete, TracebackWrapper):\n                raise AsyncCompletionException(is_complete.trace)\n            raise Exception(\"Return value of Task.complete() must be boolean (was %r)\" % is_complete)", "is_method": true, "class_name": "Worker", "function_description": "Internal validation method in Worker that ensures Task completion status is a boolean, raising specific exceptions for invalid or wrapped error values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_worker", "line_number": 889, "body": "def _add_worker(self):\n        self._worker_info.append(('first_task', self._first_task))\n        self._scheduler.add_worker(self._id, self._worker_info)", "is_method": true, "class_name": "Worker", "function_description": "Adds the current worker along with its initial task information to the scheduler for management and task assignment. This enables the scheduler to track and coordinate the worker's activities."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_remote_tasks", "line_number": 893, "body": "def _log_remote_tasks(self, get_work_response):\n        logger.debug(\"Done\")\n        logger.debug(\"There are no more tasks to run at this time\")\n        if get_work_response.running_tasks:\n            for r in get_work_response.running_tasks:\n                logger.debug('%s is currently run by worker %s', r['task_id'], r['worker'])\n        elif get_work_response.n_pending_tasks:\n            logger.debug(\n                \"There are %s pending tasks possibly being run by other workers\",\n                get_work_response.n_pending_tasks)\n            if get_work_response.n_unique_pending:\n                logger.debug(\n                    \"There are %i pending tasks unique to this worker\",\n                    get_work_response.n_unique_pending)\n            if get_work_response.n_pending_last_scheduled:\n                logger.debug(\n                    \"There are %i pending tasks last scheduled by this worker\",\n                    get_work_response.n_pending_last_scheduled)", "is_method": true, "class_name": "Worker", "function_description": "Utility method of the Worker class that logs the status of remote tasks, including currently running tasks and various counts of pending tasks, aiding in monitoring distributed task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_work_task_id", "line_number": 912, "body": "def _get_work_task_id(self, get_work_response):\n        if get_work_response.get('task_id') is not None:\n            return get_work_response['task_id']\n        elif 'batch_id' in get_work_response:\n            try:\n                task = load_task(\n                    module=get_work_response.get('task_module'),\n                    task_name=get_work_response['task_family'],\n                    params_str=get_work_response['task_params'],\n                )\n            except Exception as ex:\n                self._handle_task_load_error(ex, get_work_response['batch_task_ids'])\n                self.run_succeeded = False\n                return None\n\n            self._scheduler.add_task(\n                worker=self._id,\n                task_id=task.task_id,\n                module=get_work_response.get('task_module'),\n                family=get_work_response['task_family'],\n                params=task.to_str_params(),\n                status=RUNNING,\n                batch_id=get_work_response['batch_id'],\n            )\n            return task.task_id\n        else:\n            return None", "is_method": true, "class_name": "Worker", "function_description": "Retrieves the task identifier from a work response, loading and scheduling the task if necessary. It ensures the Worker obtains a valid task ID to proceed with processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_work", "line_number": 940, "body": "def _get_work(self):\n        if self._stop_requesting_work:\n            return GetWorkResponse(None, 0, 0, 0, 0, WORKER_STATE_DISABLED)\n\n        if self.worker_processes > 0:\n            logger.debug(\"Asking scheduler for work...\")\n            r = self._scheduler.get_work(\n                worker=self._id,\n                host=self.host,\n                assistant=self._assistant,\n                current_tasks=list(self._running_tasks.keys()),\n            )\n        else:\n            logger.debug(\"Checking if tasks are still pending\")\n            r = self._scheduler.count_pending(worker=self._id)\n\n        running_tasks = r['running_tasks']\n        task_id = self._get_work_task_id(r)\n\n        self._get_work_response_history.append({\n            'task_id': task_id,\n            'running_tasks': running_tasks,\n        })\n\n        if task_id is not None and task_id not in self._scheduled_tasks:\n            logger.info('Did not schedule %s, will load it dynamically', task_id)\n\n            try:\n                # TODO: we should obtain the module name from the server!\n                self._scheduled_tasks[task_id] = \\\n                    load_task(module=r.get('task_module'),\n                              task_name=r['task_family'],\n                              params_str=r['task_params'])\n            except TaskClassException as ex:\n                self._handle_task_load_error(ex, [task_id])\n                task_id = None\n                self.run_succeeded = False\n\n        if task_id is not None and 'batch_task_ids' in r:\n            batch_tasks = filter(None, [\n                self._scheduled_tasks.get(batch_id) for batch_id in r['batch_task_ids']])\n            self._batch_running_tasks[task_id] = batch_tasks\n\n        return GetWorkResponse(\n            task_id=task_id,\n            running_tasks=running_tasks,\n            n_pending_tasks=r['n_pending_tasks'],\n            n_unique_pending=r['n_unique_pending'],\n\n            # TODO: For a tiny amount of time (a month?) we'll keep forwards compatibility\n            #  That is you can user a newer client than server (Sep 2016)\n            n_pending_last_scheduled=r.get('n_pending_last_scheduled', 0),\n            worker_state=r.get('worker_state', WORKER_STATE_ACTIVE),\n        )", "is_method": true, "class_name": "Worker", "function_description": "Core method of the Worker class that requests and manages task assignments from a scheduler, dynamically loading new tasks and tracking running and pending jobs to coordinate work distribution and execution state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_run_task", "line_number": 995, "body": "def _run_task(self, task_id):\n        if task_id in self._running_tasks:\n            logger.debug('Got already running task id {} from scheduler, taking a break'.format(task_id))\n            next(self._sleeper())\n            return\n\n        task = self._scheduled_tasks[task_id]\n\n        task_process = self._create_task_process(task)\n\n        self._running_tasks[task_id] = task_process\n\n        if task_process.use_multiprocessing:\n            with fork_lock:\n                task_process.start()\n        else:\n            # Run in the same process\n            task_process.run()", "is_method": true, "class_name": "Worker", "function_description": "Core method of the Worker class that manages the execution of a scheduled task by starting its process or running it directly, ensuring no duplicate runs for the same task identifier."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_create_task_process", "line_number": 1014, "body": "def _create_task_process(self, task):\n        message_queue = multiprocessing.Queue() if task.accepts_messages else None\n        reporter = TaskStatusReporter(self._scheduler, task.task_id, self._id, message_queue)\n        use_multiprocessing = self._config.force_multiprocessing or bool(self.worker_processes > 1)\n        return ContextManagedTaskProcess(\n            self._config.task_process_context,\n            task, self._id, self._task_result_queue, reporter,\n            use_multiprocessing=use_multiprocessing,\n            worker_timeout=self._config.timeout,\n            check_unfulfilled_deps=self._config.check_unfulfilled_deps,\n            check_complete_on_run=self._config.check_complete_on_run,\n        )", "is_method": true, "class_name": "Worker", "function_description": "Creates and configures a managed task process for executing a given task, setting up inter-process communication, status reporting, and multiprocessing behavior based on worker settings and configuration. This enables controlled task execution with monitoring and dependency checks within the Worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_purge_children", "line_number": 1027, "body": "def _purge_children(self):\n        \"\"\"\n        Find dead children and put a response on the result queue.\n\n        :return:\n        \"\"\"\n        for task_id, p in self._running_tasks.items():\n            if not p.is_alive() and p.exitcode:\n                error_msg = 'Task {} died unexpectedly with exit code {}'.format(task_id, p.exitcode)\n                p.task.trigger_event(Event.PROCESS_FAILURE, p.task, error_msg)\n            elif p.timeout_time is not None and time.time() > float(p.timeout_time) and p.is_alive():\n                p.terminate()\n                error_msg = 'Task {} timed out after {} seconds and was terminated.'.format(task_id, p.worker_timeout)\n                p.task.trigger_event(Event.TIMEOUT, p.task, error_msg)\n            else:\n                continue\n\n            logger.info(error_msg)\n            self._task_result_queue.put((task_id, FAILED, error_msg, [], []))", "is_method": true, "class_name": "Worker", "function_description": "Internal utility of the Worker class that detects and handles tasks which have either failed or timed out, triggering appropriate events and reporting results to the task result queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_next_task", "line_number": 1047, "body": "def _handle_next_task(self):\n        \"\"\"\n        We have to catch three ways a task can be \"done\":\n\n        1. normal execution: the task runs/fails and puts a result back on the queue,\n        2. new dependencies: the task yielded new deps that were not complete and\n           will be rescheduled and dependencies added,\n        3. child process dies: we need to catch this separately.\n        \"\"\"\n        self._idle_since = None\n        while True:\n            self._purge_children()  # Deal with subprocess failures\n\n            try:\n                task_id, status, expl, missing, new_requirements = (\n                    self._task_result_queue.get(\n                        timeout=self._config.wait_interval))\n            except Queue.Empty:\n                return\n\n            task = self._scheduled_tasks[task_id]\n            if not task or task_id not in self._running_tasks:\n                continue\n                # Not a running task. Probably already removed.\n                # Maybe it yielded something?\n\n            # external task if run not implemented, retry-able if config option is enabled.\n            external_task_retryable = _is_external(task) and self._config.retry_external_tasks\n            if status == FAILED and not external_task_retryable:\n                self._email_task_failure(task, expl)\n\n            new_deps = []\n            if new_requirements:\n                new_req = [load_task(module, name, params)\n                           for module, name, params in new_requirements]\n                for t in new_req:\n                    self.add(t)\n                new_deps = [t.task_id for t in new_req]\n\n            self._add_task(worker=self._id,\n                           task_id=task_id,\n                           status=status,\n                           expl=json.dumps(expl),\n                           resources=task.process_resources(),\n                           runnable=None,\n                           params=task.to_str_params(),\n                           family=task.task_family,\n                           module=task.task_module,\n                           new_deps=new_deps,\n                           assistant=self._assistant,\n                           retry_policy_dict=_get_retry_policy_dict(task))\n\n            self._running_tasks.pop(task_id)\n\n            # re-add task to reschedule missing dependencies\n            if missing:\n                reschedule = True\n\n                # keep out of infinite loops by not rescheduling too many times\n                for task_id in missing:\n                    self.unfulfilled_counts[task_id] += 1\n                    if (self.unfulfilled_counts[task_id] >\n                            self._config.max_reschedules):\n                        reschedule = False\n                if reschedule:\n                    self.add(task)\n\n            self.run_succeeded &= (status == DONE) or (len(new_deps) > 0)\n            return", "is_method": true, "class_name": "Worker", "function_description": "Handles completion of the next task by processing its result, managing new dependencies, handling failures, and rescheduling tasks with missing dependencies to ensure reliable task execution and workflow continuity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_sleeper", "line_number": 1117, "body": "def _sleeper(self):\n        # TODO is exponential backoff necessary?\n        while True:\n            jitter = self._config.wait_jitter\n            wait_interval = self._config.wait_interval + random.uniform(0, jitter)\n            logger.debug('Sleeping for %f seconds', wait_interval)\n            time.sleep(wait_interval)\n            yield", "is_method": true, "class_name": "Worker", "function_description": "Internal generator in Worker that yields control after sleeping for a randomized interval, supporting operations requiring periodic, jittered delays such as retry wait loops or timed polling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_keep_alive", "line_number": 1126, "body": "def _keep_alive(self, get_work_response):\n        \"\"\"\n        Returns true if a worker should stay alive given.\n\n        If worker-keep-alive is not set, this will always return false.\n        For an assistant, it will always return the value of worker-keep-alive.\n        Otherwise, it will return true for nonzero n_pending_tasks.\n\n        If worker-count-uniques is true, it will also\n        require that one of the tasks is unique to this worker.\n        \"\"\"\n        if not self._config.keep_alive:\n            return False\n        elif self._assistant:\n            return True\n        elif self._config.count_last_scheduled:\n            return get_work_response.n_pending_last_scheduled > 0\n        elif self._config.count_uniques:\n            return get_work_response.n_unique_pending > 0\n        elif get_work_response.n_pending_tasks == 0:\n            return False\n        elif not self._config.max_keep_alive_idle_duration:\n            return True\n        elif not self._idle_since:\n            return True\n        else:\n            time_to_shutdown = self._idle_since + self._config.max_keep_alive_idle_duration - datetime.datetime.now()\n            logger.debug(\"[%s] %s until shutdown\", self._id, time_to_shutdown)\n            return time_to_shutdown > datetime.timedelta(0)", "is_method": true, "class_name": "Worker", "function_description": "Determines whether a worker should remain active based on task presence, worker type, and configurable keep-alive conditions. It helps manage worker lifecycle by deciding when to shut down idle or inactive workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "handle_interrupt", "line_number": 1156, "body": "def handle_interrupt(self, signum, _):\n        \"\"\"\n        Stops the assistant from asking for more work on SIGUSR1\n        \"\"\"\n        if signum == signal.SIGUSR1:\n            self._start_phasing_out()", "is_method": true, "class_name": "Worker", "function_description": "Handles the SIGUSR1 signal to initiate a phased stop of the worker's task requests, effectively pausing further work assignments when interrupted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_start_phasing_out", "line_number": 1163, "body": "def _start_phasing_out(self):\n        \"\"\"\n        Go into a mode where we dont ask for more work and quit once existing\n        tasks are done.\n        \"\"\"\n        self._config.keep_alive = False\n        self._stop_requesting_work = True", "is_method": true, "class_name": "Worker", "function_description": "Internal method of the Worker class that initiates shutdown by stopping further work requests and allowing existing tasks to complete before quitting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 1171, "body": "def run(self):\n        \"\"\"\n        Returns True if all scheduled tasks were executed successfully.\n        \"\"\"\n        logger.info('Running Worker with %d processes', self.worker_processes)\n\n        sleeper = self._sleeper()\n        self.run_succeeded = True\n\n        self._add_worker()\n\n        while True:\n            while len(self._running_tasks) >= self.worker_processes > 0:\n                logger.debug('%d running tasks, waiting for next task to finish', len(self._running_tasks))\n                self._handle_next_task()\n\n            get_work_response = self._get_work()\n\n            if get_work_response.worker_state == WORKER_STATE_DISABLED:\n                self._start_phasing_out()\n\n            if get_work_response.task_id is None:\n                if not self._stop_requesting_work:\n                    self._log_remote_tasks(get_work_response)\n                if len(self._running_tasks) == 0:\n                    self._idle_since = self._idle_since or datetime.datetime.now()\n                    if self._keep_alive(get_work_response):\n                        next(sleeper)\n                        continue\n                    else:\n                        break\n                else:\n                    self._handle_next_task()\n                    continue\n\n            # task_id is not None:\n            logger.debug(\"Pending tasks: %s\", get_work_response.n_pending_tasks)\n            self._run_task(get_work_response.task_id)\n\n        while len(self._running_tasks):\n            logger.debug('Shut down Worker, %d more tasks to go', len(self._running_tasks))\n            self._handle_next_task()\n\n        return self.run_succeeded", "is_method": true, "class_name": "Worker", "function_description": "Core method of the Worker class that manages and executes scheduled tasks concurrently until completion or shutdown, returning whether all tasks succeeded. It handles task retrieval, running limits, idling, and graceful shutdown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_rpc_message", "line_number": 1216, "body": "def _handle_rpc_message(self, message):\n        logger.info(\"Worker %s got message %s\" % (self._id, message))\n\n        # the message is a dict {'name': <function_name>, 'kwargs': <function_kwargs>}\n        name = message['name']\n        kwargs = message['kwargs']\n\n        # find the function and check if it's callable and configured to work\n        # as a message callback\n        func = getattr(self, name, None)\n        tpl = (self._id, name)\n        if not callable(func):\n            logger.error(\"Worker %s has no function '%s'\" % tpl)\n        elif not getattr(func, \"is_rpc_message_callback\", False):\n            logger.error(\"Worker %s function '%s' is not available as rpc message callback\" % tpl)\n        else:\n            logger.info(\"Worker %s successfully dispatched rpc message to function '%s'\" % tpl)\n            func(**kwargs)", "is_method": true, "class_name": "Worker", "function_description": "Handles incoming RPC messages by dispatching them to appropriately marked callable methods on the Worker instance, enabling remote procedure call functionality within the worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "set_worker_processes", "line_number": 1236, "body": "def set_worker_processes(self, n):\n        # set the new value\n        self.worker_processes = max(1, n)\n\n        # tell the scheduler\n        self._scheduler.add_worker(self._id, {'workers': self.worker_processes})", "is_method": true, "class_name": "Worker", "function_description": "Sets the number of worker processes to at least one and updates the scheduler with this new count for managing workload distribution. This function enables dynamic adjustment of worker resources within the Worker class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "dispatch_scheduler_message", "line_number": 1244, "body": "def dispatch_scheduler_message(self, task_id, message_id, content, **kwargs):\n        task_id = str(task_id)\n        if task_id in self._running_tasks:\n            task_process = self._running_tasks[task_id]\n            if task_process.status_reporter.scheduler_messages:\n                message = SchedulerMessage(self._scheduler, task_id, message_id, content, **kwargs)\n                task_process.status_reporter.scheduler_messages.put(message)", "is_method": true, "class_name": "Worker", "function_description": "Dispatches a scheduler message to a running task's status reporter, enabling communication or control signals within the Worker\u2019s task management system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "generate_email", "line_number": 134, "body": "def generate_email(sender, subject, message, recipients, image_png):\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.image import MIMEImage\n\n    msg_root = MIMEMultipart('related')\n\n    msg_text = MIMEText(message, email().format, 'utf-8')\n    msg_root.attach(msg_text)\n\n    if image_png:\n        with open(image_png, 'rb') as fp:\n            msg_image = MIMEImage(fp.read(), 'png')\n        msg_root.attach(msg_image)\n\n    msg_root['Subject'] = subject\n    msg_root['From'] = sender\n    msg_root['To'] = ','.join(recipients)\n\n    return msg_root", "is_method": false, "function_description": "Function that creates a multipart email message with a subject, sender, recipients, optional PNG image, and text content formatted using the email module's default format. Useful for constructing rich emails with embedded images for sending."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "wrap_traceback", "line_number": 156, "body": "def wrap_traceback(traceback):\n    \"\"\"\n    For internal use only (until further notice)\n    \"\"\"\n    if email().format == 'html':\n        try:\n            from pygments import highlight\n            from pygments.lexers import PythonTracebackLexer\n            from pygments.formatters import HtmlFormatter\n            with_pygments = True\n        except ImportError:\n            with_pygments = False\n\n        if with_pygments:\n            formatter = HtmlFormatter(noclasses=True)\n            wrapped = highlight(traceback, PythonTracebackLexer(), formatter)\n        else:\n            wrapped = '<pre>%s</pre>' % traceback\n    else:\n        wrapped = traceback\n\n    return wrapped", "is_method": false, "function_description": "Function that formats a Python traceback into an HTML-highlighted string if the email format is HTML and syntax highlighting is available; otherwise, it returns the plain traceback text."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_smtp", "line_number": 180, "body": "def send_email_smtp(sender, subject, message, recipients, image_png):\n    import smtplib\n\n    smtp_config = smtp()\n    kwargs = dict(\n        host=smtp_config.host,\n        port=smtp_config.port,\n        local_hostname=smtp_config.local_hostname,\n    )\n    if smtp_config.timeout:\n        kwargs['timeout'] = smtp_config.timeout\n\n    try:\n        smtp_conn = smtplib.SMTP_SSL(**kwargs) if smtp_config.ssl else smtplib.SMTP(**kwargs)\n        smtp_conn.ehlo_or_helo_if_needed()\n        if smtp_conn.has_extn('starttls') and not smtp_config.no_tls:\n            smtp_conn.starttls()\n        if smtp_config.username and smtp_config.password:\n            smtp_conn.login(smtp_config.username, smtp_config.password)\n\n        msg_root = generate_email(sender, subject, message, recipients, image_png)\n\n        smtp_conn.sendmail(sender, recipients, msg_root.as_string())\n    except socket.error as exception:\n        logger.error(\"Not able to connect to smtp server: %s\", exception)", "is_method": false, "function_description": "Function that sends an email with an optional PNG image attachment using SMTP, handling server connection, authentication, and TLS as configured. It supports secure email dispatch to specified recipients with customizable content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_ses", "line_number": 207, "body": "def send_email_ses(sender, subject, message, recipients, image_png):\n    \"\"\"\n    Sends notification through AWS SES.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import client as boto3_client\n\n    client = boto3_client('ses')\n\n    msg_root = generate_email(sender, subject, message, recipients, image_png)\n    response = client.send_raw_email(Source=sender,\n                                     Destinations=recipients,\n                                     RawMessage={'Data': msg_root.as_string()})\n\n    logger.debug((\"Message sent to SES.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))", "is_method": false, "function_description": "Function that sends an email with optional image content using AWS Simple Email Service, supporting notifications without directly handling AWS credentials."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_sendgrid", "line_number": 232, "body": "def send_email_sendgrid(sender, subject, message, recipients, image_png):\n    import sendgrid as sendgrid_lib\n    client = sendgrid_lib.SendGridAPIClient(sendgrid().apikey)\n\n    to_send = sendgrid_lib.Mail(\n            from_email=sender,\n            to_emails=recipients,\n            subject=subject)\n\n    if email().format == 'html':\n        to_send.add_content(message, 'text/html')\n    else:\n        to_send.add_content(message, 'text/plain')\n\n    if image_png:\n        to_send.add_attachment(image_png)\n\n    client.send(to_send)", "is_method": false, "function_description": "Function that sends an email with optional image attachment via the SendGrid service, supporting plain text or HTML content formats for specified recipients. It enables programmatic email delivery with multimedia support."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_email_disabled_reason", "line_number": 252, "body": "def _email_disabled_reason():\n    if email().format == 'none':\n        return \"email format is 'none'\"\n    elif email().force_send:\n        return None\n    elif sys.stdout.isatty():\n        return \"running from a tty\"\n    else:\n        return None", "is_method": false, "function_description": "Utility function that determines why email sending might be disabled, providing specific reasons based on email format, force send status, and terminal interaction context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_sns", "line_number": 263, "body": "def send_email_sns(sender, subject, message, topic_ARN, image_png):\n    \"\"\"\n    Sends notification through AWS SNS. Takes Topic ARN from recipients.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import resource as boto3_resource\n\n    sns = boto3_resource('sns')\n    topic = sns.Topic(topic_ARN[0])\n\n    # Subject is max 100 chars\n    if len(subject) > 100:\n        subject = subject[0:48] + '...' + subject[-49:]\n\n    response = topic.publish(Subject=subject, Message=message)\n\n    logger.debug((\"Message sent to SNS.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))", "is_method": false, "function_description": "Function that sends a notification message with a subject to an AWS SNS topic using a provided topic ARN, supporting configuration via AWS credentials or instance profiles."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email", "line_number": 290, "body": "def send_email(subject, message, sender, recipients, image_png=None):\n    \"\"\"\n    Decides whether to send notification. Notification is cancelled if there are\n    no recipients or if stdout is onto tty or if in debug mode.\n\n    Dispatches on config value email.method.  Default is 'smtp'.\n    \"\"\"\n    notifiers = {\n        'ses': send_email_ses,\n        'sendgrid': send_email_sendgrid,\n        'smtp': send_email_smtp,\n        'sns': send_email_sns,\n    }\n\n    subject = _prefix(subject)\n    if not recipients or recipients == (None,):\n        return\n\n    if _email_disabled_reason():\n        logger.info(\"Not sending email to %r because %s\",\n                    recipients, _email_disabled_reason())\n        return\n\n    # Clean the recipients lists to allow multiple email addresses, comma\n    # separated in luigi.cfg\n    recipients_tmp = []\n    for r in recipients:\n        recipients_tmp.extend([a.strip() for a in r.split(',') if a.strip()])\n\n    # Replace original recipients with the clean list\n    recipients = recipients_tmp\n\n    logger.info(\"Sending email to %r\", recipients)\n\n    # Get appropriate sender and call it to send the notification\n    email_sender = notifiers[email().method]\n    email_sender(sender, subject, message, recipients, image_png)", "is_method": false, "function_description": "Function that sends email notifications using the configured email service method, handling recipient validation and conditional sending based on environment and configuration. It supports multiple providers and manages recipient formatting for reliable dispatch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_email_recipients", "line_number": 329, "body": "def _email_recipients(additional_recipients=None):\n    receiver = email().receiver\n    recipients = [receiver] if receiver else []\n    if additional_recipients:\n        if isinstance(additional_recipients, str):\n            recipients.append(additional_recipients)\n        else:\n            recipients.extend(additional_recipients)\n    return recipients", "is_method": false, "function_description": "Utility function that builds a list of email recipients by including the main receiver and any additional specified recipients, supporting both single and multiple additions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_error_email", "line_number": 340, "body": "def send_error_email(subject, message, additional_recipients=None):\n    \"\"\"\n    Sends an email to the configured error email, if it's configured.\n    \"\"\"\n    recipients = _email_recipients(additional_recipients)\n    sender = email().sender\n    send_email(\n        subject=subject,\n        message=message,\n        sender=sender,\n        recipients=recipients\n    )", "is_method": false, "function_description": "Function that sends error notification emails to predefined recipients, optionally including additional addresses, facilitating automated alerting of issues via email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_prefix", "line_number": 354, "body": "def _prefix(subject):\n    \"\"\"\n    If the config has a special prefix for emails then this function adds\n    this prefix.\n    \"\"\"\n    if email().prefix:\n        return \"{} {}\".format(email().prefix, subject)\n    else:\n        return subject", "is_method": false, "function_description": "Utility function that prepends a configured email prefix to a subject line if one exists, facilitating consistent email subject formatting across an application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "format_task_error", "line_number": 365, "body": "def format_task_error(headline, task, command, formatted_exception=None):\n    \"\"\"\n    Format a message body for an error email related to a luigi.task.Task\n\n    :param headline: Summary line for the message\n    :param task: `luigi.task.Task` instance where this error occurred\n    :param formatted_exception: optional string showing traceback\n\n    :return: message body\n    \"\"\"\n\n    if formatted_exception:\n        formatted_exception = wrap_traceback(formatted_exception)\n    else:\n        formatted_exception = \"\"\n\n    if email().format == 'html':\n        msg_template = textwrap.dedent('''\n        <html>\n        <body>\n        <h2>{headline}</h2>\n\n        <table style=\"border-top: 1px solid black; border-bottom: 1px solid black\">\n        <thead>\n        <tr><th>name</th><td>{name}</td></tr>\n        </thead>\n        <tbody>\n        {param_rows}\n        </tbody>\n        </table>\n        </pre>\n\n        <h2>Command line</h2>\n        <pre>\n        {command}\n        </pre>\n\n        <h2>Traceback</h2>\n        {traceback}\n        </body>\n        </html>\n        ''')\n\n        str_params = task.to_str_params()\n        params = '\\n'.join('<tr><th>{}</th><td>{}</td></tr>'.format(*items) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, param_rows=params,\n                                   command=command, traceback=formatted_exception)\n    else:\n        msg_template = textwrap.dedent('''\\\n        {headline}\n\n        Name: {name}\n\n        Parameters:\n        {params}\n\n        Command line:\n          {command}\n\n        {traceback}\n        ''')\n\n        str_params = task.to_str_params()\n        max_width = max([0] + [len(x) for x in str_params.keys()])\n        params = '\\n'.join('  {:{width}}: {}'.format(*items, width=max_width) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, params=params,\n                                   command=command, traceback=formatted_exception)\n\n    return body", "is_method": false, "function_description": "Formats a detailed error message for a luigi.task.Task failure, supporting both HTML and plain text email formats with task details, command line, and optional traceback. Useful for automated error notifications in task pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "run", "line_number": 56, "body": "def run(self):\n        raise ValueError('Testing notifications triggering')", "is_method": true, "class_name": "TestNotificationsTask", "function_description": "This function deliberately raises an error to simulate or test the triggering of notification mechanisms during execution. It is primarily used to validate error handling and alert workflows in the TestNotificationsTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "complete", "line_number": 59, "body": "def complete(self):\n        if self.raise_in_complete:\n            raise ValueError('Testing notifications triggering')\n        return False", "is_method": true, "class_name": "TestNotificationsTask", "function_description": "Utility method in TestNotificationsTask that conditionally raises an exception for testing notification triggers or returns False, enabling simulation of task completion behaviors during testing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/retcodes.py", "function": "run_with_retcodes", "line_number": 61, "body": "def run_with_retcodes(argv):\n    \"\"\"\n    Run luigi with command line parsing, but raise ``SystemExit`` with the configured exit code.\n\n    Note: Usually you use the luigi binary directly and don't call this function yourself.\n\n    :param argv: Should (conceptually) be ``sys.argv[1:]``\n    \"\"\"\n    logger = logging.getLogger('luigi-interface')\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        retcodes = retcode()\n\n    worker = None\n    try:\n        worker = luigi.interface._run(argv).worker\n    except luigi.interface.PidLockAlreadyTakenExit:\n        sys.exit(retcodes.already_running)\n    except Exception:\n        # Some errors occur before logging is set up, we set it up now\n        env_params = luigi.interface.core()\n        InterfaceLogging.setup(env_params)\n        logger.exception(\"Uncaught exception in luigi\")\n        sys.exit(retcodes.unhandled_exception)\n\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        task_sets = luigi.execution_summary._summary_dict(worker)\n        root_task = luigi.execution_summary._root_task(worker)\n        non_empty_categories = {k: v for k, v in task_sets.items() if v}.keys()\n\n    def has(status):\n        assert status in luigi.execution_summary._ORDERED_STATUSES\n        return status in non_empty_categories\n\n    codes_and_conds = (\n        (retcodes.missing_data, has('still_pending_ext')),\n        (retcodes.task_failed, has('failed')),\n        (retcodes.already_running, has('run_by_other_worker')),\n        (retcodes.scheduling_error, has('scheduling_error')),\n        (retcodes.not_run, has('not_run')),\n    )\n    expected_ret_code = max(code * (1 if cond else 0) for code, cond in codes_and_conds)\n\n    if expected_ret_code == 0 and \\\n       root_task not in task_sets[\"completed\"] and \\\n       root_task not in task_sets[\"already_done\"]:\n        sys.exit(retcodes.not_run)\n    else:\n        sys.exit(expected_ret_code)", "is_method": false, "function_description": "Function that executes a Luigi workflow with command-line arguments, handling execution results by raising SystemExit with specific return codes to indicate failure modes or status outcomes. It enables programmatic control and error signaling for Luigi task runs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/retcodes.py", "function": "has", "line_number": 90, "body": "def has(status):\n        assert status in luigi.execution_summary._ORDERED_STATUSES\n        return status in non_empty_categories", "is_method": false, "function_description": "Checks if a given status is valid and present in a non-empty execution category, supporting status verification in task execution summaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "namespace", "line_number": 50, "body": "def namespace(namespace=None, scope=''):\n    \"\"\"\n    Call to set namespace of tasks declared after the call.\n\n    It is often desired to call this function with the keyword argument\n    ``scope=__name__``.\n\n    The ``scope`` keyword makes it so that this call is only effective for task\n    classes with a matching [*]_ ``__module__``. The default value for\n    ``scope`` is the empty string, which means all classes. Multiple calls with\n    the same scope simply replace each other.\n\n    The namespace of a :py:class:`Task` can also be changed by specifying the property\n    ``task_namespace``.\n\n    .. code-block:: python\n\n        class Task2(luigi.Task):\n            task_namespace = 'namespace2'\n\n    This explicit setting takes priority over whatever is set in the\n    ``namespace()`` method, and it's also inherited through normal python\n    inheritence.\n\n    There's no equivalent way to set the ``task_family``.\n\n    *New since Luigi 2.6.0:* ``scope`` keyword argument.\n\n    .. [*] When there are multiple levels of matching module scopes like\n           ``a.b`` vs ``a.b.c``, the more specific one (``a.b.c``) wins.\n    .. seealso:: The new and better scaling :py:func:`auto_namespace`\n    \"\"\"\n    Register._default_namespace_dict[scope] = namespace or ''", "is_method": false, "function_description": "Function that sets a namespace for subsequent task declarations, optionally scoped to specific module names, enabling organized grouping and overriding of task namespaces within a task execution framework."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "auto_namespace", "line_number": 85, "body": "def auto_namespace(scope=''):\n    \"\"\"\n    Same as :py:func:`namespace`, but instead of a constant namespace, it will\n    be set to the ``__module__`` of the task class. This is desirable for these\n    reasons:\n\n     * Two tasks with the same name will not have conflicting task families\n     * It's more pythonic, as modules are Python's recommended way to\n       do namespacing.\n     * It's traceable. When you see the full name of a task, you can immediately\n       identify where it is defined.\n\n    We recommend calling this function from your package's outermost\n    ``__init__.py`` file. The file contents could look like this:\n\n    .. code-block:: python\n\n        import luigi\n\n        luigi.auto_namespace(scope=__name__)\n\n    To reset an ``auto_namespace()`` call, you can use\n    ``namespace(scope='my_scope')``.  But this will not be\n    needed (and is also discouraged) if you use the ``scope`` kwarg.\n\n    *New since Luigi 2.6.0.*\n    \"\"\"\n    namespace(namespace=_SAME_AS_PYTHON_MODULE, scope=scope)", "is_method": false, "function_description": "Provides automatic namespacing for tasks based on their Python module, preventing naming conflicts and improving traceability. This facilitates more organized and uniquely identifiable task families in Luigi pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_id_str", "line_number": 115, "body": "def task_id_str(task_family, params):\n    \"\"\"\n    Returns a canonical string used to identify a particular task\n\n    :param task_family: The task family (class name) of the task\n    :param params: a dict mapping parameter names to their serialized values\n    :return: A unique, shortened identifier corresponding to the family and params\n    \"\"\"\n    # task_id is a concatenation of task family, the first values of the first 3 parameters\n    # sorted by parameter name and a md5hash of the family/parameters as a cananocalised json.\n    param_str = json.dumps(params, separators=(',', ':'), sort_keys=True)\n    param_hash = hashlib.md5(param_str.encode('utf-8')).hexdigest()\n\n    param_summary = '_'.join(p[:TASK_ID_TRUNCATE_PARAMS]\n                             for p in (params[p] for p in sorted(params)[:TASK_ID_INCLUDE_PARAMS]))\n    param_summary = TASK_ID_INVALID_CHAR_REGEX.sub('_', param_summary)\n\n    return '{}_{}_{}'.format(task_family, param_summary, param_hash[:TASK_ID_TRUNCATE_HASH])", "is_method": false, "function_description": "Generates a unique, concise string identifier for a task based on its family name and sorted parameters, facilitating consistent task identification and tracking across executions or systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "externalize", "line_number": 756, "body": "def externalize(taskclass_or_taskobject):\n    \"\"\"\n    Returns an externalized version of a Task. You may both pass an\n    instantiated task object or a task class. Some examples:\n\n    .. code-block:: python\n\n        class RequiringTask(luigi.Task):\n            def requires(self):\n                task_object = self.clone(MyTask)\n                return externalize(task_object)\n\n            ...\n\n    Here's mostly equivalent code, but ``externalize`` is applied to a task\n    class instead.\n\n    .. code-block:: python\n\n        @luigi.util.requires(externalize(MyTask))\n        class RequiringTask(luigi.Task):\n            pass\n            ...\n\n    Of course, it may also be used directly on classes and objects (for example\n    for reexporting or other usage).\n\n    .. code-block:: python\n\n        MyTask = externalize(MyTask)\n        my_task_2 = externalize(MyTask2(param='foo'))\n\n    If you however want a task class to be external from the beginning, you're\n    better off inheriting :py:class:`ExternalTask` rather than :py:class:`Task`.\n\n    This function tries to be side-effect free by creating a copy of the class\n    or the object passed in and then modify that object. In particular this\n    code shouldn't do anything.\n\n    .. code-block:: python\n\n        externalize(MyTask)  # BAD: This does nothing (as after luigi 2.4.0)\n    \"\"\"\n    copied_value = copy.copy(taskclass_or_taskobject)\n    if copied_value is taskclass_or_taskobject:\n        # Assume it's a class\n        clazz = taskclass_or_taskobject\n\n        @_task_wraps(clazz)\n        class _CopyOfClass(clazz):\n            # How to copy a class: http://stackoverflow.com/a/9541120/621449\n            _visible_in_registry = False\n        _CopyOfClass.run = None\n        return _CopyOfClass\n    else:\n        # We assume it's an object\n        copied_value.run = None\n        return copied_value", "is_method": false, "function_description": "Utility function that returns a modified copy of a Luigi Task class or instance with its run method disabled, enabling tasks to be treated as external dependencies without execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "getpaths", "line_number": 834, "body": "def getpaths(struct):\n    \"\"\"\n    Maps all Tasks in a structured data object to their .output().\n    \"\"\"\n    if isinstance(struct, Task):\n        return struct.output()\n    elif isinstance(struct, dict):\n        return struct.__class__((k, getpaths(v)) for k, v in struct.items())\n    elif isinstance(struct, (list, tuple)):\n        return struct.__class__(getpaths(r) for r in struct)\n    else:\n        # Remaining case: assume struct is iterable...\n        try:\n            return [getpaths(r) for r in struct]\n        except TypeError:\n            raise Exception('Cannot map %s to Task/dict/list' % str(struct))", "is_method": false, "function_description": "Utility function that recursively traverses a structured object, mapping all embedded Task instances to their output values while preserving the original container types. Useful for extracting results from nested task collections in diverse data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "flatten", "line_number": 852, "body": "def flatten(struct):\n    \"\"\"\n    Creates a flat list of all all items in structured output (dicts, lists, items):\n\n    .. code-block:: python\n\n        >>> sorted(flatten({'a': 'foo', 'b': 'bar'}))\n        ['bar', 'foo']\n        >>> sorted(flatten(['foo', ['bar', 'troll']]))\n        ['bar', 'foo', 'troll']\n        >>> flatten('foo')\n        ['foo']\n        >>> flatten(42)\n        [42]\n    \"\"\"\n    if struct is None:\n        return []\n    flat = []\n    if isinstance(struct, dict):\n        for _, result in struct.items():\n            flat += flatten(result)\n        return flat\n    if isinstance(struct, str):\n        return [struct]\n\n    try:\n        # if iterable\n        iterator = iter(struct)\n    except TypeError:\n        return [struct]\n\n    for result in iterator:\n        flat += flatten(result)\n    return flat", "is_method": false, "function_description": "Function that recursively extracts and returns all individual elements from nested dictionaries, lists, or other iterable structures as a single flat list. It enables uniform processing of arbitrarily nested data by simplifying its structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "flatten_output", "line_number": 888, "body": "def flatten_output(task):\n    \"\"\"\n    Lists all output targets by recursively walking output-less (wrapper) tasks.\n\n    FIXME order consistently.\n    \"\"\"\n    r = flatten(task.output())\n    if not r:\n        for dep in flatten(task.requires()):\n            r += flatten_output(dep)\n    return r", "is_method": false, "function_description": "Function that recursively collects and returns all ultimate output targets from a task and its dependencies, ensuring comprehensive retrieval of non-wrapper task outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_task_wraps", "line_number": 901, "body": "def _task_wraps(task_class):\n    # In order to make the behavior of a wrapper class nicer, we set the name of the\n    # new class to the wrapped class, and copy over the docstring and module as well.\n    # This makes it possible to pickle the wrapped class etc.\n    # Btw, this is a slight abuse of functools.wraps. It's meant to be used only for\n    # functions, but it works for classes too, if you pass updated=[]\n    assigned = functools.WRAPPER_ASSIGNMENTS + ('_namespace_at_class_time',)\n    return functools.wraps(task_class, assigned=assigned, updated=[])", "is_method": false, "function_description": "Utility function that customizes class wrappers by preserving the original class\u2019s name, docstring, and module, improving wrapper behavior such as pickling compatibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "batchable", "line_number": 192, "body": "def batchable(self):\n        \"\"\"\n        True if this instance can be run as part of a batch. By default, True\n        if it has any batched parameters\n        \"\"\"\n        return bool(self.batch_param_names())", "is_method": true, "class_name": "Task", "function_description": "Indicates whether the Task instance supports batch execution by checking for the presence of batched parameters. This enables batch processing optimizations when applicable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "retry_count", "line_number": 200, "body": "def retry_count(self):\n        \"\"\"\n        Override this positive integer to have different ``retry_count`` at task level\n        Check :ref:`scheduler-config`\n        \"\"\"\n        return None", "is_method": true, "class_name": "Task", "function_description": "This function provides a customizable retry count for a task, allowing task-specific override of how many times it should be retried upon failure. It supports flexible task execution control within a scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "disable_window", "line_number": 216, "body": "def disable_window(self):\n        \"\"\"\n        Override this positive integer to have different ``disable_window`` at task level.\n        Check :ref:`scheduler-config`\n        \"\"\"\n        return self.disable_window_seconds", "is_method": true, "class_name": "Task", "function_description": "Returns the task's configured disable window duration in seconds, allowing customization of the period during which the task remains disabled. This supports scheduling control within the Task class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "disable_window_seconds", "line_number": 224, "body": "def disable_window_seconds(self):\n        warnings.warn(\"Use of `disable_window_seconds` has been deprecated, use `disable_window` instead\", DeprecationWarning)\n        return None", "is_method": true, "class_name": "Task", "function_description": "This method serves as a deprecated placeholder that warns users to use a newer alternative, `disable_window`, signaling the transition away from the outdated `disable_window_seconds` interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "owner_email", "line_number": 229, "body": "def owner_email(self):\n        '''\n        Override this to send out additional error emails to task owner, in addition to the one\n        defined in the global configuration. This should return a string or a list of strings. e.g.\n        'test@exmaple.com' or ['test1@example.com', 'test2@example.com']\n        '''\n        return None", "is_method": true, "class_name": "Task", "function_description": "Returns additional email addresses of the task owner for error notifications beyond the global configuration, enabling customized error alert recipients."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_owner_list", "line_number": 237, "body": "def _owner_list(self):\n        \"\"\"\n        Turns the owner_email property into a list. This should not be overridden.\n        \"\"\"\n        owner_email = self.owner_email\n        if owner_email is None:\n            return []\n        elif isinstance(owner_email, str):\n            return owner_email.split(',')\n        else:\n            return owner_email", "is_method": true, "class_name": "Task", "function_description": "Utility method of the Task class that converts the owner_email attribute into a list of email addresses, supporting consistent handling of single or multiple owner values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "use_cmdline_section", "line_number": 250, "body": "def use_cmdline_section(self):\n        ''' Property used by core config such as `--workers` etc.\n        These will be exposed without the class as prefix.'''\n        return True", "is_method": true, "class_name": "Task", "function_description": "Indicates that command-line options for this Task class should be exposed without a class name prefix, enabling simpler command-line configuration access for core settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "event_handler", "line_number": 256, "body": "def event_handler(cls, event):\n        \"\"\"\n        Decorator for adding event handlers.\n        \"\"\"\n        def wrapped(callback):\n            cls._event_callbacks.setdefault(cls, {}).setdefault(event, set()).add(callback)\n            return callback\n        return wrapped", "is_method": true, "class_name": "Task", "function_description": "Decorator method of the Task class that registers callback functions as handlers for specific events, enabling event-driven behavior within the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "trigger_event", "line_number": 265, "body": "def trigger_event(self, event, *args, **kwargs):\n        \"\"\"\n        Trigger that calls all of the specified events associated with this class.\n        \"\"\"\n        for event_class, event_callbacks in self._event_callbacks.items():\n            if not isinstance(self, event_class):\n                continue\n            for callback in event_callbacks.get(event, []):\n                try:\n                    # callbacks are protected\n                    callback(*args, **kwargs)\n                except KeyboardInterrupt:\n                    return\n                except BaseException:\n                    logger.exception(\"Error in event callback for %r\", event)", "is_method": true, "class_name": "Task", "function_description": "Triggers all registered callbacks for a specific event relevant to the Task instance, allowing event-driven behavior by invoking associated handlers with given arguments. This facilitates customized responses to various events within the Task class hierarchy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "accepts_messages", "line_number": 282, "body": "def accepts_messages(self):\n        \"\"\"\n        For configuring which scheduler messages can be received. When falsy, this tasks does not\n        accept any message. When True, all messages are accepted.\n        \"\"\"\n        return False", "is_method": true, "class_name": "Task", "function_description": "Indicates whether the Task accepts scheduler messages, controlling if it receives any or all messages. Useful for managing task communication and message handling permissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_module", "line_number": 290, "body": "def task_module(self):\n        ''' Returns what Python module to import to get access to this class. '''\n        # TODO(erikbern): we should think about a language-agnostic mechanism\n        return self.__class__.__module__", "is_method": true, "class_name": "Task", "function_description": "Returns the Python module name where the current Task class is defined, enabling dynamic access or import of the module containing this task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_task_namespace", "line_number": 315, "body": "def get_task_namespace(cls):\n        \"\"\"\n        The task family for the given class.\n\n        Note: You normally don't want to override this.\n        \"\"\"\n        if cls.task_namespace != cls.__not_user_specified:\n            return cls.task_namespace\n        elif cls._namespace_at_class_time == _SAME_AS_PYTHON_MODULE:\n            return cls.__module__\n        return cls._namespace_at_class_time", "is_method": true, "class_name": "Task", "function_description": "Returns the namespace associated with the Task class, reflecting its task family or Python module. This helps organize tasks by their logical grouping or origin."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_family", "line_number": 328, "body": "def task_family(self):\n        \"\"\"\n        DEPRECATED since after 2.4.0. See :py:meth:`get_task_family` instead.\n        Hopefully there will be less meta magic in Luigi.\n\n        Convenience method since a property on the metaclass isn't directly\n        accessible through the class instances.\n        \"\"\"\n        return self.__class__.task_family", "is_method": true, "class_name": "Task", "function_description": "Deprecated method of the Task class that returns the task family identifier. Use get_task_family instead, as this method is kept only for backward compatibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_task_family", "line_number": 339, "body": "def get_task_family(cls):\n        \"\"\"\n        The task family for the given class.\n\n        If ``task_namespace`` is not set, then it's simply the name of the\n        class.  Otherwise, ``<task_namespace>.`` is prefixed to the class name.\n\n        Note: You normally don't want to override this.\n        \"\"\"\n        if not cls.get_task_namespace():\n            return cls.__name__\n        else:\n            return \"{}.{}\".format(cls.get_task_namespace(), cls.__name__)", "is_method": true, "class_name": "Task", "function_description": "Provides the task family identifier by combining a namespace with the class name, allowing consistent categorization of task types within the Task class hierarchy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_params", "line_number": 354, "body": "def get_params(cls):\n        \"\"\"\n        Returns all of the Parameters for this Task.\n        \"\"\"\n        # We want to do this here and not at class instantiation, or else there is no room to extend classes dynamically\n        params = []\n        for param_name in dir(cls):\n            param_obj = getattr(cls, param_name)\n            if not isinstance(param_obj, Parameter):\n                continue\n\n            params.append((param_name, param_obj))\n\n        # The order the parameters are created matters. See Parameter class\n        params.sort(key=lambda t: t[1]._counter)\n        return params", "is_method": true, "class_name": "Task", "function_description": "Returns a sorted list of all Parameter instances defined in the Task class, reflecting their creation order. This enables dynamic inspection of a task's configurable parameters for further processing or extension."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "batch_param_names", "line_number": 372, "body": "def batch_param_names(cls):\n        return [name for name, p in cls.get_params() if p._is_batchable()]", "is_method": true, "class_name": "Task", "function_description": "Returns a list of parameter names from the Task class that support batched processing, facilitating operations that handle multiple inputs efficiently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_param_names", "line_number": 376, "body": "def get_param_names(cls, include_significant=False):\n        return [name for name, p in cls.get_params() if include_significant or p.significant]", "is_method": true, "class_name": "Task", "function_description": "This class method returns a list of parameter names from the class, optionally including only those marked as significant. It helps identify key parameters for configuration or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_param_values", "line_number": 380, "body": "def get_param_values(cls, params, args, kwargs):\n        \"\"\"\n        Get the values of the parameters from the args and kwargs.\n\n        :param params: list of (param_name, Parameter).\n        :param args: positional arguments\n        :param kwargs: keyword arguments.\n        :returns: list of `(name, value)` tuples, one for each parameter.\n        \"\"\"\n        result = {}\n\n        params_dict = dict(params)\n\n        task_family = cls.get_task_family()\n\n        # In case any exceptions are thrown, create a helpful description of how the Task was invoked\n        # TODO: should we detect non-reprable arguments? These will lead to mysterious errors\n        exc_desc = '%s[args=%s, kwargs=%s]' % (task_family, args, kwargs)\n\n        # Fill in the positional arguments\n        positional_params = [(n, p) for n, p in params if p.positional]\n        for i, arg in enumerate(args):\n            if i >= len(positional_params):\n                raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))\n            param_name, param_obj = positional_params[i]\n            result[param_name] = param_obj.normalize(arg)\n\n        # Then the keyword arguments\n        for param_name, arg in kwargs.items():\n            if param_name in result:\n                raise parameter.DuplicateParameterException('%s: parameter %s was already set as a positional parameter' % (exc_desc, param_name))\n            if param_name not in params_dict:\n                raise parameter.UnknownParameterException('%s: unknown parameter %s' % (exc_desc, param_name))\n            result[param_name] = params_dict[param_name].normalize(arg)\n\n        # Then use the defaults for anything not filled in\n        for param_name, param_obj in params:\n            if param_name not in result:\n                if not param_obj.has_task_value(task_family, param_name):\n                    raise parameter.MissingParameterException(\"%s: requires the '%s' parameter to be set\" % (exc_desc, param_name))\n                result[param_name] = param_obj.task_value(task_family, param_name)\n\n        def list_to_tuple(x):\n            \"\"\" Make tuples out of lists and sets to allow hashing \"\"\"\n            if isinstance(x, list) or isinstance(x, set):\n                return tuple(x)\n            else:\n                return x\n        # Sort it by the correct order and make a list\n        return [(param_name, list_to_tuple(result[param_name])) for param_name, param_obj in params]", "is_method": true, "class_name": "Task", "function_description": "Utility method of the Task class that extracts and normalizes parameter values from given positional and keyword arguments, enforcing parameter constraints and filling in defaults for task execution consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "param_args", "line_number": 451, "body": "def param_args(self):\n        warnings.warn(\"Use of param_args has been deprecated.\", DeprecationWarning)\n        return tuple(self.param_kwargs[k] for k, v in self.get_params())", "is_method": true, "class_name": "Task", "function_description": "This method provides a deprecated way to access task parameters as a tuple, emitting a warning to encourage use of updated approaches. It helps retrieve parameter values in a tuple form from the task's parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "initialized", "line_number": 455, "body": "def initialized(self):\n        \"\"\"\n        Returns ``True`` if the Task is initialized and ``False`` otherwise.\n        \"\"\"\n        return hasattr(self, 'task_id')", "is_method": true, "class_name": "Task", "function_description": "Simple method of the Task class that checks if the task has been initialized by verifying the existence of its unique identifier attribute. Useful for confirming task readiness before execution or further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_warn_on_wrong_param_types", "line_number": 461, "body": "def _warn_on_wrong_param_types(self):\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            params[param_name]._warn_on_wrong_param_type(param_name, param_value)", "is_method": true, "class_name": "Task", "function_description": "Internal method of Task that checks parameter values against expected types and issues warnings for mismatches, helping ensure correct parameter usage during task configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "from_str_params", "line_number": 467, "body": "def from_str_params(cls, params_str):\n        \"\"\"\n        Creates an instance from a str->str hash.\n\n        :param params_str: dict of param name -> value as string.\n        \"\"\"\n        kwargs = {}\n        for param_name, param in cls.get_params():\n            if param_name in params_str:\n                param_str = params_str[param_name]\n                if isinstance(param_str, list):\n                    kwargs[param_name] = param._parse_list(param_str)\n                else:\n                    kwargs[param_name] = param.parse(param_str)\n\n        return cls(**kwargs)", "is_method": true, "class_name": "Task", "function_description": "Creates a Task instance from a dictionary of string parameters by parsing each value according to its expected type. This enables instantiating tasks from serialized or string-based configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "to_str_params", "line_number": 484, "body": "def to_str_params(self, only_significant=False, only_public=False):\n        \"\"\"\n        Convert all parameters to a str->str hash.\n        \"\"\"\n        params_str = {}\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            if (((not only_significant) or params[param_name].significant)\n                    and ((not only_public) or params[param_name].visibility == ParameterVisibility.PUBLIC)\n                    and params[param_name].visibility != ParameterVisibility.PRIVATE):\n                params_str[param_name] = params[param_name].serialize(param_value)\n\n        return params_str", "is_method": true, "class_name": "Task", "function_description": "Converts task parameters into a dictionary of stringified key-value pairs, optionally filtering by significance and visibility. This enables standardized and filtered parameter representation for logging, debugging, or serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_get_param_visibilities", "line_number": 498, "body": "def _get_param_visibilities(self):\n        param_visibilities = {}\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            if params[param_name].visibility != ParameterVisibility.PRIVATE:\n                param_visibilities[param_name] = params[param_name].visibility.serialize()\n\n        return param_visibilities", "is_method": true, "class_name": "Task", "function_description": "Utility method of the Task class that returns a dictionary of parameter names to their serialized visibility states, excluding those marked as private. It helps determine which parameters should be exposed based on their visibility settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "clone", "line_number": 507, "body": "def clone(self, cls=None, **kwargs):\n        \"\"\"\n        Creates a new instance from an existing instance where some of the args have changed.\n\n        There's at least two scenarios where this is useful (see test/clone_test.py):\n\n        * remove a lot of boiler plate when you have recursive dependencies and lots of args\n        * there's task inheritance and some logic is on the base class\n\n        :param cls:\n        :param kwargs:\n        :return:\n        \"\"\"\n        if cls is None:\n            cls = self.__class__\n\n        new_k = {}\n        for param_name, param_class in cls.get_params():\n            if param_name in kwargs:\n                new_k[param_name] = kwargs[param_name]\n            elif hasattr(self, param_name):\n                new_k[param_name] = getattr(self, param_name)\n\n        return cls(**new_k)", "is_method": true, "class_name": "Task", "function_description": "Creates a new Task instance by copying an existing one with optional argument overrides, simplifying modifications and supporting recursive dependencies or inheritance scenarios."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__hash__", "line_number": 532, "body": "def __hash__(self):\n        return self.__hash", "is_method": true, "class_name": "Task", "function_description": "Overrides the default hash behavior to provide a consistent hash value for Task instances, enabling their use in hash-based collections like sets or dictionaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__repr__", "line_number": 535, "body": "def __repr__(self):\n        \"\"\"\n        Build a task representation like `MyTask(param1=1.5, param2='5')`\n        \"\"\"\n        params = self.get_params()\n        param_values = self.get_param_values(params, [], self.param_kwargs)\n\n        # Build up task id\n        repr_parts = []\n        param_objs = dict(params)\n        for param_name, param_value in param_values:\n            if param_objs[param_name].significant:\n                repr_parts.append('%s=%s' % (param_name, param_objs[param_name].serialize(param_value)))\n\n        task_str = '{}({})'.format(self.get_task_family(), ', '.join(repr_parts))\n\n        return task_str", "is_method": true, "class_name": "Task", "function_description": "Returns a string representation of a Task instance showing its family name and significant parameter values for easy identification and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__eq__", "line_number": 553, "body": "def __eq__(self, other):\n        return self.__class__ == other.__class__ and self.task_id == other.task_id", "is_method": true, "class_name": "Task", "function_description": "Defines equality comparison for Task instances, considering two tasks equal if they are of the same class and have identical task identifiers. This enables consistent task comparisons based on identity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "complete", "line_number": 556, "body": "def complete(self):\n        \"\"\"\n        If the task has any outputs, return ``True`` if all outputs exist.\n        Otherwise, return ``False``.\n\n        However, you may freely override this method with custom logic.\n        \"\"\"\n        outputs = flatten(self.output())\n        if len(outputs) == 0:\n            warnings.warn(\n                \"Task %r without outputs has no custom complete() method\" % self,\n                stacklevel=2\n            )\n            return False\n\n        return all(map(lambda output: output.exists(), outputs))", "is_method": true, "class_name": "Task", "function_description": "Completion status checker for a Task that returns True if all expected outputs exist, otherwise False, serving to verify task completion based on output presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "bulk_complete", "line_number": 574, "body": "def bulk_complete(cls, parameter_tuples):\n        \"\"\"\n        Returns those of parameter_tuples for which this Task is complete.\n\n        Override (with an efficient implementation) for efficient scheduling\n        with range tools. Keep the logic consistent with that of complete().\n        \"\"\"\n        raise BulkCompleteNotImplementedError()", "is_method": true, "class_name": "Task", "function_description": "Returns the subset of parameter tuples for which the Task is complete, enabling efficient batch completion checks during scheduling. Intended for override to optimize bulk operations consistent with single-task completion logic."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "output", "line_number": 583, "body": "def output(self):\n        \"\"\"\n        The output that this Task produces.\n\n        The output of the Task determines if the Task needs to be run--the task\n        is considered finished iff the outputs all exist. Subclasses should\n        override this method to return a single :py:class:`Target` or a list of\n        :py:class:`Target` instances.\n\n        Implementation note\n          If running multiple workers, the output must be a resource that is accessible\n          by all workers, such as a DFS or database. Otherwise, workers might compute\n          the same output since they don't see the work done by other workers.\n\n        See :ref:`Task.output`\n        \"\"\"\n        return []", "is_method": true, "class_name": "Task", "function_description": "Core method of the Task class intended to specify the Task's expected outputs, which determine its completion status; subclasses override to define one or more targets representing the Task's results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "requires", "line_number": 601, "body": "def requires(self):\n        \"\"\"\n        The Tasks that this Task depends on.\n\n        A Task will only run if all of the Tasks that it requires are completed.\n        If your Task does not require any other Tasks, then you don't need to\n        override this method. Otherwise, a subclass can override this method\n        to return a single Task, a list of Task instances, or a dict whose\n        values are Task instances.\n\n        See :ref:`Task.requires`\n        \"\"\"\n        return []", "is_method": true, "class_name": "Task", "function_description": "Returns the dependencies of a Task, indicating which other Tasks must be completed before this one runs. It enables task sequencing and coordination within workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_requires", "line_number": 615, "body": "def _requires(self):\n        \"\"\"\n        Override in \"template\" tasks which themselves are supposed to be\n        subclassed and thus have their requires() overridden (name preserved to\n        provide consistent end-user experience), yet need to introduce\n        (non-input) dependencies.\n\n        Must return an iterable which among others contains the _requires() of\n        the superclass.\n        \"\"\"\n        return flatten(self.requires())", "is_method": true, "class_name": "Task", "function_description": "Core internal method of the Task class allowing template tasks to define additional non-input dependencies by aggregating their own requirements with those of their subclasses for consistent dependency management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "process_resources", "line_number": 627, "body": "def process_resources(self):\n        \"\"\"\n        Override in \"template\" tasks which provide common resource functionality\n        but allow subclasses to specify additional resources while preserving\n        the name for consistent end-user experience.\n        \"\"\"\n        return self.resources", "is_method": true, "class_name": "Task", "function_description": "Returns the current task's resource list, designed for subclasses to extend resources while maintaining a consistent interface. It supports customizable resource management in task subclasses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "input", "line_number": 635, "body": "def input(self):\n        \"\"\"\n        Returns the outputs of the Tasks returned by :py:meth:`requires`\n\n        See :ref:`Task.input`\n\n        :return: a list of :py:class:`Target` objects which are specified as\n                 outputs of all required Tasks.\n        \"\"\"\n        return getpaths(self.requires())", "is_method": true, "class_name": "Task", "function_description": "Returns a list of output targets from all tasks that the current task depends on, facilitating task dependency management and input retrieval within workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "deps", "line_number": 646, "body": "def deps(self):\n        \"\"\"\n        Internal method used by the scheduler.\n\n        Returns the flattened list of requires.\n        \"\"\"\n        # used by scheduler\n        return flatten(self._requires())", "is_method": true, "class_name": "Task", "function_description": "Internal Task class method that returns a flattened list of dependencies required for execution, assisting the scheduler in managing task order and prerequisites."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "on_failure", "line_number": 663, "body": "def on_failure(self, exception):\n        \"\"\"\n        Override for custom error handling.\n\n        This method gets called if an exception is raised in :py:meth:`run`.\n        The returned value of this method is json encoded and sent to the scheduler\n        as the `expl` argument. Its string representation will be used as the\n        body of the error email sent out if any.\n\n        Default behavior is to return a string representation of the stack trace.\n        \"\"\"\n\n        traceback_string = traceback.format_exc()\n        return \"Runtime error:\\n%s\" % traceback_string", "is_method": true, "class_name": "Task", "function_description": "Handles exceptions raised during task execution by generating an error description. It provides a customizable error message that can be sent to a scheduler or included in notification emails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "no_unpicklable_properties", "line_number": 690, "body": "def no_unpicklable_properties(self):\n        \"\"\"\n        Remove unpicklable properties before dump task and resume them after.\n\n        This method could be called in subtask's dump method, to ensure unpicklable\n        properties won't break dump.\n\n        This method is a context-manager which can be called as below:\n\n        .. code-block: python\n\n            class DummyTask(luigi):\n\n                def _dump(self):\n                    with self.no_unpicklable_properties():\n                        pickle.dumps(self)\n\n        \"\"\"\n        unpicklable_properties = tuple(luigi.worker.TaskProcess.forward_reporter_attributes.values())\n        reserved_properties = {}\n        for property_name in unpicklable_properties:\n            if hasattr(self, property_name):\n                reserved_properties[property_name] = getattr(self, property_name)\n                setattr(self, property_name, 'placeholder_during_pickling')\n\n        yield\n\n        for property_name, value in reserved_properties.items():\n            setattr(self, property_name, value)", "is_method": true, "class_name": "Task", "function_description": "Provides a context manager to temporarily remove unpicklable properties from a Task instance, allowing safe pickling and restoring them afterward. Useful for serializing tasks that contain non-picklable attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "bulk_complete", "line_number": 730, "body": "def bulk_complete(cls, parameter_tuples):\n        generated_tuples = []\n        for parameter_tuple in parameter_tuples:\n            if isinstance(parameter_tuple, (list, tuple)):\n                if cls(*parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n            elif isinstance(parameter_tuple, dict):\n                if cls(**parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n            else:\n                if cls(parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n        return generated_tuples", "is_method": true, "class_name": "MixinNaiveBulkComplete", "function_description": "Core method of MixinNaiveBulkComplete that filters input parameter collections by instantiating and completing each; it returns only those parameters for which the class's complete() method succeeds, enabling batch validity checking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "complete", "line_number": 821, "body": "def complete(self):\n        return all(r.complete() for r in flatten(self.requires()))", "is_method": true, "class_name": "WrapperTask", "function_description": "Checks if all prerequisite tasks are completed, indicating whether the entire task can be considered finished."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "list_to_tuple", "line_number": 422, "body": "def list_to_tuple(x):\n            \"\"\" Make tuples out of lists and sets to allow hashing \"\"\"\n            if isinstance(x, list) or isinstance(x, set):\n                return tuple(x)\n            else:\n                return x", "is_method": true, "class_name": "Task", "function_description": "Utility function in Task class converting lists or sets into tuples for hashing purposes, ensuring data types are immutable and hashable when needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "from_utc", "line_number": 227, "body": "def from_utc(utcTime, fmt=None):\n    \"\"\"convert UTC time string to time.struct_time: change datetime.datetime to time, return time.struct_time type\"\"\"\n    if fmt is None:\n        try_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\"]\n    else:\n        try_formats = [fmt]\n\n    for fmt in try_formats:\n        try:\n            time_struct = datetime.datetime.strptime(utcTime, fmt)\n        except ValueError:\n            pass\n        else:\n            date = int(time.mktime(time_struct.timetuple()))\n            return date\n    else:\n        raise ValueError(\"No UTC format matches {}\".format(utcTime))", "is_method": false, "function_description": "Function that converts a UTC time string into a Unix timestamp integer by parsing it according to specified or default datetime formats. It facilitates time format conversion for easier time-based computations or comparisons."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "app", "line_number": 300, "body": "def app(scheduler):\n    settings = {\"static_path\": os.path.join(os.path.dirname(__file__), \"static\"),\n                \"unescape\": tornado.escape.xhtml_unescape,\n                \"compress_response\": True,\n                }\n    handlers = [\n        (r'/api/(.*)', RPCHandler, {\"scheduler\": scheduler}),\n        (r'/', RootPathHandler, {'scheduler': scheduler}),\n        (r'/tasklist', AllRunHandler, {'scheduler': scheduler}),\n        (r'/tasklist/(.*?)', SelectedRunHandler, {'scheduler': scheduler}),\n        (r'/history', RecentRunHandler, {'scheduler': scheduler}),\n        (r'/history/by_name/(.*?)', ByNameHandler, {'scheduler': scheduler}),\n        (r'/history/by_id/(.*?)', ByIdHandler, {'scheduler': scheduler}),\n        (r'/history/by_params/(.*?)', ByParamsHandler, {'scheduler': scheduler}),\n        (r'/metrics', MetricsHandler, {'scheduler': scheduler})\n    ]\n    api_app = tornado.web.Application(handlers, **settings)\n    return api_app", "is_method": false, "function_description": "Constructs and returns a Tornado web application configured with URL routes and handlers for task scheduling, history, metrics, and API endpoints. It centralizes request routing tied to the provided scheduler for managing asynchronous tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_init_api", "line_number": 320, "body": "def _init_api(scheduler, api_port=None, address=None, unix_socket=None):\n    api_app = app(scheduler)\n    if unix_socket is not None:\n        api_sockets = [tornado.netutil.bind_unix_socket(unix_socket)]\n    else:\n        api_sockets = tornado.netutil.bind_sockets(api_port, address=address)\n    server = tornado.httpserver.HTTPServer(api_app)\n    server.add_sockets(api_sockets)\n\n    # Return the bound socket names.  Useful for connecting client in test scenarios.\n    return [s.getsockname() for s in api_sockets]", "is_method": false, "function_description": "Initializes and starts an HTTP API server bound to specified ports or Unix socket, returning the socket addresses to facilitate client connections, often used for testing or inter-process communication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "run", "line_number": 333, "body": "def run(api_port=8082, address=None, unix_socket=None, scheduler=None):\n    \"\"\"\n    Runs one instance of the API server.\n    \"\"\"\n    if scheduler is None:\n        scheduler = Scheduler()\n\n    # load scheduler state\n    scheduler.load()\n\n    _init_api(\n        scheduler=scheduler,\n        api_port=api_port,\n        address=address,\n        unix_socket=unix_socket,\n    )\n\n    # prune work DAG every 60 seconds\n    pruner = tornado.ioloop.PeriodicCallback(scheduler.prune, 60000)\n    pruner.start()\n\n    def shutdown_handler(signum, frame):\n        exit_handler()\n        sys.exit(0)\n\n    @atexit.register\n    def exit_handler():\n        logger.info(\"Scheduler instance shutting down\")\n        scheduler.dump()\n        stop()\n\n    signal.signal(signal.SIGINT, shutdown_handler)\n    signal.signal(signal.SIGTERM, shutdown_handler)\n    if os.name == 'nt':\n        signal.signal(signal.SIGBREAK, shutdown_handler)\n    else:\n        signal.signal(signal.SIGQUIT, shutdown_handler)\n\n    logger.info(\"Scheduler starting up\")\n\n    tornado.ioloop.IOLoop.instance().start()", "is_method": false, "function_description": "Function that starts and runs an API server instance with an optional scheduler, handles periodic cleanup, and manages graceful shutdown signals to maintain scheduler state consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "stop", "line_number": 376, "body": "def stop():\n    tornado.ioloop.IOLoop.instance().stop()", "is_method": false, "function_description": "Stops the current Tornado I/O loop, effectively halting ongoing asynchronous operations and event processing. This function is useful for gracefully terminating Tornado-based applications or servers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 102, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "RPCHandler", "function_description": "Initializes the RPCHandler with a scheduling component, enabling it to coordinate or manage task execution timing through the provided scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "options", "line_number": 105, "body": "def options(self, *args):\n        if self._cors_config.enabled:\n            self._handle_cors_preflight()\n\n        self.set_status(204)\n        self.finish()", "is_method": true, "class_name": "RPCHandler", "function_description": "Handles HTTP OPTIONS requests by managing CORS preflight checks and returning a no-content response, facilitating cross-origin resource sharing in RPC communications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 112, "body": "def get(self, method):\n        if method not in RPC_METHODS:\n            self.send_error(404)\n            return\n        payload = self.get_argument('data', default=\"{}\")\n        arguments = json.loads(payload)\n\n        if hasattr(self._scheduler, method):\n            result = getattr(self._scheduler, method)(**arguments)\n\n            if self._cors_config.enabled:\n                self._handle_cors()\n\n            self.write({\"response\": result})  # wrap all json response in a dictionary\n        else:\n            self.send_error(404)", "is_method": true, "class_name": "RPCHandler", "function_description": "Handles RPC GET requests by validating method names, invoking the corresponding scheduler method with arguments, and returning the result, supporting CORS when enabled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_handle_cors_preflight", "line_number": 131, "body": "def _handle_cors_preflight(self):\n        origin = self.request.headers.get('Origin')\n        if not origin:\n            return\n\n        if origin == 'null':\n            if self._cors_config.allow_null_origin:\n                self.set_header('Access-Control-Allow-Origin', 'null')\n                self._set_other_cors_headers()\n        else:\n            if self._cors_config.allow_any_origin:\n                self.set_header('Access-Control-Allow-Origin', '*')\n                self._set_other_cors_headers()\n            elif origin in self._cors_config.allowed_origins:\n                self.set_header('Access-Control-Allow-Origin', origin)\n                self._set_other_cors_headers()", "is_method": true, "class_name": "RPCHandler", "function_description": "Handles CORS preflight requests by setting appropriate Access-Control-Allow-Origin headers based on configured allowed origins, enabling cross-origin resource sharing for HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_handle_cors", "line_number": 148, "body": "def _handle_cors(self):\n        origin = self.request.headers.get('Origin')\n        if not origin:\n            return\n\n        if origin == 'null':\n            if self._cors_config.allow_null_origin:\n                self.set_header('Access-Control-Allow-Origin', 'null')\n        else:\n            if self._cors_config.allow_any_origin:\n                self.set_header('Access-Control-Allow-Origin', '*')\n            elif origin in self._cors_config.allowed_origins:\n                self.set_header('Access-Control-Allow-Origin', origin)\n                self.set_header('Vary', 'Origin')", "is_method": true, "class_name": "RPCHandler", "function_description": "Private method in RPCHandler that sets appropriate CORS headers based on the request origin and configured policies, enabling controlled cross-origin resource sharing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_set_other_cors_headers", "line_number": 163, "body": "def _set_other_cors_headers(self):\n        self.set_header('Access-Control-Max-Age', str(self._cors_config.max_age))\n        self.set_header('Access-Control-Allow-Methods', self._cors_config.allowed_methods)\n        self.set_header('Access-Control-Allow-Headers', self._cors_config.allowed_headers)\n        if self._cors_config.allow_credentials:\n            self.set_header('Access-Control-Allow-Credentials', 'true')\n        if self._cors_config.exposed_headers:\n            self.set_header('Access-Control-Expose-Headers', self._cors_config.exposed_headers)", "is_method": true, "class_name": "RPCHandler", "function_description": "Internal method of RPCHandler that sets additional CORS headers based on configuration to control allowed methods, headers, credentials, and exposed headers for cross-origin requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 174, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "BaseTaskHistoryHandler", "function_description": "Sets up the BaseTaskHistoryHandler with a scheduler instance, allowing it to coordinate or track tasks through the provided scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get_template_path", "line_number": 177, "body": "def get_template_path(self):\n        return pkg_resources.resource_filename(__name__, 'templates')", "is_method": true, "class_name": "BaseTaskHistoryHandler", "function_description": "Returns the filesystem path to the handler's template resources, enabling other components to locate and access template files within the package."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 182, "body": "def get(self):\n        all_tasks = self._scheduler.task_history.find_all_runs()\n        tasknames = [task.name for task in all_tasks]\n        # show all tasks with their name list to be selected\n        # why all tasks? the duration of the event history of a selected task\n        # can be more than 24 hours.\n        self.render(\"menu.html\", tasknames=tasknames)", "is_method": true, "class_name": "AllRunHandler", "function_description": "Provides a web interface that displays the names of all previously run tasks, enabling users to select tasks for further interaction or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 192, "body": "def get(self, name):\n        statusResults = {}\n        taskResults = []\n        # get all tasks that has been updated\n        all_tasks = self._scheduler.task_history.find_all_runs()\n        # get events history for all tasks\n        all_tasks_event_history = self._scheduler.task_history.find_all_events()\n\n        # build the dictionary tasks with index: id, value: task_name\n        tasks = {task.id: str(task.name) for task in all_tasks}\n\n        for task in all_tasks_event_history:\n            # if the name of user-selected task is in tasks, get its task_id\n            if tasks.get(task.task_id) == str(name):\n                status = str(task.event_name)\n                if status not in statusResults:\n                    statusResults[status] = []\n                # append the id, task_id, ts, y with 0, next_process with null\n                # for the status(running/failed/done) of the selected task\n                statusResults[status].append(({\n                                                  'id': str(task.id), 'task_id': str(task.task_id),\n                                                  'x': from_utc(str(task.ts)), 'y': 0, 'next_process': ''}))\n                # append the id, task_name, task_id, status, datetime, timestamp\n                # for the selected task\n                taskResults.append({\n                    'id': str(task.id), 'taskName': str(name), 'task_id': str(task.task_id),\n                    'status': str(task.event_name), 'datetime': str(task.ts),\n                    'timestamp': from_utc(str(task.ts))})\n        statusResults = json.dumps(statusResults)\n        taskResults = json.dumps(taskResults)\n        statusResults = tornado.escape.xhtml_unescape(str(statusResults))\n        taskResults = tornado.escape.xhtml_unescape(str(taskResults))\n        self.render('history.html', name=name, statusResults=statusResults, taskResults=taskResults)", "is_method": true, "class_name": "SelectedRunHandler", "function_description": "Retrieves and organizes event and status history for all runs of a specified task name, then renders this data for display in a web interface. Useful for visualizing the lifecycle and states of scheduled tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 247, "body": "def get(self):\n        tasks = self._scheduler.task_history.find_latest_runs()\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "RecentRunHandler", "function_description": "Retrieves the latest task runs from the scheduler's history and renders them on a webpage. This function enables displaying recent execution results in a web interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 253, "body": "def get(self, name):\n        tasks = self._scheduler.task_history.find_all_by_name(name)\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "ByNameHandler", "function_description": "Method of ByNameHandler that retrieves all tasks with a given name from the scheduler's history and renders them for display, enabling task lookup and review by name."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 259, "body": "def get(self, id):\n        task = self._scheduler.task_history.find_task_by_id(id)\n        self.render(\"show.html\", task=task)", "is_method": true, "class_name": "ByIdHandler", "function_description": "Retrieves a task by its ID from the scheduler's history and renders a web page displaying the task details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 265, "body": "def get(self, name):\n        payload = self.get_argument('data', default=\"{}\")\n        arguments = json.loads(payload)\n        tasks = self._scheduler.task_history.find_all_by_parameters(name, session=None, **arguments)\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "ByParamsHandler", "function_description": "Provides a web handler method that retrieves and displays past tasks filtered by a given name and additional parameters from a JSON payload. Enables users to view task history results dynamically via HTTP GET requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 273, "body": "def get(self):\n        # we omit the leading slash in case the visualizer is behind a different\n        # path (as in a reverse proxy setup)\n        #\n        # For example, if luigi is behind my.app.com/my/luigi/, we want / to\n        # redirect relative (so it goes to my.app.com/my/luigi/static/visualizer/index.html)\n        # instead of absolute (which would be my.app.com/static/visualizer/index.html)\n        self.redirect(\"static/visualiser/index.html\")", "is_method": true, "class_name": "RootPathHandler", "function_description": "Core method of RootPathHandler that redirects the root URL to a relative path for the visualizer\u2019s main HTML page, supporting flexible deployment setups such as reverse proxy configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "head", "line_number": 282, "body": "def head(self):\n        \"\"\"HEAD endpoint for health checking the scheduler\"\"\"\n        self.set_status(204)\n        self.finish()", "is_method": true, "class_name": "RootPathHandler", "function_description": "Provides a health check endpoint response with HTTP status 204 to indicate the scheduler is operational without returning content. Useful for monitoring the service availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 289, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "MetricsHandler", "function_description": "Sets the scheduler instance for the MetricsHandler, enabling coordinated timing or execution control of metrics-related operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 292, "body": "def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n        if metrics:\n            metrics_collector.configure_http_handler(self)\n            self.write(metrics)", "is_method": true, "class_name": "MetricsHandler", "function_description": "This method retrieves the latest collected metrics, sets up the HTTP response handler, and writes the metrics data for client consumption. It serves to provide real-time access to performance or operational metrics via HTTP."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "shutdown_handler", "line_number": 354, "body": "def shutdown_handler(signum, frame):\n        exit_handler()\n        sys.exit(0)", "is_method": false, "function_description": "Handles system signals to gracefully terminate the program by invoking an exit procedure before exiting. It ensures proper cleanup during shutdown triggered by external interrupts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "exit_handler", "line_number": 359, "body": "def exit_handler():\n        logger.info(\"Scheduler instance shutting down\")\n        scheduler.dump()\n        stop()", "is_method": false, "function_description": "Function that handles orderly shutdown by logging the event, persisting scheduler state, and stopping related processes for clean termination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_empty_retry_policy", "line_number": 90, "body": "def _get_empty_retry_policy():\n    return RetryPolicy(*[None] * len(_retry_policy_fields))", "is_method": false, "function_description": "Returns a RetryPolicy instance with all retry parameters set to None, effectively representing a policy that disables retries. This function provides a baseline retry configuration with no retry behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "rpc_method", "line_number": 94, "body": "def rpc_method(**request_args):\n    def _rpc_method(fn):\n        # If request args are passed, return this function again for use as\n        # the decorator function with the request args attached.\n        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = inspect.getfullargspec(fn)\n        assert not varargs\n        first_arg, *all_args = args\n        assert first_arg == 'self'\n        defaults = dict(zip(reversed(all_args), reversed(defaults or ())))\n        required_args = frozenset(arg for arg in all_args if arg not in defaults)\n        fn_name = fn.__name__\n\n        @functools.wraps(fn)\n        def rpc_func(self, *args, **kwargs):\n            actual_args = defaults.copy()\n            actual_args.update(dict(zip(all_args, args)))\n            actual_args.update(kwargs)\n            if not all(arg in actual_args for arg in required_args):\n                raise TypeError('{} takes {} arguments ({} given)'.format(\n                    fn_name, len(all_args), len(actual_args)))\n            return self._request('/api/{}'.format(fn_name), actual_args, **request_args)\n\n        RPC_METHODS[fn_name] = rpc_func\n        return fn\n\n    return _rpc_method", "is_method": false, "function_description": "Decorator factory that converts instance methods into RPC calls by wrapping them to validate arguments and forward requests to a specified API endpoint, registering the RPC-enabled methods for remote invocation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_default", "line_number": 159, "body": "def _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default", "is_method": false, "function_description": "Utility function that returns a given value if it is not None; otherwise, it provides a specified default value. It simplifies handling optional parameters or fallback values in other functions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_retry_policy", "line_number": 155, "body": "def _get_retry_policy(self):\n        return RetryPolicy(self.retry_count, self.disable_hard_timeout, self.disable_window)", "is_method": true, "class_name": "scheduler", "function_description": "Returns a configured RetryPolicy object based on the scheduler's retry and timeout settings, enabling consistent retry behavior within scheduling operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__len__", "line_number": 180, "body": "def __len__(self):\n        return len(self.map)", "is_method": true, "class_name": "OrderedSet", "function_description": "Returns the number of elements in the OrderedSet, providing a way to quickly determine its size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__contains__", "line_number": 183, "body": "def __contains__(self, key):\n        return key in self.map", "is_method": true, "class_name": "OrderedSet", "function_description": "Checks whether a given key exists in the OrderedSet, enabling membership testing for elements within the set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add", "line_number": 186, "body": "def add(self, key):\n        if key not in self.map:\n            end = self.end\n            curr = end[1]\n            curr[2] = end[1] = self.map[key] = [key, curr, end]", "is_method": true, "class_name": "OrderedSet", "function_description": "Adds a unique key to the OrderedSet while maintaining insertion order and ensuring no duplicates. This enables efficient order-preserving membership management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "discard", "line_number": 192, "body": "def discard(self, key):\n        if key in self.map:\n            key, prev, next = self.map.pop(key)\n            prev[2] = next\n            next[1] = prev", "is_method": true, "class_name": "OrderedSet", "function_description": "Removes an element from the OrderedSet if present, maintaining the internal order structure by updating linked references accordingly. This function supports efficient element deletion while preserving order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__iter__", "line_number": 198, "body": "def __iter__(self):\n        end = self.end\n        curr = end[2]\n        while curr is not end:\n            yield curr[0]\n            curr = curr[2]", "is_method": true, "class_name": "OrderedSet", "function_description": "Iterator method of the OrderedSet class that yields elements in their insertion order, enabling iteration over all items in the set sequentially."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__reversed__", "line_number": 205, "body": "def __reversed__(self):\n        end = self.end\n        curr = end[1]\n        while curr is not end:\n            yield curr[0]\n            curr = curr[1]", "is_method": true, "class_name": "OrderedSet", "function_description": "Provides an iterator that yields the elements of the OrderedSet in reverse order, supporting reverse traversal of its items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "peek", "line_number": 212, "body": "def peek(self, last=True):\n        if not self:\n            raise KeyError('set is empty')\n        key = self.end[1][0] if last else self.end[2][0]\n        return key", "is_method": true, "class_name": "OrderedSet", "function_description": "Provides a way to retrieve the last (or first) element from the OrderedSet without removing it, useful for checking elements in insertion order while preserving the set contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pop", "line_number": 218, "body": "def pop(self, last=True):\n        key = self.peek(last)\n        self.discard(key)\n        return key", "is_method": true, "class_name": "OrderedSet", "function_description": "Core method of OrderedSet that removes and returns either the last or first element, providing controlled element removal while maintaining order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__repr__", "line_number": 223, "body": "def __repr__(self):\n        if not self:\n            return '%s()' % (self.__class__.__name__,)\n        return '%s(%r)' % (self.__class__.__name__, list(self))", "is_method": true, "class_name": "OrderedSet", "function_description": "Provides a string representation of the OrderedSet instance displaying its contents, aiding debugging and readability by showing the class name and the ordered elements it contains."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__eq__", "line_number": 228, "body": "def __eq__(self, other):\n        if isinstance(other, OrderedSet):\n            return len(self) == len(other) and list(self) == list(other)\n        return set(self) == set(other)", "is_method": true, "class_name": "OrderedSet", "function_description": "Determines equality by comparing order and contents when the other object is an OrderedSet; otherwise, compares elements ignoring order. This enables precise set comparisons respecting element sequence when applicable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__repr__", "line_number": 275, "body": "def __repr__(self):\n        return \"Task(%r)\" % vars(self)", "is_method": true, "class_name": "Task", "function_description": "Provides a string representation of the Task instance showing its attribute dictionary, useful for debugging and logging the object's current state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_params", "line_number": 278, "body": "def set_params(self, params):\n        self.params = _get_default(params, {})\n        self.public_params = {key: value for key, value in self.params.items() if\n                              self.param_visibilities.get(key, ParameterVisibility.PUBLIC) == ParameterVisibility.PUBLIC}\n        self.hidden_params = {key: value for key, value in self.params.items() if\n                              self.param_visibilities.get(key, ParameterVisibility.PUBLIC) == ParameterVisibility.HIDDEN}", "is_method": true, "class_name": "Task", "function_description": "Sets the task's parameters while categorizing them into public and hidden groups based on their visibility settings for controlled access and management within the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_batchable", "line_number": 287, "body": "def is_batchable(self):\n        try:\n            return self.batchable\n        except AttributeError:\n            return False", "is_method": true, "class_name": "Task", "function_description": "Checks whether the Task instance supports batch processing by returning its batchable attribute or False if undefined. This helps determine if multiple tasks can be handled collectively."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_failure", "line_number": 293, "body": "def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        failure_time = time.time()\n\n        if not self.first_failure_time:\n            self.first_failure_time = failure_time\n\n        self.failures.append(failure_time)", "is_method": true, "class_name": "Task", "function_description": "Method of the Task class that records a failure occurrence by timestamp, tracking both the first failure time and all subsequent failure events for monitoring task reliability or error history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "num_failures", "line_number": 304, "body": "def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.retry_policy.disable_window\n\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)", "is_method": true, "class_name": "Task", "function_description": "Returns the count of recent failure events within a specified time window, supporting tracking and limiting retry attempts based on failure history in the Task class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_excessive_failures", "line_number": 315, "body": "def has_excessive_failures(self):\n        if self.first_failure_time is not None:\n            if time.time() >= self.first_failure_time + self.retry_policy.disable_hard_timeout:\n                return True\n\n        logger.debug('%s task num failures is %s and limit is %s', self.id, self.num_failures(), self.retry_policy.retry_count)\n        if self.num_failures() >= self.retry_policy.retry_count:\n            logger.debug('%s task num failures limit(%s) is exceeded', self.id, self.retry_policy.retry_count)\n            return True\n\n        return False", "is_method": true, "class_name": "Task", "function_description": "Determines if a task has exceeded its allowed number of failures or its hard timeout limit, indicating that it should be disabled or stopped from retrying further. This aids in enforcing retry policies for task reliability management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "clear_failures", "line_number": 327, "body": "def clear_failures(self):\n        \"\"\"\n        Clear the failures history\n        \"\"\"\n        self.failures.clear()\n        self.first_failure_time = None", "is_method": true, "class_name": "Task", "function_description": "Clears the failure history and resets the timestamp of the first failure, enabling the Task to remove error records and start fresh monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pretty_id", "line_number": 335, "body": "def pretty_id(self):\n        param_str = ', '.join(u'{}={}'.format(key, value) for key, value in sorted(self.public_params.items()))\n        return u'{}({})'.format(self.family, param_str)", "is_method": true, "class_name": "Task", "function_description": "Returns a formatted string representing the task's family name and its sorted public parameters, useful for readable identification and display of task instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_info", "line_number": 356, "body": "def add_info(self, info):\n        self.info.update(info)", "is_method": true, "class_name": "Worker", "function_description": "Updates the Worker's information dictionary with new key-value pairs, allowing dynamic extension or modification of its stored data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update", "line_number": 359, "body": "def update(self, worker_reference, get_work=False):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n        if get_work:\n            self.last_get_work = time.time()", "is_method": true, "class_name": "Worker", "function_description": "Updates the worker's reference and timestamps indicating its last activity and optionally the last time it requested work, supporting accurate tracking of worker status and engagement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "prune", "line_number": 366, "body": "def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True", "is_method": true, "class_name": "Worker", "function_description": "Determines if a worker is inactive based on a time threshold, indicating it should be removed. Useful for cleaning up unresponsive or disconnected worker instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_tasks", "line_number": 371, "body": "def get_tasks(self, state, *statuses):\n        num_self_tasks = len(self.tasks)\n        num_state_tasks = sum(len(state._status_tasks[status]) for status in statuses)\n        if num_self_tasks < num_state_tasks:\n            return filter(lambda task: task.status in statuses, self.tasks)\n        else:\n            return filter(lambda task: self.id in task.workers, state.get_active_tasks_by_status(*statuses))", "is_method": true, "class_name": "Worker", "function_description": "Method of the Worker class that retrieves tasks matching specified statuses, selecting from the worker's own tasks or the overall state to optimize result relevance based on task counts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_trivial_worker", "line_number": 379, "body": "def is_trivial_worker(self, state):\n        \"\"\"\n        If it's not an assistant having only tasks that are without\n        requirements.\n\n        We have to pass the state parameter for optimization reasons.\n        \"\"\"\n        if self.assistant:\n            return False\n        return all(not task.resources for task in self.get_tasks(state, PENDING))", "is_method": true, "class_name": "Worker", "function_description": "Determines if the worker is non-assistant and has only tasks without resource requirements, indicating trivial work. It helps identify workers with simple, requirement-free pending tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "assistant", "line_number": 391, "body": "def assistant(self):\n        return self.info.get('assistant', False)", "is_method": true, "class_name": "Worker", "function_description": "Returns whether the Worker instance has an assistant assigned, based on its stored information. This can be used to check assistant availability or presence in worker-related operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "enabled", "line_number": 395, "body": "def enabled(self):\n        return not self.disabled", "is_method": true, "class_name": "Worker", "function_description": "Returns whether the Worker instance is currently enabled by checking its disabled status. This method helps determine if the worker is active and available for tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "state", "line_number": 399, "body": "def state(self):\n        if self.enabled:\n            return WORKER_STATE_ACTIVE\n        else:\n            return WORKER_STATE_DISABLED", "is_method": true, "class_name": "Worker", "function_description": "Returns the current operational state of the Worker, indicating whether it is active or disabled based on its enabled status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_rpc_message", "line_number": 405, "body": "def add_rpc_message(self, name, **kwargs):\n        # the message has the format {'name': <function_name>, 'kwargs': <function_kwargs>}\n        self.rpc_messages.append({'name': name, 'kwargs': kwargs})", "is_method": true, "class_name": "Worker", "function_description": "Utility method of the Worker class that queues an RPC message by storing its function name and arguments, enabling asynchronous or deferred remote procedure call handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fetch_rpc_messages", "line_number": 409, "body": "def fetch_rpc_messages(self):\n        messages = self.rpc_messages[:]\n        del self.rpc_messages[:]\n        return messages", "is_method": true, "class_name": "Worker", "function_description": "Returns and clears all currently queued RPC messages, enabling retrieval and processing of pending remote procedure call data within the Worker class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__str__", "line_number": 414, "body": "def __str__(self):\n        return self.id", "is_method": true, "class_name": "Worker", "function_description": "Returns the worker's identifier as its string representation, enabling easy identification and display of Worker instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_state", "line_number": 435, "body": "def get_state(self):\n        return self._tasks, self._active_workers, self._task_batchers", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns the current internal state of SimpleTaskState, including tasks, active workers, and task batchers. This function provides a snapshot useful for monitoring or managing task execution status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_state", "line_number": 438, "body": "def set_state(self, state):\n        self._tasks, self._active_workers = state[:2]\n        if len(state) >= 3:\n            self._task_batchers = state[2]", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Sets the internal task, active worker, and optionally task batcher states from a provided state tuple, enabling state restoration or synchronization in task management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dump", "line_number": 443, "body": "def dump(self):\n        try:\n            with open(self._state_path, 'wb') as fobj:\n                pickle.dump(self.get_state(), fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Saves the current state of the SimpleTaskState instance to a file, enabling persistence and later restoration of the scheduler's state across program executions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "load", "line_number": 453, "body": "def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from empty state.\")\n                return\n\n            self.set_state(state)\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.values():\n                self._status_tasks[task.status][task.id] = task\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with empty state\", self._state_path)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Loads the saved task state from a file if available, restoring the internal task tracking structure; otherwise, it initializes the state as empty. This supports resuming or recovering task progress in task management scenarios."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_tasks", "line_number": 470, "body": "def get_active_tasks(self):\n        return self._tasks.values()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns all currently active tasks managed by the SimpleTaskState instance, providing access to their current states for monitoring or processing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_tasks_by_status", "line_number": 473, "body": "def get_active_tasks_by_status(self, *statuses):\n        return itertools.chain.from_iterable(self._status_tasks[status].values() for status in statuses)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns an iterator over all active tasks matching any of the specified statuses. This method facilitates filtering and accessing tasks by their current status within the SimpleTaskState context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_task_count_for_status", "line_number": 476, "body": "def get_active_task_count_for_status(self, status):\n        if status:\n            return len(self._status_tasks[status])\n        else:\n            return len(self._tasks)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides the count of active tasks either filtered by a specific status or in total when no status is given, enabling task tracking based on current states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_batch_running_tasks", "line_number": 482, "body": "def get_batch_running_tasks(self, batch_id):\n        assert batch_id is not None\n        return [\n            task for task in self.get_active_tasks_by_status(BATCH_RUNNING)\n            if task.batch_id == batch_id\n        ]", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns a list of active tasks currently running within a specified batch, enabling tracking and management of batch-specific task execution states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_batcher", "line_number": 489, "body": "def set_batcher(self, worker_id, family, batcher_args, max_batch_size):\n        self._task_batchers.setdefault(worker_id, {})\n        self._task_batchers[worker_id][family] = (batcher_args, max_batch_size)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Sets or updates batch processing configurations for a specific worker and task family, defining batching parameters and limits for task execution management within the SimpleTaskState class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_batcher", "line_number": 493, "body": "def get_batcher(self, worker_id, family):\n        return self._task_batchers.get(worker_id, {}).get(family, (None, 1))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides the batcher configuration for a given worker and task family, returning batcher info or defaults. Useful for managing task batching behavior per worker and task category."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "num_pending_tasks", "line_number": 496, "body": "def num_pending_tasks(self):\n        \"\"\"\n        Return how many tasks are PENDING + RUNNING. O(1).\n        \"\"\"\n        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns the count of tasks in pending or running states, enabling quick tracking of active work in progress within a task management context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task", "line_number": 502, "body": "def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Method of SimpleTaskState that fetches a task by ID, optionally inserting and indexing a default task if missing. It provides efficient access and management of tasks within the state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_task", "line_number": 510, "body": "def has_task(self, task_id):\n        return task_id in self._tasks", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Checks if a task with the specified ID exists within the SimpleTaskState's current task collection, enabling quick task presence verification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "re_enable", "line_number": 513, "body": "def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.clear_failures()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.clear_failures()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Method in SimpleTaskState that re-enables a task by clearing its scheduler disable time and failures, optionally updating its status to FAILED with a given configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_batch_running", "line_number": 520, "body": "def set_batch_running(self, task, batch_id, worker_id):\n        self.set_status(task, BATCH_RUNNING)\n        task.batch_id = batch_id\n        task.worker_running = worker_id\n        task.resources_running = task.resources\n        task.time_running = time.time()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Sets a task's state to running for a specific batch, recording its batch ID, the worker processing it, required resources, and the start time. This method helps track the execution status and allocation details of batched tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_status", "line_number": 527, "body": "def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        if new_status == DISABLED and task.status in (RUNNING, BATCH_RUNNING):\n            return\n\n        remove_on_failure = task.batch_id is not None and not task.batchable\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                return\n\n        if task.status == RUNNING and task.batch_id is not None and new_status != RUNNING:\n            for batch_task in self.get_batch_running_tasks(task.batch_id):\n                self.set_status(batch_task, new_status, config)\n                batch_task.batch_id = None\n            task.batch_id = None\n\n        if new_status == FAILED and task.status != DISABLED:\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                if not config.batch_emails:\n                    notifications.send_error_email(\n                        'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                        '{task} failed {failures} times in the last {window} seconds, so it is being '\n                        'disabled for {persist} seconds'.format(\n                            failures=task.retry_policy.retry_count,\n                            task=task.id,\n                            window=task.retry_policy.disable_window,\n                            persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        if new_status != task.status:\n            self._status_tasks[task.status].pop(task.id)\n            self._status_tasks[new_status][task.id] = task\n            task.status = new_status\n            task.updated = time.time()\n            self.update_metrics(task, config)\n\n        if new_status == FAILED:\n            task.retry = time.time() + config.retry_delay\n            if remove_on_failure:\n                task.remove = time.time()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Manages and updates the status of tasks, handling transitions, failure logic, batching behavior, and notifications, ensuring consistent task state tracking within the SimpleTaskState system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fail_dead_worker_task", "line_number": 580, "body": "def fail_dead_worker_task(self, task, config, assistants):\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status in (BATCH_RUNNING, RUNNING) and task.worker_running and task.worker_running not in task.stakeholders | assistants:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Marks tasks assigned to disconnected workers as failed and schedules them for retry, ensuring proper task state management in distributed worker environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_status", "line_number": 590, "body": "def update_status(self, task, config):\n        # Mark tasks with no remaining active stakeholders for deletion\n        if (not task.stakeholders) and (task.remove is None) and (task.status != RUNNING):\n            # We don't check for the RUNNING case, because that is already handled\n            # by the fail_dead_worker_task function.\n            logger.debug(\"Task %r has no stakeholders anymore -> might remove \"\n                         \"task in %s seconds\", task.id, config.remove_delay)\n            task.remove = time.time() + config.remove_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time is not None:\n            if time.time() - task.scheduler_disable_time > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Updates the state of a task based on its stakeholders, timing, and status; it schedules removal, re-enables disabled tasks, and retries failed tasks according to configured delays and conditions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "may_prune", "line_number": 608, "body": "def may_prune(self, task):\n        return task.remove and time.time() >= task.remove", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Determines if a task is eligible for pruning based on its removal flag and scheduled removal time. Useful for managing and cleaning up completed or obsolete tasks in a task state system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inactivate_tasks", "line_number": 611, "body": "def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Method of SimpleTaskState that removes specified tasks from active tracking, effectively marking them inactive without necessarily deleting their history. It supports flexible task lifecycle management by handling task inactivation in state storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_workers", "line_number": 619, "body": "def get_active_workers(self, last_active_lt=None, last_get_work_gt=None):\n        for worker in self._active_workers.values():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            last_get_work = worker.last_get_work\n            if last_get_work_gt is not None and (\n                            last_get_work is None or last_get_work <= last_get_work_gt):\n                continue\n            yield worker", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides an iterator over active workers filtered by recent activity and work retrieval timestamps, enabling targeted monitoring or management of worker states within the SimpleTaskState context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_assistants", "line_number": 629, "body": "def get_assistants(self, last_active_lt=None):\n        return filter(lambda w: w.assistant, self.get_active_workers(last_active_lt))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns a filtered list of active workers who are designated as assistants, optionally considering their recent activity timestamp. This enables retrieval of currently engaged assistant workers based on activity criteria."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_worker_ids", "line_number": 632, "body": "def get_worker_ids(self):\n        return self._active_workers.keys()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns the identifiers of currently active workers in the task state, enabling tracking and management of worker participation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_worker", "line_number": 635, "body": "def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns an active worker by ID, creating and storing a new Worker if none exists. This function manages worker instances, ensuring access to a valid worker for task processing or management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inactivate_workers", "line_number": 638, "body": "def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n        self._remove_workers_from_tasks(delete_workers)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Service method of SimpleTaskState that deactivates specified workers and removes their assignments from tasks, effectively updating the task-worker status management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_remove_workers_from_tasks", "line_number": 644, "body": "def _remove_workers_from_tasks(self, workers, remove_stakeholders=True):\n        for task in self.get_active_tasks():\n            if remove_stakeholders:\n                task.stakeholders.difference_update(workers)\n            task.workers -= workers", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Removes specified workers from all active tasks in the SimpleTaskState, optionally also removing them from task stakeholders. This facilitates updating task assignments and participant roles dynamically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "disable_workers", "line_number": 650, "body": "def disable_workers(self, worker_ids):\n        self._remove_workers_from_tasks(worker_ids, remove_stakeholders=False)\n        for worker_id in worker_ids:\n            worker = self.get_worker(worker_id)\n            worker.disabled = True\n            worker.tasks.clear()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Method of SimpleTaskState that disables specified workers by marking them inactive, removing their task assignments while keeping their stakeholder roles intact. It supports managing worker availability in task coordination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_metrics", "line_number": 657, "body": "def update_metrics(self, task, config):\n        if task.status == DISABLED:\n            self._metrics_collector.handle_task_disabled(task, config)\n        elif task.status == DONE:\n            self._metrics_collector.handle_task_done(task)\n        elif task.status == FAILED:\n            self._metrics_collector.handle_task_failed(task)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Service method of SimpleTaskState that updates metrics based on a task's current status, delegating handling for disabled, completed, or failed tasks to an internal metrics collector."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "load", "line_number": 700, "body": "def load(self):\n        self._state.load()", "is_method": true, "class_name": "Scheduler", "function_description": "Calls the internal state's load method to restore the scheduler's saved state. It enables the Scheduler to resume operations from a persisted checkpoint."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dump", "line_number": 703, "body": "def dump(self):\n        self._state.dump()\n        if self._config.batch_emails:\n            self._email_batcher.send_email()", "is_method": true, "class_name": "Scheduler", "function_description": "Service method of the Scheduler class that saves the current scheduler state and, if configured, sends a batch of emails. It supports state persistence alongside optional email notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "prune", "line_number": 709, "body": "def prune(self):\n        logger.debug(\"Starting pruning of task graph\")\n        self._prune_workers()\n        self._prune_tasks()\n        self._prune_emails()\n        logger.debug(\"Done pruning task graph\")", "is_method": true, "class_name": "Scheduler", "function_description": "Core method of the Scheduler class that cleans up the internal task graph by removing obsolete workers, tasks, and emails to maintain system efficiency and prevent resource clutter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_workers", "line_number": 716, "body": "def _prune_workers(self):\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.debug(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)", "is_method": true, "class_name": "Scheduler", "function_description": "Private method of the Scheduler class that identifies and deactivates workers which have timed out due to inactivity, ensuring efficient management of active worker resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_tasks", "line_number": 725, "body": "def _prune_tasks(self):\n        assistant_ids = {w.id for w in self._state.get_assistants()}\n        remove_tasks = []\n\n        for task in self._state.get_active_tasks():\n            self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            self._state.update_status(task, self._config)\n            if self._state.may_prune(task):\n                logger.info(\"Removing task %r\", task.id)\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)", "is_method": true, "class_name": "Scheduler", "function_description": "Internal method of Scheduler that removes tasks associated with inactive assistants by failing, updating, and pruning them to maintain a clean and accurate task state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_emails", "line_number": 738, "body": "def _prune_emails(self):\n        if self._config.batch_emails:\n            self._email_batcher.update()", "is_method": true, "class_name": "Scheduler", "function_description": "Internal Scheduler method that conditionally triggers the batching and updating process of emails based on configuration settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_worker", "line_number": 742, "body": "def _update_worker(self, worker_id, worker_reference=None, get_work=False):\n        # Keep track of whenever the worker was last active.\n        # For convenience also return the worker object.\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference, get_work=get_work)\n        return worker", "is_method": true, "class_name": "Scheduler", "function_description": "Internal method of Scheduler that updates a worker's status and reference, optionally querying for new tasks, while returning the updated worker object for tracking active workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_priority", "line_number": 749, "body": "def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)", "is_method": true, "class_name": "Scheduler", "function_description": "Updates a task's priority to a higher value recursively, ensuring dependent tasks' priorities are increased accordingly to maintain correct scheduling order within the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_task_batcher", "line_number": 763, "body": "def add_task_batcher(self, worker, task_family, batched_args, max_batch_size=float('inf')):\n        self._state.set_batcher(worker, task_family, batched_args, max_batch_size)", "is_method": true, "class_name": "Scheduler", "function_description": "Adds or updates a batcher configuration for a specific worker and task family, enabling grouped task execution with controlled batch sizes for efficient scheduling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "forgive_failures", "line_number": 767, "body": "def forgive_failures(self, task_id=None):\n        status = PENDING\n        task = self._state.get_task(task_id)\n        if task is None:\n            return {\"task_id\": task_id, \"status\": None}\n\n        # we forgive only failures\n        if task.status == FAILED:\n            # forgive but do not forget\n            self._update_task_history(task, status)\n            self._state.set_status(task, status, self._config)\n        return {\"task_id\": task_id, \"status\": task.status}", "is_method": true, "class_name": "Scheduler", "function_description": "Method of Scheduler that resets a failed task's status to pending, allowing it to be retried while preserving its failure history. It facilitates controlled recovery from task failures within a scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "mark_as_done", "line_number": 781, "body": "def mark_as_done(self, task_id=None):\n        status = DONE\n        task = self._state.get_task(task_id)\n        if task is None:\n            return {\"task_id\": task_id, \"status\": None}\n\n        # we can force mark DONE for running or failed tasks\n        if task.status in {RUNNING, FAILED, DISABLED}:\n            self._update_task_history(task, status)\n            self._state.set_status(task, status, self._config)\n        return {\"task_id\": task_id, \"status\": task.status}", "is_method": true, "class_name": "Scheduler", "function_description": "Marks the specified task as done, updating its status regardless of whether it is running, failed, or disabled. This function helps manage and finalize task states within the Scheduler system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_task", "line_number": 794, "body": "def add_task(self, task_id=None, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', module=None, params=None, param_visibilities=None, accepts_messages=False,\n                 assistant=False, tracking_url=None, worker=None, batchable=None,\n                 batch_id=None, retry_policy_dict=None, owners=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        assert worker is not None\n        worker_id = worker\n        worker = self._update_worker(worker_id)\n\n        resources = {} if resources is None else resources.copy()\n\n        if retry_policy_dict is None:\n            retry_policy_dict = {}\n\n        retry_policy = self._generate_retry_policy(retry_policy_dict)\n\n        if worker.enabled:\n            _default_task = self._make_task(\n                task_id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, module=module, params=params, param_visibilities=param_visibilities,\n            )\n        else:\n            _default_task = None\n\n        task = self._state.get_task(task_id, setdefault=_default_task)\n\n        if task is None or (task.status != RUNNING and not worker.enabled):\n            return\n\n        # Ignore claims that the task is PENDING if it very recently was marked as DONE.\n        if status == PENDING and task.status == DONE and (time.time() - task.updated) < self._config.stable_done_cooldown_secs:\n            return\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not getattr(task, 'module', None):\n            task.module = module\n        if not getattr(task, 'param_visibilities', None):\n            task.param_visibilities = _get_default(param_visibilities, {})\n        if not task.params:\n            task.set_params(params)\n\n        if batch_id is not None:\n            task.batch_id = batch_id\n        if status == RUNNING and not task.worker_running:\n            task.worker_running = worker_id\n            if batch_id:\n                # copy resources_running of the first batch task\n                batch_tasks = self._state.get_batch_running_tasks(batch_id)\n                task.resources_running = batch_tasks[0].resources_running.copy()\n            task.time_running = time.time()\n\n        if accepts_messages is not None:\n            task.accepts_messages = accepts_messages\n\n        if tracking_url is not None or task.status != RUNNING:\n            task.tracking_url = tracking_url\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.tracking_url = tracking_url\n\n        if batchable is not None:\n            task.batchable = batchable\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if expl is not None:\n            task.expl = expl\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.expl = expl\n\n        task_is_not_running = task.status not in (RUNNING, BATCH_RUNNING)\n        task_started_a_run = status in (DONE, FAILED, RUNNING)\n        running_on_this_worker = task.worker_running == worker_id\n        if task_is_not_running or (task_started_a_run and running_on_this_worker) or new_deps:\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it\n            if status != task.status or status == PENDING:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n\n        if status == FAILED and self._config.batch_emails:\n            batched_params, _ = self._state.get_batcher(worker_id, family)\n            if batched_params:\n                unbatched_params = {\n                    param: value\n                    for param, value in task.params.items()\n                    if param not in batched_params\n                }\n            else:\n                unbatched_params = task.params\n            try:\n                expl_raw = json.loads(expl)\n            except ValueError:\n                expl_raw = expl\n\n            self._email_batcher.add_failure(\n                task.pretty_id, task.family, unbatched_params, expl_raw, owners)\n            if task.status == DISABLED:\n                self._email_batcher.add_disable(\n                    task.pretty_id, task.family, unbatched_params, owners)\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        if resources is not None:\n            task.resources = resources\n\n        if worker.enabled and not assistant:\n            task.stakeholders.add(worker_id)\n\n            # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n            # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n            for dep in task.deps or []:\n                t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n                t.stakeholders.add(worker_id)\n\n        self._update_priority(task, priority, worker_id)\n\n        # Because some tasks (non-dynamic dependencies) are `_make_task`ed\n        # before we know their retry_policy, we always set it here\n        task.retry_policy = retry_policy\n\n        if runnable and status != FAILED and worker.enabled:\n            task.workers.add(worker_id)\n            self._state.get_worker(worker_id).tasks.add(task)\n            task.runnable = runnable", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that adds or updates a task with specified attributes, manages dependencies, priority, status, and worker assignments, enabling coordinated task scheduling and execution control within a distributed system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "announce_scheduling_failure", "line_number": 938, "body": "def announce_scheduling_failure(self, task_name, family, params, expl, owners, **kwargs):\n        if not self._config.batch_emails:\n            return\n        worker_id = kwargs['worker']\n        batched_params, _ = self._state.get_batcher(worker_id, family)\n        if batched_params:\n            unbatched_params = {\n                param: value\n                for param, value in params.items()\n                if param not in batched_params\n            }\n        else:\n            unbatched_params = params\n        self._email_batcher.add_scheduling_fail(task_name, family, unbatched_params, expl, owners)", "is_method": true, "class_name": "Scheduler", "function_description": "Service method of the Scheduler class that processes and batches scheduling failure notifications, forwarding unbatched parameters to an email handler to alert task owners about scheduling issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_worker", "line_number": 954, "body": "def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)", "is_method": true, "class_name": "Scheduler", "function_description": "Adds informational details to a specified worker within the Scheduler's managed state, supporting enhanced tracking or metadata attachment for that worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "disable_worker", "line_number": 958, "body": "def disable_worker(self, worker):\n        self._state.disable_workers({worker})", "is_method": true, "class_name": "Scheduler", "function_description": "Disables a specified worker within the Scheduler, preventing it from receiving or executing tasks. This supports managing worker availability and load distribution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_worker_processes", "line_number": 962, "body": "def set_worker_processes(self, worker, n):\n        self._state.get_worker(worker).add_rpc_message('set_worker_processes', n=n)", "is_method": true, "class_name": "Scheduler", "function_description": "Sets the number of processes for a specified worker in the Scheduler, enabling dynamic control over worker parallelism for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "send_scheduler_message", "line_number": 966, "body": "def send_scheduler_message(self, worker, task, content):\n        if not self._config.send_messages:\n            return {\"message_id\": None}\n\n        message_id = str(uuid.uuid4())\n        self._state.get_worker(worker).add_rpc_message('dispatch_scheduler_message', task_id=task,\n                                                       message_id=message_id, content=content)\n\n        return {\"message_id\": message_id}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a way to send messages from the scheduler to a specified worker about a task, returning the unique message ID if messaging is enabled in the configuration. Useful for dispatching task-related instructions or updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_scheduler_message_response", "line_number": 977, "body": "def add_scheduler_message_response(self, task_id, message_id, response):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.scheduler_message_responses[message_id] = response", "is_method": true, "class_name": "Scheduler", "function_description": "Stores a scheduler's response message linked to a specific task, enabling tracking of task-related communications within the Scheduler class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_scheduler_message_response", "line_number": 983, "body": "def get_scheduler_message_response(self, task_id, message_id):\n        response = None\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            response = task.scheduler_message_responses.pop(message_id, None)\n        return {\"response\": response}", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that retrieves and removes a specific scheduler message response for a given task, facilitating task-related message tracking and management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_task_history", "line_number": 991, "body": "def has_task_history(self):\n        return self._config.record_task_history", "is_method": true, "class_name": "Scheduler", "function_description": "Returns whether task execution history recording is enabled in the scheduler's configuration, allowing other functions to conditionally track or skip task history logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_pause_enabled", "line_number": 995, "body": "def is_pause_enabled(self):\n        return {'enabled': self._config.pause_enabled}", "is_method": true, "class_name": "Scheduler", "function_description": "Returns the current status of the pause feature, indicating whether it is enabled or disabled. This function allows other components to check if scheduling pauses are active."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_paused", "line_number": 999, "body": "def is_paused(self):\n        return {'paused': self._paused}", "is_method": true, "class_name": "Scheduler", "function_description": "Returns the current paused state of the Scheduler, indicating whether scheduled tasks are temporarily halted. This allows other components to check if the scheduler is active or paused."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pause", "line_number": 1003, "body": "def pause(self):\n        if self._config.pause_enabled:\n            self._paused = True", "is_method": true, "class_name": "Scheduler", "function_description": "Sets the Scheduler to a paused state if pausing is enabled in its configuration, temporarily halting its scheduled operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "unpause", "line_number": 1008, "body": "def unpause(self):\n        if self._config.pause_enabled:\n            self._paused = False", "is_method": true, "class_name": "Scheduler", "function_description": "Utility method in the Scheduler class that resumes operation by unpausing the scheduler if the pause feature is enabled. It allows scheduled tasks to continue after being paused."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_resources", "line_number": 1013, "body": "def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)", "is_method": true, "class_name": "Scheduler", "function_description": "Updates the Scheduler's resource settings by adding or modifying resource entries, allowing dynamic adjustment of resource availability for scheduling tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_resource", "line_number": 1019, "body": "def update_resource(self, resource, amount):\n        if not isinstance(amount, int) or amount < 0:\n            return False\n        self._resources[resource] = amount\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Updates the quantity of a specified resource with a non-negative integer value, ensuring resource allocation accuracy within the Scheduler. Useful for managing and tracking resource availability or limits."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_generate_retry_policy", "line_number": 1025, "body": "def _generate_retry_policy(self, task_retry_policy_dict):\n        retry_policy_dict = self._config._get_retry_policy()._asdict()\n        retry_policy_dict.update({k: v for k, v in task_retry_policy_dict.items() if v is not None})\n        return RetryPolicy(**retry_policy_dict)", "is_method": true, "class_name": "Scheduler", "function_description": "Constructs a combined retry policy by merging default scheduler settings with task-specific overrides, providing customized retry behavior for scheduled tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_has_resources", "line_number": 1030, "body": "def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Utility method within Scheduler that checks if the required resources are available given current usage, enabling resource-aware scheduling decisions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_used_resources", "line_number": 1040, "body": "def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                resources_running = getattr(task, \"resources_running\", task.resources)\n                if resources_running:\n                    for resource, amount in resources_running.items():\n                        used_resources[resource] += amount\n        return used_resources", "is_method": true, "class_name": "Scheduler", "function_description": "Core utility method of the Scheduler class that calculates the total amount of resources currently consumed by running tasks, supporting resource management and allocation tracking in task scheduling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_rank", "line_number": 1050, "body": "def _rank(self, task):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n\n        return task.priority, -task.time", "is_method": true, "class_name": "Scheduler", "function_description": "Internal utility of the Scheduler class that generates a ranking key based on a task's priority and negative time, facilitating prioritized task scheduling and ordering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_schedulable", "line_number": 1059, "body": "def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Checks if a task is ready to be scheduled by verifying it is pending and all its dependencies have completed. This supports task management workflows by ensuring proper execution order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_reset_orphaned_batch_running_tasks", "line_number": 1068, "body": "def _reset_orphaned_batch_running_tasks(self, worker_id):\n        running_batch_ids = {\n            task.batch_id\n            for task in self._state.get_active_tasks_by_status(RUNNING)\n            if task.worker_running == worker_id\n        }\n        orphaned_tasks = [\n            task for task in self._state.get_active_tasks_by_status(BATCH_RUNNING)\n            if task.worker_running == worker_id and task.batch_id not in running_batch_ids\n        ]\n        for task in orphaned_tasks:\n            self._state.set_status(task, PENDING)", "is_method": true, "class_name": "Scheduler", "function_description": "Internal Scheduler method that resets batch-running tasks assigned to a specific worker but lacking active running status, marking them as pending to recover from orphaned task states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "count_pending", "line_number": 1082, "body": "def count_pending(self, worker):\n        worker_id, worker = worker, self._state.get_worker(worker)\n\n        num_pending, num_unique_pending, num_pending_last_scheduled = 0, 0, 0\n        running_tasks = []\n\n        upstream_status_table = {}\n        for task in worker.get_tasks(self._state, RUNNING):\n            if self._upstream_status(task.id, upstream_status_table) == UPSTREAM_DISABLED:\n                continue\n            # Return a list of currently running tasks to the client,\n            # makes it easier to troubleshoot\n            other_worker = self._state.get_worker(task.worker_running)\n            if other_worker is not None:\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n        for task in worker.get_tasks(self._state, PENDING, FAILED):\n            if self._upstream_status(task.id, upstream_status_table) == UPSTREAM_DISABLED:\n                continue\n            num_pending += 1\n            num_unique_pending += int(len(task.workers) == 1)\n            num_pending_last_scheduled += int(task.workers.peek(last=True) == worker_id)\n\n        return {\n            'n_pending_tasks': num_pending,\n            'n_unique_pending': num_unique_pending,\n            'n_pending_last_scheduled': num_pending_last_scheduled,\n            'worker_state': worker.state,\n            'running_tasks': running_tasks,\n        }", "is_method": true, "class_name": "Scheduler", "function_description": "Returns detailed counts and statuses of pending and running tasks for a given worker, aiding in task load monitoring and troubleshooting within the Scheduler. It helps track how many tasks are queued, uniquely assigned, or last scheduled on that worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_work", "line_number": 1116, "body": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        if self._config.prune_on_get_work:\n            self.prune()\n\n        assert worker is not None\n        worker_id = worker\n        worker = self._update_worker(\n            worker_id,\n            worker_reference={'host': host},\n            get_work=True)\n        if not worker.enabled:\n            reply = {'n_pending_tasks': 0,\n                     'running_tasks': [],\n                     'task_id': None,\n                     'n_unique_pending': 0,\n                     'worker_state': worker.state,\n                     }\n            return reply\n\n        if assistant:\n            self.add_worker(worker_id, [('assistant', assistant)])\n\n        batched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\n        best_task = None\n        if current_tasks is not None:\n            ct_set = set(current_tasks)\n            for task in sorted(self._state.get_active_tasks_by_status(RUNNING), key=self._rank):\n                if task.worker_running == worker_id and task.id not in ct_set:\n                    best_task = task\n\n        if current_tasks is not None:\n            # batch running tasks that weren't claimed since the last get_work go back in the pool\n            self._reset_orphaned_batch_running_tasks(worker_id)\n\n        greedy_resources = collections.defaultdict(int)\n\n        worker = self._state.get_worker(worker_id)\n        if self._paused:\n            relevant_tasks = []\n        elif worker.is_trivial_worker(self._state):\n            relevant_tasks = worker.get_tasks(self._state, PENDING, RUNNING)\n            used_resources = collections.defaultdict(int)\n            greedy_workers = dict()  # If there's no resources, then they can grab any task\n        else:\n            relevant_tasks = self._state.get_active_tasks_by_status(PENDING, RUNNING)\n            used_resources = self._used_resources()\n            activity_limit = time.time() - self._config.worker_disconnect_delay\n            active_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)\n            greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                                  for worker in active_workers)\n        tasks = list(relevant_tasks)\n        tasks.sort(key=self._rank, reverse=True)\n\n        for task in tasks:\n            if (best_task and batched_params and task.family == best_task.family and\n                    len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    task.resources == best_task.resources and self._schedulable(task)):\n                for name, params in batched_params.items():\n                    params.append(task.params.get(name))\n                batched_tasks.append(task)\n            if best_task:\n                continue\n\n            if task.status == RUNNING and (task.worker_running in greedy_workers):\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (getattr(task, 'resources_running', task.resources) or {}).items():\n                    greedy_resources[resource] += amount\n\n            if self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                in_workers = (assistant and task.runnable) or worker_id in task.workers\n                if in_workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    batch_param_names, max_batch_size = self._state.get_batcher(\n                        worker_id, task.family)\n                    if batch_param_names and task.is_batchable():\n                        try:\n                            batched_params = {\n                                name: [task.params[name]] for name in batch_param_names\n                            }\n                            unbatched_params = {\n                                name: value for name, value in task.params.items()\n                                if name not in batched_params\n                            }\n                            batched_tasks.append(task)\n                        except KeyError:\n                            batched_params, unbatched_params = None, None\n                else:\n                    workers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers\n                    for task_worker in workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        reply = self.count_pending(worker_id)\n\n        if len(batched_tasks) > 1:\n            batch_string = '|'.join(task.id for task in batched_tasks)\n            batch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()\n            for task in batched_tasks:\n                self._state.set_batch_running(task, batch_id, worker_id)\n\n            combined_params = best_task.params.copy()\n            combined_params.update(batched_params)\n\n            reply['task_id'] = None\n            reply['task_family'] = best_task.family\n            reply['task_module'] = getattr(best_task, 'module', None)\n            reply['task_params'] = combined_params\n            reply['batch_id'] = batch_id\n            reply['batch_task_ids'] = [task.id for task in batched_tasks]\n\n        elif best_task:\n            self.update_metrics_task_started(best_task)\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker_id\n            best_task.resources_running = best_task.resources.copy()\n            best_task.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n            reply['task_id'] = best_task.id\n            reply['task_family'] = best_task.family\n            reply['task_module'] = getattr(best_task, 'module', None)\n            reply['task_params'] = best_task.params\n\n        else:\n            reply['task_id'] = None\n\n        return reply", "is_method": true, "class_name": "Scheduler", "function_description": "Core scheduling method of the Scheduler class that assigns the highest-priority available task or batch of tasks to a worker, considering resource constraints and task dependencies, enabling efficient workload distribution and resource utilization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "ping", "line_number": 1266, "body": "def ping(self, **kwargs):\n        worker_id = kwargs['worker']\n        worker = self._update_worker(worker_id)\n        return {\"rpc_messages\": worker.fetch_rpc_messages()}", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that updates a worker's state and retrieves its pending RPC messages, enabling communication management with individual workers in a distributed system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_upstream_status", "line_number": 1271, "body": "def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                dep = self._state.get_task(dep_id)\n                if dep:\n                    if dep.status == DONE:\n                        continue\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack += [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        status = max((upstream_status_table.get(a_task_id, '')\n                                      for a_task_id in dep.deps),\n                                     key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]", "is_method": true, "class_name": "Scheduler", "function_description": "Core internal method of the Scheduler class that determines and caches the aggregated status of a task by recursively evaluating its dependencies' statuses to reflect upstream task conditions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_serialize_task", "line_number": 1299, "body": "def _serialize_task(self, task_id, include_deps=True, deps=None):\n        task = self._state.get_task(task_id)\n\n        ret = {\n            'display_name': task.pretty_id,\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'last_updated': getattr(task, \"updated\", task.time),\n            'params': task.public_params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n            'resources_running': getattr(task, \"resources_running\", None),\n            'tracking_url': getattr(task, \"tracking_url\", None),\n            'status_message': getattr(task, \"status_message\", None),\n            'progress_percentage': getattr(task, \"progress_percentage\", None),\n        }\n        if task.status == DISABLED:\n            ret['re_enable_able'] = task.scheduler_disable_time is not None\n        if include_deps:\n            ret['deps'] = list(task.deps if deps is None else deps)\n        if self._config.send_messages and task.status == RUNNING:\n            ret['accepts_messages'] = task.accepts_messages\n        return ret", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a detailed dictionary representation of a scheduled task\u2019s state and metadata, optionally including dependencies, for monitoring or serialization purposes within the Scheduler class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "graph", "line_number": 1328, "body": "def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        seen = set()\n        for task in self._state.get_active_tasks():\n            serialized.update(self._traverse_graph(task.id, seen))\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "Core method of the Scheduler class that constructs and returns a serialized representation of the current active task graph after pruning inactive tasks, enabling inspection or manipulation of task dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_filter_done", "line_number": 1336, "body": "def _filter_done(self, task_ids):\n        for task_id in task_ids:\n            task = self._state.get_task(task_id)\n            if task is None or task.status != DONE:\n                yield task_id", "is_method": true, "class_name": "Scheduler", "function_description": "Internal utility method of the Scheduler class that filters out completed tasks from a list, yielding only task IDs whose corresponding tasks are pending or unknown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_traverse_graph", "line_number": 1342, "body": "def _traverse_graph(self, root_task_id, seen=None, dep_func=None, include_done=True):\n        \"\"\" Returns the dependency graph rooted at task_id\n\n        This does a breadth-first traversal to find the nodes closest to the\n        root before hitting the scheduler.max_graph_nodes limit.\n\n        :param root_task_id: the id of the graph's root\n        :return: A map of task id to serialized node\n        \"\"\"\n\n        if seen is None:\n            seen = set()\n        elif root_task_id in seen:\n            return {}\n\n        if dep_func is None:\n            def dep_func(t):\n                return t.deps\n\n        seen.add(root_task_id)\n        serialized = {}\n        queue = collections.deque([root_task_id])\n        while queue:\n            task_id = queue.popleft()\n\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.debug('Missing task for id [%s]', task_id)\n\n                # NOTE : If a dependency is missing from self._state there is no way to deduce the\n                #        task family and parameters.\n                family_match = TASK_FAMILY_RE.match(task_id)\n                family = family_match.group(1) if family_match else UNKNOWN\n                params = {'task_id': task_id}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'display_name': task_id,\n                    'priority': 0,\n                }\n            else:\n                deps = dep_func(task)\n                if not include_done:\n                    deps = list(self._filter_done(deps))\n                serialized[task_id] = self._serialize_task(task_id, deps=deps)\n                for dep in sorted(deps):\n                    if dep not in seen:\n                        seen.add(dep)\n                        queue.append(dep)\n\n            if task_id != root_task_id:\n                del serialized[task_id]['display_name']\n            if len(serialized) >= self._config.max_graph_nodes:\n                break\n\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "Core method of the Scheduler class that performs a breadth-first traversal to collect and serialize tasks in a dependency graph from a given root, optionally filtering completed tasks and limiting graph size. It supports task status inspection and visualization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dep_graph", "line_number": 1404, "body": "def dep_graph(self, task_id, include_done=True, **kwargs):\n        self.prune()\n        if not self._state.has_task(task_id):\n            return {}\n        return self._traverse_graph(task_id, include_done=include_done)", "is_method": true, "class_name": "Scheduler", "function_description": "Returns the dependency graph for a given task, optionally including completed tasks, enabling analysis of task relationships and execution order within the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inverse_dep_graph", "line_number": 1411, "body": "def inverse_dep_graph(self, task_id, include_done=True, **kwargs):\n        self.prune()\n        if not self._state.has_task(task_id):\n            return {}\n        inverse_graph = collections.defaultdict(set)\n        for task in self._state.get_active_tasks():\n            for dep in task.deps:\n                inverse_graph[dep].add(task.id)\n        return self._traverse_graph(\n            task_id, dep_func=lambda t: inverse_graph[t.id], include_done=include_done)", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the inverse dependency graph for a specified task, showing which active tasks depend on it. Useful for analyzing task dependencies and understanding the impact of task completion or failure in scheduling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_list", "line_number": 1423, "body": "def task_list(self, status='', upstream_status='', limit=True, search=None, max_shown_tasks=None,\n                  **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        if not search:\n            count_limit = max_shown_tasks or self._config.max_shown_tasks\n            pre_count = self._state.get_active_task_count_for_status(status)\n            if limit and pre_count > count_limit:\n                return {'num_tasks': -1 if upstream_status else pre_count}\n        self.prune()\n\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        if search is None:\n            def filter_func(_):\n                return True\n        else:\n            terms = search.split()\n\n            def filter_func(t):\n                return all(term in t.pretty_id for term in terms)\n\n        tasks = self._state.get_active_tasks_by_status(status) if status else self._state.get_active_tasks()\n        for task in filter(filter_func, tasks):\n            if task.status != PENDING or not upstream_status or upstream_status == self._upstream_status(task.id, upstream_status_table):\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.id] = serialized\n        if limit and len(result) > (max_shown_tasks or self._config.max_shown_tasks):\n            return {'num_tasks': len(result)}\n        return result", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a filtered and optionally limited list of tasks based on their status, upstream status, and search terms, supporting task monitoring and management within the Scheduler system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_first_task_display_name", "line_number": 1455, "body": "def _first_task_display_name(self, worker):\n        task_id = worker.info.get('first_task', '')\n        if self._state.has_task(task_id):\n            return self._state.get_task(task_id).pretty_id\n        else:\n            return task_id", "is_method": true, "class_name": "Scheduler", "function_description": "Helper method in Scheduler that returns the display name of a worker's first assigned task, falling back to the raw task ID if the task details are unavailable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "worker_list", "line_number": 1463, "body": "def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=worker.started,\n                state=worker.state,\n                first_task_display_name=self._first_task_display_name(worker),\n                num_unread_rpc_messages=len(worker.rpc_messages),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                if task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, include_deps=False)\n\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_active_tasks_by_status(PENDING):\n                for worker in task.workers:\n                    num_pending[worker] += 1\n                if len(task.workers) == 1:\n                    num_uniques[list(task.workers)[0]] += 1\n\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a detailed snapshot of active workers, including their status, tasks running or pending, and task counts, supporting monitoring and management of ongoing scheduler operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "resource_list", "line_number": 1499, "body": "def resource_list(self):\n        \"\"\"\n        Resources usage info and their consumers (tasks).\n        \"\"\"\n        self.prune()\n        resources = [\n            dict(\n                name=resource,\n                num_total=r_dict['total'],\n                num_used=r_dict['used']\n            ) for resource, r_dict in self.resources().items()]\n        if self._resources is not None:\n            consumers = collections.defaultdict(dict)\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        consumers[resource][task.id] = self._serialize_task(task.id, include_deps=False)\n            for resource in resources:\n                tasks = consumers[resource['name']]\n                resource['num_consumer'] = len(tasks)\n                resource['running'] = tasks\n        return resources", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a detailed summary of system resources including total and used counts, alongside tasks currently consuming those resources. This enables monitoring of resource allocation and task-resource relationships within the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "resources", "line_number": 1522, "body": "def resources(self):\n        ''' get total resources and available ones '''\n        used_resources = self._used_resources()\n        ret = collections.defaultdict(dict)\n        for resource, total in self._resources.items():\n            ret[resource]['total'] = total\n            if resource in used_resources:\n                ret[resource]['used'] = used_resources[resource]\n            else:\n                ret[resource]['used'] = 0\n        return ret", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a summary of total and currently used resources managed by the Scheduler, allowing clients to monitor resource availability and utilization for scheduling decisions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_search", "line_number": 1535, "body": "def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.status][task.id] = serialized\n        return result", "is_method": true, "class_name": "Scheduler", "function_description": "Core method of the Scheduler class that searches and returns active tasks matching a given substring in their IDs, organized by their status for easy filtering and retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "re_enable_task", "line_number": 1551, "body": "def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "Enables a previously disabled task in the scheduler and returns its updated serialized representation. This method supports task management by allowing reactivation of scheduled tasks based on their ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fetch_error", "line_number": 1560, "body": "def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"error\": task.expl, 'displayName': task.pretty_id}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}", "is_method": true, "class_name": "Scheduler", "function_description": "Retrieves the error details and display name of a specific task if it exists in the scheduler's state, providing diagnostic information for task execution issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_task_status_message", "line_number": 1568, "body": "def set_task_status_message(self, task_id, status_message):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.status_message = status_message\n            if task.status == RUNNING and task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.status_message = status_message", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that updates the status message of a specific task and propagates it to all running tasks within the same batch, providing synchronized status reporting across related tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task_status_message", "line_number": 1577, "body": "def get_task_status_message(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"statusMessage\": task.status_message}\n        else:\n            return {\"taskId\": task_id, \"statusMessage\": \"\"}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the current status message for a given task ID in the Scheduler, facilitating task monitoring and status reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_task_progress_percentage", "line_number": 1585, "body": "def set_task_progress_percentage(self, task_id, progress_percentage):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.progress_percentage = progress_percentage\n            if task.status == RUNNING and task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.progress_percentage = progress_percentage", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that updates a task's progress percentage and synchronizes this progress across all running tasks in the same batch, facilitating consistent progress tracking in batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task_progress_percentage", "line_number": 1594, "body": "def get_task_progress_percentage(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"progressPercentage\": task.progress_percentage}\n        else:\n            return {\"taskId\": task_id, \"progressPercentage\": None}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the current progress percentage of a specified task within the Scheduler, supporting task monitoring and status tracking. Returns None if the task does not exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "decrease_running_task_resources", "line_number": 1602, "body": "def decrease_running_task_resources(self, task_id, decrease_resources):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            if task.status != RUNNING:\n                return\n\n            def decrease(resources, decrease_resources):\n                for resource, decrease_amount in decrease_resources.items():\n                    if decrease_amount > 0 and resource in resources:\n                        resources[resource] = max(0, resources[resource] - decrease_amount)\n\n            decrease(task.resources_running, decrease_resources)\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    decrease(batch_task.resources_running, decrease_resources)", "is_method": true, "class_name": "Scheduler", "function_description": "Method of the Scheduler class that reduces the resource allocations of a running task and its associated batch tasks, enabling dynamic adjustment of resource usage during execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_running_task_resources", "line_number": 1619, "body": "def get_running_task_resources(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"resources\": getattr(task, \"resources_running\", None)}\n        else:\n            return {\"taskId\": task_id, \"resources\": None}", "is_method": true, "class_name": "Scheduler", "function_description": "Utility method in Scheduler that returns the currently allocated resources for a given running task by its ID, or None if the task is not active."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_task_history", "line_number": 1626, "body": "def _update_task_history(self, task, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task)\n            elif status == RUNNING:\n                self._task_history.task_started(task, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)", "is_method": true, "class_name": "Scheduler", "function_description": "Internal method of the Scheduler class that updates the task history based on its current status, recording events like scheduling, starting, completion, or failure to track task lifecycle progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_history", "line_number": 1639, "body": "def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history", "is_method": true, "class_name": "Scheduler", "function_description": "Returns the history of scheduled tasks tracked by the Scheduler. This enables other components to access past task information for monitoring or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_metrics_task_started", "line_number": 1644, "body": "def update_metrics_task_started(self, task):\n        self._state._metrics_collector.handle_task_started(task)", "is_method": true, "class_name": "Scheduler", "function_description": "Utility method of the Scheduler class that notifies the metrics collector when a task begins, enabling tracking of task start events for monitoring and analysis purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "filter_func", "line_number": 1438, "body": "def filter_func(_):\n                return True", "is_method": true, "class_name": "Scheduler", "function_description": "Trivial filter function within the Scheduler class that unconditionally returns True, effectively allowing all inputs to pass through without any filtering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "filter_func", "line_number": 1443, "body": "def filter_func(t):\n                return all(term in t.pretty_id for term in terms)", "is_method": true, "class_name": "Scheduler", "function_description": "Filters a task based on whether all specified terms appear in its identifier, supporting selective task selection in scheduling operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "decrease", "line_number": 1608, "body": "def decrease(resources, decrease_resources):\n                for resource, decrease_amount in decrease_resources.items():\n                    if decrease_amount > 0 and resource in resources:\n                        resources[resource] = max(0, resources[resource] - decrease_amount)", "is_method": true, "class_name": "Scheduler", "function_description": "Core utility of the Scheduler class that decreases specified resource quantities without allowing negative values, enabling safe resource reduction management in scheduling or allocation processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_section", "line_number": 35, "body": "def _section(cls, opts):\n        \"\"\"Get logging settings from config file section \"logging\".\"\"\"\n        if isinstance(cls.config, LuigiConfigParser):\n            return False\n        try:\n            logging_config = cls.config['logging']\n        except (TypeError, KeyError, NoSectionError):\n            return False\n        logging.config.dictConfig(logging_config)\n        return True", "is_method": true, "class_name": "BaseLogging", "function_description": "Utility method in BaseLogging that attempts to load and apply logging configurations from the \"logging\" section of a config file, enabling dynamic setup of logging behavior based on external settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "setup", "line_number": 47, "body": "def setup(cls,\n              opts=type('opts', (), {\n                  'background': None,\n                  'logdir': None,\n                  'logging_conf_file': None,\n                  'log_level': 'DEBUG'\n              })):\n        \"\"\"Setup logging via CLI params and config.\"\"\"\n        logger = logging.getLogger('luigi')\n\n        if cls._configured:\n            logger.info('logging already configured')\n            return False\n        cls._configured = True\n\n        if cls.config.getboolean('core', 'no_configure_logging', False):\n            logger.info('logging disabled in settings')\n            return False\n\n        configured = cls._cli(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via special settings')\n            return True\n\n        configured = cls._conf(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via *.conf file')\n            return True\n\n        configured = cls._section(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via config section')\n            return True\n\n        configured = cls._default(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured by default settings')\n        return configured", "is_method": true, "class_name": "BaseLogging", "function_description": "Class BaseLogging's setup method configures logging using command-line options, configuration files, or default settings. It ensures logging is set up once and supports multiple configuration sources for flexible initialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_cli", "line_number": 98, "body": "def _cli(cls, opts):\n        \"\"\"Setup logging via CLI options\n\n        If `--background` -- set INFO level for root logger.\n        If `--logdir` -- set logging with next params:\n            default Luigi's formatter,\n            INFO level,\n            output in logdir in `luigi-server.log` file\n        \"\"\"\n        if opts.background:\n            logging.getLogger().setLevel(logging.INFO)\n            return True\n\n        if opts.logdir:\n            logging.basicConfig(\n                level=logging.INFO,\n                format=cls._log_format,\n                filename=os.path.join(opts.logdir, \"luigi-server.log\"))\n            return True\n\n        return False", "is_method": true, "class_name": "DaemonLogging", "function_description": "Configures logging settings based on command-line options, enabling background info-level logging or file logging to a specified directory for daemon processes. Useful for flexible and context-aware logging setup in CLI-driven applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_conf", "line_number": 121, "body": "def _conf(cls, opts):\n        \"\"\"Setup logging via ini-file from logging_conf_file option.\"\"\"\n        logging_conf = cls.config.get('core', 'logging_conf_file', None)\n        if logging_conf is None:\n            return False\n\n        if not os.path.exists(logging_conf):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(logging_conf)\n        return True", "is_method": true, "class_name": "DaemonLogging", "function_description": "Sets up logging configuration from a specified ini-file, enabling customized logging behavior for the DaemonLogging class. It validates file existence before applying settings to ensure proper logging initialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_default", "line_number": 136, "body": "def _default(cls, opts):\n        \"\"\"Setup default logger\"\"\"\n        logging.basicConfig(level=logging.INFO, format=cls._log_format)\n        return True", "is_method": true, "class_name": "DaemonLogging", "function_description": "Provides a default logging setup with INFO level and a predefined format for the DaemonLogging class, ensuring consistent logger configuration across the application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_conf", "line_number": 152, "body": "def _conf(cls, opts):\n        \"\"\"Setup logging via ini-file from logging_conf_file option.\"\"\"\n        if not opts.logging_conf_file:\n            return False\n\n        if not os.path.exists(opts.logging_conf_file):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(opts.logging_conf_file, disable_existing_loggers=False)\n        return True", "is_method": true, "class_name": "InterfaceLogging", "function_description": "Initializes logging based on a configuration file path provided in options, enabling flexible logging setup or returning an error if the file is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_default", "line_number": 166, "body": "def _default(cls, opts):\n        \"\"\"Setup default logger\"\"\"\n        level = getattr(logging, opts.log_level, logging.DEBUG)\n\n        logger = logging.getLogger('luigi-interface')\n        logger.setLevel(level)\n\n        stream_handler = logging.StreamHandler()\n        stream_handler.setLevel(level)\n\n        formatter = logging.Formatter('%(levelname)s: %(message)s')\n        stream_handler.setFormatter(formatter)\n\n        logger.addHandler(stream_handler)\n        return True", "is_method": true, "class_name": "InterfaceLogging", "function_description": "Core utility of InterfaceLogging that configures a default logger with specified log level, message format, and stream output, facilitating standardized logging for the 'luigi-interface' component."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "common_params", "line_number": 230, "body": "def common_params(task_instance, task_cls):\n    \"\"\"\n    Grab all the values in task_instance that are found in task_cls.\n    \"\"\"\n    if not isinstance(task_cls, task.Register):\n        raise TypeError(\"task_cls must be an uninstantiated Task\")\n\n    task_instance_param_names = dict(task_instance.get_params()).keys()\n    task_cls_params_dict = dict(task_cls.get_params())\n    task_cls_param_names = task_cls_params_dict.keys()\n    common_param_names = set(task_instance_param_names).intersection(set(task_cls_param_names))\n    common_param_vals = [(key, task_cls_params_dict[key]) for key in common_param_names]\n    common_kwargs = dict((key, task_instance.param_kwargs[key]) for key in common_param_names)\n    vals = dict(task_instance.get_param_values(common_param_vals, [], common_kwargs))\n    return vals", "is_method": false, "function_description": "Utility function that extracts the parameter values from a task instance matching the parameters defined in a given uninstantiated task class, facilitating parameter comparison or synchronization between task objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "delegates", "line_number": 376, "body": "def delegates(task_that_delegates):\n    \"\"\" Lets a task call methods on subtask(s).\n\n    The way this works is that the subtask is run as a part of the task, but\n    the task itself doesn't have to care about the requirements of the subtasks.\n    The subtask doesn't exist from the scheduler's point of view, and\n    its dependencies are instead required by the main task.\n\n    Example:\n\n    .. code-block:: python\n\n        class PowersOfN(luigi.Task):\n            n = luigi.IntParameter()\n            def f(self, x): return x ** self.n\n\n        @delegates\n        class T(luigi.Task):\n            def subtasks(self): return PowersOfN(5)\n            def run(self): print self.subtasks().f(42)\n    \"\"\"\n    if not hasattr(task_that_delegates, 'subtasks'):\n        # This method can (optionally) define a couple of delegate tasks that\n        # will be accessible as interfaces, meaning that the task can access\n        # those tasks and run methods defined on them, etc\n        raise AttributeError('%s needs to implement the method \"subtasks\"' % task_that_delegates)\n\n    @task._task_wraps(task_that_delegates)\n    class Wrapped(task_that_delegates):\n\n        def deps(self):\n            # Overrides method in base class\n            return task.flatten(self.requires()) + task.flatten([t.deps() for t in task.flatten(self.subtasks())])\n\n        def run(self):\n            for t in task.flatten(self.subtasks()):\n                t.run()\n            task_that_delegates.run(self)\n\n    return Wrapped", "is_method": false, "function_description": "Decorator that enables a task to transparently include and run subtasks as part of its execution, allowing method calls on subtasks without exposing them as separate scheduler tasks. Useful for simplifying task composition in workflow management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "previous", "line_number": 418, "body": "def previous(task):\n    \"\"\"\n    Return a previous Task of the same family.\n\n    By default checks if this task family only has one non-global parameter and if\n    it is a DateParameter, DateHourParameter or DateIntervalParameter in which case\n    it returns with the time decremented by 1 (hour, day or interval)\n    \"\"\"\n    params = task.get_params()\n    previous_params = {}\n    previous_date_params = {}\n\n    for param_name, param_obj in params:\n        param_value = getattr(task, param_name)\n\n        if isinstance(param_obj, parameter.DateParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(days=1)\n        elif isinstance(param_obj, parameter.DateSecondParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(seconds=1)\n        elif isinstance(param_obj, parameter.DateMinuteParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(minutes=1)\n        elif isinstance(param_obj, parameter.DateHourParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(hours=1)\n        elif isinstance(param_obj, parameter.DateIntervalParameter):\n            previous_date_params[param_name] = param_value.prev()\n        else:\n            previous_params[param_name] = param_value\n\n    previous_params.update(previous_date_params)\n\n    if len(previous_date_params) == 0:\n        raise NotImplementedError(\"No task parameter - can't determine previous task\")\n    elif len(previous_date_params) > 1:\n        raise NotImplementedError(\"Too many date-related task parameters - can't determine previous task\")\n    else:\n        return task.clone(**previous_params)", "is_method": false, "function_description": "Service method that returns the previous instance of a task by decrementing its single date-related parameter (day, hour, second, minute, or interval), enabling temporal traversal within a task family."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "get_previous_completed", "line_number": 456, "body": "def get_previous_completed(task, max_steps=10):\n    prev = task\n    for _ in range(max_steps):\n        prev = previous(prev)\n        logger.debug(\"Checking if %s is complete\", prev)\n        if prev.complete():\n            return prev\n    return None", "is_method": false, "function_description": "Function that iteratively searches backward up to a specified limit to find the most recent completed task prior to a given one, supporting workflow or task sequence management by identifying last successful steps."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 289, "body": "def __call__(self, task_that_inherits):\n        # Get all parameter objects from each of the underlying tasks\n        for task_to_inherit in self.tasks_to_inherit:\n            for param_name, param_obj in task_to_inherit.get_params():\n                # Check if the parameter exists in the inheriting task\n                if not hasattr(task_that_inherits, param_name):\n                    # If not, add it to the inheriting task\n                    setattr(task_that_inherits, param_name, param_obj)\n\n        # Modify task_that_inherits by adding methods\n        def clone_parent(_self, **kwargs):\n            return _self.clone(cls=self.tasks_to_inherit[0], **kwargs)\n        task_that_inherits.clone_parent = clone_parent\n\n        def clone_parents(_self, **kwargs):\n            return [\n                _self.clone(cls=task_to_inherit, **kwargs)\n                for task_to_inherit in self.tasks_to_inherit\n            ]\n        task_that_inherits.clone_parents = clone_parents\n\n        return task_that_inherits", "is_method": true, "class_name": "inherits", "function_description": "Adds parameters and cloning methods from specified tasks to another task, enabling it to inherit attributes and cloning capabilities dynamically from multiple sources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 328, "body": "def __call__(self, task_that_requires):\n        task_that_requires = inherits(*self.tasks_to_require)(task_that_requires)\n\n        # Modify task_that_requires by adding requires method.\n        # If only one task is required, this single task is returned.\n        # Otherwise, list of tasks is returned\n        def requires(_self):\n            return _self.clone_parent() if len(self.tasks_to_require) == 1 else _self.clone_parents()\n        task_that_requires.requires = requires\n\n        return task_that_requires", "is_method": true, "class_name": "requires", "function_description": "Provides a decorator-like utility that adds a `requires` method to a task, indicating its dependencies as either a single or multiple prerequisite tasks for workflow orchestration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 359, "body": "def __call__(self, task_that_copies):\n        task_that_copies = self.requires_decorator(task_that_copies)\n\n        # Modify task_that_copies by subclassing it and adding methods\n        @task._task_wraps(task_that_copies)\n        class Wrapped(task_that_copies):\n\n            def run(_self):\n                i, o = _self.input(), _self.output()\n                f = o.open('w')  # TODO: assert that i, o are Target objects and not complex datastructures\n                for line in i.open('r'):\n                    f.write(line)\n                f.close()\n\n        return Wrapped", "is_method": true, "class_name": "copies", "function_description": "Decorator method in the copies class that wraps a task to create a modified version copying input content to output line-by-line, enabling automatic data replication within task workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "clone_parent", "line_number": 299, "body": "def clone_parent(_self, **kwargs):\n            return _self.clone(cls=self.tasks_to_inherit[0], **kwargs)", "is_method": true, "class_name": "inherits", "function_description": "Returns a cloned instance of the first parent task specified for inheritance, passing along additional arguments. This method facilitates task duplication with inherited behaviors or configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "clone_parents", "line_number": 303, "body": "def clone_parents(_self, **kwargs):\n            return [\n                _self.clone(cls=task_to_inherit, **kwargs)\n                for task_to_inherit in self.tasks_to_inherit\n            ]", "is_method": true, "class_name": "inherits", "function_description": "Creates and returns clones of the parent tasks specified in self.tasks_to_inherit, optionally applying additional parameters. This facilitates inheriting and customizing behavior from multiple parent tasks within a task hierarchy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "requires", "line_number": 334, "body": "def requires(_self):\n            return _self.clone_parent() if len(self.tasks_to_require) == 1 else _self.clone_parents()", "is_method": true, "class_name": "requires", "function_description": "Returns a cloned single or multiple parent task(s) based on the count of required tasks, supporting dependency management within the requires class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "deps", "line_number": 406, "body": "def deps(self):\n            # Overrides method in base class\n            return task.flatten(self.requires()) + task.flatten([t.deps() for t in task.flatten(self.subtasks())])", "is_method": true, "class_name": "Wrapped", "function_description": "Returns a flattened list of all direct dependencies and the dependencies of its subtasks, providing a complete set of prerequisite tasks for the Wrapped task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "run", "line_number": 410, "body": "def run(self):\n            for t in task.flatten(self.subtasks()):\n                t.run()\n            task_that_delegates.run(self)", "is_method": true, "class_name": "Wrapped", "function_description": "Runs all subtasks sequentially, then executes the wrapped task's own run method, enabling hierarchical or delegated task execution within the Wrapped class context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "run", "line_number": 366, "body": "def run(_self):\n                i, o = _self.input(), _self.output()\n                f = o.open('w')  # TODO: assert that i, o are Target objects and not complex datastructures\n                for line in i.open('r'):\n                    f.write(line)\n                f.close()", "is_method": true, "class_name": "Wrapped", "function_description": "Copies data from an input target to an output target by reading and writing line-by-line. This function facilitates transferring content between sources within the Wrapped class context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "rename_dont_move", "line_number": 160, "body": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Potentially rename ``path`` to ``dest``, but don't move it into the\n        ``dest`` folder (if it is a folder).  This relates to :ref:`AtomicWrites`.\n\n        This method has a reasonable but not bullet proof default\n        implementation.  It will just do ``move()`` if the file doesn't\n        ``exists()`` already.\n        \"\"\"\n        warnings.warn(\"File system {} client doesn't support atomic mv.\".format(self.__class__.__name__))\n        if self.exists(dest):\n            raise FileAlreadyExists()\n        self.move(path, dest)", "is_method": true, "class_name": "FileSystem", "function_description": "Provides a safe rename operation that prevents moving a file or folder into a destination directory, raising an error if the destination exists, primarily supporting atomic write semantics in file handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "rename", "line_number": 174, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Alias for ``move()``\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "FileSystem", "function_description": "Alias method in the FileSystem class that renames a file or directory by internally calling the move operation. It simplifies interface by providing rename functionality through move semantics."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "fs", "line_number": 220, "body": "def fs(self):\n        \"\"\"\n        The :py:class:`FileSystem` associated with this FileSystemTarget.\n        \"\"\"\n        raise NotImplementedError()", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Returns the associated FileSystem instance for this FileSystemTarget. This method is intended to link a target with its managing filesystem but is not implemented here."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "exists", "line_number": 241, "body": "def exists(self):\n        \"\"\"\n        Returns ``True`` if the path for this FileSystemTarget exists; ``False`` otherwise.\n\n        This method is implemented by using :py:attr:`fs`.\n        \"\"\"\n        path = self.path\n        if '*' in path or '?' in path or '[' in path or '{' in path:\n            logger.warning(\"Using wildcards in path %s might lead to processing of an incomplete dataset; \"\n                           \"override exists() to suppress the warning.\", path)\n        return self.fs.exists(path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Checks whether the specified path in the file system exists, warning about wildcard usage that may lead to incomplete dataset processing. This method is useful for validating file or directory presence before further operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "remove", "line_number": 253, "body": "def remove(self):\n        \"\"\"\n        Remove the resource at the path specified by this FileSystemTarget.\n\n        This method is implemented by using :py:attr:`fs`.\n        \"\"\"\n        self.fs.remove(self.path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Removes the file or directory at the target path within the filesystem, providing a straightforward way to delete resources managed by FileSystemTarget."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "temporary_path", "line_number": 262, "body": "def temporary_path(self):\n        \"\"\"\n        A context manager that enables a reasonably short, general and\n        magic-less way to solve the :ref:`AtomicWrites`.\n\n         * On *entering*, it will create the parent directories so the\n           temporary_path is writeable right away.\n           This step uses :py:meth:`FileSystem.mkdir`.\n         * On *exiting*, it will move the temporary file if there was no exception thrown.\n           This step uses :py:meth:`FileSystem.rename_dont_move`\n\n        The file system operations will be carried out by calling them on :py:attr:`fs`.\n\n        The typical use case looks like this:\n\n        .. code:: python\n\n            class MyTask(luigi.Task):\n                def output(self):\n                    return MyFileSystemTarget(...)\n\n                def run(self):\n                    with self.output().temporary_path() as self.temp_output_path:\n                        run_some_external_command(output_path=self.temp_output_path)\n        \"\"\"\n        num = random.randrange(0, 1e10)\n        slashless_path = self.path.rstrip('/').rstrip(\"\\\\\")\n        _temp_path = '{}-luigi-tmp-{:010}{}'.format(\n            slashless_path,\n            num,\n            self._trailing_slash())\n        # TODO: os.path doesn't make sense here as it's os-dependent\n        tmp_dir = os.path.dirname(slashless_path)\n        if tmp_dir:\n            self.fs.mkdir(tmp_dir, parents=True, raise_if_exists=False)\n\n        yield _temp_path\n        # We won't reach here if there was an user exception.\n        self.fs.rename_dont_move(_temp_path, self.path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Provides a context manager that creates a temporary writable file path for atomic writes and safely moves it to the target path upon successful completion, ensuring reliable file output with directory creation and exception safety."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "_touchz", "line_number": 302, "body": "def _touchz(self):\n        with self.open('w'):\n            pass", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Creates or updates an empty file at the target location, effectively ensuring the file's existence without writing any content. This utility is useful for flagging or initializing files in the filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "_trailing_slash", "line_number": 306, "body": "def _trailing_slash(self):\n        # I suppose one day schema-like paths, like\n        # file:///path/blah.txt?params=etc can be parsed too\n        return self.path[-1] if self.path[-1] in r'\\/' else ''", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Private helper method in FileSystemTarget that returns the trailing slash or backslash character of the path if present; assists in path format handling within the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "close", "line_number": 326, "body": "def close(self):\n        super(AtomicLocalFile, self).close()\n        self.move_to_final_destination()", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Closes the file and atomically moves it to its final destination, ensuring safe and complete file writes within the AtomicLocalFile class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "generate_tmp_path", "line_number": 330, "body": "def generate_tmp_path(self, path):\n        return os.path.join(tempfile.gettempdir(), 'luigi-s3-tmp-%09d' % random.randrange(0, 1e10))", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Generates a unique temporary file path in the system's temp directory, used for safely handling intermediate file operations in the AtomicLocalFile context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "__del__", "line_number": 336, "body": "def __del__(self):\n        if os.path.exists(self.tmp_path):\n            os.remove(self.tmp_path)", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Destructor method of the AtomicLocalFile class that ensures temporary files are cleaned up by deleting them when the instance is destroyed, preventing leftover temporary data on the filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "tmp_path", "line_number": 341, "body": "def tmp_path(self):\n        return self.__tmp_path", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Returns the temporary file path stored within the AtomicLocalFile instance. This accessor method provides other components access to the object's designated temporary storage location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "__exit__", "line_number": 344, "body": "def __exit__(self, exc_type, exc, traceback):\n        \" Close/commit the file if there are no exception \"\n        if exc_type:\n            return\n        return super(AtomicLocalFile, self).__exit__(exc_type, exc, traceback)", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Overrides the exit method of a context manager to commit changes only if no exception occurs during the managed block, ensuring atomic file operations in the AtomicLocalFile class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_fail_queue", "line_number": 45, "body": "def _fail_queue(num_messages):\n    return lambda: collections.defaultdict(lambda: ExplQueue(num_messages))", "is_method": false, "function_description": "This function provides a factory that creates default dictionaries with queues limited to a specified number of messages, supporting controlled message storage or buffering scenarios."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_plural_format", "line_number": 49, "body": "def _plural_format(template, number, plural='s'):\n    if number == 0:\n        return ''\n    return template.format(number, '' if number == 1 else plural)", "is_method": false, "function_description": "Utility function that formats a string template with a number and applies pluralization rules, returning an empty string when the number is zero."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "enqueue", "line_number": 38, "body": "def enqueue(self, item):\n        self.pop(item, None)\n        self[item] = datetime.now()\n        if len(self) > self.num_items:\n            self.popitem(last=False)", "is_method": true, "class_name": "ExplQueue", "function_description": "Core method of the ExplQueue class that adds an item with a timestamp, ensures uniqueness by removing duplicates, and maintains a fixed maximum queue size by evicting the oldest items. It supports time-aware queue management with size constraints."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_update_next_send", "line_number": 70, "body": "def _update_next_send(self):\n        self._next_send = time.time() + 60 * self._config.email_interval", "is_method": true, "class_name": "BatchNotifier", "function_description": "Private method of BatchNotifier that updates the scheduled time for the next email batch based on the configured email interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_key", "line_number": 73, "body": "def _key(self, task_name, family, unbatched_args):\n        if self._config.batch_mode == 'all':\n            return task_name\n        elif self._config.batch_mode == 'family':\n            return family\n        elif self._config.batch_mode == 'unbatched_params':\n            param_str = ', '.join('{}={}'.format(k, v) for k, v in unbatched_args.items())\n            return '{}({})'.format(family, param_str)\n        else:\n            raise ValueError('Unknown batch mode for batch notifier: {}'.format(\n                self._config.batch_mode))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Generates a unique key to group tasks based on the configured batching mode, supporting different strategies to organize notification batches within the BatchNotifier class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_expl", "line_number": 85, "body": "def _format_expl(self, expl):\n        lines = expl.rstrip().split('\\n')[-self._config.error_lines:]\n        if self._email_format == 'html':\n            return '<pre>{}</pre>'.format('\\n'.join(lines))\n        else:\n            return '\\n{}'.format('\\n'.join(map('      {}'.format, lines)))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Private method of BatchNotifier that formats the last few lines of an error explanation into either HTML or indented plain text, supporting customizable error message presentation for notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expl_body", "line_number": 92, "body": "def _expl_body(self, expls):\n        lines = [self._format_expl(expl) for expl in expls]\n        if lines and self._email_format != 'html':\n            lines.append('')\n        return '\\n'.join(lines)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Helper method in BatchNotifier that formats a list of explanations into a single string body, joining formatted lines with newline characters, optionally appending a blank line for non-HTML email formats."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_task", "line_number": 98, "body": "def _format_task(self, task_tuple):\n        task, failure_count, disable_count, scheduling_count = task_tuple\n        counts = [\n            _plural_format('{} failure{}', failure_count),\n            _plural_format('{} disable{}', disable_count),\n            _plural_format('{} scheduling failure{}', scheduling_count),\n        ]\n        count_str = ', '.join(filter(None, counts))\n        return '{} ({})'.format(task, count_str)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Formats a task description string including counts of failures, disables, and scheduling failures. This aids in presenting task status summaries within BatchNotifier notifications or logs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_tasks", "line_number": 108, "body": "def _format_tasks(self, tasks):\n        lines = map(self._format_task, sorted(tasks, key=self._expl_key))\n        if self._email_format == 'html':\n            return '<li>{}'.format('\\n<br>'.join(lines))\n        else:\n            return '- {}'.format('\\n  '.join(lines))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Helper method in BatchNotifier that formats a list of tasks into either HTML or plain text for notification purposes, supporting customizable email output styles."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_owners", "line_number": 115, "body": "def _owners(self, owners):\n        return self._default_owner | set(owners)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Combines a set of specified owners with the default owners, ensuring the full ownership list includes both. Useful for managing or notifying all relevant owners in batch processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_failure", "line_number": 118, "body": "def add_failure(self, task_name, family, unbatched_args, expl, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._fail_counts[owner][key] += 1\n            self._fail_expls[owner][key].enqueue(expl)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Method of BatchNotifier that records and tracks failure occurrences for specific tasks, associating explanations and owners, to manage and_notify about batch processing errors effectively."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_disable", "line_number": 124, "body": "def add_disable(self, task_name, family, unbatched_args, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._disabled_counts[owner][key] += 1\n            self._fail_counts[owner].setdefault(key, 0)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Adds a disable record for a specified task and owner, tracking how many times the task is disabled without batching. Useful for monitoring task disablement status across different owners in batch processing contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_scheduling_fail", "line_number": 130, "body": "def add_scheduling_fail(self, task_name, family, unbatched_args, expl, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._scheduling_fail_counts[owner][key] += 1\n            self._fail_expls[owner][key].enqueue(expl)\n            self._fail_counts[owner].setdefault(key, 0)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Method of BatchNotifier that tracks and records scheduling failures per owner by incrementing failure counts and storing failure explanations tied to specific task identifiers and arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_task_expl_groups", "line_number": 137, "body": "def _task_expl_groups(self, expls):\n        if not self._config.group_by_error_messages:\n            return [((task,), msg) for task, msg in expls.items()]\n\n        groups = collections.defaultdict(list)\n        for task, msg in expls.items():\n            groups[msg].append(task)\n        return [(tasks, msg) for msg, tasks in groups.items()]", "is_method": true, "class_name": "BatchNotifier", "function_description": "Internal method in BatchNotifier that groups tasks by their error messages if grouping is enabled, otherwise returns each task with its message individually. Useful for organizing notifications based on shared error contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expls_key", "line_number": 146, "body": "def _expls_key(self, expls_tuple):\n        expls = expls_tuple[0]\n        num_failures = sum(failures + scheduling_fails for (_1, failures, _2, scheduling_fails) in expls)\n        num_disables = sum(disables for (_1, _2, disables, _3) in expls)\n        min_name = min(expls)[0]\n        return -num_failures, -num_disables, min_name", "is_method": true, "class_name": "BatchNotifier", "function_description": "Private method of BatchNotifier that generates a sorting key based on the counts of failures and disables and the minimum name from an explanations tuple, facilitating prioritized ordering of explanation entries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expl_key", "line_number": 153, "body": "def _expl_key(self, expl):\n        return self._expls_key(((expl,), None))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Private helper method in BatchNotifier that generates a key based on a single explanation input, facilitating internal identification or caching of explanations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_email_body", "line_number": 156, "body": "def _email_body(self, fail_counts, disable_counts, scheduling_counts, fail_expls):\n        expls = {\n            (name, fail_count, disable_counts[name], scheduling_counts[name]): self._expl_body(fail_expls[name])\n            for name, fail_count in fail_counts.items()\n        }\n        expl_groups = sorted(self._task_expl_groups(expls), key=self._expls_key)\n        body_lines = []\n        for tasks, msg in expl_groups:\n            body_lines.append(self._format_tasks(tasks))\n            body_lines.append(msg)\n        body = '\\n'.join(filter(None, body_lines)).rstrip()\n        if self._email_format == 'html':\n            return '<ul>\\n{}\\n</ul>'.format(body)\n        else:\n            return body", "is_method": true, "class_name": "BatchNotifier", "function_description": "Generates a formatted email body summarizing task failure, disablement, and scheduling counts with explanations, supporting both plain text and HTML formats for batch notification purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_send_email", "line_number": 172, "body": "def _send_email(self, fail_counts, disable_counts, scheduling_counts, fail_expls, owner):\n        num_failures = sum(fail_counts.values())\n        num_disables = sum(disable_counts.values())\n        num_scheduling_failures = sum(scheduling_counts.values())\n        subject_parts = [\n            _plural_format('{} failure{}', num_failures),\n            _plural_format('{} disable{}', num_disables),\n            _plural_format('{} scheduling failure{}', num_scheduling_failures),\n        ]\n        subject_base = ', '.join(filter(None, subject_parts))\n        if subject_base:\n            prefix = '' if owner in self._default_owner else 'Your tasks have '\n            subject = 'Luigi: {}{} in the last {} minutes'.format(\n                prefix, subject_base, self._config.email_interval)\n            email_body = self._email_body(fail_counts, disable_counts, scheduling_counts, fail_expls)\n            send_email(subject, email_body, email().sender, (owner,))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Utility method in BatchNotifier that composes and sends summary emails about recent task failures, disables, and scheduling issues to task owners, facilitating timely notifications of pipeline statuses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "send_email", "line_number": 189, "body": "def send_email(self):\n        try:\n            for owner, failures in self._fail_counts.items():\n                self._send_email(\n                    fail_counts=failures,\n                    disable_counts=self._disabled_counts[owner],\n                    scheduling_counts=self._scheduling_fail_counts[owner],\n                    fail_expls=self._fail_expls[owner],\n                    owner=owner,\n                )\n        finally:\n            self._update_next_send()\n            self._fail_counts.clear()\n            self._disabled_counts.clear()\n            self._scheduling_fail_counts.clear()\n            self._fail_expls.clear()", "is_method": true, "class_name": "BatchNotifier", "function_description": "Sends notification emails summarizing failure counts and explanations for each owner, then resets all tracking counts and schedules the next notification. This supports automated batch alerting for monitoring or error reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "update", "line_number": 206, "body": "def update(self):\n        if time.time() >= self._next_send:\n            self.send_email()", "is_method": true, "class_name": "BatchNotifier", "function_description": "Checks if the scheduled time has passed and triggers sending an email if so, facilitating timed batch notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "_schedule_and_run", "line_number": 129, "body": "def _schedule_and_run(tasks, worker_scheduler_factory=None, override_defaults=None):\n    \"\"\"\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param override_defaults:\n    :return: True if all tasks and their dependencies were successfully run (or already completed);\n             False if any error occurred. It will return a detailed response of type LuigiRunResult\n             instead of a boolean if detailed_summary=True.\n    \"\"\"\n\n    if worker_scheduler_factory is None:\n        worker_scheduler_factory = _WorkerSchedulerFactory()\n    if override_defaults is None:\n        override_defaults = {}\n    env_params = core(**override_defaults)\n\n    InterfaceLogging.setup(env_params)\n\n    kill_signal = signal.SIGUSR1 if env_params.take_lock else None\n    if (not env_params.no_lock and\n            not(lock.acquire_for(env_params.lock_pid_dir, env_params.lock_size, kill_signal))):\n        raise PidLockAlreadyTakenExit()\n\n    if env_params.local_scheduler:\n        sch = worker_scheduler_factory.create_local_scheduler()\n    else:\n        if env_params.scheduler_url != '':\n            url = env_params.scheduler_url\n        else:\n            url = 'http://{host}:{port:d}/'.format(\n                host=env_params.scheduler_host,\n                port=env_params.scheduler_port,\n            )\n        sch = worker_scheduler_factory.create_remote_scheduler(url=url)\n\n    worker = worker_scheduler_factory.create_worker(\n        scheduler=sch, worker_processes=env_params.workers, assistant=env_params.assistant)\n\n    success = True\n    logger = logging.getLogger('luigi-interface')\n    with worker:\n        for t in tasks:\n            success &= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)\n        logger.info('Done scheduling tasks')\n        success &= worker.run()\n    luigi_run_result = LuigiRunResult(worker, success)\n    logger.info(luigi_run_result.summary_text)\n    return luigi_run_result", "is_method": false, "function_description": "Function that schedules and executes a set of tasks using either a local or remote scheduler, managing concurrency and locking, then returns a detailed summary of the run results and success status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "run", "line_number": 186, "body": "def run(*args, **kwargs):\n    \"\"\"\n    Please dont use. Instead use `luigi` binary.\n\n    Run from cmdline using argparse.\n\n    :param use_dynamic_argparse: Deprecated and ignored\n    \"\"\"\n    luigi_run_result = _run(*args, **kwargs)\n    return luigi_run_result if kwargs.get('detailed_summary') else luigi_run_result.scheduling_succeeded", "is_method": false, "function_description": "This function serves as a command-line interface entry point for running Luigi tasks, wrapping internal execution and returning success status or detailed results based on parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "_run", "line_number": 198, "body": "def _run(cmdline_args=None, main_task_cls=None,\n         worker_scheduler_factory=None, use_dynamic_argparse=None, local_scheduler=False, detailed_summary=False):\n    if use_dynamic_argparse is not None:\n        warnings.warn(\"use_dynamic_argparse is deprecated, don't set it.\",\n                      DeprecationWarning, stacklevel=2)\n    if cmdline_args is None:\n        cmdline_args = sys.argv[1:]\n\n    if main_task_cls:\n        cmdline_args.insert(0, main_task_cls.task_family)\n    if local_scheduler:\n        cmdline_args.append('--local-scheduler')\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)", "is_method": false, "function_description": "Internal helper function that prepares and executes a command-line task by configuring arguments, optionally setting a task class or local scheduler, then scheduling and running the task. It supports flexible task invocation and execution management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "build", "line_number": 214, "body": "def build(tasks, worker_scheduler_factory=None, detailed_summary=False, **env_params):\n    \"\"\"\n    Run internally, bypassing the cmdline parsing.\n\n    Useful if you have some luigi code that you want to run internally.\n    Example:\n\n    .. code-block:: python\n\n        luigi.build([MyTask1(), MyTask2()], local_scheduler=True)\n\n    One notable difference is that `build` defaults to not using\n    the identical process lock. Otherwise, `build` would only be\n    callable once from each process.\n\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param env_params:\n    :return: True if there were no scheduling errors, even if tasks may fail.\n    \"\"\"\n    if \"no_lock\" not in env_params:\n        env_params[\"no_lock\"] = True\n\n    luigi_run_result = _schedule_and_run(tasks, worker_scheduler_factory, override_defaults=env_params)\n    return luigi_run_result if detailed_summary else luigi_run_result.scheduling_succeeded", "is_method": false, "function_description": "Core function that runs Luigi tasks programmatically without command-line parsing, allowing internal scheduling and execution with flexible environment parameters and optional detailed results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_local_scheduler", "line_number": 118, "body": "def create_local_scheduler(self):\n        return scheduler.Scheduler(prune_on_get_work=True, record_task_history=False)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Creates and returns a local scheduler instance configured to prune tasks when fetching work and to not record task history, providing task scheduling functionality for local workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_remote_scheduler", "line_number": 121, "body": "def create_remote_scheduler(self, url):\n        return rpc.RemoteScheduler(url)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Creates and returns a remote scheduler connected to the specified URL, enabling distributed task scheduling over RPC. This function facilitates remote management of scheduled tasks in a distributed system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_worker", "line_number": 124, "body": "def create_worker(self, scheduler, worker_processes, assistant=False):\n        return worker.Worker(\n            scheduler=scheduler, worker_processes=worker_processes, assistant=assistant)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Creates and returns a Worker instance configured with the given scheduler, number of worker processes, and assistant mode option. This facilitates task execution management within the scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "load_task", "line_number": 245, "body": "def load_task(module, task_name, params_str):\n    \"\"\"\n    Imports task dynamically given a module and a task name.\n    \"\"\"\n    if module is not None:\n        __import__(module)\n    task_cls = Register.get_task_cls(task_name)\n    return task_cls.from_str_params(params_str)", "is_method": false, "function_description": "This function dynamically loads and initializes a task class by importing its module, retrieving the class by name, and creating an instance from a parameter string. It simplifies task instantiation based on flexible runtime input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "__new__", "line_number": 57, "body": "def __new__(metacls, classname, bases, classdict):\n        \"\"\"\n        Custom class creation for namespacing.\n\n        Also register all subclasses.\n\n        When the set or inherited namespace evaluates to ``None``, set the task namespace to\n        whatever the currently declared namespace is.\n        \"\"\"\n        cls = super(Register, metacls).__new__(metacls, classname, bases, classdict)\n        cls._namespace_at_class_time = metacls._get_namespace(cls.__module__)\n        metacls._reg.append(cls)\n        return cls", "is_method": true, "class_name": "Register", "function_description": "Customizes class creation to assign namespaces and automatically register all subclasses, facilitating organized class management and tracking within a namespace-aware registration system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "__call__", "line_number": 71, "body": "def __call__(cls, *args, **kwargs):\n        \"\"\"\n        Custom class instantiation utilizing instance cache.\n\n        If a Task has already been instantiated with the same parameters,\n        the previous instance is returned to reduce number of object instances.\n        \"\"\"\n        def instantiate():\n            return super(Register, cls).__call__(*args, **kwargs)\n\n        h = cls.__instance_cache\n\n        if h is None:  # disabled\n            return instantiate()\n\n        params = cls.get_params()\n        param_values = cls.get_param_values(params, args, kwargs)\n\n        k = (cls, tuple(param_values))\n\n        try:\n            hash(k)\n        except TypeError:\n            logger.debug(\"Not all parameter values are hashable so instance isn't coming from the cache\")\n            return instantiate()  # unhashable types in parameters\n\n        if k not in h:\n            h[k] = instantiate()\n\n        return h[k]", "is_method": true, "class_name": "Register", "function_description": "Overrides class instantiation to return a cached instance when called with the same parameters, minimizing duplicate objects. Useful for classes like Task that benefit from instance reuse based on initialization arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "clear_instance_cache", "line_number": 103, "body": "def clear_instance_cache(cls):\n        \"\"\"\n        Clear/Reset the instance cache.\n        \"\"\"\n        cls.__instance_cache = {}", "is_method": true, "class_name": "Register", "function_description": "Clears the cached instances stored by the Register class, effectively resetting its instance tracking or singleton behavior. Useful for freeing memory or reinitializing the state during runtime."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "disable_instance_cache", "line_number": 110, "body": "def disable_instance_cache(cls):\n        \"\"\"\n        Disables the instance cache.\n        \"\"\"\n        cls.__instance_cache = None", "is_method": true, "class_name": "Register", "function_description": "Method of the Register class that disables the instance caching mechanism, ensuring no cached instances are used or stored. Useful for managing or resetting cached state in the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "task_family", "line_number": 117, "body": "def task_family(cls):\n        \"\"\"\n        Internal note: This function will be deleted soon.\n        \"\"\"\n        if not cls.get_task_namespace():\n            return cls.__name__\n        else:\n            return \"{}.{}\".format(cls.get_task_namespace(), cls.__name__)", "is_method": true, "class_name": "Register", "function_description": "Returns a namespaced identifier for a task class, combining its namespace and class name if a namespace exists. This helps uniquely identify tasks within different namespaces."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_get_reg", "line_number": 127, "body": "def _get_reg(cls):\n        \"\"\"Return all of the registered classes.\n\n        :return:  an ``dict`` of task_family -> class\n        \"\"\"\n        # We have to do this on-demand in case task names have changed later\n        reg = dict()\n        for task_cls in cls._reg:\n            if not task_cls._visible_in_registry:\n                continue\n\n            name = task_cls.get_task_family()\n            if name in reg and \\\n                    (reg[name] == Register.AMBIGUOUS_CLASS or  # Check so issubclass doesn't crash\n                     not issubclass(task_cls, reg[name])):\n                # Registering two different classes - this means we can't instantiate them by name\n                # The only exception is if one class is a subclass of the other. In that case, we\n                # instantiate the most-derived class (this fixes some issues with decorator wrappers).\n                reg[name] = Register.AMBIGUOUS_CLASS\n            else:\n                reg[name] = task_cls\n\n        return reg", "is_method": true, "class_name": "Register", "function_description": "Returns a dictionary mapping task family names to their registered classes, resolving name conflicts by marking ambiguous entries to ensure safe instantiation. This enables retrieval of all currently visible registered classes in the Register class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_set_reg", "line_number": 152, "body": "def _set_reg(cls, reg):\n        \"\"\"The writing complement of _get_reg\n        \"\"\"\n        cls._reg = [task_cls for task_cls in reg.values() if task_cls is not cls.AMBIGUOUS_CLASS]", "is_method": true, "class_name": "Register", "function_description": "Private class method in Register that updates the class's internal registry by filtering out ambiguous task classes, maintaining a clean list of distinct registered task classes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "task_names", "line_number": 158, "body": "def task_names(cls):\n        \"\"\"\n        List of task names as strings\n        \"\"\"\n        return sorted(cls._get_reg().keys())", "is_method": true, "class_name": "Register", "function_description": "Returns a sorted list of all registered task names within the Register class, enabling users to easily access available tasks by name."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "tasks_str", "line_number": 165, "body": "def tasks_str(cls):\n        \"\"\"\n        Human-readable register contents dump.\n        \"\"\"\n        return ','.join(cls.task_names())", "is_method": true, "class_name": "Register", "function_description": "Returns a comma-separated string of all registered task names, providing a readable summary of the register's contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "get_task_cls", "line_number": 172, "body": "def get_task_cls(cls, name):\n        \"\"\"\n        Returns an unambiguous class or raises an exception.\n        \"\"\"\n        task_cls = cls._get_reg().get(name)\n        if not task_cls:\n            raise TaskClassNotFoundException(cls._missing_task_msg(name))\n\n        if task_cls == cls.AMBIGUOUS_CLASS:\n            raise TaskClassAmbigiousException('Task %r is ambiguous' % name)\n        return task_cls", "is_method": true, "class_name": "Register", "function_description": "Core method of the Register class that retrieves a unique task class by name, raising exceptions if the task is missing or ambiguous, ensuring unambiguous class resolution for task registration and lookup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "get_all_params", "line_number": 185, "body": "def get_all_params(cls):\n        \"\"\"\n        Compiles and returns all parameters for all :py:class:`Task`.\n\n        :return: a generator of tuples (TODO: we should make this more elegant)\n        \"\"\"\n        for task_name, task_cls in cls._get_reg().items():\n            if task_cls == cls.AMBIGUOUS_CLASS:\n                continue\n            for param_name, param_obj in task_cls.get_params():\n                yield task_name, (not task_cls.use_cmdline_section), param_name, param_obj", "is_method": true, "class_name": "Register", "function_description": "Provides a generator of all parameters from registered Task classes, including their task names and parameter metadata, for comprehensive access to task configurations within the Register class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_editdistance", "line_number": 198, "body": "def _editdistance(a, b):\n        \"\"\" Simple unweighted Levenshtein distance \"\"\"\n        r0 = range(0, len(b) + 1)\n        r1 = [0] * (len(b) + 1)\n\n        for i in range(0, len(a)):\n            r1[0] = i + 1\n\n            for j in range(0, len(b)):\n                c = 0 if a[i] is b[j] else 1\n                r1[j + 1] = min(r1[j] + 1, r0[j + 1] + 1, r0[j] + c)\n\n            r0 = r1[:]\n\n        return r1[len(b)]", "is_method": true, "class_name": "Register", "function_description": "Computes the Levenshtein edit distance between two sequences, measuring how many single-element edits are needed to transform one into the other. Useful for string similarity, typo correction, or sequence comparison tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_missing_task_msg", "line_number": 215, "body": "def _missing_task_msg(cls, task_name):\n        weighted_tasks = [(Register._editdistance(task_name, task_name_2), task_name_2) for task_name_2 in cls.task_names()]\n        ordered_tasks = sorted(weighted_tasks, key=lambda pair: pair[0])\n        candidates = [task for (dist, task) in ordered_tasks if dist <= 5 and dist < len(task)]\n        if candidates:\n            return \"No task %s. Did you mean:\\n%s\" % (task_name, '\\n'.join(candidates))\n        else:\n            return \"No task %s. Candidates are: %s\" % (task_name, cls.tasks_str())", "is_method": true, "class_name": "Register", "function_description": "Helper method in the Register class that suggests closest matching task names when a requested task is not found, aiding error handling by providing possible intended task names based on string similarity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_get_namespace", "line_number": 225, "body": "def _get_namespace(mcs, module_name):\n        for parent in mcs._module_parents(module_name):\n            entry = mcs._default_namespace_dict.get(parent)\n            if entry:\n                return entry\n        return ''", "is_method": true, "class_name": "Register", "function_description": "Internal helper that determines the appropriate namespace for a module by checking its parent modules in order, enabling consistent namespace resolution within the Register class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_module_parents", "line_number": 233, "body": "def _module_parents(module_name):\n        '''\n        >>> list(Register._module_parents('a.b'))\n        ['a.b', 'a', '']\n        '''\n        spl = module_name.split('.')\n        for i in range(len(spl), 0, -1):\n            yield '.'.join(spl[0:i])\n        if module_name:\n            yield ''", "is_method": true, "class_name": "Register", "function_description": "Private utility method that generates all parent module names of a given module, including the module itself and the empty string, supporting hierarchical module processing or lookup tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "dates", "line_number": 60, "body": "def dates(self):\n        ''' Returns a list of dates in this date interval.'''\n        dates = []\n        d = self.date_a\n        while d < self.date_b:\n            dates.append(d)\n            d += datetime.timedelta(1)\n\n        return dates", "is_method": true, "class_name": "DateInterval", "function_description": "Returns a list of all consecutive dates within the DateInterval, from the start (date_a) up to but excluding the end date (date_b). Useful for iterating over each day in a date range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "hours", "line_number": 70, "body": "def hours(self):\n        ''' Same as dates() but returns 24 times more info: one for each hour.'''\n        for date in self.dates():\n            for hour in range(24):\n                yield datetime.datetime.combine(date, datetime.time(hour))", "is_method": true, "class_name": "DateInterval", "function_description": "Provides an iterator over every hour within the date interval, generating datetime objects for each hour of each day covered. Useful for time series processing or hourly data analysis within the interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__str__", "line_number": 76, "body": "def __str__(self):\n        return self.to_string()", "is_method": true, "class_name": "DateInterval", "function_description": "Returns the string representation of the DateInterval instance, providing a human-readable format of the date interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__repr__", "line_number": 79, "body": "def __repr__(self):\n        return self.to_string()", "is_method": true, "class_name": "DateInterval", "function_description": "Returns a string representation of the DateInterval instance, facilitating readable display of the object's state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "prev", "line_number": 82, "body": "def prev(self):\n        ''' Returns the preceding corresponding date interval (eg. May -> April).'''\n        return self.from_date(self.date_a - datetime.timedelta(1))", "is_method": true, "class_name": "DateInterval", "function_description": "Returns the date interval immediately before the current one, enabling navigation to the preceding period (e.g., the previous month or day) in a sequence of date intervals."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "next", "line_number": 86, "body": "def next(self):\n        ''' Returns the subsequent corresponding date interval (eg. 2014 -> 2015).'''\n        return self.from_date(self.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Returns the next consecutive date interval based on the current interval's end date, facilitating sequential date range progression."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__contains__", "line_number": 107, "body": "def __contains__(self, date):\n        return date in self.dates()", "is_method": true, "class_name": "DateInterval", "function_description": "Checks if a given date falls within the DateInterval by verifying its presence in the interval's date set. This enables simple containment testing for date-based range operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__iter__", "line_number": 110, "body": "def __iter__(self):\n        for d in self.dates():\n            yield d", "is_method": true, "class_name": "DateInterval", "function_description": "Enables iteration over all dates within the DateInterval instance, allowing users to loop through each date sequentially. This method supports convenient date range traversal in iterable contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__hash__", "line_number": 114, "body": "def __hash__(self):\n        return hash(repr(self))", "is_method": true, "class_name": "DateInterval", "function_description": "Returns a hash value based on the string representation of the DateInterval instance, enabling its use in hash-based collections like sets and dictionaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__cmp__", "line_number": 117, "body": "def __cmp__(self, other):\n        if not isinstance(self, type(other)):\n            # doing this because it's not well defined if eg. 2012-01-01-2013-01-01 == 2012\n            raise TypeError('Date interval type mismatch')\n\n        return (self > other) - (self < other)", "is_method": true, "class_name": "DateInterval", "function_description": "Defines comparison behavior between two DateInterval instances, enabling their ordering by evaluating which interval is greater, lesser, or equal while ensuring type consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__lt__", "line_number": 124, "body": "def __lt__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) < (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Specialized comparison method in DateInterval that enables ordering based on start and end dates, allowing interval objects to be sorted or compared chronologically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__le__", "line_number": 129, "body": "def __le__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) <= (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Provides comparison functionality to determine if one DateInterval is less than or equal to another based on their start and end dates, ensuring type compatibility for accurate interval ordering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__gt__", "line_number": 134, "body": "def __gt__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) > (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Comparison method of DateInterval that determines if one date interval is strictly greater than another based on their start and end dates, ensuring type compatibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__ge__", "line_number": 139, "body": "def __ge__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) >= (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Comparison method of the DateInterval class that determines if one date interval is greater than or equal to another based on their start and end dates. It ensures type compatibility before comparison."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__eq__", "line_number": 144, "body": "def __eq__(self, other):\n        if not isinstance(other, DateInterval):\n            return False\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        else:\n            return (self.date_a, self.date_b) == (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Checks whether two DateInterval instances represent the exact same date range while ensuring they are of compatible types. This method enables accurate comparison of date intervals within the DateInterval class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__ne__", "line_number": 152, "body": "def __ne__(self, other):\n        return not self.__eq__(other)", "is_method": true, "class_name": "DateInterval", "function_description": "Overrides the inequality operator to determine if two DateInterval instances are not equal based on their equality comparison. It provides a standard way to compare DateInterval objects for inequality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 165, "body": "def to_string(self):\n        return self.date_a.strftime('%Y-%m-%d')", "is_method": true, "class_name": "Date", "function_description": "Returns the date as a string formatted in 'YYYY-MM-DD' format. This method provides a standardized, human-readable representation of the Date object's value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 169, "body": "def from_date(cls, d):\n        return Date(d.year, d.month, d.day)", "is_method": true, "class_name": "Date", "function_description": "Utility method of the Date class that creates a new Date instance from a standard date object, facilitating conversion between different date representations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 173, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d$', s):\n            return Date(*map(int, s.split('-')))", "is_method": true, "class_name": "Date", "function_description": "Class Date method that parses a string in \"YYYY-MM-DD\" format and returns a Date instance representing that date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 196, "body": "def to_string(self):\n        return '%d-W%02d' % self.date_a.isocalendar()[:2]", "is_method": true, "class_name": "Week", "function_description": "Returns a string representing the week of the year in \"Year-Wweek\" format based on the instance's date. Useful for standardized week identification and display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 200, "body": "def from_date(cls, d):\n        return Week(*d.isocalendar()[:2])", "is_method": true, "class_name": "Week", "function_description": "Constructs and returns a Week instance corresponding to the ISO year and week number of the given date. Useful for converting date objects into week-based representations within the Week class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 204, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-W\\d\\d$', s):\n            y, w = map(int, s.split('-W'))\n            return Week(y, w)", "is_method": true, "class_name": "Week", "function_description": "Parses a string formatted as \"YYYY-Www\" to create a corresponding Week instance representing that year and week number. This enables conversion from standardized week-string formats to Week objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 217, "body": "def to_string(self):\n        return self.date_a.strftime('%Y-%m')", "is_method": true, "class_name": "Month", "function_description": "Returns the month and year of the Month instance as a formatted string (YYYY-MM), facilitating standardized date representation and display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 221, "body": "def from_date(cls, d):\n        return Month(d.year, d.month)", "is_method": true, "class_name": "Month", "function_description": "Creates a Month instance representing the year and month extracted from a given date object. This enables easy conversion from date to Month for date-related computations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 225, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d$', s):\n            y, m = map(int, s.split('-'))\n            return Month(y, m)", "is_method": true, "class_name": "Month", "function_description": "Class Month method that parses a string in \"YYYY-MM\" format and returns a corresponding Month instance representing that year and month."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 238, "body": "def to_string(self):\n        return self.date_a.strftime('%Y')", "is_method": true, "class_name": "Year", "function_description": "Returns the year component of the date as a four-digit string, providing a simple string representation of the Year instance's date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 242, "body": "def from_date(cls, d):\n        return Year(d.year)", "is_method": true, "class_name": "Year", "function_description": "Creates a Year instance representing the year part of a given date object, providing a convenient way to extract and work with the year component."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 246, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d$', s):\n            return Year(int(s))", "is_method": true, "class_name": "Year", "function_description": "Utility method of the Year class that parses a four-digit string to create a Year instance, enabling conversion from string representations of years to Year objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 259, "body": "def to_string(self):\n        return '-'.join([d.strftime('%Y-%m-%d') for d in (self.date_a, self.date_b)])", "is_method": true, "class_name": "Custom", "function_description": "Concatenates two date attributes of the object into a single string separated by a hyphen, formatting each date as 'YYYY-MM-DD'. Useful for generating standardized string representations of paired dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 263, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\\-\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d$', s):\n            x = list(map(int, s.split('-')))\n            date_a = datetime.date(*x[:3])\n            date_b = datetime.date(*x[3:])\n            return Custom(date_a, date_b)", "is_method": true, "class_name": "Custom", "function_description": "Parses a specific date-range string format into a Custom object with two associated date attributes. This enables converting structured date strings into usable date range instances for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_history.py", "function": "task_family", "line_number": 44, "body": "def task_family(self):\n        return self._task.family", "is_method": true, "class_name": "StoredTask", "function_description": "Returns the category or group identifier of the associated task, allowing clients to understand or organize tasks by their family classification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_history.py", "function": "parameters", "line_number": 48, "body": "def parameters(self):\n        return self._task.params", "is_method": true, "class_name": "StoredTask", "function_description": "Returns the parameters associated with the stored task, providing access to its configuration or input values for further processing or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "check_pid", "line_number": 30, "body": "def check_pid(pidfile):\n    if pidfile and os.path.exists(pidfile):\n        try:\n            pid = int(open(pidfile).read().strip())\n            os.kill(pid, 0)\n            return pid\n        except BaseException:\n            return 0\n    return 0", "is_method": false, "function_description": "Utility function that checks if a process ID stored in a file is currently running, returning the PID if active or 0 otherwise. It aids in process monitoring by verifying process existence via a PID file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "write_pid", "line_number": 41, "body": "def write_pid(pidfile):\n    server_logger.info(\"Writing pid file\")\n    piddir = os.path.dirname(pidfile)\n    if piddir != '':\n        try:\n            os.makedirs(piddir)\n        except OSError:\n            pass\n\n    with open(pidfile, 'w') as fobj:\n        fobj.write(str(os.getpid()))", "is_method": false, "function_description": "Function that writes the current process ID to a specified file, creating necessary directories if they don't exist. Useful for managing daemon or server process tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "get_log_format", "line_number": 54, "body": "def get_log_format():\n    return \"%(asctime)s %(name)s[%(process)s] %(levelname)s: %(message)s\"", "is_method": false, "function_description": "Utility function that provides a standardized log message format including timestamp, logger name, process ID, log level, and message for consistent logging output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "get_spool_handler", "line_number": 58, "body": "def get_spool_handler(filename):\n    handler = logging.handlers.TimedRotatingFileHandler(\n        filename=filename,\n        when='d',\n        encoding='utf8',\n        backupCount=7  # keep one week of historical logs\n    )\n    formatter = logging.Formatter(get_log_format())\n    handler.setFormatter(formatter)\n    return handler", "is_method": false, "function_description": "Provides a configured logging handler that rotates log files daily, retaining one week of backups, ensuring efficient log file management and formatted output for consistent log records."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "_server_already_running", "line_number": 70, "body": "def _server_already_running(pidfile):\n    existing_pid = check_pid(pidfile)\n    if pidfile and existing_pid:\n        return True\n    return False", "is_method": false, "function_description": "Utility function to check if a server process is already running by verifying the existence of a valid PID in the given pidfile."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "daemonize", "line_number": 77, "body": "def daemonize(cmd, pidfile=None, logdir=None, api_port=8082, address=None, unix_socket=None):\n    import daemon\n\n    logdir = logdir or \"/var/log/luigi\"\n    if not os.path.exists(logdir):\n        os.makedirs(logdir)\n\n    log_path = os.path.join(logdir, \"luigi-server.log\")\n\n    # redirect stdout/stderr\n    today = datetime.date.today()\n    stdout_path = os.path.join(\n        logdir,\n        \"luigi-server-{0:%Y-%m-%d}.out\".format(today)\n    )\n    stderr_path = os.path.join(\n        logdir,\n        \"luigi-server-{0:%Y-%m-%d}.err\".format(today)\n    )\n    stdout_proxy = open(stdout_path, 'a+')\n    stderr_proxy = open(stderr_path, 'a+')\n\n    try:\n        ctx = daemon.DaemonContext(\n            stdout=stdout_proxy,\n            stderr=stderr_proxy,\n            working_directory='.',\n            initgroups=False,\n        )\n    except TypeError:\n        # Older versions of python-daemon cannot deal with initgroups arg.\n        ctx = daemon.DaemonContext(\n            stdout=stdout_proxy,\n            stderr=stderr_proxy,\n            working_directory='.',\n        )\n\n    with ctx:\n        loghandler = get_spool_handler(log_path)\n        rootlogger.addHandler(loghandler)\n\n        if pidfile:\n            server_logger.info(\"Checking pid file\")\n            existing_pid = check_pid(pidfile)\n            if pidfile and existing_pid:\n                server_logger.info(\"Server already running (pid=%s)\", existing_pid)\n                return\n            write_pid(pidfile)\n\n        cmd(api_port=api_port, address=address, unix_socket=unix_socket)", "is_method": false, "function_description": "Function that runs a given command as a daemon process, managing logging, PID files, and process detachment to enable background execution of server or service tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_urljoin", "line_number": 53, "body": "def _urljoin(base, url):\n    \"\"\"\n    Join relative URLs to base URLs like urllib.parse.urljoin but support\n    arbitrary URIs (esp. 'http+unix://').\n    \"\"\"\n    parsed = urlparse(base)\n    scheme = parsed.scheme\n    return urlparse(\n        urljoin(parsed._replace(scheme='http').geturl(), url)\n    )._replace(scheme=scheme).geturl()", "is_method": false, "function_description": "Utility function that joins a relative URL to a base URL while preserving non-standard URI schemes, enabling handling of arbitrary URI formats beyond typical HTTP/HTTPS URLs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_create_request", "line_number": 75, "body": "def _create_request(self, full_url, body=None):\n        # when full_url contains basic auth info, extract it and set the Authorization header\n        url = urlparse(full_url)\n        if url.username:\n            # base64 encoding of username:password\n            auth = base64.b64encode('{}:{}'.format(url.username, url.password or '').encode('utf-8'))\n            auth = auth.decode('utf-8')\n            # update full_url and create a request object with the auth header set\n            full_url = url._replace(netloc=url.netloc.split('@', 1)[-1]).geturl()\n            req = Request(full_url)\n            req.add_header('Authorization', 'Basic {}'.format(auth))\n        else:\n            req = Request(full_url)\n\n        # add the request body\n        if body:\n            req.data = urlencode(body).encode('utf-8')\n\n        return req", "is_method": true, "class_name": "URLLibFetcher", "function_description": "Creates and returns an HTTP request object from a URL, handling embedded basic authentication and optionally encoding form data as the request body. Useful for preparing authorized HTTP requests in web fetch operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "fetch", "line_number": 95, "body": "def fetch(self, full_url, body, timeout):\n        req = self._create_request(full_url, body=body)\n        return urlopen(req, timeout=timeout).read().decode('utf-8')", "is_method": true, "class_name": "URLLibFetcher", "function_description": "Core method of URLLibFetcher that sends an HTTP request to a specified URL with an optional body and returns the response content decoded as a UTF-8 string within a given timeout period."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "check_pid", "line_number": 107, "body": "def check_pid(self):\n        # if the process id change changed from when the session was created\n        # a new session needs to be setup since requests isn't multiprocessing safe.\n        if os.getpid() != self.process_id:\n            self.session = requests.Session()\n            self.process_id = os.getpid()", "is_method": true, "class_name": "RequestsFetcher", "function_description": "Ensures the RequestsFetcher maintains a valid HTTP session by resetting it if the process ID changes, supporting safe usage across multiprocessing environments. This prevents session-related errors when forking processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "fetch", "line_number": 114, "body": "def fetch(self, full_url, body, timeout):\n        self.check_pid()\n        resp = self.session.post(full_url, data=body, timeout=timeout)\n        resp.raise_for_status()\n        return resp.text", "is_method": true, "class_name": "RequestsFetcher", "function_description": "Core method of RequestsFetcher that sends a POST request to a specified URL with a given body and timeout, returning the response text after ensuring the request was successful."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_get_retryer", "line_number": 147, "body": "def _get_retryer(self):\n        def retry_logging(retry_state):\n            if self._rpc_log_retries:\n                logger.warning(\"Failed connecting to remote scheduler %r\", self._url, exc_info=True)\n                logger.info(\"Retrying attempt %r of %r (max)\" % (retry_state.attempt_number + 1, self._rpc_retry_attempts))\n                logger.info(\"Wait for %d seconds\" % self._rpc_retry_wait)\n\n        return Retrying(wait=wait_fixed(self._rpc_retry_wait),\n                        stop=stop_after_attempt(self._rpc_retry_attempts),\n                        reraise=True,\n                        after=retry_logging)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Internal helper method of RemoteScheduler that configures and returns a retry mechanism with logging for connecting attempts to a remote scheduler endpoint."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_fetch", "line_number": 159, "body": "def _fetch(self, url_suffix, body):\n        full_url = _urljoin(self._url, url_suffix)\n        scheduler_retry = self._get_retryer()\n\n        try:\n            response = scheduler_retry(self._fetcher.fetch, full_url, body, self._connect_timeout)\n        except self._fetcher.raises as e:\n            raise RPCError(\n                \"Errors (%d attempts) when connecting to remote scheduler %r\" %\n                (self._rpc_retry_attempts, self._url),\n                e\n            )\n        return response", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Private method in RemoteScheduler that performs an HTTP fetch with retries to a constructed remote URL, raising an error if all retry attempts fail."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_request", "line_number": 173, "body": "def _request(self, url, data, attempts=3, allow_null=True):\n        body = {'data': json.dumps(data)}\n\n        for _ in range(attempts):\n            page = self._fetch(url, body)\n            response = json.loads(page)[\"response\"]\n            if allow_null or response is not None:\n                return response\n        raise RPCError(\"Received null response from remote scheduler %r\" % self._url)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Internal helper method in RemoteScheduler that sends JSON-encoded data to a specified URL, retrying multiple times to obtain a non-null response, facilitating reliable remote procedure calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "retry_logging", "line_number": 148, "body": "def retry_logging(retry_state):\n            if self._rpc_log_retries:\n                logger.warning(\"Failed connecting to remote scheduler %r\", self._url, exc_info=True)\n                logger.info(\"Retrying attempt %r of %r (max)\" % (retry_state.attempt_number + 1, self._rpc_retry_attempts))\n                logger.info(\"Wait for %d seconds\" % self._rpc_retry_wait)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Logs retry attempts and failure details when connecting to a remote scheduler, providing visibility into retry progress and wait times during RPC connection issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "has_value", "line_number": 55, "body": "def has_value(cls, value):\n        return any(value == item.value for item in cls)", "is_method": true, "class_name": "ParameterVisibility", "function_description": "Checks if a given value exists among the enumeration members of the ParameterVisibility class, facilitating validation or membership checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 58, "body": "def serialize(self):\n        return self.value", "is_method": true, "class_name": "ParameterVisibility", "function_description": "Returns the underlying value representing the parameter's visibility state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_get_value_from_config", "line_number": 185, "body": "def _get_value_from_config(self, section, name):\n        \"\"\"Loads the default from the config. Returns _no_value if it doesn't exist\"\"\"\n\n        conf = configuration.get_config()\n\n        try:\n            value = conf.get(section, name)\n        except (NoSectionError, NoOptionError, KeyError):\n            return _no_value\n\n        return self.parse(value)", "is_method": true, "class_name": "Parameter", "function_description": "Method of the Parameter class that retrieves a configuration value by section and name, returning a parsed value or a sentinel if the option is missing. It facilitates fetching default parameters from a configuration source."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_get_value", "line_number": 197, "body": "def _get_value(self, task_name, param_name):\n        for value, warn in self._value_iterator(task_name, param_name):\n            if value != _no_value:\n                if warn:\n                    warnings.warn(warn, DeprecationWarning)\n                return value\n        return _no_value", "is_method": true, "class_name": "Parameter", "function_description": "Internal helper of the Parameter class that retrieves the first valid value for a given task and parameter, optionally issuing deprecation warnings. It supports parameter resolution with backward compatibility checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_value_iterator", "line_number": 205, "body": "def _value_iterator(self, task_name, param_name):\n        \"\"\"\n        Yield the parameter values, with optional deprecation warning as second tuple value.\n\n        The parameter value will be whatever non-_no_value that is yielded first.\n        \"\"\"\n        cp_parser = CmdlineParser.get_instance()\n        if cp_parser:\n            dest = self._parser_global_dest(param_name, task_name)\n            found = getattr(cp_parser.known_args, dest, None)\n            yield (self._parse_or_no_value(found), None)\n        yield (self._get_value_from_config(task_name, param_name), None)\n        if self._config_path:\n            yield (self._get_value_from_config(self._config_path['section'], self._config_path['name']),\n                   'The use of the configuration [{}] {} is deprecated. Please use [{}] {}'.format(\n                       self._config_path['section'], self._config_path['name'], task_name, param_name))\n        yield (self._default, None)", "is_method": true, "class_name": "Parameter", "function_description": "Provides an iterator over possible parameter values from multiple sources, optionally including a deprecation warning. Useful for retrieving configuration values with fallback and backward compatibility awareness."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "has_task_value", "line_number": 223, "body": "def has_task_value(self, task_name, param_name):\n        return self._get_value(task_name, param_name) != _no_value", "is_method": true, "class_name": "Parameter", "function_description": "Checks if a specified task parameter has an assigned value within the Parameter class, enabling validation of parameter presence before use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "task_value", "line_number": 226, "body": "def task_value(self, task_name, param_name):\n        value = self._get_value(task_name, param_name)\n        if value == _no_value:\n            raise MissingParameterException(\"No default specified\")\n        else:\n            return self.normalize(value)", "is_method": true, "class_name": "Parameter", "function_description": "Provides the value of a specific parameter for a given task, raising an exception if no value is set. It ensures the returned parameter value is normalized for consistent use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_is_batchable", "line_number": 233, "body": "def _is_batchable(self):\n        return self._batch_method is not None", "is_method": true, "class_name": "Parameter", "function_description": "Utility method in the Parameter class that checks if the parameter supports batch processing by verifying the presence of a batching method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 236, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        The default implementation is the identity function, but subclasses should override\n        this method for specialized parsing.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        return x", "is_method": true, "class_name": "Parameter", "function_description": "Basic parsing method of the Parameter class that returns the input value unchanged by default. It serves as a placeholder for subclasses to implement specialized value parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parse_list", "line_number": 248, "body": "def _parse_list(self, xs):\n        \"\"\"\n        Parse a list of values from the scheduler.\n\n        Only possible if this is_batchable() is True. This will combine the list into a single\n        parameter value using batch method. This should never need to be overridden.\n\n        :param xs: list of values to parse and combine\n        :return: the combined parsed values\n        \"\"\"\n        if not self._is_batchable():\n            raise NotImplementedError('No batch method found')\n        elif not xs:\n            raise ValueError('Empty parameter list passed to parse_list')\n        else:\n            return self._batch_method(map(self.parse, xs))", "is_method": true, "class_name": "Parameter", "function_description": "Utility method in the Parameter class that combines and parses a list of values into a single batchable parameter using a batch method. It ensures batch processing capability is available before combining inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 265, "body": "def serialize(self, x):\n        \"\"\"\n        Opposite of :py:meth:`parse`.\n\n        Converts the value ``x`` to a string.\n\n        :param x: the value to serialize.\n        \"\"\"\n        return str(x)", "is_method": true, "class_name": "Parameter", "function_description": "Simple utility of the Parameter class that converts a given value into its string representation, effectively serializing it for storage or transmission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 275, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != Parameter:\n            return\n        if not isinstance(param_value, str):\n            warnings.warn('Parameter \"{}\" with value \"{}\" is not of type string.'.format(param_name, param_value))", "is_method": true, "class_name": "Parameter", "function_description": "Utility method in the Parameter class that issues a warning if a parameter value is not a string and the current instance is exactly of type Parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 281, "body": "def normalize(self, x):\n        \"\"\"\n        Given a parsed parameter value, normalizes it.\n\n        The value can either be the result of parse(), the default value or\n        arguments passed into the task's constructor by instantiation.\n\n        This is very implementation defined, but can be used to validate/clamp\n        valid values. For example, if you wanted to only accept even integers,\n        and \"correct\" odd values to the nearest integer, you can implement\n        normalize as ``x // 2 * 2``.\n        \"\"\"\n        return x", "is_method": true, "class_name": "Parameter", "function_description": "Provides a customizable hook to validate or adjust a parameter's value after parsing, enforcing constraints or normalization rules before use. Useful for ensuring parameter correctness or applying value transformations in derived classes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 295, "body": "def next_in_enumeration(self, _value):\n        \"\"\"\n        If your Parameter type has an enumerable ordering of values. You can\n        choose to override this method. This method is used by the\n        :py:mod:`luigi.execution_summary` module for pretty printing\n        purposes. Enabling it to pretty print tasks like ``MyTask(num=1),\n        MyTask(num=2), MyTask(num=3)`` to ``MyTask(num=1..3)``.\n\n        :param value: The value\n        :return: The next value, like \"value + 1\". Or ``None`` if there's no enumerable ordering.\n        \"\"\"\n        return None", "is_method": true, "class_name": "Parameter", "function_description": "Returns the next value in the enumeration of a parameter if an ordered sequence exists, facilitating human-readable task summaries. By default, it indicates no enumerable ordering is defined."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parse_or_no_value", "line_number": 308, "body": "def _parse_or_no_value(self, x):\n        if not x:\n            return _no_value\n        else:\n            return self.parse(x)", "is_method": true, "class_name": "Parameter", "function_description": "Utility method of the Parameter class that returns a special no-value indicator if input is empty; otherwise, it parses the input using the class's parse logic."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_global_dest", "line_number": 315, "body": "def _parser_global_dest(param_name, task_name):\n        return task_name + '_' + param_name", "is_method": true, "class_name": "Parameter", "function_description": "Private utility function in the Parameter class that generates a unique identifier by combining a task name and parameter name. It helps namespace parameters to avoid naming conflicts across different tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_kwargs", "line_number": 319, "body": "def _parser_kwargs(cls, param_name, task_name=None):\n        return {\n            \"action\": \"store\",\n            \"dest\": cls._parser_global_dest(param_name, task_name) if task_name else param_name,\n        }", "is_method": true, "class_name": "Parameter", "function_description": "Utility method in the Parameter class that constructs and returns keyword arguments for argument parsers, facilitating consistent parameter destination naming optionally scoped by task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 329, "body": "def serialize(self, x):\n        if x is None:\n            return ''\n        else:\n            return str(x)", "is_method": true, "class_name": "OptionalParameter", "function_description": "Converts an optional parameter to its string representation, returning an empty string if the value is None. This simplifies serialization by handling both present and absent values uniformly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 335, "body": "def parse(self, x):\n        return x or None", "is_method": true, "class_name": "OptionalParameter", "function_description": "Utility method in OptionalParameter that returns the input value or None if the input is falsy, facilitating optional parameter handling in function calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 338, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != OptionalParameter:\n            return\n        if not isinstance(param_value, str) and param_value is not None:\n            warnings.warn('OptionalParameter \"{}\" with value \"{}\" is not of type string or None.'.format(\n                param_name, param_value))", "is_method": true, "class_name": "OptionalParameter", "function_description": "Internal method that warns if a parameter's value is neither a string nor None, helping to enforce type expectations when using the OptionalParameter class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 367, "body": "def parse(self, s):\n        \"\"\"\n        Parses a date string formatted like ``YYYY-MM-DD``.\n        \"\"\"\n        return datetime.datetime.strptime(s, self.date_format).date()", "is_method": true, "class_name": "_DateParameterBase", "function_description": "Parses a date string in the format YYYY-MM-DD into a date object, enabling standardized date interpretation and conversion within the _DateParameterBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 373, "body": "def serialize(self, dt):\n        \"\"\"\n        Converts the date to a string using the :py:attr:`~_DateParameterBase.date_format`.\n        \"\"\"\n        if dt is None:\n            return str(dt)\n        return dt.strftime(self.date_format)", "is_method": true, "class_name": "_DateParameterBase", "function_description": "Converts a date object into a formatted string based on a predefined date format, handling None values gracefully. Useful for consistent date serialization across different components."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 415, "body": "def next_in_enumeration(self, value):\n        return value + datetime.timedelta(days=self.interval)", "is_method": true, "class_name": "DateParameter", "function_description": "Returns the next date value in a sequence spaced by a defined interval in days, facilitating date enumeration or iteration within the DateParameter class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 418, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, datetime.datetime):\n            value = value.date()\n\n        delta = (value - self.start).days % self.interval\n        return value - datetime.timedelta(days=delta)", "is_method": true, "class_name": "DateParameter", "function_description": "Core method of DateParameter that normalizes a date value by aligning it to the nearest interval boundary starting from a specific start date, useful for consistent date grouping or bucketing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_add_months", "line_number": 441, "body": "def _add_months(self, date, months):\n        \"\"\"\n        Add ``months`` months to ``date``.\n\n        Unfortunately we can't use timedeltas to add months because timedelta counts in days\n        and there's no foolproof way to add N months in days without counting the number of\n        days per month.\n        \"\"\"\n        year = date.year + (date.month + months - 1) // 12\n        month = (date.month + months - 1) % 12 + 1\n        return datetime.date(year=year, month=month, day=1)", "is_method": true, "class_name": "MonthParameter", "function_description": "Utility method in MonthParameter that calculates a new date by adding a specified number of months to a given date, correctly handling year and month boundaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 453, "body": "def next_in_enumeration(self, value):\n        return self._add_months(value, self.interval)", "is_method": true, "class_name": "MonthParameter", "function_description": "Returns the month value obtained by adding a predefined interval to the given month value, useful for iterating through months in a defined step size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 456, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, date_interval.Month):\n            value = value.date_a\n\n        months_since_start = (value.year - self.start.year) * 12 + (value.month - self.start.month)\n        months_since_start -= months_since_start % self.interval\n\n        return self._add_months(self.start, months_since_start)", "is_method": true, "class_name": "MonthParameter", "function_description": "Normalizes a given date to the nearest preceding month aligned with a set interval from a start date, ensuring consistent monthly intervals for time-based computations or categorizations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 480, "body": "def next_in_enumeration(self, value):\n        return value.replace(year=value.year + self.interval)", "is_method": true, "class_name": "YearParameter", "function_description": "Returns a new date by advancing the given date's year by the class's interval value, supporting date progression based on year increments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 483, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, date_interval.Year):\n            value = value.date_a\n\n        delta = (value.year - self.start.year) % self.interval\n        return datetime.date(year=value.year - delta, month=1, day=1)", "is_method": true, "class_name": "YearParameter", "function_description": "Normalizes a given date to the nearest interval-aligned year boundary based on the starting year and interval of the YearParameter. Useful for grouping or binning dates into consistent yearly segments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 520, "body": "def parse(self, s):\n        \"\"\"\n        Parses a string to a :py:class:`~datetime.datetime`.\n        \"\"\"\n        return datetime.datetime.strptime(s, self.date_format)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Parses a string into a datetime object based on a predefined format, enabling consistent datetime conversion for downstream processing or validation tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 526, "body": "def serialize(self, dt):\n        \"\"\"\n        Converts the date to a string using the :py:attr:`~_DatetimeParameterBase.date_format`.\n        \"\"\"\n        if dt is None:\n            return str(dt)\n        return dt.strftime(self.date_format)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Converts a datetime object to its string representation based on a predefined date format, supporting consistent date serialization within the _DatetimeParameterBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_convert_to_dt", "line_number": 535, "body": "def _convert_to_dt(dt):\n        if not isinstance(dt, datetime.datetime):\n            dt = datetime.datetime.combine(dt, datetime.time.min)\n        return dt", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Utility function in _DatetimeParameterBase that converts a date or datetime input to a datetime object, ensuring consistent datetime representation for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 540, "body": "def normalize(self, dt):\n        \"\"\"\n        Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at\n        :py:attr:`~_DatetimeParameterBase.start`.\n        \"\"\"\n        if dt is None:\n            return None\n\n        dt = self._convert_to_dt(dt)\n\n        dt = dt.replace(microsecond=0)  # remove microseconds, to avoid float rounding issues.\n        delta = (dt - self.start).total_seconds()\n        granularity = (self._timedelta * self.interval).total_seconds()\n        return dt - datetime.timedelta(seconds=delta % granularity)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Core method of _DatetimeParameterBase that adjusts a datetime to align with fixed intervals starting from a set point, ensuring normalized timestamps for consistent time-based computations or scheduling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 555, "body": "def next_in_enumeration(self, value):\n        return value + self._timedelta * self.interval", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Core method of the _DatetimeParameterBase class that calculates the next datetime value by advancing the current value by a fixed interval multiplied by a timedelta unit. It supports sequential iteration over datetime parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 587, "body": "def parse(self, s):\n        try:\n            value = datetime.datetime.strptime(s, self.deprecated_date_format)\n            warnings.warn(\n                'Using \"H\" between hours and minutes is deprecated, omit it instead.',\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return value\n        except ValueError:\n            return super(DateMinuteParameter, self).parse(s)", "is_method": true, "class_name": "DateMinuteParameter", "function_description": "Parses a date-time string using a deprecated format while issuing a deprecation warning, falling back to a superclass parser if parsing fails. Useful for handling legacy date inputs with gradual migration to a new format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 620, "body": "def parse(self, s):\n        \"\"\"\n        Parses an ``int`` from the string using ``int()``.\n        \"\"\"\n        return int(s)", "is_method": true, "class_name": "IntParameter", "function_description": "Core functionality of IntParameter that converts a string representation into an integer value, providing simple string-to-int parsing for configurations or inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 626, "body": "def next_in_enumeration(self, value):\n        return value + 1", "is_method": true, "class_name": "IntParameter", "function_description": "Simple utility method of the IntParameter class that returns the next integer following the given value, supporting enumeration over integer sequences."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 635, "body": "def parse(self, s):\n        \"\"\"\n        Parses a ``float`` from the string using ``float()``.\n        \"\"\"\n        return float(s)", "is_method": true, "class_name": "FloatParameter", "function_description": "Parses a string input into a floating-point number, providing a simple utility for converting textual numeric representations within the FloatParameter class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 682, "body": "def parse(self, val):\n        \"\"\"\n        Parses a ``bool`` from the string, matching 'true' or 'false' ignoring case.\n        \"\"\"\n        s = str(val).lower()\n        if s == \"true\":\n            return True\n        elif s == \"false\":\n            return False\n        else:\n            raise ValueError(\"cannot interpret '{}' as boolean\".format(val))", "is_method": true, "class_name": "BoolParameter", "function_description": "Method of BoolParameter that interprets a string value as a boolean by matching case-insensitive 'true' or 'false', raising an error for invalid inputs. It ensures consistent boolean parsing from text representations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 694, "body": "def normalize(self, value):\n        try:\n            return self.parse(value)\n        except ValueError:\n            return None", "is_method": true, "class_name": "BoolParameter", "function_description": "Core method of BoolParameter that attempts to parse a given value into a boolean, returning None if parsing fails, thus providing a normalized boolean interpretation with graceful error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_kwargs", "line_number": 700, "body": "def _parser_kwargs(self, *args, **kwargs):\n        parser_kwargs = super(BoolParameter, self)._parser_kwargs(*args, **kwargs)\n        if self.parsing == self.IMPLICIT_PARSING:\n            parser_kwargs[\"action\"] = \"store_true\"\n        elif self.parsing == self.EXPLICIT_PARSING:\n            parser_kwargs[\"nargs\"] = \"?\"\n            parser_kwargs[\"const\"] = True\n        else:\n            raise ValueError(\"unknown parsing value '{}'\".format(self.parsing))\n        return parser_kwargs", "is_method": true, "class_name": "BoolParameter", "function_description": "Constructs and returns argument parser settings based on the BoolParameter's parsing mode, configuring how boolean values are interpreted during command-line argument parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 722, "body": "def parse(self, s):\n        \"\"\"\n        Parses a :py:class:`~luigi.date_interval.DateInterval` from the input.\n\n        see :py:mod:`luigi.date_interval`\n          for details on the parsing of DateIntervals.\n        \"\"\"\n        # TODO: can we use xml.utils.iso8601 or something similar?\n\n        from luigi import date_interval as d\n\n        for cls in [d.Year, d.Month, d.Week, d.Date, d.Custom]:\n            i = cls.parse(s)\n            if i:\n                return i\n\n        raise ValueError('Invalid date interval - could not be parsed')", "is_method": true, "class_name": "DateIntervalParameter", "function_description": "Parses a string input into a corresponding DateInterval object by attempting different interval types, enabling flexible interpretation of date ranges for scheduling or temporal operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_apply_regex", "line_number": 753, "body": "def _apply_regex(self, regex, input):\n        import re\n        re_match = re.match(regex, input)\n        if re_match and any(re_match.groups()):\n            kwargs = {}\n            has_val = False\n            for k, v in re_match.groupdict(default=\"0\").items():\n                val = int(v)\n                if val > -1:\n                    has_val = True\n                    kwargs[k] = val\n            if has_val:\n                return datetime.timedelta(**kwargs)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Method of TimeDeltaParameter that parses a string using a regex to extract time components and returns a corresponding timedelta object if valid values are found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parseIso8601", "line_number": 767, "body": "def _parseIso8601(self, input):\n        def field(key):\n            return r\"(?P<%s>\\d+)%s\" % (key, key[0].upper())\n\n        def optional_field(key):\n            return \"(%s)?\" % field(key)\n\n        # A little loose: ISO 8601 does not allow weeks in combination with other fields, but this regex does (as does python timedelta)\n        regex = \"P(%s|%s(T%s)?)\" % (field(\"weeks\"), optional_field(\"days\"),\n                                    \"\".join([optional_field(key) for key in [\"hours\", \"minutes\", \"seconds\"]]))\n        return self._apply_regex(regex, input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Parses an ISO 8601 duration string into its individual time components, supporting weeks, days, hours, minutes, and seconds for flexible time interval representation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parseSimple", "line_number": 779, "body": "def _parseSimple(self, input):\n        keys = [\"weeks\", \"days\", \"hours\", \"minutes\", \"seconds\"]\n        # Give the digits a regex group name from the keys, then look for text with the first letter of the key,\n        # optionally followed by the rest of the word, with final char (the \"s\") optional\n        regex = \"\".join([r\"((?P<%s>\\d+) ?%s(%s)?(%s)? ?)?\" % (k, k[0], k[1:-1], k[-1]) for k in keys])\n        return self._apply_regex(regex, input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Parses a time duration string into components like weeks, days, hours, minutes, and seconds by matching numeric values with their respective time units. It supports flexible input formats with optional pluralization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 786, "body": "def parse(self, input):\n        \"\"\"\n        Parses a time delta from the input.\n\n        See :py:class:`TimeDeltaParameter` for details on supported formats.\n        \"\"\"\n        result = self._parseIso8601(input)\n        if not result:\n            result = self._parseSimple(input)\n        if result is not None:\n            return result\n        else:\n            raise ParameterException(\"Invalid time delta - could not parse %s\" % input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Service method of TimeDeltaParameter that parses input strings into time delta objects, supporting multiple formats and raising an error on invalid input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 800, "body": "def serialize(self, x):\n        \"\"\"\n        Converts datetime.timedelta to a string\n\n        :param x: the value to serialize.\n        \"\"\"\n        weeks = x.days // 7\n        days = x.days % 7\n        hours = x.seconds // 3600\n        minutes = (x.seconds % 3600) // 60\n        seconds = (x.seconds % 3600) % 60\n        result = \"{} w {} d {} h {} m {} s\".format(weeks, days, hours, minutes, seconds)\n        return result", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Provides a utility to convert a timedelta object into a human-readable string format showing weeks, days, hours, minutes, and seconds. Useful for presenting time durations in a clear and standardized textual form."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 814, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != TimeDeltaParameter:\n            return\n        if not isinstance(param_value, datetime.timedelta):\n            warnings.warn('Parameter \"{}\" with value \"{}\" is not of type timedelta.'.format(param_name, param_value))", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Utility method in TimeDeltaParameter that issues a warning if a given parameter is not of the expected datetime.timedelta type, helping enforce type correctness during parameter assignment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 840, "body": "def parse(self, input):\n        \"\"\"\n        Parse a task_famly using the :class:`~luigi.task_register.Register`\n        \"\"\"\n        return task_register.Register.get_task_cls(input)", "is_method": true, "class_name": "TaskParameter", "function_description": "Utility method in TaskParameter that parses and returns a task class based on the given input using a centralized task registry."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 846, "body": "def serialize(self, cls):\n        \"\"\"\n        Converts the :py:class:`luigi.task.Task` (sub) class to its family name.\n        \"\"\"\n        return cls.get_task_family()", "is_method": true, "class_name": "TaskParameter", "function_description": "Returns the family name of a given task class, providing a standardized identifier for task categorization and management within workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 882, "body": "def parse(self, s):\n        try:\n            return self._enum[s]\n        except KeyError:\n            raise ValueError('Invalid enum value - could not be parsed')", "is_method": true, "class_name": "EnumParameter", "function_description": "Core function of EnumParameter that converts a string into its corresponding enum value, raising an error if the input is invalid or unrecognized."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 888, "body": "def serialize(self, e):\n        return e.name", "is_method": true, "class_name": "EnumParameter", "function_description": "Converts an enum member to its string name representation for serialization purposes, facilitating easy storage or transmission of enum values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 925, "body": "def parse(self, s):\n        values = [] if s == '' else s.split(self._sep)\n\n        for i, v in enumerate(values):\n            try:\n                values[i] = self._enum[v]\n            except KeyError:\n                raise ValueError('Invalid enum value \"{}\" index {} - could not be parsed'.format(v, i))\n\n        return tuple(values)", "is_method": true, "class_name": "EnumListParameter", "function_description": "Parses a string into a tuple of enum members based on a separator, validating each part against defined enum values. Useful for converting delimited enum name lists into corresponding enum instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 936, "body": "def serialize(self, enum_values):\n        return self._sep.join([e.name for e in enum_values])", "is_method": true, "class_name": "EnumListParameter", "function_description": "Serializes a list of enum members into a single string by joining their names with a defined separator. This enables easy storage or transmission of enum lists as strings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "default", "line_number": 945, "body": "def default(self, obj):\n        if isinstance(obj, FrozenOrderedDict):\n            return obj.get_wrapped()\n        return json.JSONEncoder.default(self, obj)", "is_method": true, "class_name": "_DictParamEncoder", "function_description": "Method of _DictParamEncoder that defines custom JSON encoding for FrozenOrderedDict objects by extracting their wrapped data; otherwise, it defaults to standard JSON encoding."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 984, "body": "def normalize(self, value):\n        \"\"\"\n        Ensure that dictionary parameter is converted to a FrozenOrderedDict so it can be hashed.\n        \"\"\"\n        return recursively_freeze(value)", "is_method": true, "class_name": "DictParameter", "function_description": "Core utility method of DictParameter that converts a dictionary-like value into an immutable, hashable FrozenOrderedDict to enable consistent use in contexts requiring hashable parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 990, "body": "def parse(self, source):\n        \"\"\"\n        Parses an immutable and ordered ``dict`` from a JSON string using standard JSON library.\n\n        We need to use an immutable dictionary, to create a hashable parameter and also preserve the internal structure\n        of parsing. The traversal order of standard ``dict`` is undefined, which can result various string\n        representations of this parameter, and therefore a different task id for the task containing this parameter.\n        This is because task id contains the hash of parameters' JSON representation.\n\n        :param s: String to be parse\n        \"\"\"\n        # TOML based config convert params to python types itself.\n        if not isinstance(source, str):\n            return source\n        return json.loads(source, object_pairs_hook=FrozenOrderedDict)", "is_method": true, "class_name": "DictParameter", "function_description": "Core method of DictParameter that converts a JSON string into a hashable, immutable, and order-preserving dictionary, ensuring consistent parameter representation for reliably generating task identifiers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 1006, "body": "def serialize(self, x):\n        return json.dumps(x, cls=_DictParamEncoder)", "is_method": true, "class_name": "DictParameter", "function_description": "Utility method of the DictParameter class that serializes an object into a JSON string using a custom encoder, enabling consistent and customized JSON representation of complex data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 1041, "body": "def normalize(self, x):\n        \"\"\"\n        Ensure that struct is recursively converted to a tuple so it can be hashed.\n\n        :param str x: the value to parse.\n        :return: the normalized (hashable/immutable) value.\n        \"\"\"\n        return recursively_freeze(x)", "is_method": true, "class_name": "ListParameter", "function_description": "Utility method in ListParameter that converts a given input into a hashable, immutable structure by recursively freezing it, ensuring consistent normalization for use as keys or in sets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1050, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        i = json.loads(x, object_pairs_hook=FrozenOrderedDict)\n        if i is None:\n            return None\n        return list(i)", "is_method": true, "class_name": "ListParameter", "function_description": "Parses a JSON-formatted string representing an ordered collection and converts it into a list, preserving element order. Useful for extracting ordered data from string inputs in the ListParameter context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 1062, "body": "def serialize(self, x):\n        \"\"\"\n        Opposite of :py:meth:`parse`.\n\n        Converts the value ``x`` to a string.\n\n        :param x: the value to serialize.\n        \"\"\"\n        return json.dumps(x, cls=_DictParamEncoder)", "is_method": true, "class_name": "ListParameter", "function_description": "Utility method of the ListParameter class that converts a given value into its JSON string representation, enabling consistent serialization for storage or transmission purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1102, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        # Since the result of json.dumps(tuple) differs from a tuple string, we must handle either case.\n        # A tuple string may come from a config file or from cli execution.\n\n        # t = ((1, 2), (3, 4))\n        # t_str = '((1,2),(3,4))'\n        # t_json_str = json.dumps(t)\n        # t_json_str == '[[1, 2], [3, 4]]'\n        # json.loads(t_json_str) == t\n        # json.loads(t_str) == ValueError: No JSON object could be decoded\n\n        # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).\n        # ast.literal_eval(t_str) == t\n        try:\n            # loop required to parse tuple of tuples\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))", "is_method": true, "class_name": "TupleParameter", "function_description": "Utility method of the TupleParameter class that parses a string representation of nested tuples into actual tuple objects, supporting both JSON and Python literal formats for flexible input parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1194, "body": "def parse(self, s):\n        value = self._var_type(s)\n        if (self._left_op(self._min_value, value) and self._right_op(value, self._max_value)):\n            return value\n        else:\n            raise ValueError(\n                \"{s} is not in the set of {permitted_range}\".format(\n                    s=s, permitted_range=self._permitted_range))", "is_method": true, "class_name": "NumericalParameter", "function_description": "Parses and validates a string input as a numerical value within predefined bounds, raising an error if the value is outside the permitted range. This ensures inputs conform to specified numerical constraints."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1250, "body": "def parse(self, s):\n        var = self._var_type(s)\n        return self.normalize(var)", "is_method": true, "class_name": "ChoiceParameter", "function_description": "Method of ChoiceParameter that parses an input string by converting it to a specified variable type and then normalizes the result, facilitating consistent processing of choice-based parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 1254, "body": "def normalize(self, var):\n        if var in self._choices:\n            return var\n        else:\n            raise ValueError(\"{var} is not a valid choice from {choices}\".format(\n                var=var, choices=self._choices))", "is_method": true, "class_name": "ChoiceParameter", "function_description": "Validates and returns a value if it matches one of the predefined choices in ChoiceParameter; otherwise, it raises an error. This ensures only allowed options are accepted for processing or configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "field", "line_number": 768, "body": "def field(key):\n            return r\"(?P<%s>\\d+)%s\" % (key, key[0].upper())", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Utility method in TimeDeltaParameter that generates a named regex pattern to capture numeric values associated with specific time units, facilitating parsing of time duration strings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "optional_field", "line_number": 771, "body": "def optional_field(key):\n            return \"(%s)?\" % field(key)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Helper function in TimeDeltaParameter that returns an optional regex field pattern for a given key, useful for building flexible time delta parsing expressions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_upgrade_schema", "line_number": 242, "body": "def _upgrade_schema(engine):\n    \"\"\"\n    Ensure the database schema is up to date with the codebase.\n\n    :param engine: SQLAlchemy engine of the underlying database.\n    \"\"\"\n    inspector = reflection.Inspector.from_engine(engine)\n    with engine.connect() as conn:\n\n        # Upgrade 1.  Add task_id column and index to tasks\n        if 'task_id' not in [x['name'] for x in inspector.get_columns('tasks')]:\n            logger.warning('Upgrading DbTaskHistory schema: Adding tasks.task_id')\n            conn.execute('ALTER TABLE tasks ADD COLUMN task_id VARCHAR(200)')\n            conn.execute('CREATE INDEX ix_task_id ON tasks (task_id)')\n\n        # Upgrade 2. Alter value column to be TEXT, note that this is idempotent so no if-guard\n        if 'mysql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY COLUMN value TEXT')\n        elif 'oracle' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY value TEXT')\n        elif 'mssql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TEXT')\n        elif 'postgresql' in engine.dialect.name:\n            if str([x for x in inspector.get_columns('task_parameters')\n                    if x['name'] == 'value'][0]['type']) != 'TEXT':\n                conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TYPE TEXT')\n        elif 'sqlite' in engine.dialect.name:\n            # SQLite does not support changing column types. A database file will need\n            # to be used to pickup this migration change.\n            for i in conn.execute('PRAGMA table_info(task_parameters);').fetchall():\n                if i['name'] == 'value' and i['type'] != 'TEXT':\n                    logger.warning(\n                        'SQLite can not change column types. Please use a new database '\n                        'to pickup column type changes.'\n                    )\n        else:\n            logger.warning(\n                'SQLAlcheny dialect {} could not be migrated to the TEXT type'.format(\n                    engine.dialect\n                )\n            )", "is_method": false, "function_description": "Utility function that ensures the database schema matches the current codebase by adding missing columns and updating column types across different SQL dialects. It supports schema migrations for task-related tables to maintain compatibility and data integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_session", "line_number": 65, "body": "def _session(self, session=None):\n        if session:\n            yield session\n        else:\n            session = self.session_factory()\n            try:\n                yield session\n            except BaseException:\n                session.rollback()\n                raise\n            else:\n                session.commit()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Core utility method of the DbTaskHistory class that provides a managed database session context, handling session creation, commit, and rollback to ensure transactional integrity during database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_scheduled", "line_number": 88, "body": "def task_scheduled(self, task):\n        htask = self._get_task(task, status=PENDING)\n        self._add_task_event(htask, TaskEvent(event_name=PENDING, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Records that a given task has been scheduled by logging a pending status event in the task history. This method supports tracking task lifecycle changes within the DbTaskHistory system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_finished", "line_number": 92, "body": "def task_finished(self, task, successful):\n        event_name = DONE if successful else FAILED\n        htask = self._get_task(task, status=event_name)\n        self._add_task_event(htask, TaskEvent(event_name=event_name, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Marks a given task as finished with a success or failure status and records the corresponding event with a timestamp in the task history. This enables tracking and updating task completion outcomes within the DbTaskHistory system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_started", "line_number": 97, "body": "def task_started(self, task, worker_host):\n        htask = self._get_task(task, status=RUNNING, host=worker_host)\n        self._add_task_event(htask, TaskEvent(event_name=RUNNING, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Records the start of a task by updating its status to running and logging the start event with a timestamp, tracking task execution on a specific worker host."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_get_task", "line_number": 101, "body": "def _get_task(self, task, status, host=None):\n        if task.id in self.tasks:\n            htask = self.tasks[task.id]\n            htask.status = status\n            if host:\n                htask.host = host\n        else:\n            htask = self.tasks[task.id] = task_history.StoredTask(task, status, host)\n        return htask", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Core method of DbTaskHistory that retrieves or creates a stored task entry, updating its status and host information as needed, to track the lifecycle and assignment of background tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_add_task_event", "line_number": 111, "body": "def _add_task_event(self, task, event):\n        for (task_record, session) in self._find_or_create_task(task):\n            task_record.events.append(event)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Private method of DbTaskHistory that appends a specified event to all records associated with a given task, ensuring task event histories are updated consistently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_find_or_create_task", "line_number": 115, "body": "def _find_or_create_task(self, task):\n        with self._session() as session:\n            if task.record_id is not None:\n                logger.debug(\"Finding task with record_id [%d]\", task.record_id)\n                task_record = session.query(TaskRecord).get(task.record_id)\n                if not task_record:\n                    raise Exception(\"Task with record_id, but no matching Task record!\")\n                yield (task_record, session)\n            else:\n                task_record = TaskRecord(task_id=task._task.id, name=task.task_family, host=task.host)\n                for k, v in task.parameters.items():\n                    task_record.parameters[k] = TaskParameter(name=k, value=v)\n                session.add(task_record)\n                yield (task_record, session)\n            if task.host:\n                task_record.host = task.host\n        task.record_id = task_record.id", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Core method of DbTaskHistory that retrieves an existing task record by ID or creates and stores a new task record with parameters in the database, ensuring task persistence and consistent task identification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_by_parameters", "line_number": 133, "body": "def find_all_by_parameters(self, task_name, session=None, **task_params):\n        \"\"\"\n        Find tasks with the given task_name and the same parameters as the kwargs.\n        \"\"\"\n        with self._session(session) as session:\n            query = session.query(TaskRecord).join(TaskEvent).filter(TaskRecord.name == task_name)\n            for k, v in task_params.items():\n                alias = sqlalchemy.orm.aliased(TaskParameter)\n                query = query.join(alias).filter(alias.name == k, alias.value == v)\n\n            tasks = query.order_by(TaskEvent.ts)\n            for task in tasks:\n                # Sanity check\n                assert all(k in task.parameters and v == str(task.parameters[k].value) for k, v in task_params.items())\n\n                yield task", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Core method of DbTaskHistory that retrieves all tasks matching a specified name and exact parameter values, enabling filtered historical task lookups based on dynamic criteria."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_by_name", "line_number": 150, "body": "def find_all_by_name(self, task_name, session=None):\n        \"\"\"\n        Find all tasks with the given task_name.\n        \"\"\"\n        return self.find_all_by_parameters(task_name, session)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Provides a way to retrieve all task records matching a specific name, enabling clients to query task history by task identifier."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_latest_runs", "line_number": 156, "body": "def find_latest_runs(self, session=None):\n        \"\"\"\n        Return tasks that have been updated in the past 24 hours.\n        \"\"\"\n        with self._session(session) as session:\n            yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n            return session.query(TaskRecord).\\\n                join(TaskEvent).\\\n                filter(TaskEvent.ts >= yesterday).\\\n                group_by(TaskRecord.id, TaskEvent.event_name, TaskEvent.ts).\\\n                order_by(TaskEvent.ts.desc()).\\\n                all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Provides a list of tasks updated within the last 24 hours, supporting recent task activity monitoring and management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_runs", "line_number": 169, "body": "def find_all_runs(self, session=None):\n        \"\"\"\n        Return all tasks that have been updated.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Returns all task records with updates from the database, providing a complete history of task runs for analysis or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_events", "line_number": 176, "body": "def find_all_events(self, session=None):\n        \"\"\"\n        Return all running/failed/done events.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskEvent).all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Method of DbTaskHistory that retrieves all task events regardless of their status, supporting comprehensive tracking of running, failed, or completed events for analysis or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_task_by_id", "line_number": 183, "body": "def find_task_by_id(self, id, session=None):\n        \"\"\"\n        Find task with the given record ID.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).get(id)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Utility method in DbTaskHistory that retrieves a task record by its unique ID from the database, optionally using a provided session. It enables direct access to specific task entries for further processing or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 200, "body": "def __repr__(self):\n        return \"TaskParameter(task_id=%d, name=%s, value=%s)\" % (self.task_id, self.name, self.value)", "is_method": true, "class_name": "TaskParameter", "function_description": "Provides a string representation of a TaskParameter instance, displaying its task ID, name, and value for easy identification and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 214, "body": "def __repr__(self):\n        return \"TaskEvent(task_id=%s, event_name=%s, ts=%s\" % (self.task_id, self.event_name, self.ts)", "is_method": true, "class_name": "TaskEvent", "function_description": "Returns a string representation of the TaskEvent instance showing its task ID, event name, and timestamp for easier debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 238, "body": "def __repr__(self):\n        return \"TaskRecord(name=%s, host=%s)\" % (self.name, self.host)", "is_method": true, "class_name": "TaskRecord", "function_description": "Returns a concise string representation of the TaskRecord instance, showing its name and host attributes. Useful for debugging and logging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline.py", "function": "luigi_run", "line_number": 8, "body": "def luigi_run(argv=sys.argv[1:]):\n    run_with_retcodes(argv)", "is_method": false, "function_description": "This function executes a Luigi pipeline with command-line arguments, facilitating automated task scheduling and workflow management. It acts as a simple entry point to run Luigi commands programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline.py", "function": "luigid", "line_number": 12, "body": "def luigid(argv=sys.argv[1:]):\n    import luigi.server\n    import luigi.process\n    import luigi.configuration\n    parser = argparse.ArgumentParser(description=u'Central luigi server')\n    parser.add_argument(u'--background', help=u'Run in background mode', action='store_true')\n    parser.add_argument(u'--pidfile', help=u'Write pidfile')\n    parser.add_argument(u'--logdir', help=u'log directory')\n    parser.add_argument(u'--state-path', help=u'Pickled state file')\n    parser.add_argument(u'--address', help=u'Listening interface')\n    parser.add_argument(u'--unix-socket', help=u'Unix socket path')\n    parser.add_argument(u'--port', default=8082, help=u'Listening port')\n\n    opts = parser.parse_args(argv)\n\n    if opts.state_path:\n        config = luigi.configuration.get_config()\n        config.set('scheduler', 'state_path', opts.state_path)\n\n    DaemonLogging.setup(opts)\n    if opts.background:\n        luigi.process.daemonize(luigi.server.run, api_port=opts.port,\n                                address=opts.address, pidfile=opts.pidfile,\n                                logdir=opts.logdir, unix_socket=opts.unix_socket)\n    else:\n        luigi.server.run(api_port=opts.port, address=opts.address, unix_socket=opts.unix_socket)", "is_method": false, "function_description": "Function that parses command-line arguments and starts the central Luigi server, optionally running it as a background daemon with configurable networking, logging, and state persistence options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move_to_final_destination", "line_number": 39, "body": "def move_to_final_destination(self):\n        os.rename(self.tmp_path, self.path)", "is_method": true, "class_name": "atomic_file", "function_description": "Method of the atomic_file class that finalizes a file write operation by renaming a temporary file to its intended final path, ensuring atomic file replacement or creation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "generate_tmp_path", "line_number": 42, "body": "def generate_tmp_path(self, path):\n        return path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)", "is_method": true, "class_name": "atomic_file", "function_description": "Returns a temporary file path by appending a unique randomized suffix to the given path. Useful for safely creating temporary files without name collisions during file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "copy", "line_number": 53, "body": "def copy(self, old_path, new_path, raise_if_exists=False):\n        if raise_if_exists and os.path.exists(new_path):\n            raise RuntimeError('Destination exists: %s' % new_path)\n        d = os.path.dirname(new_path)\n        if d and not os.path.exists(d):\n            self.mkdir(d)\n        shutil.copy(old_path, new_path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Core file operation method of the LocalFileSystem class that copies a file to a new location, optionally preventing overwrites, and automatically creates target directories if they do not exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "exists", "line_number": 61, "body": "def exists(self, path):\n        return os.path.exists(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Utility method of the LocalFileSystem class that checks if a given file or directory path exists in the local filesystem. It enables validation of file presence before performing file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "mkdir", "line_number": 64, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise FileAlreadyExists()\n            elif not self.isdir(path):\n                raise NotADirectory()\n            else:\n                return\n\n        if parents:\n            try:\n                os.makedirs(path)\n            except OSError as err:\n                # somebody already created the path\n                if err.errno != errno.EEXIST:\n                    raise\n        else:\n            if not os.path.exists(os.path.dirname(path)):\n                raise MissingParentDirectory()\n            os.mkdir(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Creates a directory at the specified path, optionally creating parent directories and handling existing paths with configurable error behavior. It ensures directory structure setup for file system operations within the LocalFileSystem context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "isdir", "line_number": 85, "body": "def isdir(self, path):\n        return os.path.isdir(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Method of LocalFileSystem that checks if a given path corresponds to an existing directory, enabling functions to verify directory presence before performing filesystem operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "listdir", "line_number": 88, "body": "def listdir(self, path):\n        for dir_, _, files in os.walk(path):\n            assert dir_.startswith(path)\n            for name in files:\n                yield os.path.join(dir_, name)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Provides an iterator over all file paths within a directory and its subdirectories on the local filesystem, enabling recursive file listing for downstream file processing or management tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "remove", "line_number": 94, "body": "def remove(self, path, recursive=True):\n        if recursive and self.isdir(path):\n            shutil.rmtree(path)\n        else:\n            os.remove(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Utility method of LocalFileSystem that deletes a file or directory at a given path, with optional recursive removal for directories. It enables managing local filesystem cleanup operations programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move", "line_number": 100, "body": "def move(self, old_path, new_path, raise_if_exists=False):\n        \"\"\"\n        Move file atomically. If source and destination are located\n        on different filesystems, atomicity is approximated\n        but cannot be guaranteed.\n        \"\"\"\n        if raise_if_exists and os.path.exists(new_path):\n            raise FileAlreadyExists('Destination exists: %s' % new_path)\n        d = os.path.dirname(new_path)\n        if d and not os.path.exists(d):\n            self.mkdir(d)\n        try:\n            os.rename(old_path, new_path)\n        except OSError as err:\n            if err.errno == errno.EXDEV:\n                new_path_tmp = '%s-%09d' % (new_path, random.randint(0, 999999999))\n                shutil.copy(old_path, new_path_tmp)\n                os.rename(new_path_tmp, new_path)\n                os.remove(old_path)\n            else:\n                raise err", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Method of the LocalFileSystem class that moves a file atomically between paths, ensuring the destination directory exists and optionally raising an error if the target file already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "rename_dont_move", "line_number": 122, "body": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Rename ``path`` to ``dest``, but don't move it into the ``dest``\n        folder (if it is a folder). This method is just a wrapper around the\n        ``move`` method of LocalTarget.\n        \"\"\"\n        self.move(path, dest, raise_if_exists=True)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Method of LocalFileSystem that renames a file or folder to a new name without relocating it into the destination directory, ensuring no overwriting by raising an error if the destination exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "makedirs", "line_number": 146, "body": "def makedirs(self):\n        \"\"\"\n        Create all parent folders if they do not exist.\n        \"\"\"\n        normpath = os.path.normpath(self.path)\n        parentfolder = os.path.dirname(normpath)\n        if parentfolder:\n            try:\n                os.makedirs(parentfolder)\n            except OSError:\n                pass", "is_method": true, "class_name": "LocalTarget", "function_description": "Creates all necessary parent directories for the target's file path if they do not already exist, ensuring the directory structure is prepared for file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "open", "line_number": 158, "body": "def open(self, mode='r'):\n        rwmode = mode.replace('b', '').replace('t', '')\n        if rwmode == 'w':\n            self.makedirs()\n            return self.format.pipe_writer(atomic_file(self.path))\n\n        elif rwmode == 'r':\n            fileobj = FileWrapper(io.BufferedReader(io.FileIO(self.path, mode)))\n            return self.format.pipe_reader(fileobj)\n\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "LocalTarget", "function_description": "Method of LocalTarget that opens the target file for reading or writing with atomicity and format-specific processing, ensuring directories exist before writing. It supports safely accessing file content in chosen mode with automatic resource handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move", "line_number": 171, "body": "def move(self, new_path, raise_if_exists=False):\n        self.fs.move(self.path, new_path, raise_if_exists=raise_if_exists)", "is_method": true, "class_name": "LocalTarget", "function_description": "Core function of the LocalTarget class that moves a file or directory to a new location, optionally raising an error if the destination already exists. It facilitates file system manipulation by relocating data as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move_dir", "line_number": 174, "body": "def move_dir(self, new_path):\n        self.move(new_path)", "is_method": true, "class_name": "LocalTarget", "function_description": "Moves the directory represented by this LocalTarget instance to a new location specified by new_path. This method provides a simple interface for relocating directories within the local file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "remove", "line_number": 177, "body": "def remove(self):\n        self.fs.remove(self.path)", "is_method": true, "class_name": "LocalTarget", "function_description": "Removes the specified file or directory at the given path from the file system managed by this LocalTarget instance. This is typically used to delete local resources after processing or to clean up temporary data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "copy", "line_number": 180, "body": "def copy(self, new_path, raise_if_exists=False):\n        self.fs.copy(self.path, new_path, raise_if_exists)", "is_method": true, "class_name": "LocalTarget", "function_description": "Utility method of LocalTarget that copies the file from its current path to a new location, optionally raising an error if the destination file already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "fn", "line_number": 184, "body": "def fn(self):\n        warnings.warn(\"Use LocalTarget.path to reference filename\", DeprecationWarning, stacklevel=2)\n        return self.path", "is_method": true, "class_name": "LocalTarget", "function_description": "This method provides a deprecated way to access the file path, issuing a warning to encourage use of the preferred attribute instead. It supports backward compatibility while guiding users toward updated usage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "__del__", "line_number": 188, "body": "def __del__(self):\n        if self.is_tmp and self.exists():\n            self.remove()", "is_method": true, "class_name": "LocalTarget", "function_description": "Destructor method of the LocalTarget class that automatically removes temporary files if they exist when the object is deleted, ensuring cleanup of temporary resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "get_instance", "line_number": 38, "body": "def get_instance(cls):\n        \"\"\" Singleton getter \"\"\"\n        return cls._instance", "is_method": true, "class_name": "CmdlineParser", "function_description": "Returns the singleton instance of the CmdlineParser class, ensuring only one parser instance is used throughout the application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "global_instance", "line_number": 44, "body": "def global_instance(cls, cmdline_args, allow_override=False):\n        \"\"\"\n        Meant to be used as a context manager.\n        \"\"\"\n        orig_value = cls._instance\n        assert (orig_value is None) or allow_override\n        new_value = None\n        try:\n            new_value = CmdlineParser(cmdline_args)\n            cls._instance = new_value\n            yield new_value\n        finally:\n            assert cls._instance is new_value\n            cls._instance = orig_value", "is_method": true, "class_name": "CmdlineParser", "function_description": "Provides a context manager to temporarily set or override the global CmdlineParser instance based on given command-line arguments, ensuring the original instance is restored afterward."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_build_parser", "line_number": 81, "body": "def _build_parser(root_task=None, help_all=False):\n        parser = argparse.ArgumentParser(add_help=False)\n\n        # Unfortunately, we have to set it as optional to argparse, so we can\n        # parse out stuff like `--module` before we call for `--help`.\n        parser.add_argument('root_task',\n                            nargs='?',\n                            help='Task family to run. Is not optional.',\n                            metavar='Required root task',\n                            )\n\n        for task_name, is_without_section, param_name, param_obj in Register.get_all_params():\n            is_the_root_task = task_name == root_task\n            help = param_obj.description if any((is_the_root_task, help_all, param_obj.always_in_help)) else argparse.SUPPRESS\n            flag_name_underscores = param_name if is_without_section else task_name + '_' + param_name\n            global_flag_name = '--' + flag_name_underscores.replace('_', '-')\n            parser.add_argument(global_flag_name,\n                                help=help,\n                                **param_obj._parser_kwargs(param_name, task_name)\n                                )\n            if is_the_root_task:\n                local_flag_name = '--' + param_name.replace('_', '-')\n                parser.add_argument(local_flag_name,\n                                    help=help,\n                                    **param_obj._parser_kwargs(param_name)\n                                    )\n\n        return parser", "is_method": true, "class_name": "CmdlineParser", "function_description": "Constructs a customized command-line argument parser that dynamically includes task-specific and global flags based on registered parameters, supporting flexible help display and root task specification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "get_task_obj", "line_number": 110, "body": "def get_task_obj(self):\n        \"\"\"\n        Get the task object\n        \"\"\"\n        return self._get_task_cls()(**self._get_task_kwargs())", "is_method": true, "class_name": "CmdlineParser", "function_description": "Returns an instance of the task class with configured parameters, providing a ready-to-use task object based on the command line parser's setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_get_task_cls", "line_number": 116, "body": "def _get_task_cls(self):\n        \"\"\"\n        Get the task class\n        \"\"\"\n        return Register.get_task_cls(self.known_args.root_task)", "is_method": true, "class_name": "CmdlineParser", "function_description": "Utility method in CmdlineParser that retrieves the class associated with the specified root task, enabling dynamic task handling based on command-line input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_get_task_kwargs", "line_number": 122, "body": "def _get_task_kwargs(self):\n        \"\"\"\n        Get the local task arguments as a dictionary. The return value is in\n        the form ``dict(my_param='my_value', ...)``\n        \"\"\"\n        res = {}\n        for (param_name, param_obj) in self._get_task_cls().get_params():\n            attr = getattr(self.known_args, param_name)\n            if attr:\n                res.update(((param_name, param_obj.parse(attr)),))\n\n        return res", "is_method": true, "class_name": "CmdlineParser", "function_description": "Provides a dictionary of parsed command-line task parameters, converting argument values into appropriately typed keyword arguments for task execution based on task class definitions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_attempt_load_module", "line_number": 136, "body": "def _attempt_load_module(known_args):\n        \"\"\"\n        Load the --module parameter\n        \"\"\"\n        module = known_args.core_module\n        if module:\n            __import__(module)", "is_method": true, "class_name": "CmdlineParser", "function_description": "Private method in CmdlineParser that attempts to dynamically import a module specified by the --module command-line argument, enabling runtime extension or customization based on user input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_possibly_exit_with_help", "line_number": 145, "body": "def _possibly_exit_with_help(parser, known_args):\n        \"\"\"\n        Check if the user passed --help[-all], if so, print a message and exit.\n        \"\"\"\n        if known_args.core_help or known_args.core_help_all:\n            parser.print_help()\n            sys.exit()", "is_method": true, "class_name": "CmdlineParser", "function_description": "Helper method in CmdlineParser that detects help flags and, if present, displays the help message then terminates the program immediately."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/metrics.py", "function": "get", "line_number": 13, "body": "def get(cls, which):\n        if which == MetricsCollectors.none:\n            return NoMetricsCollector()\n        elif which == MetricsCollectors.datadog:\n            from luigi.contrib.datadog_metric import DatadogMetricsCollector\n            return DatadogMetricsCollector()\n        elif which == MetricsCollectors.prometheus:\n            from luigi.contrib.prometheus_metric import PrometheusMetricsCollector\n            return PrometheusMetricsCollector()\n        else:\n            raise ValueError(\"MetricsCollectors value ' {0} ' isn't supported\", which)", "is_method": true, "class_name": "MetricsCollectors", "function_description": "Factory method of MetricsCollectors that returns an instance of the specified metrics collector implementation or raises an error for unsupported types. It enables selecting different metrics collection services dynamically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "get_default_format", "line_number": 514, "body": "def get_default_format():\n    return Text", "is_method": false, "function_description": "Returns the default format type used by the system, providing a standard reference for format-related operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 36, "body": "def __getattr__(self, name):\n        # forward calls to 'write', 'close' and other methods not defined below\n        return getattr(self._subpipe, name)", "is_method": true, "class_name": "FileWrapper", "function_description": "Delegates attribute access to an internal subpipe object, allowing the FileWrapper to expose its methods dynamically. This enables seamless forwarding of calls like 'write' or 'close' to the wrapped file-like resource."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 40, "body": "def __enter__(self, *args, **kwargs):\n        # instead of returning whatever is returned by __enter__ on the subpipe\n        # this returns self, so whatever custom injected methods are still available\n        # this might cause problems with custom file_objects, but seems to work\n        # fine with standard python `file` objects which is the only default use\n        return self", "is_method": true, "class_name": "FileWrapper", "function_description": "Provides context management support by returning the FileWrapper instance itself upon entering a with-statement, ensuring custom methods on the wrapper remain accessible during the context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 47, "body": "def __exit__(self, *args, **kwargs):\n        return self._subpipe.__exit__(*args, **kwargs)", "is_method": true, "class_name": "FileWrapper", "function_description": "This method ensures proper cleanup by delegating exit operations to an internal resource, enabling use of the FileWrapper in context management (with statements) for safe resource handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 50, "body": "def __iter__(self):\n        return iter(self._subpipe)", "is_method": true, "class_name": "FileWrapper", "function_description": "Enables iteration over the FileWrapper by returning an iterator for its underlying data pipeline, allowing clients to loop through contained elements seamlessly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "create_subprocess", "line_number": 91, "body": "def create_subprocess(self, command):\n        \"\"\"\n        http://www.chiark.greenend.org.uk/ucgi/~cjwatson/blosxom/2009-07-02-python-sigpipe.html\n        \"\"\"\n\n        def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n\n        return subprocess.Popen(command,\n                                stdin=self._input_pipe,\n                                stdout=subprocess.PIPE,\n                                preexec_fn=subprocess_setup,\n                                close_fds=True)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Creates a subprocess with customized signal handling and input/output pipe setup, enabling controlled inter-process communication within the InputPipeProcessWrapper class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_finish", "line_number": 107, "body": "def _finish(self):\n        # Need to close this before input_pipe to get all SIGPIPE messages correctly\n        self._process.stdout.close()\n        if not self._original_input and os.path.exists(self._tmp_file):\n            os.remove(self._tmp_file)\n\n        if self._input_pipe is not None:\n            self._input_pipe.close()\n\n        self._process.wait()  # deadlock?\n        if self._process.returncode not in (0, 141, 128 - 141):\n            # 141 == 128 + 13 == 128 + SIGPIPE - normally processes exit with 128 + {reiceived SIG}\n            # 128 - 141 == -13 == -SIGPIPE, sometimes python receives -13 for some subprocesses\n            raise RuntimeError('Error reading from pipe. Subcommand exited with non-zero exit status %s.' % self._process.returncode)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Finalization method in InputPipeProcessWrapper that closes related pipes, cleans up temporary files, waits for process completion, and raises an error if the subprocess ends with an unexpected exit code."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "close", "line_number": 122, "body": "def close(self):\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Method of InputPipeProcessWrapper that finalizes and closes the input processing pipe, ensuring all resources are properly released."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 125, "body": "def __del__(self):\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Destructor method in InputPipeProcessWrapper that ensures cleanup by finalizing resources when an instance is destroyed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 128, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Enables the InputPipeProcessWrapper instance to be used as a context manager, supporting the with statement for resource management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_abort", "line_number": 131, "body": "def _abort(self):\n        \"\"\"\n        Call _finish, but eat the exception (if any).\n        \"\"\"\n        try:\n            self._finish()\n        except KeyboardInterrupt:\n            raise\n        except BaseException:\n            pass", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Internal helper method of InputPipeProcessWrapper that attempts to finalize processing while silently ignoring all exceptions except KeyboardInterrupt, ensuring graceful abortion without error propagation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 142, "body": "def __exit__(self, type, value, traceback):\n        if type:\n            self._abort()\n        else:\n            self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Special method in InputPipeProcessWrapper ensuring proper cleanup by either aborting on errors or finishing successfully when exiting a context block. It manages resource finalization based on whether an exception occurred."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 148, "body": "def __getattr__(self, name):\n        if name in ['_process', '_input_pipe']:\n            raise AttributeError(name)\n        try:\n            return getattr(self._process.stdout, name)\n        except AttributeError:\n            return getattr(self._input_pipe, name)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Overrides attribute access to delegate calls to either the wrapped process's stdout or the input pipe, providing seamless attribute forwarding while protecting internal attributes. This supports flexible stream handling within the InputPipeProcessWrapper class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 156, "body": "def __iter__(self):\n        for line in self._process.stdout:\n            yield line\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Iterator method of the InputPipeProcessWrapper class that yields lines from a subprocess's standard output stream and performs cleanup after iteration completes. It facilitates streaming output processing from an external process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "readable", "line_number": 161, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Returns True indicating that the process wrapper is readable, likely signaling readiness for input operations or data retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writable", "line_number": 164, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Indicates that the InputPipeProcessWrapper instance does not support writing operations. It provides a simple read-only status check for the pipe process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "seekable", "line_number": 167, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Indicates that the InputPipeProcessWrapper does not support seeking operations, specifying its data stream cannot be repositioned. This informs consumers about the non-seekable nature of the input source."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 184, "body": "def write(self, *args, **kwargs):\n        self._process.stdin.write(*args, **kwargs)\n        self._flushcount += 1\n        if self._flushcount == self.WRITES_BEFORE_FLUSH:\n            self._process.stdin.flush()\n            self._flushcount = 0", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Utility method in OutputPipeProcessWrapper that writes data to a subprocess's standard input stream and manages flushing after a set number of writes to ensure efficient data transmission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writeLine", "line_number": 191, "body": "def writeLine(self, line):\n        assert '\\n' not in line\n        self.write(line + '\\n')", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Utility method in OutputPipeProcessWrapper that writes a single line followed by a newline character, ensuring the input line itself does not contain newline characters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_finish", "line_number": 195, "body": "def _finish(self):\n        \"\"\"\n        Closes and waits for subprocess to exit.\n        \"\"\"\n        if self._process.returncode is None:\n            self._process.stdin.flush()\n            self._process.stdin.close()\n            self._process.wait()\n            self.closed = True", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Utility method in OutputPipeProcessWrapper that ensures the subprocess's input stream is flushed and closed, then waits for the subprocess to terminate cleanly, finalizing its lifecycle."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 205, "body": "def __del__(self):\n        if not self.closed:\n            self.abort()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Destructor method that ensures the OutputPipeProcessWrapper cleans up resources by aborting any ongoing process if not already closed when the instance is being destroyed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 209, "body": "def __exit__(self, type, value, traceback):\n        if type is None:\n            self.close()\n        else:\n            self.abort()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Handles resource cleanup by closing normally on successful exit or aborting if an exception occurs within the OutputPipeProcessWrapper context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 215, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Enables the OutputPipeProcessWrapper instance to be used as a context manager, ensuring proper resource handling within a with-statement block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "close", "line_number": 218, "body": "def close(self):\n        self._finish()\n        if self._process.returncode == 0:\n            if self._output_pipe is not None:\n                self._output_pipe.close()\n        else:\n            raise RuntimeError('Error when executing command %s' % self._command)", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Method of OutputPipeProcessWrapper that finalizes the process and closes the output pipe if successful; raises an error if the process ended with a failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "abort", "line_number": 226, "body": "def abort(self):\n        self._finish()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Service method in OutputPipeProcessWrapper that terminates the ongoing process or operation by invoking internal completion procedures. It enables clean abortion of processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 229, "body": "def __getattr__(self, name):\n        if name in ['_process', '_output_pipe']:\n            raise AttributeError(name)\n        try:\n            return getattr(self._process.stdin, name)\n        except AttributeError:\n            return getattr(self._output_pipe, name)", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Provides dynamic attribute access by delegating to either the wrapped process's stdin or an output pipe, enabling seamless interaction with these internal components while protecting specific private attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "readable", "line_number": 237, "body": "def readable(self):\n        return False", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Returns False to indicate the output pipe process is not readable, signaling that reading from this process stream is unsupported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writable", "line_number": 240, "body": "def writable(self):\n        return True", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Indicates that the OutputPipeProcessWrapper instance supports write operations, signaling it's writable for output handling or data streaming purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "seekable", "line_number": 243, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Indicates whether the wrapped output pipe process supports seeking. Always returns False, signaling non-seekable behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 256, "body": "def __getattr__(self, name):\n        if name == '_stream':\n            raise AttributeError(name)\n        return getattr(self._stream, name)", "is_method": true, "class_name": "BaseWrapper", "function_description": "This method provides dynamic access to attributes of the wrapped _stream object, enabling transparent delegation while protecting the internal _stream attribute from direct access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 261, "body": "def __enter__(self):\n        self._stream.__enter__()\n        return self", "is_method": true, "class_name": "BaseWrapper", "function_description": "Enables use of the BaseWrapper instance as a context manager, initializing the underlying stream resource upon entering the context. This facilitates safe and convenient resource management with the with statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 265, "body": "def __exit__(self, *args):\n        self._stream.__exit__(*args)", "is_method": true, "class_name": "BaseWrapper", "function_description": "This method ensures proper resource cleanup by delegating exit behavior to an underlying stream, supporting use of the class in context management (with statement) for safe acquisition and release of resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 268, "body": "def __iter__(self):\n        try:\n            for line in self._stream:\n                yield line\n        finally:\n            self.close()", "is_method": true, "class_name": "BaseWrapper", "function_description": "Enables iteration over a streaming data source, yielding each line until completion, then ensures proper resource cleanup by closing the stream. This facilitates safe and convenient processing of streamed content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "read", "line_number": 288, "body": "def read(self, n=-1):\n        b = self._stream.read(n)\n\n        if self.newline == b'':\n            return b\n\n        if self.newline is None:\n            newline = b'\\n'\n\n        return re.sub(b'(\\n|\\r\\n|\\r)', newline, b)", "is_method": true, "class_name": "NewlineWrapper", "function_description": "Core method of NewlineWrapper that reads data from an underlying stream and normalizes all newline characters to a specified newline style, supporting consistent newline handling across different platforms or formats."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writelines", "line_number": 299, "body": "def writelines(self, lines):\n        if self.newline is None or self.newline == '':\n            newline = os.linesep.encode('ascii')\n        else:\n            newline = self.newline\n\n        self._stream.writelines(\n            (re.sub(b'(\\n|\\r\\n|\\r)', newline, line) for line in lines)\n        )", "is_method": true, "class_name": "NewlineWrapper", "function_description": "This method writes multiple lines to a stream, replacing all line endings with a consistent newline sequence. It ensures uniform line breaks based on the configured newline setting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 309, "body": "def write(self, b):\n        if self.newline is None or self.newline == '':\n            newline = os.linesep.encode('ascii')\n        else:\n            newline = self.newline\n\n        self._stream.write(re.sub(b'(\\n|\\r\\n|\\r)', newline, b))", "is_method": true, "class_name": "NewlineWrapper", "function_description": "Core method of the NewlineWrapper class that writes byte data to a stream while normalizing all newline characters to a specified byte sequence, ensuring consistent newline formatting across different platforms."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 328, "body": "def write(self, b):\n        self._stream.write(self._convert(b))", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Method of the MixedUnicodeBytesWrapper class that writes a converted byte or string sequence to the underlying stream, enabling seamless handling of mixed Unicode and byte inputs during output operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writelines", "line_number": 331, "body": "def writelines(self, lines):\n        self._stream.writelines((self._convert(line) for line in lines))", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Method of MixedUnicodeBytesWrapper that writes multiple lines to an underlying stream, converting each line to a specific format before writing. It enables seamless writing of mixed Unicode and byte lines to the wrapped stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_convert", "line_number": 334, "body": "def _convert(self, b):\n        if isinstance(b, str):\n            b = b.encode(self.encoding)\n            warnings.warn('Writing unicode to byte stream', stacklevel=2)\n        return b", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Converts input strings to bytes using the instance's encoding, warning when converting Unicode to bytes. It ensures consistent byte representation for data handling in mixed Unicode and byte streams."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__rshift__", "line_number": 354, "body": "def __rshift__(a, b):\n        return ChainFormat(a, b)", "is_method": true, "class_name": "Format", "function_description": "Defines a right-shift operator to combine two format objects into a chained format, enabling sequential formatting operations within the Format class context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 387, "body": "def pipe_reader(self, input_pipe):\n        for x in reversed(self.args):\n            input_pipe = x.pipe_reader(input_pipe)\n        return input_pipe", "is_method": true, "class_name": "ChainFormat", "function_description": "Core utility method of the ChainFormat class that applies a series of transformations to input data by processing it through multiple chained readers in reverse order. It enables composable and sequential data processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 392, "body": "def pipe_writer(self, output_pipe):\n        for x in reversed(self.args):\n            output_pipe = x.pipe_writer(output_pipe)\n        return output_pipe", "is_method": true, "class_name": "ChainFormat", "function_description": "Core utility of the ChainFormat class that sequentially processes its arguments in reverse order through pipe_writer calls, enabling composable data transformations or output formatting in a pipeline structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 400, "body": "def __exit__(self, *args):\n        # io.TextIOWrapper close the file on __exit__, let the underlying file decide\n        if not self.closed and self.writable():\n            super(TextWrapper, self).flush()\n\n        self._stream.__exit__(*args)", "is_method": true, "class_name": "TextWrapper", "function_description": "Handles cleanup when exiting a context by flushing writable text streams and delegating closure responsibility to the underlying stream, ensuring proper resource management in text I/O operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 407, "body": "def __del__(self, *args):\n        # io.TextIOWrapper close the file on __del__, let the underlying file decide\n        if not self.closed and self.writable():\n            super(TextWrapper, self).flush()\n\n        try:\n            self._stream.__del__(*args)\n        except AttributeError:\n            pass", "is_method": true, "class_name": "TextWrapper", "function_description": "Destructor method for TextWrapper that ensures buffered data is flushed before object deletion, delegating stream cleanup while preventing errors if underlying stream lacks a destructor."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 424, "body": "def __getattr__(self, name):\n        if name == '_stream':\n            raise AttributeError(name)\n        return getattr(self._stream, name)", "is_method": true, "class_name": "TextWrapper", "function_description": "Provides attribute access delegation to an internal stream object, except blocking access to the '_stream' attribute to prevent direct manipulation. This supports transparent forwarding of method calls to the wrapped stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 429, "body": "def __enter__(self):\n        self._stream.__enter__()\n        return self", "is_method": true, "class_name": "TextWrapper", "function_description": "Enables the TextWrapper instance to be used as a context manager, managing resource acquisition and release automatically during with statements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 435, "body": "def pipe_reader(self, input_pipe):\n        return input_pipe", "is_method": true, "class_name": "NopFormat", "function_description": "Pass-through method in NopFormat that returns the given input stream unchanged, serving as a placeholder or default operation in data processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 438, "body": "def pipe_writer(self, output_pipe):\n        return output_pipe", "is_method": true, "class_name": "NopFormat", "function_description": "Returns the given output pipe unchanged, effectively serving as a pass-through writer in data processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 448, "body": "def pipe_reader(self, input_pipe):\n        return self.wrapper_cls(input_pipe, *self.args, **self.kwargs)", "is_method": true, "class_name": "WrappedFormat", "function_description": "Returns an instance of the wrapper class initialized with a given input pipe and preset arguments, facilitating flexible wrapping or processing of streamed data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 451, "body": "def pipe_writer(self, output_pipe):\n        return self.wrapper_cls(output_pipe, *self.args, **self.kwargs)", "is_method": true, "class_name": "WrappedFormat", "function_description": "Returns a wrapped writer instance that directs output through a specified pipe, applying predefined arguments and keyword arguments. This enables flexible output handling within the WrappedFormat context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 483, "body": "def pipe_reader(self, input_pipe):\n        return InputPipeProcessWrapper(['gunzip'], input_pipe)", "is_method": true, "class_name": "GzipFormat", "function_description": "Provides a wrapper to decompress data from an input pipe using the 'gunzip' utility, enabling seamless processing of gzip-compressed streams within a pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 486, "body": "def pipe_writer(self, output_pipe):\n        args = ['gzip']\n        if self.compression_level is not None:\n            args.append('-' + str(int(self.compression_level)))\n        return OutputPipeProcessWrapper(args, output_pipe)", "is_method": true, "class_name": "GzipFormat", "function_description": "Provides a writable pipe that compresses data using gzip with an optional compression level, enabling streaming compression through an output pipe."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 498, "body": "def pipe_reader(self, input_pipe):\n        return InputPipeProcessWrapper(['bzcat'], input_pipe)", "is_method": true, "class_name": "Bzip2Format", "function_description": "Utility method in Bzip2Format that wraps a pipe to decompress bzip2-compressed input streams using the 'bzcat' command for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 501, "body": "def pipe_writer(self, output_pipe):\n        return OutputPipeProcessWrapper(['bzip2'], output_pipe)", "is_method": true, "class_name": "Bzip2Format", "function_description": "Returns a process wrapper that compresses data using bzip2, directing the output to the given pipe. This enables seamless bzip2 compression within data pipelines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "subprocess_setup", "line_number": 96, "body": "def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Sets up the subprocess environment by restoring the default SIGPIPE signal handler, ensuring non-Python subprocesses receive the expected signal behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "getpcmd", "line_number": 30, "body": "def getpcmd(pid):\n    \"\"\"\n    Returns command of process.\n\n    :param pid:\n    \"\"\"\n    if os.name == \"nt\":\n        # Use wmic command instead of ps on Windows.\n        cmd = 'wmic path win32_process where ProcessID=%s get Commandline 2> nul' % (pid, )\n        with os.popen(cmd, 'r') as p:\n            lines = [line for line in p.readlines() if line.strip(\"\\r\\n \") != \"\"]\n            if lines:\n                _, val = lines\n                return val\n    elif sys.platform == \"darwin\":\n        # Use pgrep instead of /proc on macOS.\n        pidfile = \".%d.pid\" % (pid, )\n        with open(pidfile, 'w') as f:\n            f.write(str(pid))\n        try:\n            p = Popen(['pgrep', '-lf', '-F', pidfile], stdout=PIPE)\n            stdout, _ = p.communicate()\n            line = stdout.decode('utf8').strip()\n            if line:\n                _, scmd = line.split(' ', 1)\n                return scmd\n        finally:\n            os.unlink(pidfile)\n    else:\n        # Use the /proc filesystem\n        # At least on android there have been some issues with not all\n        # process infos being readable. In these cases using the `ps` command\n        # worked. See the pull request at\n        # https://github.com/spotify/luigi/pull/1876\n        try:\n            with open('/proc/{0}/cmdline'.format(pid), 'r') as fh:\n                return fh.read().replace('\\0', ' ').rstrip()\n        except IOError:\n            # the system may not allow reading the command line\n            # of a process owned by another user\n            pass\n\n    # Fallback instead of None, for e.g. Cygwin where -o is an \"unknown option\" for the ps command:\n    return '[PROCESS_WITH_PID={}]'.format(pid)", "is_method": false, "function_description": "Utility function that retrieves and returns the command line string used to start a process identified by its PID, supporting multiple operating systems with fallbacks for restricted access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "get_info", "line_number": 76, "body": "def get_info(pid_dir, my_pid=None):\n    # Check the name and pid of this process\n    if my_pid is None:\n        my_pid = os.getpid()\n\n    my_cmd = getpcmd(my_pid)\n    cmd_hash = my_cmd.encode('utf8')\n    pid_file = os.path.join(pid_dir, hashlib.md5(cmd_hash).hexdigest()) + '.pid'\n\n    return my_pid, my_cmd, pid_file", "is_method": false, "function_description": "Returns the current process ID, its command name, and a unique PID file path based on the command, facilitating process identification and management within a given directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "acquire_for", "line_number": 88, "body": "def acquire_for(pid_dir, num_available=1, kill_signal=None):\n    \"\"\"\n    Makes sure the process is only run once at the same time with the same name.\n\n    Notice that we since we check the process name, different parameters to the same\n    command can spawn multiple processes at the same time, i.e. running\n    \"/usr/bin/my_process\" does not prevent anyone from launching\n    \"/usr/bin/my_process --foo bar\".\n    \"\"\"\n\n    my_pid, my_cmd, pid_file = get_info(pid_dir)\n\n    # Create a pid file if it does not exist\n    try:\n        os.mkdir(pid_dir)\n        os.chmod(pid_dir, 0o777)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise\n        pass\n\n    # Let variable \"pids\" be all pids who exist in the .pid-file who are still\n    # about running the same command.\n    pids = {pid for pid in _read_pids_file(pid_file) if getpcmd(pid) == my_cmd}\n\n    if kill_signal is not None:\n        for pid in pids:\n            os.kill(pid, kill_signal)\n        print('Sent kill signal to Pids: {}'.format(pids))\n        # We allow for the killer to progress, yet we don't want these to stack\n        # up! So we only allow it once.\n        num_available += 1\n\n    if len(pids) >= num_available:\n        # We are already running under a different pid\n        print('Pid(s) {} already running'.format(pids))\n        if kill_signal is not None:\n            print('Note: There have (probably) been 1 other \"--take-lock\"'\n                  ' process which continued to run! Probably no need to run'\n                  ' this one as well.')\n        return False\n\n    _write_pids_file(pid_file, pids | {my_pid})\n\n    return True", "is_method": false, "function_description": "Function that ensures only a specified number of instances with the same process name run concurrently, optionally sending a kill signal to existing instances. It helps manage process concurrency based on process names within a given directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "_read_pids_file", "line_number": 135, "body": "def _read_pids_file(pid_file):\n    # First setup a python 2 vs 3 compatibility\n    # http://stackoverflow.com/a/21368622/621449\n    try:\n        FileNotFoundError  # noqa: F823\n    except NameError:\n        # Should only happen on python 2\n        FileNotFoundError = IOError\n    # If the file happen to not exist, simply return\n    # an empty set()\n    try:\n        with open(pid_file, 'r') as f:\n            return {int(pid_str.strip()) for pid_str in f if pid_str.strip()}\n    except FileNotFoundError:\n        return set()", "is_method": false, "function_description": "Function that reads a given file containing process IDs and returns them as a set of integers. It gracefully handles missing files by returning an empty set, ensuring compatibility across Python versions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "_write_pids_file", "line_number": 152, "body": "def _write_pids_file(pid_file, pids_set):\n    with open(pid_file, 'w') as f:\n        f.writelines('{}\\n'.format(pid) for pid in pids_set)\n\n    # Make the .pid-file writable by all (when the os allows for it)\n    if os.name != 'nt':\n        s = os.stat(pid_file)\n        if os.getuid() == s.st_uid:\n            os.chmod(pid_file, s.st_mode | 0o777)", "is_method": false, "function_description": "Private function that writes a set of process IDs to a file and adjusts file permissions to be writable by all users on compatible operating systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "recursively_freeze", "line_number": 50, "body": "def recursively_freeze(value):\n    \"\"\"\n    Recursively walks ``Mapping``s and ``list``s and converts them to ``FrozenOrderedDict`` and ``tuples``, respectively.\n    \"\"\"\n    if isinstance(value, Mapping):\n        return FrozenOrderedDict(((k, recursively_freeze(v)) for k, v in value.items()))\n    elif isinstance(value, list) or isinstance(value, tuple):\n        return tuple(recursively_freeze(v) for v in value)\n    return value", "is_method": false, "function_description": "Function that recursively converts nested mappings and lists into immutable FrozenOrderedDicts and tuples, enabling hashable, read-only representations of complex data structures for safe usage as dictionary keys or cache keys."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__getitem__", "line_number": 26, "body": "def __getitem__(self, key):\n        return self.__dict[key]", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Core access method of FrozenOrderedDict that retrieves the value associated with a given key, supporting key-based lookup in an immutable ordered dictionary structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__iter__", "line_number": 29, "body": "def __iter__(self):\n        return iter(self.__dict)", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides iteration over the keys of the FrozenOrderedDict, enabling users to traverse its elements in order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__len__", "line_number": 32, "body": "def __len__(self):\n        return len(self.__dict)", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Returns the number of key-value pairs stored in the FrozenOrderedDict, allowing other functions to determine its size or iterate based on length."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__repr__", "line_number": 35, "body": "def __repr__(self):\n        # We should use short representation for beautiful console output\n        return repr(dict(self.__dict))", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides a concise string representation of the FrozenOrderedDict by displaying its contents as a standard dictionary for clearer console output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__hash__", "line_number": 39, "body": "def __hash__(self):\n        if self.__hash is None:\n            hashes = map(hash, self.items())\n            self.__hash = functools.reduce(operator.xor, hashes, 0)\n\n        return self.__hash", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Core method of FrozenOrderedDict that computes and caches a consistent hash value based on its items, enabling its instances to be used as hashable keys in dictionaries or sets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "get_wrapped", "line_number": 46, "body": "def get_wrapped(self):\n        return self.__dict", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Returns the internal dictionary wrapped by the FrozenOrderedDict instance, providing access to its underlying data structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "copy", "line_number": 38, "body": "def copy(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Copies the contents of a single file path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data()[path]\n        self.get_all_data()[dest] = contents", "is_method": true, "class_name": "MockFileSystem", "function_description": "Provides functionality to copy a file's contents to a new destination within the mock file system, optionally raising an error if the destination already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "get_all_data", "line_number": 47, "body": "def get_all_data(self):\n        # This starts a server in the background, so we don't want to do it in the global scope\n        if MockFileSystem._data is None:\n            MockFileSystem._data = multiprocessing.Manager().dict()\n        return MockFileSystem._data", "is_method": true, "class_name": "MockFileSystem", "function_description": "Provides access to a shared, multiprocessing-safe dictionary storing filesystem data, initializing it if necessary to enable concurrent data management within the MockFileSystem context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "get_data", "line_number": 53, "body": "def get_data(self, fn):\n        return self.get_all_data()[fn]", "is_method": true, "class_name": "MockFileSystem", "function_description": "Core utility of MockFileSystem that retrieves the data associated with a specific filename from the stored file collection. It enables quick access to individual file contents within the mock system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "exists", "line_number": 56, "body": "def exists(self, path):\n        return MockTarget(path).exists()", "is_method": true, "class_name": "MockFileSystem", "function_description": "Utility method in the MockFileSystem class that checks if a given path exists, facilitating file existence verification in a mock filesystem environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "remove", "line_number": 59, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        \"\"\"\n        Removes the given mockfile. skip_trash doesn't have any meaning.\n        \"\"\"\n        if recursive:\n            to_delete = []\n            for s in self.get_all_data().keys():\n                if s.startswith(path):\n                    to_delete.append(s)\n            for s in to_delete:\n                self.get_all_data().pop(s)\n        else:\n            self.get_all_data().pop(path)", "is_method": true, "class_name": "MockFileSystem", "function_description": "Provides functionality to delete a file or directory path from the mock file system, optionally removing all nested files when recursive is enabled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "move", "line_number": 73, "body": "def move(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Moves a single file from path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data().pop(path)\n        self.get_all_data()[dest] = contents", "is_method": true, "class_name": "MockFileSystem", "function_description": "Utility method of MockFileSystem that moves a file from one path to another, optionally raising an error if the destination path already exists. It facilitates file management within the mock filesystem environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "listdir", "line_number": 82, "body": "def listdir(self, path):\n        \"\"\"\n        listdir does a prefix match of self.get_all_data(), but doesn't yet support globs.\n        \"\"\"\n        return [s for s in self.get_all_data().keys()\n                if s.startswith(path)]", "is_method": true, "class_name": "MockFileSystem", "function_description": "Provides a list of all filesystem entries starting with the given path prefix, enabling prefix-based directory listing within a mock filesystem environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "isdir", "line_number": 89, "body": "def isdir(self, path):\n        return any(self.listdir(path))", "is_method": true, "class_name": "MockFileSystem", "function_description": "Checks if a given path in the mock file system contains any entries, indicating it is a directory. This method helps to verify directory existence in testing environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "clear", "line_number": 98, "body": "def clear(self):\n        self.get_all_data().clear()", "is_method": true, "class_name": "MockFileSystem", "function_description": "Clears all stored data within the mock file system, effectively resetting its state. This allows tests or operations to start fresh without residual data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "exists", "line_number": 110, "body": "def exists(self,):\n        return self.path in self.fs.get_all_data()", "is_method": true, "class_name": "MockTarget", "function_description": "Method of the MockTarget class that checks if the target's path exists within the associated filesystem's stored data. It provides a simple existence check to confirm the presence of data at the given path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "move", "line_number": 113, "body": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        Call MockFileSystem's move command\n        \"\"\"\n        self.fs.move(self.path, path, raise_if_exists)", "is_method": true, "class_name": "MockTarget", "function_description": "Method of MockTarget that moves a file or directory to a new path using the MockFileSystem, optionally raising an error if the destination exists. It provides a simple interface for simulated file operations in testing environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "rename", "line_number": 119, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Call move to rename self\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "MockTarget", "function_description": "Utility method in the MockTarget class that renames the object by invoking its move functionality, effectively providing an alias for renaming operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "open", "line_number": 125, "body": "def open(self, mode='r'):\n        fn = self.path\n        mock_target = self\n\n        class Buffer(BytesIO):\n            # Just to be able to do writing + reading from the same buffer\n\n            _write_line = True\n\n            def set_wrapper(self, wrapper):\n                self.wrapper = wrapper\n\n            def write(self, data):\n                if mock_target._mirror_on_stderr:\n                    if self._write_line:\n                        sys.stderr.write(fn + \": \")\n                    if bytes:\n                        sys.stderr.write(data.decode('utf8'))\n                    else:\n                        sys.stderr.write(data)\n                    if (data[-1]) == '\\n':\n                        self._write_line = True\n                    else:\n                        self._write_line = False\n                super(Buffer, self).write(data)\n\n            def close(self):\n                if mode[0] == 'w':\n                    try:\n                        mock_target.wrapper.flush()\n                    except AttributeError:\n                        pass\n                    mock_target.fs.get_all_data()[fn] = self.getvalue()\n                super(Buffer, self).close()\n\n            def __exit__(self, exc_type, exc_val, exc_tb):\n                if not exc_type:\n                    self.close()\n\n            def __enter__(self):\n                return self\n\n            def readable(self):\n                return mode[0] == 'r'\n\n            def writeable(self):\n                return mode[0] == 'w'\n\n            def seekable(self):\n                return False\n\n        if mode[0] == 'w':\n            wrapper = self.format.pipe_writer(Buffer())\n            wrapper.set_wrapper(wrapper)\n            return wrapper\n        else:\n            return self.format.pipe_reader(Buffer(self.fs.get_all_data()[fn]))", "is_method": true, "class_name": "MockTarget", "function_description": "Provides a file-like interface for reading from or writing to in-memory data associated with the MockTarget, optionally mirroring written data to stderr. This enables mock file operations with customizable data handling and buffering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "set_wrapper", "line_number": 134, "body": "def set_wrapper(self, wrapper):\n                self.wrapper = wrapper", "is_method": true, "class_name": "Buffer", "function_description": "Sets or updates the wrapper attribute of the Buffer instance, allowing customization or decoration of its functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "close", "line_number": 151, "body": "def close(self):\n                if mode[0] == 'w':\n                    try:\n                        mock_target.wrapper.flush()\n                    except AttributeError:\n                        pass\n                    mock_target.fs.get_all_data()[fn] = self.getvalue()\n                super(Buffer, self).close()", "is_method": true, "class_name": "Buffer", "function_description": "Closes the buffer, saving its contents to an associated file system if opened in write mode, and ensures any buffered data is flushed before finalizing. This enables consistent persistence of in-memory data to storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "__exit__", "line_number": 160, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n                if not exc_type:\n                    self.close()", "is_method": true, "class_name": "Buffer", "function_description": "Handles the exit of a context manager by closing the buffer if no exceptions occurred, ensuring proper resource cleanup during the buffer's lifecycle."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "__enter__", "line_number": 164, "body": "def __enter__(self):\n                return self", "is_method": true, "class_name": "Buffer", "function_description": "Enables the Buffer instance to be used as a context manager, allowing setup and teardown actions with the 'with' statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "readable", "line_number": 167, "body": "def readable(self):\n                return mode[0] == 'r'", "is_method": true, "class_name": "Buffer", "function_description": "Determines whether the buffer is in a readable mode, indicating if data can be read from it. This helps other functions decide if reading operations are allowed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "writeable", "line_number": 170, "body": "def writeable(self):\n                return mode[0] == 'w'", "is_method": true, "class_name": "Buffer", "function_description": "Returns whether the Buffer instance is opened in write mode, indicating if data can be written to it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "seekable", "line_number": 173, "body": "def seekable(self):\n                return False", "is_method": true, "class_name": "Buffer", "function_description": "Indicates whether the Buffer supports seeking operations; always returns False to show it is not seekable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "_update_data", "line_number": 39, "body": "def _update_data(data, new_data):\n        if not new_data:\n            return data\n        if not data:\n            return new_data\n        for section, content in new_data.items():\n            if section not in data:\n                data[section] = dict()\n            data[section].update(content)\n        return data", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Internal helper function in LuigiTomlParser that merges two nested dictionaries, combining their sections and contents to update configuration data structures efficiently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "read", "line_number": 50, "body": "def read(self, config_paths):\n        self.data = dict()\n        for path in config_paths:\n            if os.path.isfile(path):\n                self.data = self._update_data(self.data, toml.load(path))\n\n        # freeze dict params\n        for section, content in self.data.items():\n            for key, value in content.items():\n                if isinstance(value, dict):\n                    self.data[section][key] = recursively_freeze(value)\n\n        return self.data", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Service method of the LuigiTomlParser class that reads and merges multiple TOML configuration files, returning a nested dictionary with all parameters frozen to prevent modification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "get", "line_number": 64, "body": "def get(self, section, option, default=NO_DEFAULT, **kwargs):\n        try:\n            return self.data[section][option]\n        except KeyError:\n            if default is self.NO_DEFAULT:\n                raise\n            return default", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Utility method of LuigiTomlParser that retrieves a configuration value from a specified section and option, returning a default if the key is missing to ensure robust access to parsed TOML data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getboolean", "line_number": 72, "body": "def getboolean(self, section, option, default=NO_DEFAULT):\n        return self.get(section, option, default)", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Returns the value of a specified option from a given section, with an optional default if not found. It provides a basic retrieval interface within the LuigiTomlParser configuration context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getint", "line_number": 75, "body": "def getint(self, section, option, default=NO_DEFAULT):\n        return self.get(section, option, default)", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Utility method in LuigiTomlParser that retrieves an integer value from a specified section and option in a TOML configuration, returning a default if the option is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getfloat", "line_number": 78, "body": "def getfloat(self, section, option, default=NO_DEFAULT):\n        return self.get(section, option, default)", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Returns a floating-point value from a specified section and option in the TOML configuration, optionally providing a default if the value is absent."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getintdict", "line_number": 81, "body": "def getintdict(self, section):\n        return self.data.get(section, {})", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Returns a dictionary of integer values for a specified section from the parsed TOML data. This method provides easy access to section-specific integer configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "set", "line_number": 84, "body": "def set(self, section, option, value=None):\n        if section not in self.data:\n            self.data[section] = {}\n        self.data[section][option] = value", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Sets or updates a configuration option within a given section of the TOML data, creating the section if it doesn't exist. Useful for dynamically modifying parsed TOML configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "has_option", "line_number": 89, "body": "def has_option(self, section, option):\n        return section in self.data and option in self.data[section]", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Checks if a specified option exists within a given section of the parsed TOML data, allowing users to verify configuration presence before access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "__getitem__", "line_number": 92, "body": "def __getitem__(self, name):\n        return self.data[name]", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Direct accessor method in LuigiTomlParser that retrieves a value from its internal data storage by key, enabling dictionary-like access to parsed TOML content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "_check_parser", "line_number": 43, "body": "def _check_parser(parser_class, parser):\n    if not parser_class.enabled:\n        msg = (\n            \"Parser not installed yet. \"\n            \"Please, install luigi with required parser:\\n\"\n            \"pip install luigi[{parser}]\"\n        )\n        raise ImportError(msg.format(parser=parser))", "is_method": false, "function_description": "Utility function that verifies if a parser class is enabled, raising an informative ImportError with installation instructions if the parser is not installed or available."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "get_config", "line_number": 53, "body": "def get_config(parser=PARSER):\n    \"\"\"Get configs singleton for parser\n    \"\"\"\n    parser_class = PARSERS[parser]\n    _check_parser(parser_class, parser)\n    return parser_class.instance()", "is_method": false, "function_description": "Utility function that returns a singleton configuration instance for a specified parser, ensuring consistent parser settings throughout the application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "add_config_path", "line_number": 61, "body": "def add_config_path(path):\n    \"\"\"Select config parser by file extension and add path into parser.\n    \"\"\"\n    if not os.path.isfile(path):\n        warnings.warn(\"Config file does not exist: {path}\".format(path=path))\n        return False\n\n    # select parser by file extension\n    _base, ext = os.path.splitext(path)\n    if ext and ext[1:] in PARSERS:\n        parser = ext[1:]\n    else:\n        parser = PARSER\n    parser_class = PARSERS[parser]\n\n    _check_parser(parser_class, parser)\n    if parser != PARSER:\n        msg = (\n            \"Config for {added} parser added, but used {used} parser. \"\n            \"Set up right parser via env var: \"\n            \"export LUIGI_CONFIG_PARSER={added}\"\n        )\n        warnings.warn(msg.format(added=parser, used=PARSER))\n\n    # add config path to parser\n    parser_class.add_config_path(path)\n    return True", "is_method": false, "function_description": "Function that adds a configuration file path to the appropriate parser based on file extension, warning if the file doesn't exist or if the configured parser differs from the detected one."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_get", "line_number": 62, "body": "def before_get(self, parser, section, option, value, defaults):\n        return self._interpolate_env(option, section, value)", "is_method": true, "class_name": "EnvironmentInterpolation", "function_description": "Method in EnvironmentInterpolation that processes configuration values by interpolating environment variables before retrieval, enabling dynamic resolution of options based on the current environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "_interpolate_env", "line_number": 65, "body": "def _interpolate_env(self, option, section, value):\n        rawval = value\n        parts = []\n        while value:\n            match = self._ENVRE.search(value)\n            if match is None:\n                parts.append(value)\n                break\n            envvar = match.groups()[0]\n            try:\n                envval = os.environ[envvar]\n            except KeyError:\n                raise InterpolationMissingEnvvarError(\n                    option, section, rawval, envvar)\n            start, end = match.span()\n            parts.append(value[:start])\n            parts.append(envval)\n            value = value[end:]\n        return \"\".join(parts)", "is_method": true, "class_name": "EnvironmentInterpolation", "function_description": "Core method of EnvironmentInterpolation that replaces environment variable placeholders in a string with their actual values, raising errors if any referenced variables are missing. Useful for dynamic configuration resolution from environment data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_get", "line_number": 96, "body": "def before_get(self, parser, section, option, value, defaults):\n        for interp in self._interpolations:\n            value = interp.before_get(parser, section, option, value, defaults)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "Method of CombinedInterpolation that sequentially applies multiple interpolation handlers before retrieving a configuration value, enabling layered preprocessing of option values in parsers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_read", "line_number": 101, "body": "def before_read(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_read(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "Method of CombinedInterpolation that sequentially applies multiple interpolations to a configuration value before it is read, enabling layered or combined value processing across interpolation strategies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_set", "line_number": 106, "body": "def before_set(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_set(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "Core method of CombinedInterpolation that applies multiple preprocessing interpolation steps sequentially to a value before setting it in a configuration parser. It enables combined transformations or validations from several interpolations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_write", "line_number": 111, "body": "def before_write(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_write(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "Method in CombinedInterpolation that sequentially applies multiple interpolation transformations to a value before it is written, enabling combined preprocessing or adjustment of configuration entries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "reload", "line_number": 130, "body": "def reload(cls):\n        # Warn about deprecated old-style config paths.\n        deprecated_paths = [p for p in cls._config_paths if os.path.basename(p) == 'client.cfg' and os.path.exists(p)]\n        if deprecated_paths:\n            warnings.warn(\"Luigi configuration files named 'client.cfg' are deprecated if favor of 'luigi.cfg'. \" +\n                          \"Found: {paths!r}\".format(paths=deprecated_paths),\n                          DeprecationWarning)\n\n        return cls.instance().read(cls._config_paths)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Reloads configuration files while warning about deprecated 'client.cfg' filenames, then refreshes the parser's settings from the current config paths. Useful for updating Luigi's configuration at runtime with deprecation alerts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "_get_with_default", "line_number": 140, "body": "def _get_with_default(self, method, section, option, default, expected_type=None, **kwargs):\n        \"\"\"\n        Gets the value of the section/option using method.\n\n        Returns default if value is not found.\n\n        Raises an exception if the default value is not None and doesn't match the expected_type.\n        \"\"\"\n        try:\n            try:\n                # Underscore-style is the recommended configuration style\n                option = option.replace('-', '_')\n                return method(self, section, option, **kwargs)\n            except (NoOptionError, NoSectionError):\n                # Support dash-style option names (with deprecation warning).\n                option_alias = option.replace('_', '-')\n                value = method(self, section, option_alias, **kwargs)\n                warn = 'Configuration [{s}] {o} (with dashes) should be avoided. Please use underscores: {u}.'.format(\n                    s=section, o=option_alias, u=option)\n                warnings.warn(warn, DeprecationWarning)\n                return value\n        except (NoOptionError, NoSectionError):\n            if default is LuigiConfigParser.NO_DEFAULT:\n                raise\n            if expected_type is not None and default is not None and \\\n               not isinstance(default, expected_type):\n                raise\n            return default", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method of LuigiConfigParser that retrieves a configuration value from a section with fallback to differently styled option names and returns a default if missing, ensuring type consistency for default values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "has_option", "line_number": 169, "body": "def has_option(self, section, option):\n        \"\"\"modified has_option\n        Check for the existence of a given option in a given section. If the\n        specified 'section' is None or an empty string, DEFAULT is assumed. If\n        the specified 'section' does not exist, returns False.\n        \"\"\"\n\n        # Underscore-style is the recommended configuration style\n        option = option.replace('-', '_')\n        if ConfigParser.has_option(self, section, option):\n            return True\n\n        # Support dash-style option names (with deprecation warning).\n        option_alias = option.replace('_', '-')\n        if ConfigParser.has_option(self, section, option_alias):\n            warn = 'Configuration [{s}] {o} (with dashes) should be avoided. Please use underscores: {u}.'.format(\n                s=section, o=option_alias, u=option)\n            warnings.warn(warn, DeprecationWarning)\n            return True\n\n        return False", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Checks if a configuration option exists within a specified section, supporting both underscore and deprecated dash-style option names, with fallback to a default section when none is specified."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "get", "line_number": 191, "body": "def get(self, section, option, default=NO_DEFAULT, **kwargs):\n        return self._get_with_default(ConfigParser.get, section, option, default, **kwargs)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method in LuigiConfigParser that retrieves a configuration option's value from a specified section, returning a default if the option is missing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getboolean", "line_number": 194, "body": "def getboolean(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getboolean, section, option, default, bool)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method of LuigiConfigParser that retrieves a boolean configuration value from a given section and option, returning a default if the value is absent."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getint", "line_number": 197, "body": "def getint(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getint, section, option, default, int)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method of LuigiConfigParser that retrieves an integer value for a given section and option from the configuration, returning a default if the option is missing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getfloat", "line_number": 200, "body": "def getfloat(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getfloat, section, option, default, float)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method in LuigiConfigParser that retrieves a floating-point configuration value from a specified section and option, returning a default if the value is missing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getintdict", "line_number": 203, "body": "def getintdict(self, section):\n        try:\n            # Exclude keys from [DEFAULT] section because in general they do not hold int values\n            return dict((key, int(value)) for key, value in self.items(section)\n                        if key not in {k for k, _ in self.items('DEFAULT')})\n        except NoSectionError:\n            return {}", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Utility method of LuigiConfigParser that returns a dictionary of integer values from a specified section, excluding keys inherited from the default section. It simplifies retrieval of typed configuration data with error handling for missing sections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "set", "line_number": 211, "body": "def set(self, section, option, value=None):\n        if not ConfigParser.has_section(self, section):\n            ConfigParser.add_section(self, section)\n\n        return ConfigParser.set(self, section, option, value)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Sets a configuration option's value, creating the specified section if it doesn't exist, enabling seamless updates or additions in configuration management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "instance", "line_number": 25, "body": "def instance(cls, *args, **kwargs):\n        \"\"\" Singleton getter \"\"\"\n        if cls._instance is None:\n            cls._instance = cls(*args, **kwargs)\n            loaded = cls._instance.reload()\n            logging.getLogger('luigi-interface').info('Loaded %r', loaded)\n\n        return cls._instance", "is_method": true, "class_name": "BaseParser", "function_description": "Provides a singleton instance of BaseParser, ensuring only one object exists and is reloaded when first created. This supports consistent, centralized parsing throughout applications using the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "add_config_path", "line_number": 35, "body": "def add_config_path(cls, path):\n        cls._config_paths.append(path)\n        cls.reload()", "is_method": true, "class_name": "BaseParser", "function_description": "Adds a new configuration file path to the parser's list and triggers a reload to apply updated settings. This enables dynamic configuration management within the BaseParser class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "reload", "line_number": 40, "body": "def reload(cls):\n        return cls.instance().read(cls._config_paths)", "is_method": true, "class_name": "BaseParser", "function_description": "Reloads the parser instance\u2019s configuration from predefined paths, ensuring the parser reflects the latest settings or data. This method facilitates dynamic configuration updates without recreating the parser."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_constrain_glob", "line_number": 489, "body": "def _constrain_glob(glob, paths, limit=5):\n    \"\"\"\n    Tweaks glob into a list of more specific globs that together still cover paths and not too much extra.\n\n    Saves us minutes long listings for long dataset histories.\n\n    Specifically, in this implementation the leftmost occurrences of \"[0-9]\"\n    give rise to a few separate globs that each specialize the expression to\n    digits that actually occur in paths.\n    \"\"\"\n\n    def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)\n\n    current = {glob: paths}\n    while True:\n        pos = list(current.keys())[0].find('[0-9]')\n        if pos == -1:\n            # no wildcard expressions left to specialize in the glob\n            return list(current.keys())\n        char_sets = {}\n        for g, p in current.items():\n            char_sets[g] = sorted({path[pos] for path in p})\n        if sum(len(s) for s in char_sets.values()) > limit:\n            return [g.replace('[0-9]', digit_set_wildcard(char_sets[g]), 1) for g in current]\n        for g, s in char_sets.items():\n            for c in s:\n                new_glob = g.replace('[0-9]', c, 1)\n                new_paths = list(filter(lambda p: p[pos] == c, current[g]))\n                current[new_glob] = new_paths\n            del current[g]", "is_method": false, "function_description": "Function that refines a glob pattern containing digit wildcards into a limited set of more specific globs matching actual digits in given paths, optimizing path matching and reducing expensive directory listings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "most_common", "line_number": 529, "body": "def most_common(items):\n    [(element, counter)] = Counter(items).most_common(1)\n    return element, counter", "is_method": false, "function_description": "Utility function that returns the most frequent element and its count from a list, useful for quickly identifying dominant items in data collections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_get_per_location_glob", "line_number": 534, "body": "def _get_per_location_glob(tasks, outputs, regexes):\n    \"\"\"\n    Builds a glob listing existing output paths.\n\n    Esoteric reverse engineering, but worth it given that (compared to an\n    equivalent contiguousness guarantee by naive complete() checks)\n    requests to the filesystem are cut by orders of magnitude, and users\n    don't even have to retrofit existing tasks anyhow.\n    \"\"\"\n    paths = [o.path for o in outputs]\n    # naive, because some matches could be confused by numbers earlier\n    # in path, e.g. /foo/fifa2000k/bar/2000-12-31/00\n    matches = [r.search(p) for r, p in zip(regexes, paths)]\n\n    for m, p, t in zip(matches, paths, tasks):\n        if m is None:\n            raise NotImplementedError(\"Couldn't deduce datehour representation in output path %r of task %s\" % (p, t))\n\n    n_groups = len(matches[0].groups())\n    # the most common position of every group is likely\n    # to be conclusive hit or miss\n    positions = [most_common((m.start(i), m.end(i)) for m in matches)[0] for i in range(1, n_groups + 1)]\n\n    glob = list(paths[0])  # FIXME sanity check that it's the same for all paths\n    for start, end in positions:\n        glob = glob[:start] + ['[0-9]'] * (end - start) + glob[end:]\n    # chop off the last path item\n    # (wouldn't need to if `hadoop fs -ls -d` equivalent were available)\n    return ''.join(glob).rsplit('/', 1)[0]", "is_method": false, "function_description": "Utility function that constructs a pattern glob to match existing output file paths, optimizing filesystem queries by generalizing common numeric segments across task outputs for efficient data location discovery."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_get_filesystems_and_globs", "line_number": 565, "body": "def _get_filesystems_and_globs(datetime_to_task, datetime_to_re):\n    \"\"\"\n    Yields a (filesystem, glob) tuple per every output location of task.\n\n    The task can have one or several FileSystemTarget outputs.\n\n    For convenience, the task can be a luigi.WrapperTask,\n    in which case outputs of all its dependencies are considered.\n    \"\"\"\n    # probe some scattered datetimes unlikely to all occur in paths, other than by being sincere datetime parameter's representations\n    # TODO limit to [self.start, self.stop) so messages are less confusing? Done trivially it can kill correctness\n    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]\n    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]\n    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]\n    sample_outputs = [flatten_output(t) for t in sample_tasks]\n\n    for o, t in zip(sample_outputs, sample_tasks):\n        if len(o) != len(sample_outputs[0]):\n            raise NotImplementedError(\"Outputs must be consistent over time, sorry; was %r for %r and %r for %r\" % (o, t, sample_outputs[0], sample_tasks[0]))\n            # TODO fall back on requiring last couple of days? to avoid astonishing blocking when changes like that are deployed\n            # erm, actually it's not hard to test entire hours_back..hours_forward and split into consistent subranges FIXME?\n        for target in o:\n            if not isinstance(target, FileSystemTarget):\n                raise NotImplementedError(\"Output targets must be instances of FileSystemTarget; was %r for %r\" % (target, t))\n\n    for o in zip(*sample_outputs):  # transposed, so here we're iterating over logical outputs, not datetimes\n        glob = _get_per_location_glob(sample_tasks, o, regexes)\n        yield o[0].fs, glob", "is_method": false, "function_description": "Utility function that generates filesystem and glob pattern pairs for each output location of time-parameterized tasks, supporting multiple FileSystemTarget outputs and wrapper tasks by analyzing sample datetimes to ensure output consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_list_existing", "line_number": 595, "body": "def _list_existing(filesystem, glob, paths):\n    \"\"\"\n    Get all the paths that do in fact exist. Returns a set of all existing paths.\n\n    Takes a luigi.target.FileSystem object, a str which represents a glob and\n    a list of strings representing paths.\n    \"\"\"\n    globs = _constrain_glob(glob, paths)\n    time_start = time.time()\n    listing = []\n    for g in sorted(globs):\n        logger.debug('Listing %s', g)\n        if filesystem.exists(g):\n            listing.extend(filesystem.listdir(g))\n    logger.debug('%d %s listings took %f s to return %d items',\n                 len(globs), filesystem.__class__.__name__, time.time() - time_start, len(listing))\n    return set(listing)", "is_method": false, "function_description": "Utility function that returns a set of existing file paths matching constrained glob patterns from a given filesystem, helping to identify which specified paths actually exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "infer_bulk_complete_from_fs", "line_number": 614, "body": "def infer_bulk_complete_from_fs(datetimes, datetime_to_task, datetime_to_re):\n    \"\"\"\n    Efficiently determines missing datetimes by filesystem listing.\n\n    The current implementation works for the common case of a task writing\n    output to a ``FileSystemTarget`` whose path is built using strftime with\n    format like '...%Y...%m...%d...%H...', without custom ``complete()`` or\n    ``exists()``.\n\n    (Eventually Luigi could have ranges of completion as first-class citizens.\n    Then this listing business could be factored away/be provided for\n    explicitly in target API or some kind of a history server.)\n    \"\"\"\n    filesystems_and_globs_by_location = _get_filesystems_and_globs(datetime_to_task, datetime_to_re)\n    paths_by_datetime = [[o.path for o in flatten_output(datetime_to_task(d))] for d in datetimes]\n    listing = set()\n    for (f, g), p in zip(filesystems_and_globs_by_location, zip(*paths_by_datetime)):  # transposed, so here we're iterating over logical outputs, not datetimes\n        listing |= _list_existing(f, g, p)\n\n    # quickly learn everything that's missing\n    missing_datetimes = []\n    for d, p in zip(datetimes, paths_by_datetime):\n        if not set(p) <= listing:\n            missing_datetimes.append(d)\n\n    return missing_datetimes", "is_method": false, "function_description": "This function identifies which datetimes lack corresponding output files by checking the filesystem, helping determine incomplete tasks in batch processing workflows such as Luigi pipelines. It efficiently detects missing outputs based on timestamped file patterns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "of_cls", "line_number": 115, "body": "def of_cls(self):\n        \"\"\"\n        DONT USE. Will be deleted soon. Use ``self.of``!\n        \"\"\"\n        if isinstance(self.of, str):\n            warnings.warn('When using Range programatically, dont pass \"of\" param as string!')\n            return Register.get_task_cls(self.of)\n        return self.of", "is_method": true, "class_name": "RangeBase", "function_description": "Deprecated internal method of RangeBase that returns the class referenced by the 'of' attribute, warning against using string types; intended to be replaced by 'self.of'."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_emit_metrics", "line_number": 164, "body": "def _emit_metrics(self, missing_datetimes, finite_start, finite_stop):\n        \"\"\"\n        For consistent metrics one should consider the entire range, but\n        it is open (infinite) if stop or start is None.\n\n        Hence make do with metrics respective to the finite simplification.\n        \"\"\"\n        datetimes = self.finite_datetimes(\n            finite_start if self.start is None else min(finite_start, self.parameter_to_datetime(self.start)),\n            finite_stop if self.stop is None else max(finite_stop, self.parameter_to_datetime(self.stop)))\n\n        delay_in_jobs = len(datetimes) - datetimes.index(missing_datetimes[0]) if datetimes and missing_datetimes else 0\n        self.trigger_event(RangeEvent.DELAY, self.of_cls.task_family, delay_in_jobs)\n\n        expected_count = len(datetimes)\n        complete_count = expected_count - len(missing_datetimes)\n        self.trigger_event(RangeEvent.COMPLETE_COUNT, self.of_cls.task_family, complete_count)\n        self.trigger_event(RangeEvent.COMPLETE_FRACTION, self.of_cls.task_family, float(complete_count) / expected_count if expected_count else 1)", "is_method": true, "class_name": "RangeBase", "function_description": "Utility method of RangeBase that calculates and triggers metrics related to missing and completed datetime events within a specified finite range, supporting progress tracking and delay measurement in task scheduling contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 183, "body": "def _format_datetime(self, dt):\n        return self.datetime_to_parameter(dt)", "is_method": true, "class_name": "RangeBase", "function_description": "Internal helper method in RangeBase that formats a datetime object into a specific parameter representation for further use or processing within the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_range", "line_number": 186, "body": "def _format_range(self, datetimes):\n        param_first = self._format_datetime(datetimes[0])\n        param_last = self._format_datetime(datetimes[-1])\n        return '[%s, %s]' % (param_first, param_last)", "is_method": true, "class_name": "RangeBase", "function_description": "Utility method of the RangeBase class that formats a list of datetime objects into a string representing the range between the first and last datetimes. It provides a standardized textual representation of datetime intervals."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_instantiate_task_cls", "line_number": 191, "body": "def _instantiate_task_cls(self, param):\n        return self.of(**self._task_parameters(param))", "is_method": true, "class_name": "RangeBase", "function_description": "Core method in RangeBase that creates and returns an instance of a task class using dynamically generated task parameters based on the given input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_param_name", "line_number": 195, "body": "def _param_name(self):\n        if self.param_name is None:\n            return next(x[0] for x in self.of.get_params() if x[1].positional)\n        else:\n            return self.param_name", "is_method": true, "class_name": "RangeBase", "function_description": "Utility method in RangeBase that determines a parameter name, either returning a preset name or extracting the first positional parameter name from an associated object's parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_task_parameters", "line_number": 201, "body": "def _task_parameters(self, param):\n        kwargs = dict(**self.of_params)\n        kwargs[self._param_name] = param\n        return kwargs", "is_method": true, "class_name": "RangeBase", "function_description": "Utility method in RangeBase that creates a parameter dictionary by merging existing parameters with a new key-value pair, facilitating configuration updates for tasks or operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "requires", "line_number": 206, "body": "def requires(self):\n        # cache because we anticipate a fair amount of computation\n        if hasattr(self, '_cached_requires'):\n            return self._cached_requires\n\n        if not self.start and not self.stop:\n            raise ParameterException(\"At least one of start and stop needs to be specified\")\n        if not self.start and not self.reverse:\n            raise ParameterException(\"Either start needs to be specified or reverse needs to be True\")\n        if self.start and self.stop and self.start > self.stop:\n            raise ParameterException(\"Can't have start > stop\")\n        # TODO check overridden complete() and exists()\n\n        now = datetime.utcfromtimestamp(time.time() if self.now is None else self.now)\n\n        moving_start = self.moving_start(now)\n        finite_start = moving_start if self.start is None else max(self.parameter_to_datetime(self.start), moving_start)\n        moving_stop = self.moving_stop(now)\n        finite_stop = moving_stop if self.stop is None else min(self.parameter_to_datetime(self.stop), moving_stop)\n\n        datetimes = self.finite_datetimes(finite_start, finite_stop) if finite_start <= finite_stop else []\n\n        if datetimes:\n            logger.debug('Actually checking if range %s of %s is complete',\n                         self._format_range(datetimes), self.of_cls.task_family)\n            missing_datetimes = sorted(self._missing_datetimes(datetimes))\n            logger.debug('Range %s lacked %d of expected %d %s instances',\n                         self._format_range(datetimes), len(missing_datetimes), len(datetimes), self.of_cls.task_family)\n        else:\n            missing_datetimes = []\n            logger.debug('Empty range. No %s instances expected', self.of_cls.task_family)\n\n        self._emit_metrics(missing_datetimes, finite_start, finite_stop)\n\n        if self.reverse:\n            required_datetimes = missing_datetimes[-self.task_limit:]\n        else:\n            required_datetimes = missing_datetimes[:self.task_limit]\n        if required_datetimes:\n            logger.debug('Requiring %d missing %s instances in range %s',\n                         len(required_datetimes), self.of_cls.task_family, self._format_range(required_datetimes))\n        if self.reverse:\n            required_datetimes.reverse()  # TODO priorities, so that within the batch tasks are ordered too\n\n        self._cached_requires = [self._instantiate_task_cls(self.datetime_to_parameter(d)) for d in required_datetimes]\n        return self._cached_requires", "is_method": true, "class_name": "RangeBase", "function_description": "Core method of RangeBase that identifies and returns incomplete task instances within a specified datetime range, enforcing parameter consistency and supporting reverse ordering and task limiting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 253, "body": "def missing_datetimes(self, finite_datetimes):\n        \"\"\"\n        Override in subclasses to do bulk checks.\n\n        Returns a sorted list.\n\n        This is a conservative base implementation that brutally checks completeness, instance by instance.\n\n        Inadvisable as it may be slow.\n        \"\"\"\n        return [d for d in finite_datetimes if not self._instantiate_task_cls(self.datetime_to_parameter(d)).complete()]", "is_method": true, "class_name": "RangeBase", "function_description": "Provides a baseline method to identify which datetime instances in a given list are incomplete, enabling subclasses to efficiently check task completion over bulk datetime inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_missing_datetimes", "line_number": 265, "body": "def _missing_datetimes(self, finite_datetimes):\n        \"\"\"\n        Backward compatible wrapper. Will be deleted eventually (stated on Dec 2015)\n        \"\"\"\n        try:\n            return self.missing_datetimes(finite_datetimes)\n        except TypeError as ex:\n            if 'missing_datetimes()' in repr(ex):\n                warnings.warn('In your Range* subclass, missing_datetimes() should only take 1 argument (see latest docs)')\n                return self.missing_datetimes(self.of_cls, finite_datetimes)\n            else:\n                raise", "is_method": true, "class_name": "RangeBase", "function_description": "Helper method in RangeBase that calls a subclass\u2019s missing_datetimes function with backward compatibility, handling different argument signatures to maintain legacy support during a transitional period."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 302, "body": "def datetime_to_parameter(self, dt):\n        return dt.date()", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Converts a datetime object to its date component, providing a simplified date parameter representation. This utility supports operations requiring date-only values within the RangeDailyBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 305, "body": "def parameter_to_datetime(self, p):\n        return datetime(p.year, p.month, p.day)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Converts a given date-like parameter to a datetime object representing midnight of that day, standardizing date inputs for daily range operations within the RangeDailyBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 308, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt.date())", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Provides a dictionary of task parameters for a given date-time by combining daily range parameters with the specific date's parameters. Useful for normalizing task inputs based on date within the RangeDailyBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 314, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, dt.day)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Utility method in RangeDailyBase that extracts and converts a date-related parameter from a dictionary into a datetime object representing that specific day."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 321, "body": "def moving_start(self, now):\n        return now - timedelta(days=self.days_back)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "This function calculates a date offset by a predefined number of days before the given date. It provides a utility for determining the start date in a daily range context based on a specified look-back period."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 324, "body": "def moving_stop(self, now):\n        return now + timedelta(days=self.days_forward)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Returns a future date by adding a predefined number of days to the given date, supporting forward-looking date calculations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 327, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to turn of day.\n        \"\"\"\n        date_start = datetime(finite_start.year, finite_start.month, finite_start.day)\n        dates = []\n        for i in itertools.count():\n            t = date_start + timedelta(days=i)\n            if t >= finite_stop:\n                return dates\n            if t >= finite_start:\n                dates.append(t)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Returns a list of datetime points representing each day\u2019s start within a specified finite time range, useful for iterating over daily boundaries between two datetimes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 365, "body": "def datetime_to_parameter(self, dt):\n        return dt", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Trivial method in RangeHourlyBase that returns the input datetime as-is, likely serving as a placeholder or identity transformer for datetime parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 368, "body": "def parameter_to_datetime(self, p):\n        return p", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "This method returns the input parameter unchanged, likely serving as a placeholder for converting parameters to datetime objects in subclasses of RangeHourlyBase."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 371, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Returns a dictionary of combined task parameters based on a given datetime, integrating range-specific settings for scheduling or time-based tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 377, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        return p[self._param_name]", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Returns the datetime value associated with the class's specific parameter name from a given parameters dictionary. Useful for extracting timestamp-related data in ranged tasks within the RangeHourlyBase context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 383, "body": "def moving_start(self, now):\n        return now - timedelta(hours=self.hours_back)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Returns a timestamp offset by a configured number of hours before the given time, supporting time range calculations relative to the current moment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 386, "body": "def moving_stop(self, now):\n        return now + timedelta(hours=self.hours_forward)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Returns a future timestamp by adding a preset number of hours to the given time, enabling time-based calculations or scheduling within a predefined forward range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 389, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to whole hours.\n        \"\"\"\n        datehour_start = datetime(finite_start.year, finite_start.month, finite_start.day, finite_start.hour)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(hours=i)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Generates and returns a list of datetime objects representing each whole hour within a given start and stop time range. Useful for iterating over or analyzing hourly intervals within arbitrary datetime boundaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 402, "body": "def _format_datetime(self, dt):\n        return luigi.DateHourParameter().serialize(dt)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "This method formats a datetime object into a serialized string according to hourly granularity. It provides a standardized way to represent datetimes within the RangeHourlyBase context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 435, "body": "def datetime_to_parameter(self, dt):\n        return dt", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Returns the given datetime parameter without modification. This method can serve as a placeholder or a pass-through for datetime processing in the RangeByMinutesBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 438, "body": "def parameter_to_datetime(self, p):\n        return p", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "This method returns the input parameter as is, serving as a placeholder for converting parameters to datetime objects within the RangeByMinutesBase class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 441, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Returns a dictionary of combined task parameters based on a given date-time, integrating range-specific parameters for task execution within time intervals."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 447, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Method of RangeByMinutesBase that extracts a datetime value from given parameters by constructing a datetime object using year, month, day, hour, and minute from the specified parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 454, "body": "def moving_start(self, now):\n        return now - timedelta(minutes=self.minutes_back)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Returns a datetime representing the start time offset by a configured number of minutes before the given timestamp. Useful for defining time ranges for data queries or event filtering relative to the current time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 457, "body": "def moving_stop(self, now):\n        return now + timedelta(minutes=self.minutes_forward)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Returns a future datetime offset by a configured number of minutes, supporting time range calculations relative to a given moment. Useful for scheduling or time window adjustments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 460, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to a whole number of minutes intervals.\n        \"\"\"\n        # Validate that the minutes_interval can divide 60 and it is greater than 0 and lesser than 60\n        if not (0 < self.minutes_interval < 60):\n            raise ParameterException('minutes-interval must be within 0..60')\n        if 60 % self.minutes_interval != 0:\n            raise ParameterException('minutes-interval does not evenly divide 60')\n        # start of a complete interval, e.g. 20:13 and the interval is 5 -> 20:10\n        start_minute = int(finite_start.minute/self.minutes_interval)*self.minutes_interval\n        datehour_start = datetime(\n            year=finite_start.year,\n            month=finite_start.month,\n            day=finite_start.day,\n            hour=finite_start.hour,\n            minute=start_minute)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(minutes=i*self.minutes_interval)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Generates a list of datetime points between given start and stop times that align with exact intervals of whole minutes. Useful for creating evenly spaced time ranges based on a specified minute interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 485, "body": "def _format_datetime(self, dt):\n        return luigi.DateMinuteParameter().serialize(dt)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Utility method in RangeByMinutesBase that formats a datetime object into a serialized string representation suitable for minute-level date parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "digit_set_wildcard", "line_number": 500, "body": "def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)", "is_method": false, "function_description": "Function that creates a concise wildcard character set expression, using ranges when characters form a consecutive sequence, for use in pattern matching or regular expressions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 670, "body": "def datetime_to_parameter(self, dt):\n        return date(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Converts a given datetime to the first day of its month as a date object, standardizing dates for monthly range operations within the RangeMonthly class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 673, "body": "def parameter_to_datetime(self, p):\n        return datetime(p.year, p.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Converts a given date-like parameter to a datetime object set to the first day of its month. This standardizes dates for monthly range calculations or comparisons within the RangeMonthly class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 676, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt.date())", "is_method": true, "class_name": "RangeMonthly", "function_description": "Converts a given datetime into a dictionary of parameters combining date-specific and range-based task settings, facilitating standardized input for ranged monthly operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 682, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Core method of the RangeMonthly class that extracts a datetime object representing the first day of the month from provided parameters, facilitating date-based range operations in monthly contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 689, "body": "def _format_datetime(self, dt):\n        return dt.strftime('%Y-%m')", "is_method": true, "class_name": "RangeMonthly", "function_description": "Formats a datetime object into a string representing the year and month in \"YYYY-MM\" format. This internal method standardizes date formatting for monthly range representations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 692, "body": "def moving_start(self, now):\n        return self._align(now) - relativedelta(months=self.months_back)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Returns a date aligned to the start of the current range, shifted backward by a specified number of months. This helps define the starting point of a monthly time window for analysis or display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 695, "body": "def moving_stop(self, now):\n        return self._align(now) + relativedelta(months=self.months_forward)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Returns a date shifted forward by a set number of months from a reference date, aligned according to the class's criteria. This helps calculate future monthly ranges based on the current date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_align", "line_number": 698, "body": "def _align(self, dt):\n        return datetime(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Returns the first day of the month for a given datetime, aligning any date to its monthly start. This supports monthly range calculations by standardizing dates to their month beginnings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 701, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to turn of month.\n        \"\"\"\n        start_date = self._align(finite_start)\n        aligned_stop = self._align(finite_stop)\n        dates = []\n        for m in itertools.count():\n            t = start_date + relativedelta(months=m)\n            if t >= aligned_stop:\n                return dates\n            if t >= finite_start:\n                dates.append(t)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Method of RangeMonthly that returns all datetime points marking month boundaries within a specified finite time interval. It helps identify monthly turnover dates between given start and stop datetimes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 730, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, map(self.datetime_to_parameter, finite_datetimes))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d)'))", "is_method": true, "class_name": "RangeDaily", "function_description": "Identifies and returns the set of missing datetime entries from a given collection by comparing expected complete parameters, supporting detection of gaps in date-based data sequences."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 758, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            # TODO: Why is there a list() here but not for the RangeDaily??\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, list(map(self.datetime_to_parameter, finite_datetimes)))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d).*(%H)'))", "is_method": true, "class_name": "RangeHourly", "function_description": "Method of the RangeHourly class that identifies missing datetime entries by comparing provided datetimes against a completed set, facilitating detection of gaps in hourly time ranges for data processing or validation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 787, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, map(self.datetime_to_parameter, finite_datetimes))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d).*(%H).*(%M)'))", "is_method": true, "class_name": "RangeByMinutes", "function_description": "Identifies datetime values missing from a given finite set by comparing them against a completed range, supporting detection of gaps in time-based sequences. Useful for validating or completing datetime intervals within time range operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "get_task_requires", "line_number": 54, "body": "def get_task_requires(task):\n    return set(flatten(task.requires()))", "is_method": false, "function_description": "Returns a set of all prerequisite tasks required for the given task, consolidating nested dependencies into a single collection. This aids in understanding and managing task execution order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "dfs_paths", "line_number": 58, "body": "def dfs_paths(start_task, goal_task_family, path=None):\n    if path is None:\n        path = [start_task]\n    if start_task.task_family == goal_task_family or goal_task_family is None:\n        for item in path:\n            yield item\n    for next in get_task_requires(start_task) - set(path):\n        for t in dfs_paths(next, goal_task_family, path + [next]):\n            yield t", "is_method": false, "function_description": "This function performs a depth-first traversal starting from a task, yielding tasks along paths that belong to a specified task family or all tasks if no family is specified. It helps explore dependent tasks within or across task families."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "find_deps", "line_number": 76, "body": "def find_deps(task, upstream_task_family):\n    '''\n    Finds all dependencies that start with the given task and have a path\n    to upstream_task_family\n\n    Returns all deps on all paths between task and upstream\n    '''\n    return {t for t in dfs_paths(task, upstream_task_family)}", "is_method": false, "function_description": "This function identifies all dependency tasks on any path from a specified starting task to an upstream task family, enabling analysis of task execution order or dependency chains in workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "find_deps_cli", "line_number": 86, "body": "def find_deps_cli():\n    '''\n    Finds all tasks on all paths from provided CLI task\n    '''\n    cmdline_args = sys.argv[1:]\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        return find_deps(cp.get_task_obj(), upstream().family)", "is_method": false, "function_description": "Utility function that extracts a CLI task from command-line arguments and finds all related dependent tasks along execution paths, supporting task dependency analysis in command-line workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "get_task_output_description", "line_number": 95, "body": "def get_task_output_description(task_output):\n    '''\n    Returns a task's output as a string\n    '''\n    output_description = \"n/a\"\n\n    if isinstance(task_output, RemoteTarget):\n        output_description = \"[SSH] {0}:{1}\".format(task_output._fs.remote_context.host, task_output.path)\n    elif isinstance(task_output, S3Target):\n        output_description = \"[S3] {0}\".format(task_output.path)\n    elif isinstance(task_output, FileSystemTarget):\n        output_description = \"[FileSystem] {0}\".format(task_output.path)\n    elif isinstance(task_output, PostgresTarget):\n        output_description = \"[DB] {0}:{1}\".format(task_output.host, task_output.table)\n    else:\n        output_description = \"to be determined\"\n\n    return output_description", "is_method": false, "function_description": "Utility function that converts various task output target types into descriptive strings, summarizing their storage location or access method to aid in task result reporting or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "main", "line_number": 115, "body": "def main():\n    deps = find_deps_cli()\n    for task in deps:\n        task_output = task.output()\n\n        if isinstance(task_output, dict):\n            output_descriptions = [get_task_output_description(output) for label, output in task_output.items()]\n        elif isinstance(task_output, Iterable):\n            output_descriptions = [get_task_output_description(output) for output in task_output]\n        else:\n            output_descriptions = [get_task_output_description(task_output)]\n\n        print(\"   TASK: {0}\".format(task))\n        for desc in output_descriptions:\n            print(\"                       : {0}\".format(desc))", "is_method": false, "function_description": "Main entry point that discovers tasks with dependencies, retrieves their outputs, generates descriptive summaries for each output, and prints task details along with these descriptions for user inspection or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "main", "line_number": 55, "body": "def main():\n    parser = argparse.ArgumentParser(\n        \"luigi-grep is used to search for workflows using the luigi scheduler's json api\")\n    parser.add_argument(\n        \"--scheduler-host\", default=\"localhost\", help=\"hostname of the luigi scheduler\")\n    parser.add_argument(\n        \"--scheduler-port\", default=\"8082\", help=\"port of the luigi scheduler\")\n    parser.add_argument(\"--prefix\", help=\"prefix of a task query to search for\", default=None)\n    parser.add_argument(\"--status\", help=\"search for jobs with the given status\", default=None)\n\n    args = parser.parse_args()\n    grep = LuigiGrep(args.scheduler_host, args.scheduler_port)\n\n    results = []\n    if args.prefix:\n        results = grep.prefix_search(args.prefix)\n    elif args.status:\n        results = grep.status_search(args.status)\n\n    for job in results:\n        print(\"{name}: {status}, Dependencies:\".format(name=job['name'], status=job['status']))\n        for status, jobs in job['deps_by_status'].items():\n            print(\"  status={status}\".format(status=status))\n            for job in jobs:\n                print(\"    {job}\".format(job=job))", "is_method": false, "function_description": "Main entry point of a command-line tool that queries a Luigi scheduler for workflow tasks filtered by name prefix or status, then prints the matching jobs and their dependencies. It enables users to easily search and inspect scheduled Luigi workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "graph_url", "line_number": 17, "body": "def graph_url(self):\n        return \"http://{0}:{1}/api/graph\".format(self._host, self._port)", "is_method": true, "class_name": "LuigiGrep", "function_description": "Returns the API endpoint URL for accessing the graph data from the LuigiGrep service, using its configured host and port."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "_fetch_json", "line_number": 20, "body": "def _fetch_json(self):\n        \"\"\"Returns the json representation of the dep graph\"\"\"\n        print(\"Fetching from url: \" + self.graph_url)\n        resp = urlopen(self.graph_url).read()\n        return json.loads(resp.decode('utf-8'))", "is_method": true, "class_name": "LuigiGrep", "function_description": "Internal method of LuigiGrep that retrieves and parses a JSON representation of a dependency graph from a specified URL. It provides access to the raw graph data for further processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "_build_results", "line_number": 26, "body": "def _build_results(self, jobs, job):\n        job_info = jobs[job]\n        deps = job_info['deps']\n        deps_status = defaultdict(list)\n        for j in deps:\n            if j in jobs:\n                deps_status[jobs[j]['status']].append(j)\n            else:\n                deps_status['UNKNOWN'].append(j)\n        return {\"name\": job, \"status\": job_info['status'], \"deps_by_status\": deps_status}", "is_method": true, "class_name": "LuigiGrep", "function_description": "Private method in LuigiGrep that summarizes a job's status along with the statuses of its dependent jobs, aiding in job dependency tracking and status reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "prefix_search", "line_number": 37, "body": "def prefix_search(self, job_name_prefix):\n        \"\"\"Searches for jobs matching the given ``job_name_prefix``.\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            if job.startswith(job_name_prefix):\n                yield self._build_results(jobs, job)", "is_method": true, "class_name": "LuigiGrep", "function_description": "Provides an iterable search over jobs whose names start with a given prefix, enabling efficient filtering of jobs by name within the LuigiGrep context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "status_search", "line_number": 45, "body": "def status_search(self, status):\n        \"\"\"Searches for jobs matching the given ``status``.\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            job_info = jobs[job]\n            if job_info['status'].lower() == status.lower():\n                yield self._build_results(jobs, job)", "is_method": true, "class_name": "LuigiGrep", "function_description": "Core method of LuigiGrep that iterates through jobs and yields those matching a specified status, facilitating status-based filtering in job monitoring or querying workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps_tree.py", "function": "print_tree", "line_number": 41, "body": "def print_tree(task, indent='', last=True):\n    '''\n    Return a string representation of the tasks, their statuses/parameters in a dependency tree format\n    '''\n    # dont bother printing out warnings about tasks with no output\n    with warnings.catch_warnings():\n        warnings.filterwarnings(action='ignore', message='Task .* without outputs has no custom complete\\\\(\\\\) method')\n        is_task_complete = task.complete()\n    is_complete = (bcolors.OKGREEN + 'COMPLETE' if is_task_complete else bcolors.OKBLUE + 'PENDING') + bcolors.ENDC\n    name = task.__class__.__name__\n    params = task.to_str_params(only_significant=True)\n    result = '\\n' + indent\n    if(last):\n        result += '\u2514\u2500--'\n        indent += '    '\n    else:\n        result += '|---'\n        indent += '|   '\n    result += '[{0}-{1} ({2})]'.format(name, params, is_complete)\n    children = flatten(task.requires())\n    for index, child in enumerate(children):\n        result += print_tree(child, indent, (index+1) == len(children))\n    return result", "is_method": false, "function_description": "Utility function that generates a formatted string representation of a task dependency tree, showing each task's name, parameters, and completion status for easy visualization of workflow progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "_do_work_on_compute_node", "line_number": 42, "body": "def _do_work_on_compute_node(work_dir, tarball=True):\n\n    if tarball:\n        # Extract the necessary dependencies\n        # This can create a lot of I/O overhead when running many SGEJobTasks,\n        # so is optional if the luigi project is accessible from the cluster node\n        _extract_packages_archive(work_dir)\n\n    # Open up the pickle file with the work to be done\n    os.chdir(work_dir)\n    with open(\"job-instance.pickle\", \"r\") as f:\n        job = pickle.load(f)\n\n    # Do the work contained\n    job.work()", "is_method": false, "function_description": "Executes a serialized job's work method on a compute node, optionally extracting dependencies beforehand to prepare the execution environment. It supports distributed task processing by running pickled job instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "_extract_packages_archive", "line_number": 59, "body": "def _extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)", "is_method": false, "function_description": "Utility function that extracts a \"packages.tar\" archive within a given directory and updates the Python module search path to include the current directory for package accessibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "main", "line_number": 77, "body": "def main(args=sys.argv):\n    \"\"\"Run the work() method from the class instance in the file \"job-instance.pickle\".\n    \"\"\"\n    try:\n        tarball = \"--no-tarball\" not in args\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to sge_runner.py must be a directory that exists\"\n        project_dir = args[2]\n        sys.path.append(project_dir)\n        _do_work_on_compute_node(work_dir, tarball)\n    except Exception as e:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(e)\n        raise", "is_method": false, "function_description": "Entrypoint function that initializes logging and runs a serialized job's work method from a specified directory, facilitating execution of prepackaged tasks on compute nodes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_parse_qstat_state", "line_number": 110, "body": "def _parse_qstat_state(qstat_out, job_id):\n    \"\"\"Parse \"state\" column from `qstat` output for given job_id\n\n    Returns state for the *first* job matching job_id. Returns 'u' if\n    `qstat` output is empty or job_id is not found.\n\n    \"\"\"\n    if qstat_out.strip() == '':\n        return 'u'\n    lines = qstat_out.split('\\n')\n    # skip past header\n    while not lines.pop(0).startswith('---'):\n        pass\n    for line in lines:\n        if line:\n            job, prior, name, user, state = line.strip().split()[0:5]\n            if int(job) == int(job_id):\n                return state\n    return 'u'", "is_method": false, "function_description": "Utility function that extracts the job state from qstat command output for a specified job ID, returning 'u' if the job is not found or output is empty. It enables status checking of jobs in cluster scheduling environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_parse_qsub_job_id", "line_number": 131, "body": "def _parse_qsub_job_id(qsub_out):\n    \"\"\"Parse job id from qsub output string.\n\n    Assume format:\n\n        \"Your job <job_id> (\"<job_name>\") has been submitted\"\n\n    \"\"\"\n    return int(qsub_out.split()[2])", "is_method": false, "function_description": "Utility function that extracts and returns the job ID integer from the standard output string of a qsub job submission command. It enables tracking or referencing submitted jobs based on their assigned IDs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_build_qsub_command", "line_number": 142, "body": "def _build_qsub_command(cmd, job_name, outfile, errfile, pe, n_cpu):\n    \"\"\"Submit shell command to SGE queue via `qsub`\"\"\"\n    qsub_template = \"\"\"echo {cmd} | qsub -o \":{outfile}\" -e \":{errfile}\" -V -r y -pe {pe} {n_cpu} -N {job_name}\"\"\"\n    return qsub_template.format(\n        cmd=cmd, job_name=job_name, outfile=outfile, errfile=errfile,\n        pe=pe, n_cpu=n_cpu)", "is_method": false, "function_description": "Constructs a command string to submit a shell job to an SGE cluster using `qsub`, specifying job details like output files, parallel environment, CPU count, and job name for batch processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_fetch_task_failures", "line_number": 216, "body": "def _fetch_task_failures(self):\n        if not os.path.exists(self.errfile):\n            logger.info('No error file')\n            return []\n        with open(self.errfile, \"r\") as f:\n            errors = f.readlines()\n        if errors == []:\n            return errors\n        if errors[0].strip() == 'stdin: is not a tty':  # SGE complains when we submit through a pipe\n            errors.pop(0)\n        return errors", "is_method": true, "class_name": "SGEJobTask", "function_description": "Private method of SGEJobTask that reads and returns error messages from a job's error file, filtering out non-critical initial warnings if present. It helps identify task failures from stored error logs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_init_local", "line_number": 228, "body": "def _init_local(self):\n\n        # Set up temp folder in shared directory (trim to max filename length)\n        base_tmp_dir = self.shared_tmp_dir\n        random_id = '%016x' % random.getrandbits(64)\n        folder_name = self.task_id + '-' + random_id\n        self.tmp_dir = os.path.join(base_tmp_dir, folder_name)\n        max_filename_length = os.fstatvfs(0).f_namemax\n        self.tmp_dir = self.tmp_dir[:max_filename_length]\n        logger.info(\"Tmp dir: %s\", self.tmp_dir)\n        os.makedirs(self.tmp_dir)\n\n        # Dump the code to be run into a pickle file\n        logging.debug(\"Dumping pickled class\")\n        self._dump(self.tmp_dir)\n\n        if not self.no_tarball:\n            # Make sure that all the class's dependencies are tarred and available\n            # This is not necessary if luigi is importable from the cluster node\n            logging.debug(\"Tarballing dependencies\")\n            # Grab luigi and the module containing the code to be run\n            packages = [luigi] + [__import__(self.__module__, None, None, 'dummy')]\n            create_packages_archive(packages, os.path.join(self.tmp_dir, \"packages.tar\"))", "is_method": true, "class_name": "SGEJobTask", "function_description": "Initializes a local temporary working directory with a unique name, storing serialized task code and optionally packaging necessary dependencies for isolated task execution in a shared environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "run", "line_number": 252, "body": "def run(self):\n        if self.run_locally:\n            self.work()\n        else:\n            self._init_local()\n            self._run_job()", "is_method": true, "class_name": "SGEJobTask", "function_description": "Method in SGEJobTask that executes the task either locally or remotely by deciding the appropriate execution path based on configuration. It enables flexible task running environments for job processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_dump", "line_number": 270, "body": "def _dump(self, out_dir=''):\n        \"\"\"Dump instance to file.\"\"\"\n        with self.no_unpicklable_properties():\n            self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace('(c__main__', \"(c\" + module_name)\n                with open(self.job_file, \"w\") as f:\n                    f.write(d)\n            else:\n                with open(self.job_file, \"wb\") as f:\n                    pickle.dump(self, f)", "is_method": true, "class_name": "SGEJobTask", "function_description": "This method serializes and saves the SGEJobTask instance to a file, handling module naming for pickling compatibility. It enables persisting task state for later retrieval or execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_run_job", "line_number": 284, "body": "def _run_job(self):\n\n        # Build a qsub argument that will run sge_runner.py on the directory we've specified\n        runner_path = sge_runner.__file__\n        if runner_path.endswith(\"pyc\"):\n            runner_path = runner_path[:-3] + \"py\"\n        job_str = 'python {0} \"{1}\" \"{2}\"'.format(\n            runner_path, self.tmp_dir, os.getcwd())  # enclose tmp_dir in quotes to protect from special escape chars\n        if self.no_tarball:\n            job_str += ' \"--no-tarball\"'\n\n        # Build qsub submit command\n        self.outfile = os.path.join(self.tmp_dir, 'job.out')\n        self.errfile = os.path.join(self.tmp_dir, 'job.err')\n        submit_cmd = _build_qsub_command(job_str, self.task_family, self.outfile,\n                                         self.errfile, self.parallel_env, self.n_cpu)\n        logger.debug('qsub command: \\n' + submit_cmd)\n\n        # Submit the job and grab job ID\n        output = subprocess.check_output(submit_cmd, shell=True)\n        self.job_id = _parse_qsub_job_id(output)\n        logger.debug(\"Submitted job to qsub with response:\\n\" + output)\n\n        self._track_job()\n\n        # Now delete the temporaries, if they're there.\n        if (self.tmp_dir and os.path.exists(self.tmp_dir) and not self.dont_remove_tmp_dir):\n            logger.info('Removing temporary directory %s' % self.tmp_dir)\n            subprocess.call([\"rm\", \"-rf\", self.tmp_dir])", "is_method": true, "class_name": "SGEJobTask", "function_description": "Core method of the SGEJobTask class that submits and tracks a job on an SGE cluster using qsub, managing job execution and temporary directories for task isolation and cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_track_job", "line_number": 314, "body": "def _track_job(self):\n        while True:\n            # Sleep for a little bit\n            time.sleep(self.poll_time)\n\n            # See what the job's up to\n            # ASSUMPTION\n            qstat_out = subprocess.check_output(['qstat'])\n            sge_status = _parse_qstat_state(qstat_out, self.job_id)\n            if sge_status == 'r':\n                logger.info('Job is running...')\n            elif sge_status == 'qw':\n                logger.info('Job is pending...')\n            elif 'E' in sge_status:\n                logger.error('Job has FAILED:\\n' + '\\n'.join(self._fetch_task_failures()))\n                break\n            elif sge_status == 't' or sge_status == 'u':\n                # Then the job could either be failed or done.\n                errors = self._fetch_task_failures()\n                if not errors:\n                    logger.info('Job is done')\n                else:\n                    logger.error('Job has FAILED:\\n' + '\\n'.join(errors))\n                break\n            else:\n                logger.info('Job status is UNKNOWN!')\n                logger.info('Status is : %s' % sge_status)\n                raise Exception(\"job status isn't one of ['r', 'qw', 'E*', 't', 'u']: %s\" % sge_status)", "is_method": true, "class_name": "SGEJobTask", "function_description": "Monitors the status of a grid engine job by periodically polling its state, logging progress, and detecting completion or failure to enable responsive job tracking and error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "run", "line_number": 352, "body": "def run(self):\n        self.work()", "is_method": true, "class_name": "LocalSGEJobTask", "function_description": "This method executes the primary work task defined for the LocalSGEJobTask instance. It serves as the entry point to perform the job's main operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcp.py", "function": "get_authenticate_kwargs", "line_number": 15, "body": "def get_authenticate_kwargs(oauth_credentials=None, http_=None):\n    \"\"\"Returns a dictionary with keyword arguments for use with discovery\n\n    Prioritizes oauth_credentials or a http client provided by the user\n    If none provided, falls back to default credentials provided by google's command line\n    utilities. If that also fails, tries using httplib2.Http()\n\n    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client\n    \"\"\"\n    if oauth_credentials:\n        authenticate_kwargs = {\n            \"credentials\": oauth_credentials\n        }\n    elif http_:\n        authenticate_kwargs = {\n            \"http\": http_\n        }\n    else:\n        # neither http_ or credentials provided\n        try:\n            # try default credentials\n            credentials, _ = google.auth.default()\n            authenticate_kwargs = {\n                \"credentials\": credentials\n            }\n        except google.auth.exceptions.DefaultCredentialsError:\n            # try http using httplib2\n            authenticate_kwargs = {\n                \"http\": httplib2.Http()\n            }\n\n    return authenticate_kwargs", "is_method": false, "function_description": "Utility function that constructs authentication parameters for API clients by selecting appropriate credentials or HTTP clients, supporting OAuth, user-provided clients, or fallback defaults for Google Cloud service initialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "engine", "line_number": 193, "body": "def engine(self):\n        \"\"\"\n        Return an engine instance, creating it if it doesn't exist.\n\n        Recreate the engine connection if it wasn't originally created\n        by the current process.\n        \"\"\"\n        pid = os.getpid()\n        conn = SQLAlchemyTarget._engine_dict.get(self.connection_string)\n        if not conn or conn.pid != pid:\n            # create and reset connection\n            engine = sqlalchemy.create_engine(\n                self.connection_string,\n                connect_args=self.connect_args,\n                echo=self.echo\n            )\n            SQLAlchemyTarget._engine_dict[self.connection_string] = self.Connection(engine, pid)\n        return SQLAlchemyTarget._engine_dict[self.connection_string].engine", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Core method of the SQLAlchemyTarget class that provides a SQLAlchemy engine instance, ensuring it is created or recreated if missing or from a different process, enabling reliable database connections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "touch", "line_number": 212, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n        \"\"\"\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n\n        table = self.marker_table_bound\n        id_exists = self.exists()\n        with self.engine.begin() as conn:\n            if not id_exists:\n                ins = table.insert().values(update_id=self.update_id, target_table=self.target_table,\n                                            inserted=datetime.datetime.now())\n            else:\n                ins = table.update().where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                           table.c.target_table == self.target_table)).\\\n                    values(update_id=self.update_id, target_table=self.target_table,\n                           inserted=datetime.datetime.now())\n            conn.execute(ins)\n        assert self.exists()", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Service method of SQLAlchemyTarget that marks an update as complete by inserting or updating a record in a marker table, tracking the update status and timestamp for synchronization or auditing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "exists", "line_number": 233, "body": "def exists(self):\n        row = None\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n        with self.engine.begin() as conn:\n            table = self.marker_table_bound\n            s = sqlalchemy.select([table]).where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                                 table.c.target_table == self.target_table)).limit(1)\n            row = conn.execute(s).fetchone()\n        return row is not None", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Checks if a specific update and target table record exists in the associated marker table. This method helps verify the presence of a marker entry to track database update progress or status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "create_marker_table", "line_number": 244, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        if self.marker_table is None:\n            self.marker_table = luigi.configuration.get_config().get('sqlalchemy', 'marker-table', 'table_updates')\n\n        engine = self.engine\n\n        with engine.begin() as con:\n            metadata = sqlalchemy.MetaData()\n            if not con.dialect.has_table(con, self.marker_table):\n                self.marker_table_bound = sqlalchemy.Table(\n                    self.marker_table, metadata,\n                    sqlalchemy.Column(\"update_id\", sqlalchemy.String(128), primary_key=True),\n                    sqlalchemy.Column(\"target_table\", sqlalchemy.String(128)),\n                    sqlalchemy.Column(\"inserted\", sqlalchemy.DateTime, default=datetime.datetime.now()))\n                metadata.create_all(engine)\n            else:\n                metadata.reflect(only=[self.marker_table], bind=engine)\n                self.marker_table_bound = metadata.tables[self.marker_table]", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Creates a marker table in the database if it doesn't already exist, supporting tracking of table updates within the SQLAlchemyTarget context. This ensures a dedicated structure for recording update metadata during data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "connection_string", "line_number": 289, "body": "def connection_string(self):\n        return None", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns None, indicating no connection string is provided or configured by this method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "create_table", "line_number": 320, "body": "def create_table(self, engine):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types specified in columns.\n        If the table exists, then it binds to the existing table.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        :param engine: The sqlalchemy engine instance\n        :type engine: object\n        \"\"\"\n        def construct_sqla_columns(columns):\n            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]\n            return retval\n\n        needs_setup = (len(self.columns) == 0) or (False in [len(c) == 2 for c in self.columns]) if not self.reflect else False\n        if needs_setup:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        else:\n            # if columns is specified as (name, type) tuples\n            with engine.begin() as con:\n\n                if self.schema:\n                    metadata = sqlalchemy.MetaData(schema=self.schema)\n                else:\n                    metadata = sqlalchemy.MetaData()\n\n                try:\n                    if not con.dialect.has_table(con, self.table, self.schema or None):\n                        sqla_columns = construct_sqla_columns(self.columns)\n                        self.table_bound = sqlalchemy.Table(self.table, metadata, *sqla_columns)\n                        metadata.create_all(engine)\n                    else:\n                        full_table = '.'.join([self.schema, self.table]) if self.schema else self.table\n                        metadata.reflect(only=[self.table], bind=engine)\n                        self.table_bound = metadata.tables[full_table]\n                except Exception as e:\n                    self._logger.exception(self.table + str(e))", "is_method": true, "class_name": "CopyToTable", "function_description": "Core method of CopyToTable that creates or binds to a database table using SQLAlchemy, ensuring the table matches specified columns. It supports transactional table setup and handles schema reflection for existing tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "update_id", "line_number": 361, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns a unique identifier representing the current insert operation in the table, enabling tracking or referencing of tasks within the CopyToTable context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "output", "line_number": 367, "body": "def output(self):\n        return SQLAlchemyTarget(\n            connection_string=self.connection_string,\n            target_table=self.table,\n            update_id=self.update_id(),\n            connect_args=self.connect_args,\n            echo=self.echo)", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides a configured SQLAlchemyTarget instance for writing data to a specific database table, using connection details and update identification to manage target output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "rows", "line_number": 375, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n\n        This method can be overridden for custom file types or formats.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip(\"\\n\").split(self.column_separator)", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides an iterable of rows as tuples or lists for insertion, parsing lines from an input source based on a column separator. Can be customized for different file formats or data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "run", "line_number": 385, "body": "def run(self):\n        self._logger.info(\"Running task copy to table for update id %s for table %s\" % (self.update_id(), self.table))\n        output = self.output()\n        engine = output.engine\n        self.create_table(engine)\n        with engine.begin() as conn:\n            rows = iter(self.rows())\n            ins_rows = [dict(zip((\"_\" + c.key for c in self.table_bound.c), row))\n                        for row in itertools.islice(rows, self.chunk_size)]\n            while ins_rows:\n                self.copy(conn, ins_rows, self.table_bound)\n                ins_rows = [dict(zip((\"_\" + c.key for c in self.table_bound.c), row))\n                            for row in itertools.islice(rows, self.chunk_size)]\n                self._logger.info(\"Finished inserting %d rows into SQLAlchemy target\" % len(ins_rows))\n        output.touch()\n        self._logger.info(\"Finished inserting rows into SQLAlchemy target\")", "is_method": true, "class_name": "CopyToTable", "function_description": "Executes a batch process that creates a target table if needed and copies rows into it in chunks, ensuring efficient insertion into a database table while logging progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "copy", "line_number": 402, "body": "def copy(self, conn, ins_rows, table_bound):\n        \"\"\"\n        This method does the actual insertion of the rows of data given by ins_rows into the\n        database. A task that needs row updates instead of insertions should overload this method.\n        :param conn: The sqlalchemy connection object\n        :param ins_rows: The dictionary of rows with the keys in the format _<column_name>. For example\n        if you have a table with a column name \"property\", then the key in the dictionary\n        would be \"_property\". This format is consistent with the bindparam usage in sqlalchemy.\n        :param table_bound: The object referring to the table\n        :return:\n        \"\"\"\n        bound_cols = dict((c, sqlalchemy.bindparam(\"_\" + c.key)) for c in table_bound.columns)\n        ins = table_bound.insert().values(bound_cols)\n        conn.execute(ins, ins_rows)", "is_method": true, "class_name": "CopyToTable", "function_description": "Service method of CopyToTable that inserts multiple rows into a specified database table using a SQLAlchemy connection, designed for straightforward data insertion tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "construct_sqla_columns", "line_number": 332, "body": "def construct_sqla_columns(columns):\n            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]\n            return retval", "is_method": true, "class_name": "CopyToTable", "function_description": "Core utility of the CopyToTable class that builds SQLAlchemy Column objects from given column specifications, facilitating dynamic table column construction for database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "do_work_on_compute_node", "line_number": 34, "body": "def do_work_on_compute_node(work_dir):\n    # Extract the necessary dependencies\n    extract_packages_archive(work_dir)\n\n    # Open up the pickle file with the work to be done\n    os.chdir(work_dir)\n    with open(\"job-instance.pickle\", \"r\") as pickle_file_handle:\n        job = pickle.load(pickle_file_handle)\n\n    # Do the work contained\n    job.work()", "is_method": false, "function_description": "This function sets up a compute node environment by extracting dependencies, loading a serialized job, and executing its work method. It automates running predefined tasks on specified directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "extract_packages_archive", "line_number": 47, "body": "def extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)", "is_method": false, "function_description": "Function that extracts a packages archive from a specified directory and ensures the current working directory is restored, supporting dynamic package management in that directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "main", "line_number": 65, "body": "def main(args=sys.argv):\n    \"\"\"Run the work() method from the class instance in the file \"job-instance.pickle\".\n    \"\"\"\n    try:\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to lsf_runner.py must be a directory that exists\"\n        do_work_on_compute_node(work_dir)\n    except Exception as exc:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(exc)\n        raise", "is_method": false, "function_description": "Function that initializes logging, validates the first argument as an existing directory, and triggers a compute node processing workflow; it primarily serves as an entry point to run distributed or batch jobs with error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "marker_index_document_id", "line_number": 159, "body": "def marker_index_document_id(self):\n        \"\"\"\n        Generate an id for the indicator document.\n        \"\"\"\n        params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n        return hashlib.sha1(params.encode('utf-8')).hexdigest()", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Generates a unique SHA-1 hash identifier for an indicator document based on index, document type, and update ID. This ID can be used to uniquely reference or track documents within ElasticsearchTarget."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "touch", "line_number": 166, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n\n        The document id would be sufficient but,\n        for documentation,\n        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.\n        \"\"\"\n        self.create_marker_index()\n        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n                      id=self.marker_index_document_id(), body={\n                          'update_id': self.update_id,\n                          'target_index': self.index,\n                          'target_doc_type': self.doc_type,\n                          'date': datetime.datetime.now()})\n        self.es.indices.flush(index=self.marker_index)\n        self.ensure_hist_size()", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Class ElasticsearchTarget\n\nCore utility that marks an update as complete by indexing metadata about the update in a dedicated marker index, supporting tracking and monitoring of update operations within Elasticsearch targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "exists", "line_number": 184, "body": "def exists(self):\n        \"\"\"\n        Test, if this task has been run.\n        \"\"\"\n        try:\n            self.es.get(index=self.marker_index, doc_type=self.marker_doc_type, id=self.marker_index_document_id())\n            return True\n        except elasticsearch.NotFoundError:\n            logger.debug('Marker document not found.')\n        except elasticsearch.ElasticsearchException as err:\n            logger.warn(err)\n        return False", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Method of ElasticsearchTarget that checks whether a specific marker document exists, indicating if the related task has been executed before. It helps prevent redundant task runs by confirming prior completion status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "create_marker_index", "line_number": 197, "body": "def create_marker_index(self):\n        \"\"\"\n        Create the index that will keep track of the tasks if necessary.\n        \"\"\"\n        if not self.es.indices.exists(index=self.marker_index):\n            self.es.indices.create(index=self.marker_index)", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Creates an Elasticsearch index to track task statuses, ensuring it exists before use. This function supports task management within the ElasticsearchTarget class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "ensure_hist_size", "line_number": 204, "body": "def ensure_hist_size(self):\n        \"\"\"\n        Shrink the history of updates for\n        a `index/doc_type` combination down to `self.marker_index_hist_size`.\n        \"\"\"\n        if self.marker_index_hist_size == 0:\n            return\n        result = self.es.search(index=self.marker_index,\n                                doc_type=self.marker_doc_type,\n                                body={'query': {\n                                    'term': {'target_index': self.index}}},\n                                sort=('date:desc',))\n\n        for i, hit in enumerate(result.get('hits').get('hits'), start=1):\n            if i > self.marker_index_hist_size:\n                marker_document_id = hit.get('_id')\n                self.es.delete(id=marker_document_id, index=self.marker_index,\n                               doc_type=self.marker_doc_type)\n        self.es.indices.flush(index=self.marker_index)", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Method of ElasticsearchTarget that limits update history size by removing oldest update markers beyond a configured threshold, helping maintain controlled storage use and efficient index management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "host", "line_number": 254, "body": "def host(self):\n        \"\"\"\n        ES hostname.\n        \"\"\"\n        return 'localhost'", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the hostname for the Elasticsearch instance, defaulting to 'localhost'. This function provides connection configuration within the CopyToIndex class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "port", "line_number": 261, "body": "def port(self):\n        \"\"\"\n        ES port.\n        \"\"\"\n        return 9200", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the default port number used for connecting to an Elasticsearch instance within the CopyToIndex class. This function standardizes the port configuration for Elasticsearch interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "doc_type", "line_number": 286, "body": "def doc_type(self):\n        \"\"\"\n        The target doc_type.\n        \"\"\"\n        return 'default'", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the target document type string associated with the CopyToIndex operation, defaulting to 'default'. This method identifies the classification of documents for indexing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "mapping", "line_number": 293, "body": "def mapping(self):\n        \"\"\"\n        Dictionary with custom mapping or `None`.\n        \"\"\"\n        return None", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the custom mapping configuration for the index, or None if no specific mapping is defined. This supports flexibility in how data is structured when copied to the index."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "settings", "line_number": 300, "body": "def settings(self):\n        \"\"\"\n        Settings to be used at index creation time.\n        \"\"\"\n        return {'settings': {}}", "is_method": true, "class_name": "CopyToIndex", "function_description": "Provides default configuration settings for index creation, serving as a basic template that can be customized or extended by other components during indexing setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "chunk_size", "line_number": 307, "body": "def chunk_size(self):\n        \"\"\"\n        Single API call for this number of docs.\n        \"\"\"\n        return 2000", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the maximum number of documents to process in a single API call, facilitating efficient batching in the CopyToIndex class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "raise_on_error", "line_number": 314, "body": "def raise_on_error(self):\n        \"\"\"\n        Whether to fail fast.\n        \"\"\"\n        return True", "is_method": true, "class_name": "CopyToIndex", "function_description": "Core configuration method of the CopyToIndex class that indicates whether operations should immediately raise an error upon failure, enabling fail-fast behavior for robust error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "purge_existing_index", "line_number": 321, "body": "def purge_existing_index(self):\n        \"\"\"\n        Whether to delete the `index` completely before any indexing.\n        \"\"\"\n        return False", "is_method": true, "class_name": "CopyToIndex", "function_description": "Method of CopyToIndex that indicates if the existing index should be fully deleted before reindexing. It provides a control flag to manage index reset behavior during the copy operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "marker_index_hist_size", "line_number": 328, "body": "def marker_index_hist_size(self):\n        \"\"\"\n        Number of event log entries in the marker index. 0: unlimited.\n        \"\"\"\n        return 0", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the size limit of the marker index's event log entries, where zero indicates no limit. It provides a configuration reference for managing event log capacity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "timeout", "line_number": 335, "body": "def timeout(self):\n        \"\"\"\n        Timeout.\n        \"\"\"\n        return 10", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns a fixed timeout value used to limit operation duration within the CopyToIndex context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "extra_elasticsearch_args", "line_number": 342, "body": "def extra_elasticsearch_args(self):\n        \"\"\"\n        Extra arguments to pass to the Elasticsearch constructor\n        \"\"\"\n        return {}", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method provides additional configuration arguments for initializing an Elasticsearch client. It allows customizing the connection settings when integrating with Elasticsearch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "docs", "line_number": 348, "body": "def docs(self):\n        \"\"\"\n        Return the documents to be indexed.\n\n        Beside the user defined fields, the document may contain an `_index`, `_type` and `_id`.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line", "is_method": true, "class_name": "CopyToIndex", "function_description": "Method of the CopyToIndex class that yields documents from an input source for indexing, including user-defined and system-specific fields. It provides a stream of documents ready to be processed or inserted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "_docs", "line_number": 360, "body": "def _docs(self):\n        \"\"\"\n        Since `self.docs` may yield documents that do not explicitly contain `_index` or `_type`,\n        add those attributes here, if necessary.\n        \"\"\"\n        iterdocs = iter(self.docs())\n        first = next(iterdocs)\n        needs_parsing = False\n        if isinstance(first, str):\n            needs_parsing = True\n        elif isinstance(first, dict):\n            pass\n        else:\n            raise RuntimeError('Document must be either JSON strings or dict.')\n        for doc in itertools.chain([first], iterdocs):\n            if needs_parsing:\n                doc = json.loads(doc)\n            if '_index' not in doc:\n                doc['_index'] = self.index\n            if '_type' not in doc:\n                doc['_type'] = self.doc_type\n            yield doc", "is_method": true, "class_name": "CopyToIndex", "function_description": "Processes an iterable of documents by ensuring each has necessary metadata attributes (_index and _type), parsing JSON strings if needed, and yielding documents in a consistent dictionary format for indexing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "_init_connection", "line_number": 383, "body": "def _init_connection(self):\n        return elasticsearch.Elasticsearch(\n            connection_class=Urllib3HttpConnection,\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            timeout=self.timeout,\n            **self.extra_elasticsearch_args\n        )", "is_method": true, "class_name": "CopyToIndex", "function_description": "Private method in CopyToIndex that establishes and returns a configured Elasticsearch connection using provided host, port, authentication, and additional settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "create_index", "line_number": 393, "body": "def create_index(self):\n        \"\"\"\n        Override to provide code for creating the target index.\n\n        By default it will be created without any special settings or mappings.\n        \"\"\"\n        es = self._init_connection()\n        if not es.indices.exists(index=self.index):\n            es.indices.create(index=self.index, body=self.settings)", "is_method": true, "class_name": "CopyToIndex", "function_description": "Creates the target index in the connected Elasticsearch service if it does not already exist, applying any specified settings. This enables subsequent data insertion or querying against the defined index."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "delete_index", "line_number": 403, "body": "def delete_index(self):\n        \"\"\"\n        Delete the index, if it exists.\n        \"\"\"\n        es = self._init_connection()\n        if es.indices.exists(index=self.index):\n            es.indices.delete(index=self.index)", "is_method": true, "class_name": "CopyToIndex", "function_description": "Method of the CopyToIndex class that removes the specified search index if it currently exists, allowing cleanup or reinitialization of index data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "update_id", "line_number": 411, "body": "def update_id(self):\n        \"\"\"\n        This id will be a unique identifier for this indexing task.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns the unique identifier for the current indexing task, enabling consistent reference and tracking within the CopyToIndex process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "output", "line_number": 417, "body": "def output(self):\n        \"\"\"\n        Returns a ElasticsearchTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return ElasticsearchTarget(\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            index=self.index,\n            doc_type=self.doc_type,\n            update_id=self.update_id(),\n            marker_index_hist_size=self.marker_index_hist_size,\n            timeout=self.timeout,\n            extra_elasticsearch_args=self.extra_elasticsearch_args\n        )", "is_method": true, "class_name": "CopyToIndex", "function_description": "Provides an ElasticsearchTarget object representing the destination index for the inserted dataset, encapsulating connection and configuration details for downstream operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "run", "line_number": 435, "body": "def run(self):\n        \"\"\"\n        Run task, namely:\n\n        * purge existing index, if requested (`purge_existing_index`),\n        * create the index, if missing,\n        * apply mappings, if given,\n        * set refresh interval to -1 (disable) for performance reasons,\n        * bulk index in batches of size `chunk_size` (2000),\n        * set refresh interval to 1s,\n        * refresh Elasticsearch,\n        * create entry in marker index.\n        \"\"\"\n        if self.purge_existing_index:\n            self.delete_index()\n        self.create_index()\n        es = self._init_connection()\n        if self.mapping:\n            es.indices.put_mapping(index=self.index, doc_type=self.doc_type,\n                                   body=self.mapping)\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n                                index=self.index)\n\n        bulk(es, self._docs(), chunk_size=self.chunk_size,\n             raise_on_error=self.raise_on_error)\n\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n                                index=self.index)\n        es.indices.refresh()\n        self.output().touch()", "is_method": true, "class_name": "CopyToIndex", "function_description": "Service method of CopyToIndex that (optionally) clears, creates, configures, and bulk loads documents into an Elasticsearch index efficiently, managing refresh settings for performance and maintaining indexing metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_soql_fields", "line_number": 39, "body": "def get_soql_fields(soql):\n    \"\"\"\n    Gets queried columns names.\n    \"\"\"\n    soql_fields = re.search('(?<=select)(?s)(.*)(?=from)', soql, re.IGNORECASE)     # get fields\n    soql_fields = re.sub(' ', '', soql_fields.group())                              # remove extra spaces\n    soql_fields = re.sub('\\t', '', soql_fields)                                     # remove tabs\n    fields = re.split(',|\\n|\\r|', soql_fields)                                      # split on commas and newlines\n    fields = [field for field in fields if field != '']                             # remove empty strings\n    return fields", "is_method": false, "function_description": "Function that extracts and returns the list of column names specified in the SELECT clause of a SOQL query string. It parses the query to identify which fields are being queried for further processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "ensure_utf", "line_number": 51, "body": "def ensure_utf(value):\n    return value.encode(\"utf-8\") if isinstance(value, unicode) else value", "is_method": false, "function_description": "Function that ensures a string value is UTF-8 encoded by encoding it if it is a Unicode string, otherwise returning it unchanged. Useful for consistent string encoding handling in text processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "parse_results", "line_number": 55, "body": "def parse_results(fields, data):\n    \"\"\"\n    Traverses ordered dictionary, calls _traverse_results() to recursively read into the dictionary depth of data\n    \"\"\"\n    master = []\n\n    for record in data['records']:  # for each 'record' in response\n        row = [None] * len(fields)  # create null list the length of number of columns\n        for obj, value in record.items():  # for each obj in record\n            if not isinstance(value, (dict, list, tuple)):  # if not data structure\n                if obj in fields:\n                    row[fields.index(obj)] = ensure_utf(value)\n\n            elif isinstance(value, dict) and obj != 'attributes':  # traverse down into object\n                path = obj\n                _traverse_results(value, fields, row, path)\n\n        master.append(row)\n    return master", "is_method": false, "function_description": "Utility function that processes nested dictionary data records into a list of rows, extracting specified fields for each record. It supports flattening complex data structures for easier downstream data handling or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_traverse_results", "line_number": 76, "body": "def _traverse_results(value, fields, row, path):\n    \"\"\"\n    Helper method for parse_results().\n\n    Traverses through ordered dict and recursively calls itself when encountering a dictionary\n    \"\"\"\n    for f, v in value.items():  # for each item in obj\n        field_name = '{path}.{name}'.format(path=path, name=f) if path else f\n\n        if not isinstance(v, (dict, list, tuple)):  # if not data structure\n            if field_name in fields:\n                row[fields.index(field_name)] = ensure_utf(v)\n\n        elif isinstance(v, dict) and f != 'attributes':  # it is a dict\n            _traverse_results(v, fields, row, field_name)", "is_method": false, "function_description": "Internal helper function that recursively traverses nested dictionaries to extract and assign specified field values to a row structure, supporting detailed parsing of hierarchical data in parse_results()."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "object_name", "line_number": 110, "body": "def object_name(self):\n        \"\"\"\n        Override to return the SF object we are querying.\n        Must have the SF \"__c\" suffix if it is a customer object.\n        \"\"\"\n        return None", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Returns the name of the Salesforce object targeted by the query, typically including the \"__c\" suffix for custom objects. Provides a customizable identifier for specific Salesforce queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "use_sandbox", "line_number": 118, "body": "def use_sandbox(self):\n        \"\"\"\n        Override to specify use of SF sandbox.\n        True iff we should be uploading to a sandbox environment instead of the production organization.\n        \"\"\"\n        return False", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Indicates whether Salesforce operations should target a sandbox environment instead of production, enabling safe testing and development separate from live data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "sandbox_name", "line_number": 126, "body": "def sandbox_name(self):\n        \"\"\"Override to specify the sandbox name if it is intended to be used.\"\"\"\n        return None", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Returns the name of the Salesforce sandbox environment if specified; otherwise, indicates no sandbox is used. This allows customization of the target environment for Salesforce API interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "soql", "line_number": 132, "body": "def soql(self):\n        \"\"\"Override to return the raw string SOQL or the path to it.\"\"\"\n        return None", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Returns the raw SOQL query string or its file path for Salesforce queries. Provides the base query definition for Salesforce data retrieval operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "is_soql_file", "line_number": 137, "body": "def is_soql_file(self):\n        \"\"\"Override to True if soql property is a file path.\"\"\"\n        return False", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Returns False to indicate the SOQL property is not a file path; used to differentiate between query strings and file-based SOQL inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "content_type", "line_number": 142, "body": "def content_type(self):\n        \"\"\"\n        Override to use a different content type. Salesforce allows XML, CSV, ZIP_CSV, or ZIP_XML. Defaults to CSV.\n        \"\"\"\n        return \"CSV\"", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Overrides the content type to specify the format for Salesforce queries, defaulting to CSV among supported formats like XML and ZIP variants. It configures data retrieval output suited for Salesforce integrations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "run", "line_number": 148, "body": "def run(self):\n        if self.use_sandbox and not self.sandbox_name:\n            raise Exception(\"Parameter sf_sandbox_name must be provided when uploading to a Salesforce Sandbox\")\n\n        sf = SalesforceAPI(salesforce().username,\n                           salesforce().password,\n                           salesforce().security_token,\n                           salesforce().sb_security_token,\n                           self.sandbox_name)\n\n        job_id = sf.create_operation_job('query', self.object_name, content_type=self.content_type)\n        logger.info(\"Started query job %s in salesforce for object %s\" % (job_id, self.object_name))\n\n        batch_id = ''\n        msg = ''\n        try:\n            if self.is_soql_file:\n                with open(self.soql, 'r') as infile:\n                    self.soql = infile.read()\n\n            batch_id = sf.create_batch(job_id, self.soql, self.content_type)\n            logger.info(\"Creating new batch %s to query: %s for job: %s.\" % (batch_id, self.object_name, job_id))\n            status = sf.block_on_batch(job_id, batch_id)\n            if status['state'].lower() == 'failed':\n                msg = \"Batch failed with message: %s\" % status['state_message']\n                logger.error(msg)\n                # don't raise exception if it's b/c of an included relationship\n                # normal query will execute (with relationship) after bulk job is closed\n                if 'foreign key relationships not supported' not in status['state_message'].lower():\n                    raise Exception(msg)\n            else:\n                result_ids = sf.get_batch_result_ids(job_id, batch_id)\n\n                # If there's only one result, just download it, otherwise we need to merge the resulting downloads\n                if len(result_ids) == 1:\n                    data = sf.get_batch_result(job_id, batch_id, result_ids[0])\n                    with open(self.output().path, 'wb') as outfile:\n                        outfile.write(data)\n                else:\n                    # Download each file to disk, and then merge into one.\n                    # Preferring to do it this way so as to minimize memory consumption.\n                    for i, result_id in enumerate(result_ids):\n                        logger.info(\"Downloading batch result %s for batch: %s and job: %s\" % (result_id, batch_id, job_id))\n                        with open(\"%s.%d\" % (self.output().path, i), 'wb') as outfile:\n                            outfile.write(sf.get_batch_result(job_id, batch_id, result_id))\n\n                    logger.info(\"Merging results of batch %s\" % batch_id)\n                    self.merge_batch_results(result_ids)\n        finally:\n            logger.info(\"Closing job %s\" % job_id)\n            sf.close_job(job_id)\n\n        if 'state_message' in status and 'foreign key relationships not supported' in status['state_message'].lower():\n            logger.info(\"Retrying with REST API query\")\n            data_file = sf.query_all(self.soql)\n\n            reader = csv.reader(data_file)\n            with open(self.output().path, 'wb') as outfile:\n                writer = csv.writer(outfile, dialect='excel')\n                for row in reader:\n                    writer.writerow(row)", "is_method": true, "class_name": "QuerySalesforce", "function_description": "This method executes a bulk query against Salesforce, handling batch processing, result downloading, and fallback to REST API if needed. It provides reliable retrieval of Salesforce object data, supporting sandbox usage and large query results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "merge_batch_results", "line_number": 210, "body": "def merge_batch_results(self, result_ids):\n        \"\"\"\n        Merges the resulting files of a multi-result batch bulk query.\n        \"\"\"\n        outfile = open(self.output().path, 'w')\n\n        if self.content_type.lower() == 'csv':\n            for i, result_id in enumerate(result_ids):\n                with open(\"%s.%d\" % (self.output().path, i), 'r') as f:\n                    header = f.readline()\n                    if i == 0:\n                        outfile.write(header)\n                    for line in f:\n                        outfile.write(line)\n        else:\n            raise Exception(\"Batch result merging not implemented for %s\" % self.content_type)\n\n        outfile.close()", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Merges multiple batch query result files into a single CSV output file, consolidating headers and data lines for easier post-processing of Salesforce query results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "start_session", "line_number": 253, "body": "def start_session(self):\n        \"\"\"\n        Starts a Salesforce session and determines which SF instance to use for future requests.\n        \"\"\"\n        if self.has_active_session():\n            raise Exception(\"Session already in progress.\")\n\n        response = requests.post(self._get_login_url(),\n                                 headers=self._get_login_headers(),\n                                 data=self._get_login_xml())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        for e in root.iter(\"%ssessionId\" % self.SOAP_NS):\n            if self.session_id:\n                raise Exception(\"Invalid login attempt.  Multiple session ids found.\")\n            self.session_id = e.text\n\n        for e in root.iter(\"%sserverUrl\" % self.SOAP_NS):\n            if self.server_url:\n                raise Exception(\"Invalid login attempt.  Multiple server urls found.\")\n            self.server_url = e.text\n\n        if not self.has_active_session():\n            raise Exception(\"Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s].\" %\n                            (self.session_id, self.server_url))\n        self.hostname = urlsplit(self.server_url).hostname", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Service method of SalesforceAPI that initiates and validates a new Salesforce session, identifying the server instance for future API interactions while ensuring no duplicate or invalid sessions exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "has_active_session", "line_number": 281, "body": "def has_active_session(self):\n        return self.session_id and self.server_url", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Checks if the SalesforceAPI instance currently holds an active session by verifying the presence of session credentials. Useful for determining connection status before making API calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query", "line_number": 284, "body": "def query(self, query, **kwargs):\n        \"\"\"\n        Return the result of a Salesforce SOQL query as a dict decoded from the Salesforce response JSON payload.\n\n        :param query: the SOQL query to send to Salesforce, e.g. \"SELECT id from Lead WHERE email = 'a@b.com'\"\n        \"\"\"\n        params = {'q': query}\n        response = requests.get(self._get_norm_query_url(),\n                                headers=self._get_rest_headers(),\n                                params=params,\n                                **kwargs)\n        if response.status_code != requests.codes.ok:\n            raise Exception(response.content)\n\n        return response.json()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Core method of the SalesforceAPI class that executes a SOQL query and returns the Salesforce response as a JSON-decoded dictionary for easy data retrieval from Salesforce objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query_more", "line_number": 300, "body": "def query_more(self, next_records_identifier, identifier_is_url=False, **kwargs):\n        \"\"\"\n        Retrieves more results from a query that returned more results\n        than the batch maximum. Returns a dict decoded from the Salesforce\n        response JSON payload.\n\n        :param next_records_identifier: either the Id of the next Salesforce\n                                     object in the result, or a URL to the\n                                     next record in the result.\n        :param identifier_is_url: True if `next_records_identifier` should be\n                               treated as a URL, False if\n                               `next_records_identifer` should be treated as\n                               an Id.\n        \"\"\"\n        if identifier_is_url:\n            # Don't use `self.base_url` here because the full URI is provided\n            url = (u'https://{instance}{next_record_url}'\n                   .format(instance=self.hostname,\n                           next_record_url=next_records_identifier))\n        else:\n            url = self._get_norm_query_url() + '{next_record_id}'\n            url = url.format(next_record_id=next_records_identifier)\n        response = requests.get(url, headers=self._get_rest_headers(), **kwargs)\n\n        response.raise_for_status()\n\n        return response.json()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Utility method of SalesforceAPI that fetches additional query results beyond the initial batch limit, supporting retrieval via either a record ID or a full URL to seamlessly paginate through Salesforce data responses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query_all", "line_number": 328, "body": "def query_all(self, query, **kwargs):\n        \"\"\"\n        Returns the full set of results for the `query`. This is a\n        convenience wrapper around `query(...)` and `query_more(...)`.\n        The returned dict is the decoded JSON payload from the final call to\n        Salesforce, but with the `totalSize` field representing the full\n        number of results retrieved and the `records` list representing the\n        full list of records retrieved.\n\n        :param query: the SOQL query to send to Salesforce, e.g.\n                   `SELECT Id FROM Lead WHERE Email = \"waldo@somewhere.com\"`\n        \"\"\"\n        # Make the initial query to Salesforce\n        response = self.query(query, **kwargs)\n\n        # get fields\n        fields = get_soql_fields(query)\n\n        # put fields and first page of results into a temp list to be written to TempFile\n        tmp_list = [fields]\n        tmp_list.extend(parse_results(fields, response))\n\n        tmp_dir = luigi.configuration.get_config().get('salesforce', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(mode='a+b', dir=tmp_dir)\n\n        writer = csv.writer(tmp_file)\n        writer.writerows(tmp_list)\n\n        # The number of results might have exceeded the Salesforce batch limit\n        # so check whether there are more results and retrieve them if so.\n\n        length = len(response['records'])\n        while not response['done']:\n            response = self.query_more(response['nextRecordsUrl'], identifier_is_url=True, **kwargs)\n\n            writer.writerows(parse_results(fields, response))\n            length += len(response['records'])\n            if not length % 10000:\n                logger.info('Requested {0} lines...'.format(length))\n\n        logger.info('Requested a total of {0} lines.'.format(length))\n\n        tmp_file.seek(0)\n        return tmp_file", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Core method of the SalesforceAPI class that executes a SOQL query and retrieves all matching records, handling pagination transparently and returning a temporary file containing the complete dataset for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "restful", "line_number": 374, "body": "def restful(self, path, params):\n        \"\"\"\n        Allows you to make a direct REST call if you know the path\n        Arguments:\n        :param path: The path of the request. Example: sobjects/User/ABC123/password'\n        :param params: dict of parameters to pass to the path\n        \"\"\"\n\n        url = self._get_norm_base_url() + path\n        response = requests.get(url, headers=self._get_rest_headers(), params=params)\n\n        if response.status_code != 200:\n            raise Exception(response)\n        json_result = response.json(object_pairs_hook=OrderedDict)\n        if len(json_result) == 0:\n            return None\n        else:\n            return json_result", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides a low-level interface to perform authenticated REST API calls to Salesforce using a specified path and parameters, returning the JSON response or raising an error on failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "create_operation_job", "line_number": 393, "body": "def create_operation_job(self, operation, obj, external_id_field_name=None, content_type=None):\n        \"\"\"\n        Creates a new SF job that for doing any operation (insert, upsert, update, delete, query)\n\n        :param operation: delete, insert, query, upsert, update, hardDelete. Must be lowercase.\n        :param obj: Parent SF object\n        :param external_id_field_name: Optional.\n        \"\"\"\n        if not self.has_active_session():\n            self.start_session()\n\n        response = requests.post(self._get_create_job_url(),\n                                 headers=self._get_create_job_headers(),\n                                 data=self._get_create_job_xml(operation, obj, external_id_field_name, content_type))\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        job_id = root.find('%sid' % self.API_NS).text\n        return job_id", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Creates and initiates a Salesforce bulk API job for specified data operations like insert, update, delete, or query, returning the job ID for tracking and managing asynchronous data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_job_details", "line_number": 413, "body": "def get_job_details(self, job_id):\n        \"\"\"\n        Gets all details for existing job\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: job info as xml\n        \"\"\"\n        response = requests.get(self._get_job_details_url(job_id))\n\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides detailed information about an existing job identified by job_id within Salesforce via an API call. It returns the job's info in XML format for monitoring or processing purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "abort_job", "line_number": 426, "body": "def abort_job(self, job_id):\n        \"\"\"\n        Abort an existing job. When a job is aborted, no more records are processed.\n        Changes to data may already have been committed and aren't rolled back.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: abort response as xml\n        \"\"\"\n        response = requests.post(self._get_abort_job_url(job_id),\n                                 headers=self._get_abort_job_headers(),\n                                 data=self._get_abort_job_xml())\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Method of the SalesforceAPI class that aborts a running job, stopping further processing without rolling back any already committed changes. Useful for canceling in-progress operations prematurely."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "close_job", "line_number": 441, "body": "def close_job(self, job_id):\n        \"\"\"\n        Closes job\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: close response as xml\n        \"\"\"\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not close job without valid job_id and an active session.\")\n\n        response = requests.post(self._get_close_job_url(job_id),\n                                 headers=self._get_close_job_headers(),\n                                 data=self._get_close_job_xml())\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Method in SalesforceAPI that closes an existing job identified by job_id, ensuring an active session, and returns the server's XML response to confirm the job closure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "create_batch", "line_number": 458, "body": "def create_batch(self, job_id, data, file_type):\n        \"\"\"\n        Creates a batch with either a string of data or a file containing data.\n\n        If a file is provided, this will pull the contents of the file_target into memory when running.\n        That shouldn't be a problem for any files that meet the Salesforce single batch upload\n        size limit (10MB) and is done to ensure compressed files can be uploaded properly.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param data:\n\n        :return: Returns batch_id\n        \"\"\"\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not create a batch without a valid job_id and an active session.\")\n\n        headers = self._get_create_batch_content_headers(file_type)\n        headers['Content-Length'] = str(len(data))\n\n        response = requests.post(self._get_create_batch_url(job_id),\n                                 headers=headers,\n                                 data=data)\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        batch_id = root.find('%sid' % self.API_NS).text\n        return batch_id", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Creates and uploads a data batch to a specified Salesforce job, returning the batch ID for tracking. It supports both raw data strings and files, enabling efficient bulk data operations within active sessions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "block_on_batch", "line_number": 486, "body": "def block_on_batch(self, job_id, batch_id, sleep_time_seconds=5, max_wait_time_seconds=-1):\n        \"\"\"\n        Blocks until @batch_id is completed or failed.\n        :param job_id:\n        :param batch_id:\n        :param sleep_time_seconds:\n        :param max_wait_time_seconds:\n        \"\"\"\n        if not job_id or not batch_id or not self.has_active_session():\n            raise Exception(\"Can not block on a batch without a valid batch_id, job_id and an active session.\")\n\n        start_time = time.time()\n        status = {}\n        while max_wait_time_seconds < 0 or time.time() - start_time < max_wait_time_seconds:\n            status = self._get_batch_info(job_id, batch_id)\n            logger.info(\"Batch %s Job %s in state %s.  %s records processed.  %s records failed.\" %\n                        (batch_id, job_id, status['state'], status['num_processed'], status['num_failed']))\n            if status['state'].lower() in [\"completed\", \"failed\"]:\n                return status\n            time.sleep(sleep_time_seconds)\n\n        raise Exception(\"Batch did not complete in %s seconds.  Final status was: %s\" % (sleep_time_seconds, status))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Blocks execution until a Salesforce batch job completes or fails, with optional polling intervals and timeout. It helps monitor asynchronous batch processing by waiting for job completion status updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_results", "line_number": 509, "body": "def get_batch_results(self, job_id, batch_id):\n        \"\"\"\n        DEPRECATED: Use `get_batch_result_ids`\n        \"\"\"\n        warnings.warn(\"get_batch_results is deprecated and only returns one batch result. Please use get_batch_result_ids\")\n        return self.get_batch_result_ids(job_id, batch_id)[0]", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Deprecated method in SalesforceAPI that retrieves a single batch result ID for a given job and batch; users are advised to use `get_batch_result_ids` for multiple results instead."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_result_ids", "line_number": 516, "body": "def get_batch_result_ids(self, job_id, batch_id):\n        \"\"\"\n        Get result IDs of a batch that has completed processing.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param batch_id: batch_id as returned by 'create_batch(...)'\n        :return: list of batch result IDs to be used in 'get_batch_result(...)'\n        \"\"\"\n        response = requests.get(self._get_batch_results_url(job_id, batch_id),\n                                headers=self._get_batch_info_headers())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        result_ids = [r.text for r in root.findall('%sresult' % self.API_NS)]\n\n        return result_ids", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Method of the SalesforceAPI class that retrieves a list of result IDs for a completed batch job, facilitating further detailed result fetching in Salesforce batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_result", "line_number": 533, "body": "def get_batch_result(self, job_id, batch_id, result_id):\n        \"\"\"\n        Gets result back from Salesforce as whatever type was originally sent in create_batch (xml, or csv).\n        :param job_id:\n        :param batch_id:\n        :param result_id:\n\n        \"\"\"\n        response = requests.get(self._get_batch_result_url(job_id, batch_id, result_id),\n                                headers=self._get_session_headers())\n        response.raise_for_status()\n\n        return response.content", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Service method of SalesforceAPI that retrieves results from a specified batch job in the original data format (XML or CSV), enabling processing or analysis of Salesforce bulk data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info", "line_number": 547, "body": "def _get_batch_info(self, job_id, batch_id):\n        response = requests.get(self._get_batch_info_url(job_id, batch_id),\n                                headers=self._get_batch_info_headers())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n\n        result = {\n            \"state\": root.find('%sstate' % self.API_NS).text,\n            \"num_processed\": root.find('%snumberRecordsProcessed' % self.API_NS).text,\n            \"num_failed\": root.find('%snumberRecordsFailed' % self.API_NS).text,\n        }\n        if root.find('%sstateMessage' % self.API_NS) is not None:\n            result['state_message'] = root.find('%sstateMessage' % self.API_NS).text\n        return result", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Core utility method of the SalesforceAPI class that retrieves and returns the status and processing details of a specific batch job from Salesforce. It provides insights into the batch state, number of processed and failed records, and optional state messages."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_url", "line_number": 563, "body": "def _get_login_url(self):\n        server = \"login\" if not self.sandbox_name else \"test\"\n        return \"https://%s.salesforce.com/services/Soap/u/%s\" % (server, self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper method in SalesforceAPI that constructs the appropriate Salesforce SOAP login URL, switching between production and sandbox environments based on configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_base_url", "line_number": 567, "body": "def _get_base_url(self):\n        return \"https://%s/services\" % self.hostname", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the base URL for Salesforce API endpoints using the instance's hostname. This URL serves as the foundation for constructing specific service requests within the SalesforceAPI class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_bulk_base_url", "line_number": 570, "body": "def _get_bulk_base_url(self):\n        # Expands on Base Url for Bulk\n        return \"%s/async/%s\" % (self._get_base_url(), self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the base URL for Salesforce's Bulk API by appending the API version to the standard base URL, facilitating access to asynchronous bulk data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_base_url", "line_number": 574, "body": "def _get_norm_base_url(self):\n        # Expands on Base Url for Norm\n        return \"%s/data/v%s\" % (self._get_base_url(), self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Utility method in SalesforceAPI that constructs the normalized base URL incorporating the API version for data requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_query_url", "line_number": 578, "body": "def _get_norm_query_url(self):\n        # Expands on Norm Base Url\n        return \"%s/query\" % self._get_norm_base_url()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Helper method in SalesforceAPI that constructs the full normalized query URL by appending \"/query\" to the base normalized URL. It provides the precise endpoint for executing query requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_url", "line_number": 582, "body": "def _get_create_job_url(self):\n        # Expands on Bulk url\n        return \"%s/job\" % (self._get_bulk_base_url())", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the URL endpoint for creating a new job in the Salesforce Bulk API, facilitating operations that require job management within Salesforce integrations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_job_id_url", "line_number": 586, "body": "def _get_job_id_url(self, job_id):\n        # Expands on Job Creation url\n        return \"%s/%s\" % (self._get_create_job_url(), job_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper method in SalesforceAPI that constructs a full URL for a specific job by appending the job ID to the job creation URL. It assists in forming endpoint URLs for job-related API requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_job_details_url", "line_number": 590, "body": "def _get_job_details_url(self, job_id):\n        # Expands on basic Job Id url\n        return self._get_job_id_url(job_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the URL associated with a specific job identifier, extending the basic job ID URL functionality for accessing job details within the SalesforceAPI context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_abort_job_url", "line_number": 594, "body": "def _get_abort_job_url(self, job_id):\n        # Expands on basic Job Id url\n        return self._get_job_id_url(job_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper method in SalesforceAPI that returns the URL associated with a specific job ID, facilitating access to job-related endpoints."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_close_job_url", "line_number": 598, "body": "def _get_close_job_url(self, job_id):\n        # Expands on basic Job Id url\n        return self._get_job_id_url(job_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Helper method in SalesforceAPI that generates the full URL for a job using its ID, based on the basic job URL structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_batch_url", "line_number": 602, "body": "def _get_create_batch_url(self, job_id):\n        # Expands on basic Job Id url\n        return \"%s/batch\" % (self._get_job_id_url(job_id))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Internal helper method of SalesforceAPI that constructs the URL endpoint to create a batch for a specified job ID, facilitating batch operations in Salesforce data processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info_url", "line_number": 606, "body": "def _get_batch_info_url(self, job_id, batch_id):\n        # Expands on Batch Creation url\n        return \"%s/%s\" % (self._get_create_batch_url(job_id), batch_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Constructs and returns the URL to access specific batch information within a Salesforce job, facilitating batch-level operations in the SalesforceAPI class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_results_url", "line_number": 610, "body": "def _get_batch_results_url(self, job_id, batch_id):\n        # Expands on Batch Info url\n        return \"%s/result\" % (self._get_batch_info_url(job_id, batch_id))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method in SalesforceAPI that constructs the URL to retrieve results for a specific batch within a job, facilitating API interactions involving batch job result fetching."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_result_url", "line_number": 614, "body": "def _get_batch_result_url(self, job_id, batch_id, result_id):\n        # Expands on Batch Results url\n        return \"%s/%s\" % (self._get_batch_results_url(job_id, batch_id), result_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Constructs a complete URL to access a specific batch result within a Salesforce job by combining job, batch, and result identifiers. This aids in retrieving detailed results for asynchronous API batch operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_headers", "line_number": 618, "body": "def _get_login_headers(self):\n        headers = {\n            'Content-Type': \"text/xml; charset=UTF-8\",\n            'SOAPAction': 'login'\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Internal helper method of the SalesforceAPI class that constructs and returns the necessary HTTP headers for initiating a SOAP login request."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_session_headers", "line_number": 625, "body": "def _get_session_headers(self):\n        headers = {\n            'X-SFDC-Session': self.session_id\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method of SalesforceAPI that constructs and returns HTTP headers containing the current session ID for authenticated API requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_session_headers", "line_number": 631, "body": "def _get_norm_session_headers(self):\n        headers = {\n            'Authorization': 'Bearer %s' % self.session_id\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method of SalesforceAPI that constructs authorization headers using the current session ID for authenticated API requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_rest_headers", "line_number": 637, "body": "def _get_rest_headers(self):\n        headers = self._get_norm_session_headers()\n        headers['Content-Type'] = 'application/json'\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper method in SalesforceAPI that constructs HTTP headers for REST API requests, including normalized session headers and JSON content type specification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_job_headers", "line_number": 642, "body": "def _get_job_headers(self):\n        headers = self._get_session_headers()\n        headers['Content-Type'] = \"application/xml; charset=UTF-8\"\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper method in SalesforceAPI that prepares HTTP headers for job-related requests, ensuring the correct content type and session authentication are set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_headers", "line_number": 647, "body": "def _get_create_job_headers(self):\n        return self._get_job_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the HTTP headers required to create a new job in the Salesforce API. This helper method supports job creation requests by providing the necessary header configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_abort_job_headers", "line_number": 650, "body": "def _get_abort_job_headers(self):\n        return self._get_job_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Returns the HTTP headers needed to abort a job in the Salesforce API context, reusing the standard job header retrieval method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_close_job_headers", "line_number": 653, "body": "def _get_close_job_headers(self):\n        return self._get_job_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method of SalesforceAPI that retrieves job headers related to \"close\" operations by delegating to a general job headers retrieval function. It provides a specialized shorthand for accessing specific job header information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_batch_content_headers", "line_number": 656, "body": "def _get_create_batch_content_headers(self, content_type):\n        headers = self._get_session_headers()\n        content_type = 'text/csv' if content_type.lower() == 'csv' else 'application/xml'\n        headers['Content-Type'] = \"%s; charset=UTF-8\" % content_type\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method of the SalesforceAPI class that generates HTTP headers for batch creation requests, setting the Content-Type based on the specified format for CSV or XML."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info_headers", "line_number": 662, "body": "def _get_batch_info_headers(self):\n        return self._get_session_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private method in SalesforceAPI that retrieves the HTTP headers required for batch information requests by reusing the current session headers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_xml", "line_number": 665, "body": "def _get_login_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n            <env:Envelope xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n                xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xmlns:env=\"http://schemas.xmlsoap.org/soap/envelope/\">\n              <env:Body>\n                <n1:login xmlns:n1=\"urn:partner.soap.sforce.com\">\n                  <n1:username>%s</n1:username>\n                  <n1:password>%s%s</n1:password>\n                </n1:login>\n              </env:Body>\n            </env:Envelope>\n        \"\"\" % (self.username, self.password, self.security_token if self.sandbox_name is None else self.sb_security_token)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the XML payload template for authenticating a Salesforce user via SOAP API, embedding credentials and security tokens dynamically based on environment settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_xml", "line_number": 679, "body": "def _get_create_job_xml(self, operation, obj, external_id_field_name, content_type):\n        external_id_field_name_element = \"\" if not external_id_field_name else \\\n            \"\\n<externalIdFieldName>%s</externalIdFieldName>\" % external_id_field_name\n\n        # Note: \"Unable to parse job\" error may be caused by reordering fields.\n        #       ExternalIdFieldName element must be before contentType element.\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n                <operation>%s</operation>\n                <object>%s</object>\n                %s\n                <contentType>%s</contentType>\n            </jobInfo>\n        \"\"\" % (operation, obj, external_id_field_name_element, content_type)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Constructs XML payload for creating a Salesforce data load job specifying operation, object type, external ID field, and content type. Used internally to prepare job creation requests in Salesforce API integration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_abort_job_xml", "line_number": 694, "body": "def _get_abort_job_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n              <state>Aborted</state>\n            </jobInfo>\n        \"\"\"", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Private helper of SalesforceAPI that provides the XML payload to mark a data load job as aborted, supporting job control operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_close_job_xml", "line_number": 701, "body": "def _get_close_job_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n              <state>Closed</state>\n            </jobInfo>\n        \"\"\"", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides an XML representation to mark a Salesforce data load job as closed, used internally to update job status in asynchronous data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_daily_snapshot.py", "function": "latest", "line_number": 51, "body": "def latest(cls, *args, **kwargs):\n        \"\"\"This is cached so that requires() is deterministic.\"\"\"\n        date = kwargs.pop(\"date\", datetime.date.today())\n        lookback = kwargs.pop(\"lookback\", 14)\n        # hashing kwargs deterministically would be hard. Let's just lookup by equality\n        key = (cls, args, kwargs, lookback, date)\n        for k, v in ExternalDailySnapshot.__cache:\n            if k == key:\n                return v\n        val = cls.__latest(date, lookback, args, kwargs)\n        ExternalDailySnapshot.__cache.append((key, val))\n        return val", "is_method": true, "class_name": "ExternalDailySnapshot", "function_description": "Class: ExternalDailySnapshot\n\nMethod latest retrieves a cached snapshot or computes the latest snapshot based on given criteria like date and lookback period, ensuring consistent and efficient retrieval of daily snapshot data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_daily_snapshot.py", "function": "__latest", "line_number": 65, "body": "def __latest(cls, date, lookback, args, kwargs):\n        assert lookback > 0\n        t = None\n        for i in range(lookback):\n            d = date - datetime.timedelta(i)\n            t = cls(date=d, *args, **kwargs)\n            if t.complete():\n                return t\n        logger.debug(\"Could not find last dump for %s (looked back %d days)\",\n                     cls.__name__, lookback)\n        return t", "is_method": true, "class_name": "ExternalDailySnapshot", "function_description": "Utility method of ExternalDailySnapshot that finds the most recent complete snapshot within a specified lookback period, starting from a given date. It aids in retrieving the latest valid data snapshot for a date range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "_get_jars", "line_number": 81, "body": "def _get_jars(self, path):\n        return [os.path.join(path, j) for j in os.listdir(path)\n                if j.endswith('.jar')]", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Private helper method in ScaldingJobRunner that lists all .jar files in a specified directory path, aiding in locating necessary Java archives for job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scala_jars", "line_number": 85, "body": "def get_scala_jars(self, include_compiler=False):\n        lib_dir = os.path.join(self.scala_home, 'lib')\n        jars = [os.path.join(lib_dir, 'scala-library.jar')]\n\n        # additional jar for scala 2.10 only\n        reflect = os.path.join(lib_dir, 'scala-reflect.jar')\n        if os.path.exists(reflect):\n            jars.append(reflect)\n\n        if include_compiler:\n            jars.append(os.path.join(lib_dir, 'scala-compiler.jar'))\n\n        return jars", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Provides a list of essential Scala library JAR file paths needed for running Scala jobs, optionally including the Scala compiler JAR. Useful for configuring classpaths in Scala job execution environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scalding_jars", "line_number": 99, "body": "def get_scalding_jars(self):\n        lib_dir = os.path.join(self.scalding_home, 'lib')\n        return self._get_jars(lib_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Returns the collection of JAR files from the Scalding library directory, supporting dependencies management for running Scalding jobs within the ScaldingJobRunner class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scalding_core", "line_number": 103, "body": "def get_scalding_core(self):\n        lib_dir = os.path.join(self.scalding_home, 'lib')\n        for j in os.listdir(lib_dir):\n            if j.startswith('scalding-core-'):\n                p = os.path.join(lib_dir, j)\n                logger.debug('Found scalding-core: %s', p)\n                return p\n        raise luigi.contrib.hadoop.HadoopJobError('Could not find scalding-core.')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Method of ScaldingJobRunner that locates and returns the file path of the scalding-core library within the Scalding installation, essential for running Scalding jobs requiring this core dependency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_provided_jars", "line_number": 112, "body": "def get_provided_jars(self):\n        return self._get_jars(self.provided_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Utility method in ScaldingJobRunner that retrieves JAR files from a predefined directory of provided dependencies for use in job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_libjars", "line_number": 115, "body": "def get_libjars(self):\n        return self._get_jars(self.libjars_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Returns a list of jar files from the designated library directory, supporting job configuration by providing necessary external dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_tmp_job_jar", "line_number": 118, "body": "def get_tmp_job_jar(self, source):\n        job_name = os.path.basename(os.path.splitext(source)[0])\n        return os.path.join(self.tmp_dir.path, job_name + '.jar')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Returns the file path for a temporary job jar based on the source file name, facilitating the management of job artifacts in temporary storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_build_dir", "line_number": 122, "body": "def get_build_dir(self, source):\n        build_dir = os.path.join(self.tmp_dir.path, 'build')\n        return build_dir", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Returns the build directory path within the temporary directory managed by the ScaldingJobRunner, providing a consistent location for build-related files during job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_job_class", "line_number": 126, "body": "def get_job_class(self, source):\n        # find name of the job class\n        # usually the one that matches file name or last class that extends Job\n        job_name = os.path.splitext(os.path.basename(source))[0]\n        package = None\n        job_class = None\n        for line in open(source).readlines():\n            p = re.search(r'package\\s+([^\\s\\(]+)', line)\n            if p:\n                package = p.groups()[0]\n            p = re.search(r'class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)\n            if p:\n                job_class = p.groups()[0]\n                if job_class == job_name:\n                    break\n        if job_class:\n            if package:\n                job_class = package + '.' + job_class\n            logger.debug('Found scalding job class: %s', job_class)\n            return job_class\n        else:\n            raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Identifies and returns the fully qualified name of the Scalding job class defined in a given source file, aiding job execution by locating the relevant class extending the Job base class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "build_job_jar", "line_number": 149, "body": "def build_job_jar(self, job):\n        job_jar = job.jar()\n        if job_jar:\n            if not os.path.exists(job_jar):\n                logger.error(\"Can't find jar: %s, full path %s\", job_jar, os.path.abspath(job_jar))\n                raise Exception(\"job jar does not exist\")\n            if not job.job_class():\n                logger.error(\"Undefined job_class()\")\n                raise Exception(\"Undefined job_class()\")\n            return job_jar\n\n        job_src = job.source()\n        if not job_src:\n            logger.error(\"Both source() and jar() undefined\")\n            raise Exception(\"Both source() and jar() undefined\")\n        if not os.path.exists(job_src):\n            logger.error(\"Can't find source: %s, full path %s\", job_src, os.path.abspath(job_src))\n            raise Exception(\"job source does not exist\")\n\n        job_src = job.source()\n        job_jar = self.get_tmp_job_jar(job_src)\n\n        build_dir = self.get_build_dir(job_src)\n        if not os.path.exists(build_dir):\n            os.makedirs(build_dir)\n\n        classpath = ':'.join(filter(None,\n                                    self.get_scalding_jars() +\n                                    self.get_provided_jars() +\n                                    self.get_libjars() +\n                                    job.extra_jars()))\n        scala_cp = ':'.join(self.get_scala_jars(include_compiler=True))\n\n        # compile scala source\n        arglist = ['java', '-cp', scala_cp, 'scala.tools.nsc.Main',\n                   '-classpath', classpath,\n                   '-d', build_dir, job_src]\n        logger.info('Compiling scala source: %s', subprocess.list2cmdline(arglist))\n        subprocess.check_call(arglist)\n\n        # build job jar file\n        arglist = ['jar', 'cf', job_jar, '-C', build_dir, '.']\n        logger.info('Building job jar: %s', subprocess.list2cmdline(arglist))\n        subprocess.check_call(arglist)\n        return job_jar", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Builds and returns a runnable job JAR file by validating existing JAR or compiling Scala source code with dependencies. This enables execution of Scalding jobs by preparing their deployable artifact."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "run_job", "line_number": 195, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        job_jar = self.build_job_jar(job)\n        jars = [job_jar] + self.get_libjars() + job.extra_jars()\n        scalding_core = self.get_scalding_core()\n        libjars = ','.join(filter(None, jars))\n        arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', scalding_core, '-libjars', libjars]\n        arglist += ['-D%s' % c for c in job.jobconfs()]\n\n        job_class = job.job_class() or self.get_job_class(job.source())\n        arglist += [job_class, '--hdfs']\n\n        # scalding does not parse argument with '=' properly\n        arglist += ['--name', job.task_id.replace('=', ':')]\n\n        (tmp_files, job_args) = luigi.contrib.hadoop_jar.fix_paths(job)\n        arglist += job_args\n\n        env = os.environ.copy()\n        jars.append(scalding_core)\n        hadoop_cp = ':'.join(filter(None, jars))\n        env['HADOOP_CLASSPATH'] = hadoop_cp\n        logger.info(\"Submitting Hadoop job: HADOOP_CLASSPATH=%s %s\",\n                    hadoop_cp, subprocess.list2cmdline(arglist))\n        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url, env=env)\n\n        for a, b in tmp_files:\n            a.move(b)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Runs a Scalding Hadoop job by building the job jar, setting up classpaths and arguments, and submitting the job while tracking its execution and handling temporary files. It enables executing complex data processing tasks within the ScaldingJobRunner context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "relpath", "line_number": 243, "body": "def relpath(self, current_file, rel_path):\n        \"\"\"\n        Compute path given current file and relative path.\n        \"\"\"\n        script_dir = os.path.dirname(os.path.abspath(current_file))\n        rel_path = os.path.abspath(os.path.join(script_dir, rel_path))\n        return rel_path", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Utility method in ScaldingJobTask that computes an absolute file path from a given file and a relative path, facilitating reliable path resolution within job scripts or resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "source", "line_number": 251, "body": "def source(self):\n        \"\"\"\n        Path to the scala source for this Scalding Job\n\n        Either one of source() or jar() must be specified.\n        \"\"\"\n        return None", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns the file path to the Scala source code for the Scalding job. It indicates the source location required to run or build the job, with either this or a jar file path mandatory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "extra_jars", "line_number": 267, "body": "def extra_jars(self):\n        \"\"\"\n        Extra jars for building and running this Scalding Job.\n        \"\"\"\n        return []", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns a list of additional JAR files needed to build or run the Scalding job. This allows customization of the job\u2019s runtime dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "job_class", "line_number": 273, "body": "def job_class(self):\n        \"\"\"\n        optional main job class for this Scalding Job.\n        \"\"\"\n        return None", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns None as a placeholder for specifying the main job class in a Scalding job. This method indicates the optional nature of defining a primary job class in ScaldingJobTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "job_runner", "line_number": 279, "body": "def job_runner(self):\n        return ScaldingJobRunner()", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns a new instance of ScaldingJobRunner, providing a means to initiate or manage Scalding job executions within the ScaldingJobTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "atomic_output", "line_number": 282, "body": "def atomic_output(self):\n        \"\"\"\n        If True, then rewrite output arguments to be temp locations and\n        atomically move them into place after the job finishes.\n        \"\"\"\n        return True", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Service method of ScaldingJobTask indicating that job outputs should be written to temporary locations and atomically moved to their final destinations upon completion for safer and consistent job output handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "requires", "line_number": 289, "body": "def requires(self):\n        return {}", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns an empty dictionary indicating no dependencies or prerequisite tasks are required for this ScaldingJobTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "job_args", "line_number": 292, "body": "def job_args(self):\n        \"\"\"\n        Extra arguments to pass to the Scalding job.\n        \"\"\"\n        return []", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Returns additional arguments for configuring the Scalding job execution. This method provides job-specific parameters other functions can use to customize job behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "args", "line_number": 298, "body": "def args(self):\n        \"\"\"\n        Returns an array of args to pass to the job.\n        \"\"\"\n        arglist = []\n        for k, v in self.requires_hadoop().items():\n            arglist.append('--' + k)\n            arglist.extend([t.output().path for t in flatten(v)])\n        arglist.extend(['--output', self.output()])\n        arglist.extend(self.job_args())\n        return arglist", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Constructs and returns the complete list of command-line arguments needed to run a Scalding job, incorporating Hadoop requirements, output paths, and additional job-specific parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "_init_kubernetes", "line_number": 72, "body": "def _init_kubernetes(self):\n        self.__logger = logger\n        self.__logger.debug(\"Kubernetes auth method: \" + self.auth_method)\n        if self.auth_method == \"kubeconfig\":\n            self.__kube_api = HTTPClient(KubeConfig.from_file(self.kubeconfig_path))\n        elif self.auth_method == \"service-account\":\n            self.__kube_api = HTTPClient(KubeConfig.from_service_account())\n        else:\n            raise ValueError(\"Illegal auth_method\")\n        self.job_uuid = str(uuid.uuid4().hex)\n        now = datetime.utcnow()\n        self.uu_name = \"%s-%s-%s\" % (self.name, now.strftime('%Y%m%d%H%M%S'), self.job_uuid[:16])", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Initializes Kubernetes API client based on the specified authentication method and prepares unique job identifiers for managing Kubernetes jobs within the KubernetesJobTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "auth_method", "line_number": 86, "body": "def auth_method(self):\n        \"\"\"\n        This can be set to ``kubeconfig`` or ``service-account``.\n        It defaults to ``kubeconfig``.\n\n        For more details, please refer to:\n\n        - kubeconfig: http://kubernetes.io/docs/user-guide/kubeconfig-file\n        - service-account: http://kubernetes.io/docs/user-guide/service-accounts\n        \"\"\"\n        return self.kubernetes_config.auth_method", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the current authentication method configured for Kubernetes access, indicating whether kubeconfig or service-account is used for authorization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubeconfig_path", "line_number": 99, "body": "def kubeconfig_path(self):\n        \"\"\"\n        Path to kubeconfig file used for cluster authentication.\n        It defaults to \"~/.kube/config\", which is the default location\n        when using minikube (http://kubernetes.io/docs/getting-started-guides/minikube).\n        When auth_method is ``service-account`` this property is ignored.\n\n        **WARNING**: For Python versions < 3.5 kubeconfig must point to a Kubernetes API\n        hostname, and NOT to an IP address.\n\n        For more details, please refer to:\n        http://kubernetes.io/docs/user-guide/kubeconfig-file\n        \"\"\"\n        return self.kubernetes_config.kubeconfig_path", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides the file path to the kubeconfig used for authenticating Kubernetes cluster access, defaulting to the standard location unless a service-account authentication method is used."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubernetes_namespace", "line_number": 115, "body": "def kubernetes_namespace(self):\n        \"\"\"\n        Namespace in Kubernetes where the job will run.\n        It defaults to the default namespace in Kubernetes\n\n        For more details, please refer to:\n        https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\n        \"\"\"\n        return self.kubernetes_config.kubernetes_namespace", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the Kubernetes namespace configured for the job to run in, defaulting to the standard namespace if none is set. This enables job placement within the appropriate Kubernetes environment context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "labels", "line_number": 134, "body": "def labels(self):\n        \"\"\"\n        Return custom labels for kubernetes job.\n        example::\n        ``{\"run_dt\": datetime.date.today().strftime('%F')}``\n        \"\"\"\n        return {}", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides custom label metadata for a Kubernetes job, enabling identification or categorization. Returns an empty dictionary by default and can be overridden to supply specific labels."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "max_retrials", "line_number": 170, "body": "def max_retrials(self):\n        \"\"\"\n        Maximum number of retrials in case of failure.\n        \"\"\"\n        return self.kubernetes_config.max_retrials", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the maximum allowed retry count for a Kubernetes job task, indicating how many times the task should be retried upon failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "backoff_limit", "line_number": 177, "body": "def backoff_limit(self):\n        \"\"\"\n        Maximum number of retries before considering the job as failed.\n        See: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy\n        \"\"\"\n        return 6", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides the maximum retry count for a Kubernetes job before marking it as failed. This constant controls job failure thresholds to manage job execution retries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "delete_on_success", "line_number": 185, "body": "def delete_on_success(self):\n        \"\"\"\n        Delete the Kubernetes workload if the job has ended successfully.\n        \"\"\"\n        return True", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns a flag indicating that the Kubernetes job workload should be deleted upon successful completion. This supports automated cleanup of completed jobs to manage cluster resources efficiently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "print_pod_logs_on_exit", "line_number": 192, "body": "def print_pod_logs_on_exit(self):\n        \"\"\"\n        Fetch and print the pod logs once the job is completed.\n        \"\"\"\n        return False", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns False and does not fetch or print pod logs on job completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "active_deadline_seconds", "line_number": 199, "body": "def active_deadline_seconds(self):\n        \"\"\"\n        Time allowed to successfully schedule pods.\n        See: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-termination-and-cleanup\n        \"\"\"\n        return None", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the maximum time allowed for scheduling pods before job termination, according to Kubernetes job constraints.  \n(Note: This method currently serves as a placeholder and returns None.)"}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubernetes_config", "line_number": 207, "body": "def kubernetes_config(self):\n        if not self._kubernetes_config:\n            self._kubernetes_config = kubernetes()\n        return self._kubernetes_config", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides access to a Kubernetes configuration object, initializing it on first use to manage Kubernetes-related operations within the KubernetesJobTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "poll_interval", "line_number": 213, "body": "def poll_interval(self):\n        \"\"\"How often to poll Kubernetes for job status, in seconds.\"\"\"\n        return self.__DEFAULT_POLL_INTERVAL", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "This method provides the polling interval duration in seconds used to check the status of a Kubernetes job. It enables other components to use a consistent timing strategy for job status updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "pod_creation_wait_interal", "line_number": 218, "body": "def pod_creation_wait_interal(self):\n        \"\"\"Delay for initial pod creation for just submitted job in seconds\"\"\"\n        return self.__DEFAULT_POD_CREATION_INTERVAL", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the default waiting interval time in seconds for the initial creation of a pod for a newly submitted Kubernetes job. This helps coordinate timing-related logic when managing Kubernetes job pods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__track_job", "line_number": 222, "body": "def __track_job(self):\n        \"\"\"Poll job status while active\"\"\"\n        while not self.__verify_job_has_started():\n            time.sleep(self.poll_interval)\n            self.__logger.debug(\"Waiting for Kubernetes job \" + self.uu_name + \" to start\")\n        self.__print_kubectl_hints()\n\n        status = self.__get_job_status()\n        while status == \"RUNNING\":\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name + \" is running\")\n            time.sleep(self.poll_interval)\n            status = self.__get_job_status()\n\n        assert status != \"FAILED\", \"Kubernetes job \" + self.uu_name + \" failed\"\n\n        # status == \"SUCCEEDED\"\n        self.__logger.info(\"Kubernetes job \" + self.uu_name + \" succeeded\")\n        self.signal_complete()", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Monitors and waits for a Kubernetes job to start, tracks its running status until completion, and signals success or failure accordingly. This method provides real-time job status polling for workflow management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_pods", "line_number": 252, "body": "def __get_pods(self):\n        pod_objs = Pod.objects(self.__kube_api, namespace=self.kubernetes_namespace) \\\n            .filter(selector=\"job-name=\" + self.uu_name) \\\n            .response['items']\n        return [Pod(self.__kube_api, p) for p in pod_objs]", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Retrieves and returns all pod objects associated with a specific Kubernetes job in the configured namespace, enabling management or inspection of the job's pods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_job", "line_number": 258, "body": "def __get_job(self):\n        jobs = Job.objects(self.__kube_api, namespace=self.kubernetes_namespace) \\\n            .filter(selector=\"luigi_task_id=\" + self.job_uuid) \\\n            .response['items']\n        assert len(jobs) == 1, \"Kubernetes job \" + self.uu_name + \" not found\"\n        return Job(self.__kube_api, jobs[0])", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Private method in KubernetesJobTask that fetches a unique Kubernetes job matching a specific task identifier within a namespace, ensuring exactly one job is returned for further operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__print_pod_logs", "line_number": 265, "body": "def __print_pod_logs(self):\n        for pod in self.__get_pods():\n            logs = pod.logs(timestamps=True).strip()\n            self.__logger.info(\"Fetching logs from \" + pod.name)\n            if len(logs) > 0:\n                for line in logs.split('\\n'):\n                    self.__logger.info(line)", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "This function fetches and logs the timestamped output of all pods related to a Kubernetes job. It enables monitoring of pod activity by recording their logs for debugging or audit purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__print_kubectl_hints", "line_number": 273, "body": "def __print_kubectl_hints(self):\n        self.__logger.info(\"To stream Pod logs, use:\")\n        for pod in self.__get_pods():\n            self.__logger.info(\"`kubectl logs -f pod/%s -n %s`\" % (pod.name, pod.namespace))", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Utility method in KubernetesJobTask that logs kubectl commands to stream logs for each associated Pod, aiding users in monitoring job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__verify_job_has_started", "line_number": 278, "body": "def __verify_job_has_started(self):\n        \"\"\"Asserts that the job has successfully started\"\"\"\n        # Verify that the job started\n        self.__get_job()\n\n        # Verify that the pod started\n        pods = self.__get_pods()\n        if not pods:\n            self.__logger.debug(\n                'No pods found for %s, waiting for cluster state to match the job definition' % self.uu_name\n            )\n            time.sleep(self.pod_creation_wait_interal)\n            pods = self.__get_pods()\n\n        assert len(pods) > 0, \"No pod scheduled by \" + self.uu_name\n        for pod in pods:\n            status = pod.obj['status']\n            for cont_stats in status.get('containerStatuses', []):\n                if 'terminated' in cont_stats['state']:\n                    t = cont_stats['state']['terminated']\n                    err_msg = \"Pod %s %s (exit code %d). Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, t['reason'], t['exitCode'], pod.name)\n                    assert t['exitCode'] == 0, err_msg\n\n                if 'waiting' in cont_stats['state']:\n                    wr = cont_stats['state']['waiting']['reason']\n                    assert wr == 'ContainerCreating', \"Pod %s %s. Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, wr, pod.name)\n\n            for cond in status.get('conditions', []):\n                if 'message' in cond:\n                    if cond['reason'] == 'ContainersNotReady':\n                        return False\n                    assert cond['status'] != 'False', \\\n                        \"[ERROR] %s - %s\" % (cond['reason'], cond['message'])\n        return True", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Checks and asserts that the Kubernetes job and its pods have started successfully, ensuring containers are running without errors or premature termination. Useful for verifying job readiness before proceeding with dependent operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_job_status", "line_number": 315, "body": "def __get_job_status(self):\n        \"\"\"Return the Kubernetes job status\"\"\"\n        # Figure out status and return it\n        job = self.__get_job()\n\n        if \"succeeded\" in job.obj[\"status\"] and job.obj[\"status\"][\"succeeded\"] > 0:\n            job.scale(replicas=0)\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if self.delete_on_success:\n                self.__delete_job_cascade(job)\n            return \"SUCCEEDED\"\n\n        if \"failed\" in job.obj[\"status\"]:\n            failed_cnt = job.obj[\"status\"][\"failed\"]\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name\n                                + \" status.failed: \" + str(failed_cnt))\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if failed_cnt > self.max_retrials:\n                job.scale(replicas=0)  # avoid more retrials\n                return \"FAILED\"\n        return \"RUNNING\"", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Method of KubernetesJobTask that retrieves the current status of a Kubernetes job, managing job scaling, logging, and cleanup based on success or failure states for efficient job lifecycle handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__delete_job_cascade", "line_number": 339, "body": "def __delete_job_cascade(self, job):\n        delete_options_cascade = {\n            \"kind\": \"DeleteOptions\",\n            \"apiVersion\": \"v1\",\n            \"propagationPolicy\": \"Background\"\n        }\n        r = self.__kube_api.delete(json=delete_options_cascade, **job.api_kwargs())\n        if r.status_code != 200:\n            self.__kube_api.raise_for_status(r)", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Private method in KubernetesJobTask that deletes a Kubernetes job with cascading background propagation, ensuring dependent resources are also removed. It provides controlled job cleanup within Kubernetes clusters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "run", "line_number": 349, "body": "def run(self):\n        self._init_kubernetes()\n        # Render job\n        job_json = {\n            \"apiVersion\": \"batch/v1\",\n            \"kind\": \"Job\",\n            \"metadata\": {\n                \"name\": self.uu_name,\n                \"labels\": {\n                    \"spawned_by\": \"luigi\",\n                    \"luigi_task_id\": self.job_uuid\n                }\n            },\n            \"spec\": {\n                \"backoffLimit\": self.backoff_limit,\n                \"template\": {\n                    \"metadata\": {\n                        \"name\": self.uu_name,\n                        \"labels\": {}\n                    },\n                    \"spec\": self.spec_schema\n                }\n            }\n        }\n        if self.kubernetes_namespace is not None:\n            job_json['metadata']['namespace'] = self.kubernetes_namespace\n        if self.active_deadline_seconds is not None:\n            job_json['spec']['activeDeadlineSeconds'] = \\\n                self.active_deadline_seconds\n        # Update user labels\n        job_json['metadata']['labels'].update(self.labels)\n        job_json['spec']['template']['metadata']['labels'].update(self.labels)\n\n        # Add default restartPolicy if not specified\n        if \"restartPolicy\" not in self.spec_schema:\n            job_json[\"spec\"][\"template\"][\"spec\"][\"restartPolicy\"] = \"Never\"\n        # Submit job\n        self.__logger.info(\"Submitting Kubernetes Job: \" + self.uu_name)\n        job = Job(self.__kube_api, job_json)\n        job.create()\n        # Track the Job (wait while active)\n        self.__logger.info(\"Start tracking Kubernetes Job: \" + self.uu_name)\n        self.__track_job()", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Runs a Kubernetes Job by constructing its specification, submitting it to the cluster, and monitoring its execution. This method automates job creation and lifecycle tracking within a Kubernetes environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "load_hive_cmd", "line_number": 47, "body": "def load_hive_cmd():\n    return luigi.configuration.get_config().get('hive', 'command', 'hive').split(' ')", "is_method": false, "function_description": "Function that retrieves and splits the configured Hive command string from the system configuration, enabling execution of Hive commands with customizable command paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_hive_syntax", "line_number": 51, "body": "def get_hive_syntax():\n    return luigi.configuration.get_config().get('hive', 'release', 'cdh4')", "is_method": false, "function_description": "Function that retrieves the configured Hive syntax release version from the Luigi configuration, defaulting to 'cdh4'. It provides access to the Hive dialect setting for components requiring Hive syntax details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_hive_warehouse_location", "line_number": 55, "body": "def get_hive_warehouse_location():\n    return luigi.configuration.get_config().get('hive', 'warehouse_location', '/user/hive/warehouse')", "is_method": false, "function_description": "Function that retrieves the configured Hive warehouse directory path, defaulting to '/user/hive/warehouse' if not set. Useful for components needing Hive data storage location information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_ignored_file_masks", "line_number": 59, "body": "def get_ignored_file_masks():\n    return luigi.configuration.get_config().get('hive', 'ignored_file_masks', None)", "is_method": false, "function_description": "Retrieves the configured file name patterns to ignore during processing from the 'hive' configuration section. This helps other functions determine which files should be excluded."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive", "line_number": 63, "body": "def run_hive(args, check_return_code=True):\n    \"\"\"\n    Runs the `hive` from the command line, passing in the given args, and\n    returning stdout.\n\n    With the apache release of Hive, so of the table existence checks\n    (which are done using DESCRIBE do not exit with a return code of 0\n    so we need an option to ignore the return code and just return stdout for parsing\n    \"\"\"\n    cmd = load_hive_cmd() + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout.decode('utf-8')", "is_method": false, "function_description": "Function that executes a Hive command-line query with specified arguments, returning its output while optionally validating the command's success based on its return code. It supports flexible error handling for Hive's non-standard return codes during table checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive_cmd", "line_number": 81, "body": "def run_hive_cmd(hivecmd, check_return_code=True):\n    \"\"\"\n    Runs the given hive query and returns stdout.\n    \"\"\"\n    return run_hive(['-e', hivecmd], check_return_code)", "is_method": false, "function_description": "Utility function to execute a Hive query and return its output, optionally ensuring successful execution by checking the return code. It simplifies running Hive commands programmatically for data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive_script", "line_number": 88, "body": "def run_hive_script(script):\n    \"\"\"\n    Runs the contents of the given script in hive and returns stdout.\n    \"\"\"\n    if not os.path.isfile(script):\n        raise RuntimeError(\"Hive script: {0} does not exist.\".format(script))\n    return run_hive(['-f', script])", "is_method": false, "function_description": "Function that executes a Hive script file and returns its standard output, enabling automated running of Hive queries from script files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_is_ordered_dict", "line_number": 97, "body": "def _is_ordered_dict(dikt):\n    if isinstance(dikt, collections.OrderedDict):\n        return True\n\n    if sys.version_info >= (3, 7):\n        return isinstance(dikt, dict)\n\n    return False", "is_method": false, "function_description": "Utility function that checks if an object is an ordered dictionary, accounting for Python version differences in dict ordering behavior. It helps determine if dictionary iteration order is preserved."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_validate_partition", "line_number": 107, "body": "def _validate_partition(partition):\n    \"\"\"\n    If partition is set and its size is more than one and not ordered,\n    then we're unable to restore its path in the warehouse\n    \"\"\"\n    if (\n            partition\n            and len(partition) > 1\n            and not _is_ordered_dict(partition)\n    ):\n        raise ValueError('Unable to restore table/partition location')", "is_method": false, "function_description": "Internal validation function that checks whether a given partition can be reliably restored; it ensures multi-element partitions are ordered to prevent path resolution errors in a warehouse context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_default_client", "line_number": 326, "body": "def get_default_client():\n    syntax = get_hive_syntax()\n    if syntax == \"apache\":\n        return ApacheHiveCommandClient()\n    elif syntax == \"metastore\":\n        return MetastoreClient()\n    elif syntax == 'warehouse':\n        return WarehouseHiveClient()\n    else:\n        return HiveCommandClient()", "is_method": false, "function_description": "Utility function that returns a default Hive client instance based on the current Hive syntax configuration, enabling seamless interaction with different Hive service implementations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 156, "body": "def table_location(self, table, database='default', partition=None):\n        cmd = \"use {0}; describe formatted {1}\".format(database, table)\n        if partition is not None:\n            cmd += \" PARTITION ({0})\".format(self.partition_spec(partition))\n\n        stdout = run_hive_cmd(cmd)\n\n        for line in stdout.split(\"\\n\"):\n            if \"Location:\" in line:\n                return line.split(\"\\t\")[1]", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Returns the file system location of a specified Hive table or partition, enabling other functions to access or manage the physical data storage within Hive databases."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 167, "body": "def table_exists(self, table, database='default', partition=None):\n        if partition is None:\n            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n\n            return stdout and table.lower() in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Checks if a specified table or its partition exists within a given Hive database, enabling validation before querying or modifying Hive data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 181, "body": "def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table))\n        if not describe or \"does not exist\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Core method of HiveCommandClient that retrieves the schema of a specified Hive table, returning its column details as a list of tuples or None if the table does not exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 187, "body": "def partition_spec(self, partition):\n        \"\"\"\n        Turns a dict into the a Hive partition specification string.\n        \"\"\"\n        return ','.join([\"`{0}`='{1}'\".format(k, v) for (k, v) in\n                         sorted(partition.items(), key=operator.itemgetter(0))])", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Converts a dictionary of partition keys and values into a properly formatted Hive partition specification string, facilitating query construction in HiveCommandClient."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 201, "body": "def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table), False)\n        if not describe or \"Table not found\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]", "is_method": true, "class_name": "ApacheHiveCommandClient", "function_description": "Service method of ApacheHiveCommandClient that retrieves and returns the schema of a specified Hive table as a list of tuples, facilitating schema inspection for database and table management tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 210, "body": "def table_location(self, table, database='default', partition=None):\n        with HiveThriftContext() as client:\n            if partition is not None:\n                try:\n                    import hive_metastore.ttypes\n                    partition_str = self.partition_spec(partition)\n                    thrift_table = client.get_partition_by_name(database, table, partition_str)\n                except hive_metastore.ttypes.NoSuchObjectException:\n                    return ''\n            else:\n                thrift_table = client.get_table(database, table)\n            return thrift_table.sd.location", "is_method": true, "class_name": "MetastoreClient", "function_description": "Service method of MetastoreClient that retrieves the storage location URI of a specified Hive table or partition within a database, facilitating access to table data paths for downstream processing or metadata management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 223, "body": "def table_exists(self, table, database='default', partition=None):\n        with HiveThriftContext() as client:\n            if partition is None:\n                return table in client.get_all_tables(database)\n            else:\n                return partition in self._existing_partitions(table, database, client)", "is_method": true, "class_name": "MetastoreClient", "function_description": "Checks if a specified table or its partition exists within a given database in the metastore. This enables other components to verify data availability before queries or operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_existing_partitions", "line_number": 230, "body": "def _existing_partitions(self, table, database, client):\n        def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def\n\n        # -1 is max_parts, the # of partition names to return (-1 = unlimited)\n        partition_strings = client.get_partition_names(database, table, -1)\n        return [_parse_partition_string(existing_partition) for existing_partition in partition_strings]", "is_method": true, "class_name": "MetastoreClient", "function_description": "Private utility method of MetastoreClient that retrieves and parses all partition definitions for a given table and database, facilitating partition management and metadata inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 242, "body": "def table_schema(self, table, database='default'):\n        with HiveThriftContext() as client:\n            return [(field_schema.name, field_schema.type) for field_schema in client.get_schema(database, table)]", "is_method": true, "class_name": "MetastoreClient", "function_description": "Provides the schema of a specified table, returning field names and types from the metastore. This aids in understanding table structure for database interaction or query construction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 246, "body": "def partition_spec(self, partition):\n        return \"/\".join(\"%s=%s\" % (k, v) for (k, v) in sorted(partition.items(), key=operator.itemgetter(0)))", "is_method": true, "class_name": "MetastoreClient", "function_description": "Generates a standardized string representation of a partition dictionary by sorting its keys and formatting key-value pairs as path segments. This aids in consistent partition identification and organization within data storage systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "__enter__", "line_number": 255, "body": "def __enter__(self):\n        try:\n            from thrift.transport import TSocket\n            from thrift.transport import TTransport\n            from thrift.protocol import TBinaryProtocol\n            # Note that this will only work with a CDH release.\n            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.\n            # If using the Apache release of Hive this import will fail.\n            from hive_metastore import ThriftHiveMetastore\n            config = luigi.configuration.get_config()\n            host = config.get('hive', 'metastore_host')\n            port = config.getint('hive', 'metastore_port')\n            transport = TSocket.TSocket(host, port)\n            transport = TTransport.TBufferedTransport(transport)\n            protocol = TBinaryProtocol.TBinaryProtocol(transport)\n            transport.open()\n            self.transport = transport\n            return ThriftHiveMetastore.Client(protocol)\n        except ImportError as e:\n            raise Exception('Could not import Hive thrift library:' + str(e))", "is_method": true, "class_name": "HiveThriftContext", "function_description": "Provides a context manager entry method that establishes and returns a Thrift client connection to the Hive metastore, facilitating interaction with Hive metadata services within a managed resource scope."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "__exit__", "line_number": 276, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.transport.close()", "is_method": true, "class_name": "HiveThriftContext", "function_description": "Provides cleanup by closing the transport connection when exiting a HiveThriftContext, ensuring proper release of resources in context management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 292, "body": "def table_location(self, table, database='default', partition=None):\n        return os.path.join(\n            self.warehouse_location,\n            database + '.db',\n            table,\n            self.partition_spec(partition)\n        )", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Returns the filesystem path for a specified table and optional partition within a database in the warehouse, enabling location-based access to table data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 300, "body": "def table_exists(self, table, database='default', partition=None):\n        \"\"\"\n        The table/partition is considered existing if corresponding path in hdfs exists\n        and contains file except those which match pattern set in  `ignored_file_masks`\n        \"\"\"\n        path = self.table_location(table, database, partition)\n        if self.hdfs_client.exists(path):\n            ignored_files = get_ignored_file_masks()\n            if ignored_files is None:\n                return True\n\n            filenames = self.hdfs_client.listdir(path)\n            pattern = re.compile(ignored_files)\n            for filename in filenames:\n                if not pattern.match(filename):\n                    return True\n\n        return False", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Checks if a table or partition exists in HDFS by verifying the presence of non-ignored files at its storage location. This aids in validating data availability before operations in the WarehouseHiveClient context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 319, "body": "def partition_spec(self, partition):\n        _validate_partition(partition)\n        return '/'.join([\n            '{}={}'.format(k, v) for (k, v) in (partition or {}).items()\n        ])", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Utility method of WarehouseHiveClient that converts a partition dictionary into a Hive-style partition string, facilitating partition specification in Hive table operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hiverc", "line_number": 356, "body": "def hiverc(self):\n        \"\"\"\n        Location of an rc file to run before the query\n        if hiverc-location key is specified in luigi.cfg, will default to the value there\n        otherwise returns None.\n\n        Returning a list of rc files will load all of them in order.\n        \"\"\"\n        return luigi.configuration.get_config().get('hive', 'hiverc-location', default=None)", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Provides the configured location(s) of Hive rc file(s) to execute before running a query, enabling custom Hive session initialization based on configuration settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hivevars", "line_number": 366, "body": "def hivevars(self):\n        \"\"\"\n        Returns a dict of key=value settings to be passed along\n        to the hive command line via --hivevar.\n        This option can be used as a separated namespace for script local variables.\n        See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VariableSubstitution\n        \"\"\"\n        return {}", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Returns a dictionary of key-value pairs intended for passing as Hive variable substitutions via command line, enabling script-local variable management for Hive queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hiveconfs", "line_number": 375, "body": "def hiveconfs(self):\n        \"\"\"\n        Returns a dict of key=value settings to be passed along\n        to the hive command line via --hiveconf. By default, sets\n        mapred.job.name to task_id and if not None, sets:\n\n        * mapred.reduce.tasks (n_reduce_tasks)\n        * mapred.fairscheduler.pool (pool) or mapred.job.queue.name (pool)\n        * hive.exec.reducers.bytes.per.reducer (bytes_per_reducer)\n        * hive.exec.reducers.max (reducers_max)\n        \"\"\"\n        jcs = {}\n        jcs['mapred.job.name'] = \"'\" + self.task_id + \"'\"\n        if self.n_reduce_tasks is not None:\n            jcs['mapred.reduce.tasks'] = self.n_reduce_tasks\n        if self.pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = luigi.configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs['mapred.fairscheduler.pool'] = self.pool\n            elif scheduler_type == 'capacity':\n                jcs['mapred.job.queue.name'] = self.pool\n        if self.bytes_per_reducer is not None:\n            jcs['hive.exec.reducers.bytes.per.reducer'] = self.bytes_per_reducer\n        if self.reducers_max is not None:\n            jcs['hive.exec.reducers.max'] = self.reducers_max\n        return jcs", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Returns a dictionary of Hive configuration settings based on the task's attributes, preparing parameters to customize Hive job execution such as job name, reducer tasks, scheduler pool, and reducer limits."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "job_runner", "line_number": 403, "body": "def job_runner(self):\n        return HiveQueryRunner()", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Returns a new instance of HiveQueryRunner to execute Hive queries. Provides a standardized way to obtain a query runner within HiveQueryTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "prepare_outputs", "line_number": 412, "body": "def prepare_outputs(self, job):\n        \"\"\"\n        Called before job is started.\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail\n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Prepares output directories for a Hive job by ensuring parent directories of filesystem targets exist before execution, preventing job failures due to missing paths. This supports reliable job initialization in Hadoop environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_arglist", "line_number": 431, "body": "def get_arglist(self, f_name, job):\n        arglist = load_hive_cmd() + ['-f', f_name]\n        hiverc = job.hiverc()\n        if hiverc:\n            if isinstance(hiverc, str):\n                hiverc = [hiverc]\n            for rcfile in hiverc:\n                arglist += ['-i', rcfile]\n        hiveconfs = job.hiveconfs()\n        if hiveconfs:\n            for k, v in hiveconfs.items():\n                arglist += ['--hiveconf', '{0}={1}'.format(k, v)]\n        hivevars = job.hivevars()\n        if hivevars:\n            for k, v in hivevars.items():\n                arglist += ['--hivevar', '{0}={1}'.format(k, v)]\n        logger.info(arglist)\n        return arglist", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Constructs the command-line argument list needed to execute a Hive query file with job-specific configurations and variables included. This enables customizable and flexible Hive query execution within the HiveQueryRunner context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_job", "line_number": 450, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        self.prepare_outputs(job)\n        with tempfile.NamedTemporaryFile() as f:\n            query = job.query()\n            if isinstance(query, str):\n                query = query.encode('utf8')\n            f.write(query)\n            f.flush()\n            arglist = self.get_arglist(f.name, job)\n            return luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url)", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Executes a Hive job by preparing outputs, writing the query to a temporary file, and running the Hadoop job while tracking its progress, facilitating automated Hive query execution within workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "exists", "line_number": 488, "body": "def exists(self):\n        \"\"\"\n        returns `True` if the partition/table exists\n        \"\"\"\n        try:\n            logger.debug(\n                \"Checking Hive table '{d}.{t}' for partition {p}\".format(\n                    d=self.database,\n                    t=self.table,\n                    p=str(self.partition or {})\n                )\n            )\n\n            return self.client.table_exists(self.table, self.database, self.partition)\n        except HiveCommandError:\n            if self.fail_missing_table:\n                raise\n            else:\n                if self.client.table_exists(self.table, self.database):\n                    # a real error occurred\n                    raise\n                else:\n                    # oh the table just doesn't exist\n                    return False", "is_method": true, "class_name": "HivePartitionTarget", "function_description": "Checks whether a specific Hive partition or table exists in the database, handling errors and optionally failing if the table is missing. This enables conditional logic based on the presence of Hive data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "path", "line_number": 514, "body": "def path(self):\n        \"\"\"\n        Returns the path for this HiveTablePartitionTarget's data.\n        \"\"\"\n        location = self.client.table_location(self.table, self.database, self.partition)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location", "is_method": true, "class_name": "HivePartitionTarget", "function_description": "Returns the filesystem path where the Hive table partition's data is stored, enabling access to the partition's physical location for downstream processing or management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "output", "line_number": 550, "body": "def output(self):\n        return HivePartitionTarget(\n            database=self.database,\n            table=self.table,\n            partition=self.partition,\n        )", "is_method": true, "class_name": "ExternalHiveTask", "function_description": "Returns a HivePartitionTarget object representing the database, table, and partition details associated with the ExternalHiveTask. This method provides a standardized reference to the task's Hive output location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_parse_partition_string", "line_number": 231, "body": "def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def", "is_method": true, "class_name": "MetastoreClient", "function_description": "Private helper method of MetastoreClient that parses a partition string into a dictionary mapping partition keys to their values for easier partition definition handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "touch", "line_number": 71, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n\n        connection.execute_non_query(\n            \"\"\"IF NOT EXISTS(SELECT 1\n                            FROM {marker_table}\n                            WHERE update_id = %(update_id)s)\n                    INSERT INTO {marker_table} (update_id, target_table)\n                        VALUES (%(update_id)s, %(table)s)\n                ELSE\n                    UPDATE t\n                    SET target_table = %(table)s\n                        , inserted = GETDATE()\n                    FROM {marker_table} t\n                    WHERE update_id = %(update_id)s\n              \"\"\".format(marker_table=self.marker_table),\n            {\"update_id\": self.update_id, \"table\": self.table})\n\n        # make sure update is properly marked\n        assert self.exists(connection)", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Marks an update as complete by inserting or updating a record in a marker table, creating the table if it doesn't exist. It ensures reliable tracking of update status within the MSSqlTarget workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "exists", "line_number": 102, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n        try:\n            row = connection.execute_row(\"\"\"SELECT 1 FROM {marker_table}\n                                            WHERE update_id = %s\n                                    \"\"\".format(marker_table=self.marker_table),\n                                         (self.update_id,))\n        except _mssql.MSSQLDatabaseException as e:\n            # Error number for table doesn't exist\n            if e.number == 208:\n                row = None\n            else:\n                raise\n\n        return row is not None", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Checks if a specific update marker exists in the designated database table, indicating whether a particular update has been applied in the MSSqlTarget context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "connect", "line_number": 119, "body": "def connect(self):\n        \"\"\"\n        Create a SQL Server connection and return a connection object\n        \"\"\"\n        connection = _mssql.connect(user=self.user,\n                                    password=self.password,\n                                    server=self.host,\n                                    port=self.port,\n                                    database=self.database)\n        return connection", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Core function of MSSqlTarget that establishes and returns a connection to a SQL Server database using stored credentials, enabling database operations for other functions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "create_marker_table", "line_number": 130, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n        Use a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        try:\n            connection.execute_non_query(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT    NOT NULL IDENTITY(1,1),\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      DATETIME DEFAULT(GETDATE()),\n                        PRIMARY KEY (update_id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except _mssql.MSSQLDatabaseException as e:\n            # Table already exists code\n            if e.number == 2714:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Provides a method in MSSqlTarget to create a marker table for tracking updates, ensuring the table exists without interruption if it already does. Useful for managing data synchronization or state in database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "is_error_5xx", "line_number": 73, "body": "def is_error_5xx(err):\n    return isinstance(err, errors.HttpError) and err.resp.status >= 500", "is_method": false, "function_description": "Function that checks whether an error is an HTTP server error (status code 500 or higher), useful for identifying server-side failures in network requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_wait_for_consistency", "line_number": 84, "body": "def _wait_for_consistency(checker):\n    \"\"\"Eventual consistency: wait until GCS reports something is true.\n\n    This is necessary for e.g. create/delete where the operation might return,\n    but won't be reflected for a bit.\n    \"\"\"\n    for _ in range(EVENTUAL_CONSISTENCY_MAX_SLEEPS):\n        if checker():\n            return\n\n        time.sleep(EVENTUAL_CONSISTENCY_SLEEP_INTERVAL)\n\n    logger.warning('Exceeded wait for eventual GCS consistency - this may be a'\n                   'bug in the library or something is terribly wrong.')", "is_method": false, "function_description": "Helper function that waits for an external condition to become true, ensuring eventual consistency (e.g., after create or delete operations) by repeatedly checking a condition with delays."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_path_to_bucket_and_key", "line_number": 141, "body": "def _path_to_bucket_and_key(self, path):\n        (scheme, netloc, path, _, _) = urlsplit(path)\n        assert scheme == 'gs'\n        path_without_initial_slash = path[1:]\n        return netloc, path_without_initial_slash", "is_method": true, "class_name": "GCSClient", "function_description": "Utility method in GCSClient that extracts and returns the bucket name and object key from a Google Cloud Storage URI, supporting internal path parsing for storage operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_is_root", "line_number": 147, "body": "def _is_root(self, key):\n        return len(key) == 0 or key == '/'", "is_method": true, "class_name": "GCSClient", "function_description": "Private helper method in GCSClient that determines whether a given key represents the root directory or root path in the storage hierarchy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_add_path_delimiter", "line_number": 150, "body": "def _add_path_delimiter(self, key):\n        return key if key[-1:] == '/' else key + '/'", "is_method": true, "class_name": "GCSClient", "function_description": "Helper method in GCSClient that ensures a given path string ends with a forward slash, facilitating consistent path formatting for storage operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_obj_exists", "line_number": 154, "body": "def _obj_exists(self, bucket, obj):\n        try:\n            self.client.objects().get(bucket=bucket, object=obj).execute()\n        except errors.HttpError as ex:\n            if ex.resp['status'] == '404':\n                return False\n            raise\n        else:\n            return True", "is_method": true, "class_name": "GCSClient", "function_description": "Checks if a specified object exists within a given cloud storage bucket, enabling validation before operations like download or deletion in the GCSClient class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_list_iter", "line_number": 164, "body": "def _list_iter(self, bucket, prefix):\n        request = self.client.objects().list(bucket=bucket, prefix=prefix)\n        response = request.execute()\n\n        while response is not None:\n            for it in response.get('items', []):\n                yield it\n\n            request = self.client.objects().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "GCSClient", "function_description": "Internal generator method of GCSClient that iterates through all objects in a Google Cloud Storage bucket matching a prefix, enabling efficient, paginated retrieval of file metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_do_put", "line_number": 179, "body": "def _do_put(self, media, dest_path):\n        bucket, obj = self._path_to_bucket_and_key(dest_path)\n\n        request = self.client.objects().insert(bucket=bucket, name=obj, media_body=media)\n        if not media.resumable():\n            return request.execute()\n\n        response = None\n        while response is None:\n            status, response = request.next_chunk()\n            if status:\n                logger.debug('Upload progress: %.2f%%', 100 * status.progress())\n\n        _wait_for_consistency(lambda: self._obj_exists(bucket, obj))\n        return response", "is_method": true, "class_name": "GCSClient", "function_description": "Core internal method of GCSClient that uploads media data to a specified Google Cloud Storage path, handling both resumable and non-resumable uploads with progress tracking and ensuring eventual storage consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "exists", "line_number": 195, "body": "def exists(self, path):\n        bucket, obj = self._path_to_bucket_and_key(path)\n        if self._obj_exists(bucket, obj):\n            return True\n\n        return self.isdir(path)", "is_method": true, "class_name": "GCSClient", "function_description": "Checks whether a specified path exists in the storage, returning True if it corresponds to a file or a directory. It helps confirm the presence of objects or folders in Google Cloud Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "isdir", "line_number": 202, "body": "def isdir(self, path):\n        bucket, obj = self._path_to_bucket_and_key(path)\n        if self._is_root(obj):\n            try:\n                self.client.buckets().get(bucket=bucket).execute()\n            except errors.HttpError as ex:\n                if ex.resp['status'] == '404':\n                    return False\n                raise\n\n        obj = self._add_path_delimiter(obj)\n        if self._obj_exists(bucket, obj):\n            return True\n\n        # Any objects with this prefix\n        resp = self.client.objects().list(bucket=bucket, prefix=obj, maxResults=20).execute()\n        lst = next(iter(resp.get('items', [])), None)\n        return bool(lst)", "is_method": true, "class_name": "GCSClient", "function_description": "Determines whether a given path in a Google Cloud Storage bucket represents a directory by checking for matching objects or prefixes. This method helps in distinguishing folders from files within GCS storage structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "remove", "line_number": 221, "body": "def remove(self, path, recursive=True):\n        (bucket, obj) = self._path_to_bucket_and_key(path)\n\n        if self._is_root(obj):\n            raise InvalidDeleteException(\n                'Cannot delete root of bucket at path {}'.format(path))\n\n        if self._obj_exists(bucket, obj):\n            self.client.objects().delete(bucket=bucket, object=obj).execute()\n            _wait_for_consistency(lambda: not self._obj_exists(bucket, obj))\n            return True\n\n        if self.isdir(path):\n            if not recursive:\n                raise InvalidDeleteException(\n                    'Path {} is a directory. Must use recursive delete'.format(path))\n\n            req = http.BatchHttpRequest(batch_uri=GCS_BATCH_URI)\n            for it in self._list_iter(bucket, self._add_path_delimiter(obj)):\n                req.add(self.client.objects().delete(bucket=bucket, object=it['name']))\n            req.execute()\n\n            _wait_for_consistency(lambda: not self.isdir(path))\n            return True\n\n        return False", "is_method": true, "class_name": "GCSClient", "function_description": "Removes a file or directory at the given GCS path, supporting recursive deletion for directories while ensuring safety checks like forbidding bucket root deletion. It enables reliable cleanup of storage objects in Google Cloud Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put", "line_number": 248, "body": "def put(self, filename, dest_path, mimetype=None, chunksize=None):\n        chunksize = chunksize or self.chunksize\n        resumable = os.path.getsize(filename) > 0\n\n        mimetype = mimetype or mimetypes.guess_type(dest_path)[0] or DEFAULT_MIMETYPE\n        media = http.MediaFileUpload(filename, mimetype=mimetype, chunksize=chunksize, resumable=resumable)\n\n        self._do_put(media, dest_path)", "is_method": true, "class_name": "GCSClient", "function_description": "Uploads a local file to a specified cloud storage path, supporting resumable and chunked transfers with optional MIME type specification to ensure correct content handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_forward_args_to_put", "line_number": 257, "body": "def _forward_args_to_put(self, kwargs):\n        return self.put(**kwargs)", "is_method": true, "class_name": "GCSClient", "function_description": "Simple internal method in GCSClient that forwards all keyword arguments to the put method, serving as a transparent passthrough to reuse put's functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put_multiple", "line_number": 260, "body": "def put_multiple(self, filepaths, remote_directory, mimetype=None, chunksize=None, num_process=1):\n        if isinstance(filepaths, str):\n            raise ValueError(\n                'filenames must be a list of strings. If you want to put a single file, '\n                'use the `put(self, filename, ...)` method'\n            )\n\n        put_kwargs_list = [\n            {\n                'filename': filepath,\n                'dest_path': os.path.join(remote_directory, os.path.basename(filepath)),\n                'mimetype': mimetype,\n                'chunksize': chunksize,\n            }\n            for filepath in filepaths\n        ]\n\n        if num_process > 1:\n            from multiprocessing import Pool\n            from contextlib import closing\n            with closing(Pool(num_process)) as p:\n                return p.map(self._forward_args_to_put, put_kwargs_list)\n        else:\n            for put_kwargs in put_kwargs_list:\n                self._forward_args_to_put(put_kwargs)", "is_method": true, "class_name": "GCSClient", "function_description": "Batch uploads multiple local files to a specified remote directory in Google Cloud Storage, optionally using parallel processing for faster transfers. It supports configurable MIME types and chunk sizes for each file upload."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put_string", "line_number": 286, "body": "def put_string(self, contents, dest_path, mimetype=None):\n        mimetype = mimetype or mimetypes.guess_type(dest_path)[0] or DEFAULT_MIMETYPE\n        assert isinstance(mimetype, str)\n        if not isinstance(contents, bytes):\n            contents = contents.encode(\"utf-8\")\n        media = http.MediaIoBaseUpload(BytesIO(contents), mimetype, resumable=bool(contents))\n        self._do_put(media, dest_path)", "is_method": true, "class_name": "GCSClient", "function_description": "Utility method of the GCSClient class that uploads a string or byte content to a specified destination path in Google Cloud Storage, automatically handling MIME type detection and encoding."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "mkdir", "line_number": 294, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            elif not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            else:\n                return\n\n        self.put_string(b\"\", self._add_path_delimiter(path), mimetype='text/plain')", "is_method": true, "class_name": "GCSClient", "function_description": "Creates a directory at the specified path in Google Cloud Storage, optionally handling existing paths and parent directories, facilitating organized storage management within GCS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "copy", "line_number": 305, "body": "def copy(self, source_path, destination_path):\n        src_bucket, src_obj = self._path_to_bucket_and_key(source_path)\n        dest_bucket, dest_obj = self._path_to_bucket_and_key(destination_path)\n\n        if self.isdir(source_path):\n            src_prefix = self._add_path_delimiter(src_obj)\n            dest_prefix = self._add_path_delimiter(dest_obj)\n\n            source_path = self._add_path_delimiter(source_path)\n            copied_objs = []\n            for obj in self.listdir(source_path):\n                suffix = obj[len(source_path):]\n\n                self.client.objects().copy(\n                    sourceBucket=src_bucket,\n                    sourceObject=src_prefix + suffix,\n                    destinationBucket=dest_bucket,\n                    destinationObject=dest_prefix + suffix,\n                    body={}).execute()\n                copied_objs.append(dest_prefix + suffix)\n\n            _wait_for_consistency(\n                lambda: all(self._obj_exists(dest_bucket, obj)\n                            for obj in copied_objs))\n        else:\n            self.client.objects().copy(\n                sourceBucket=src_bucket,\n                sourceObject=src_obj,\n                destinationBucket=dest_bucket,\n                destinationObject=dest_obj,\n                body={}).execute()\n            _wait_for_consistency(lambda: self._obj_exists(dest_bucket, dest_obj))", "is_method": true, "class_name": "GCSClient", "function_description": "Utility method of GCSClient that copies a file or all objects under a directory path within Google Cloud Storage, ensuring eventual consistency after the copy operation completes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "rename", "line_number": 338, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Alias for ``move()``\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "GCSClient", "function_description": "Alias method in GCSClient that renames storage objects by delegating the operation to the move function, enabling convenient renaming functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "move", "line_number": 344, "body": "def move(self, source_path, destination_path):\n        \"\"\"\n        Rename/move an object from one GCS location to another.\n        \"\"\"\n        self.copy(source_path, destination_path)\n        self.remove(source_path)", "is_method": true, "class_name": "GCSClient", "function_description": "Method of GCSClient that renames or moves an object within Google Cloud Storage by copying it to a new location and deleting the original. It enables efficient object relocation across GCS paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "listdir", "line_number": 351, "body": "def listdir(self, path):\n        \"\"\"\n        Get an iterable with GCS folder contents.\n        Iterable contains paths relative to queried path.\n        \"\"\"\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        obj_prefix = self._add_path_delimiter(obj)\n        if self._is_root(obj_prefix):\n            obj_prefix = ''\n\n        obj_prefix_len = len(obj_prefix)\n        for it in self._list_iter(bucket, obj_prefix):\n            yield self._add_path_delimiter(path) + it['name'][obj_prefix_len:]", "is_method": true, "class_name": "GCSClient", "function_description": "Provides an iterable list of objects within a specified Google Cloud Storage path, yielding their relative paths for convenient folder content enumeration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "list_wildcard", "line_number": 366, "body": "def list_wildcard(self, wildcard_path):\n        \"\"\"Yields full object URIs matching the given wildcard.\n\n        Currently only the '*' wildcard after the last path delimiter is supported.\n\n        (If we need \"full\" wildcard functionality we should bring in gsutil dependency with its\n        https://github.com/GoogleCloudPlatform/gsutil/blob/master/gslib/wildcard_iterator.py...)\n        \"\"\"\n        path, wildcard_obj = wildcard_path.rsplit('/', 1)\n        assert '*' not in path, \"The '*' wildcard character is only supported after the last '/'\"\n        wildcard_parts = wildcard_obj.split('*')\n        assert len(wildcard_parts) == 2, \"Only one '*' wildcard is supported\"\n\n        for it in self.listdir(path):\n            if it.startswith(path + '/' + wildcard_parts[0]) and it.endswith(wildcard_parts[1]) and \\\n                   len(it) >= len(path + '/' + wildcard_parts[0]) + len(wildcard_parts[1]):\n                yield it", "is_method": true, "class_name": "GCSClient", "function_description": "Provides an iterator that yields full Google Cloud Storage object URIs matching a simple wildcard pattern, supporting only a single '*' wildcard at the end of the path. Useful for listing objects with common name patterns without external dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "download", "line_number": 385, "body": "def download(self, path, chunksize=None, chunk_callback=lambda _: False):\n        \"\"\"Downloads the object contents to local file system.\n\n        Optionally stops after the first chunk for which chunk_callback returns True.\n        \"\"\"\n        chunksize = chunksize or self.chunksize\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        with tempfile.NamedTemporaryFile(delete=False) as fp:\n            # We can't return the tempfile reference because of a bug in python: http://bugs.python.org/issue18879\n            return_fp = _DeleteOnCloseFile(fp.name, 'r')\n\n            # Special case empty files because chunk-based downloading doesn't work.\n            result = self.client.objects().get(bucket=bucket, object=obj).execute()\n            if int(result['size']) == 0:\n                return return_fp\n\n            request = self.client.objects().get_media(bucket=bucket, object=obj)\n            downloader = http.MediaIoBaseDownload(fp, request, chunksize=chunksize)\n\n            done = False\n            while not done:\n                _, done = downloader.next_chunk()\n                if chunk_callback(fp):\n                    done = True\n\n        return return_fp", "is_method": true, "class_name": "GCSClient", "function_description": "Downloads an object from Google Cloud Storage to a local temporary file, supporting optional incremental retrieval and early stopping based on a user-provided callback per downloaded chunk."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "close", "line_number": 415, "body": "def close(self):\n        super(_DeleteOnCloseFile, self).close()\n        try:\n            os.remove(self.name)\n        except OSError:\n            # Catch a potential threading race condition and also allow this\n            # method to be called multiple times.\n            pass", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Method of _DeleteOnCloseFile that closes the file and ensures its deletion from the filesystem, handling possible race conditions for safe, repeated calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "readable", "line_number": 424, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that the file object supports reading operations by always returning True. This method helps verify the file's readability status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "writable", "line_number": 427, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that the file is not writable by always returning False, signaling that write operations are not supported on this file-like object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "seekable", "line_number": 430, "body": "def seekable(self):\n        return True", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that the file supports random access operations, allowing other functions to move the read/write pointer within the file as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "move_to_final_destination", "line_number": 443, "body": "def move_to_final_destination(self):\n        self.gcs_client.put(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicGCSFile", "function_description": "Core method of AtomicGCSFile that moves a file from a temporary Google Cloud Storage location to its final destination, ensuring atomic file updates or uploads."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "open", "line_number": 458, "body": "def open(self, mode='r'):\n        if mode == 'r':\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(self.fs.download(self.path))))\n        elif mode == 'w':\n            return self.format.pipe_writer(AtomicGCSFile(self.path, self.fs))\n        else:\n            raise ValueError(\"Unsupported open mode '{}'\".format(mode))", "is_method": true, "class_name": "GCSTarget", "function_description": "Provides a file-like interface to read from or write to a cloud storage path using specified formatting, supporting mode-controlled data streaming for flexible file operations in GCSTarget."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "exists", "line_number": 512, "body": "def exists(self):\n        flag_target = self.path + self.flag\n        return self.fs.exists(flag_target)", "is_method": true, "class_name": "GCSFlagTarget", "function_description": "Checks if a specific flag file exists at the combined path in the filesystem, indicating the presence or status of the target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "dataset", "line_number": 106, "body": "def dataset(self):\n        return BQDataset(project_id=self.project_id, dataset_id=self.dataset_id, location=self.location)", "is_method": true, "class_name": "BQTable", "function_description": "Returns a BQDataset instance representing the dataset associated with the table's project, dataset ID, and location. It provides convenient access to the parent dataset of the table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "uri", "line_number": 110, "body": "def uri(self):\n        return \"bq://\" + self.project_id + \"/\" + \\\n               self.dataset.dataset_id + \"/\" + self.table_id", "is_method": true, "class_name": "BQTable", "function_description": "Returns the unique BigQuery URI string identifying the table by combining its project, dataset, and table IDs for easy reference and access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "dataset_exists", "line_number": 131, "body": "def dataset_exists(self, dataset):\n        \"\"\"Returns whether the given dataset exists.\n        If regional location is specified for the dataset, that is also checked\n        to be compatible with the remote dataset, otherwise an exception is thrown.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        try:\n            response = self.client.datasets().get(projectId=dataset.project_id,\n                                                  datasetId=dataset.dataset_id).execute()\n            if dataset.location is not None:\n                fetched_location = response.get('location')\n                if dataset.location != fetched_location:\n                    raise Exception('''Dataset already exists with regional location {}. Can't use {}.'''.format(\n                        fetched_location if fetched_location is not None else 'unspecified',\n                        dataset.location))\n\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "is_method": true, "class_name": "BigQueryClient", "function_description": "Checks if a specified BigQuery dataset exists and verifies its regional location compatibility, raising an error if location conflicts occur. Useful for validating dataset presence and location before data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "table_exists", "line_number": 157, "body": "def table_exists(self, table):\n        \"\"\"Returns whether the given table exists.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n        if not self.dataset_exists(table.dataset):\n            return False\n\n        try:\n            self.client.tables().get(projectId=table.project_id,\n                                     datasetId=table.dataset_id,\n                                     tableId=table.table_id).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "is_method": true, "class_name": "BigQueryClient", "function_description": "Checks if a specified BigQuery table exists within its dataset, facilitating conditional logic or error handling before performing operations on that table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "make_dataset", "line_number": 177, "body": "def make_dataset(self, dataset, raise_if_exists=False, body=None):\n        \"\"\"Creates a new dataset with the default permissions.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :param raise_if_exists: whether to raise an exception if the dataset already exists.\n           :raises luigi.target.FileAlreadyExists: if raise_if_exists=True and the dataset exists\n        \"\"\"\n\n        if body is None:\n            body = {}\n\n        try:\n            # Construct a message body in the format required by\n            # https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.datasets.html#insert\n            body['datasetReference'] = {\n                'projectId': dataset.project_id,\n                'datasetId': dataset.dataset_id\n            }\n            if dataset.location is not None:\n                body['location'] = dataset.location\n            self.client.datasets().insert(projectId=dataset.project_id, body=body).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 409:\n                if raise_if_exists:\n                    raise luigi.target.FileAlreadyExists()\n            else:\n                raise", "is_method": true, "class_name": "BigQueryClient", "function_description": "Creates a new BigQuery dataset with default permissions, optionally raising an error if the dataset already exists. This function supports dataset management by handling dataset creation and conflict detection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "delete_dataset", "line_number": 206, "body": "def delete_dataset(self, dataset, delete_nonempty=True):\n        \"\"\"Deletes a dataset (and optionally any tables in it), if it exists.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :param delete_nonempty: if true, will delete any tables before deleting the dataset\n        \"\"\"\n\n        if not self.dataset_exists(dataset):\n            return\n\n        self.client.datasets().delete(projectId=dataset.project_id,\n                                      datasetId=dataset.dataset_id,\n                                      deleteContents=delete_nonempty).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Deletes a specified BigQuery dataset, optionally removing all tables within it beforehand if non-empty. This method ensures safe dataset removal only if it exists, supporting clean-up operations in data management workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "delete_table", "line_number": 221, "body": "def delete_table(self, table):\n        \"\"\"Deletes a table, if it exists.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n\n        if not self.table_exists(table):\n            return\n\n        self.client.tables().delete(projectId=table.project_id,\n                                    datasetId=table.dataset_id,\n                                    tableId=table.table_id).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Utility method of the BigQueryClient class that deletes a specified table if it exists, preventing errors from attempting to remove non-existent tables. It ensures safe table deletion within Google BigQuery environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "list_datasets", "line_number": 235, "body": "def list_datasets(self, project_id):\n        \"\"\"Returns the list of datasets in a given project.\n\n           :param project_id:\n           :type project_id: str\n        \"\"\"\n\n        request = self.client.datasets().list(projectId=project_id,\n                                              maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for ds in response.get('datasets', []):\n                yield ds['datasetReference']['datasetId']\n\n            request = self.client.datasets().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Provides an iterable list of dataset IDs within a specified BigQuery project, facilitating discovery and enumeration of datasets for data management or analysis tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "list_tables", "line_number": 256, "body": "def list_tables(self, dataset):\n        \"\"\"Returns the list of tables in a given dataset.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        request = self.client.tables().list(projectId=dataset.project_id,\n                                            datasetId=dataset.dataset_id,\n                                            maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for t in response.get('tables', []):\n                yield t['tableReference']['tableId']\n\n            request = self.client.tables().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Provides an iterator to list all table IDs in a specified BigQuery dataset, supporting efficient retrieval across potentially large table collections through paginated requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "get_view", "line_number": 278, "body": "def get_view(self, table):\n        \"\"\"Returns the SQL query for a view, or None if it doesn't exist or is not a view.\n\n        :param table: The table containing the view.\n        :type table: BQTable\n        \"\"\"\n\n        request = self.client.tables().get(projectId=table.project_id,\n                                           datasetId=table.dataset_id,\n                                           tableId=table.table_id)\n\n        try:\n            response = request.execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return None\n            raise\n\n        return response['view']['query'] if 'view' in response else None", "is_method": true, "class_name": "BigQueryClient", "function_description": "Retrieves the SQL query defining a BigQuery view from a specified table, returning None if the table does not exist or is not a view. This enables identification and access of view definitions within BigQuery datasets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "update_view", "line_number": 298, "body": "def update_view(self, table, view):\n        \"\"\"Updates the SQL query for a view.\n\n        If the output table exists, it is replaced with the supplied view query. Otherwise a new\n        table is created with this view.\n\n        :param table: The table to contain the view.\n        :type table: BQTable\n        :param view: The SQL query for the view.\n        :type view: str\n        \"\"\"\n\n        body = {\n            'tableReference': {\n                'projectId': table.project_id,\n                'datasetId': table.dataset_id,\n                'tableId': table.table_id\n            },\n            'view': {\n                'query': view\n            }\n        }\n\n        if self.table_exists(table):\n            self.client.tables().update(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        tableId=table.table_id,\n                                        body=body).execute()\n        else:\n            self.client.tables().insert(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        body=body).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Method of the BigQueryClient class that creates or replaces a BigQuery table with a view defined by the given SQL query, ensuring the table accurately reflects the specified view query."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run_job", "line_number": 331, "body": "def run_job(self, project_id, body, dataset=None):\n        \"\"\"Runs a BigQuery \"job\". See the documentation for the format of body.\n\n           .. note::\n               You probably don't need to use this directly. Use the tasks defined below.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :return: the job id of the job.\n           :rtype: str\n           :raises luigi.contrib.BigQueryExecutionError: if the job fails.\n        \"\"\"\n\n        if dataset and not self.dataset_exists(dataset):\n            self.make_dataset(dataset)\n\n        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()\n        job_id = new_job['jobReference']['jobId']\n        logger.info('Started import job %s:%s', project_id, job_id)\n        while True:\n            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)\n            if status['status']['state'] == 'DONE':\n                if status['status'].get('errorResult'):\n                    raise BigQueryExecutionError(job_id, status['status']['errorResult'])\n                return job_id\n\n            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)\n            time.sleep(5)", "is_method": true, "class_name": "BigQueryClient", "function_description": "Core utility of the BigQueryClient class that executes a BigQuery job, optionally creating the dataset first, and waits for completion while handling errors. It returns the job ID upon successful execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "copy", "line_number": 360, "body": "def copy(self,\n             source_table,\n             dest_table,\n             create_disposition=CreateDisposition.CREATE_IF_NEEDED,\n             write_disposition=WriteDisposition.WRITE_TRUNCATE):\n        \"\"\"Copies (or appends) a table to another table.\n\n            :param source_table:\n            :type source_table: BQTable\n            :param dest_table:\n            :type dest_table: BQTable\n            :param create_disposition: whether to create the table if needed\n            :type create_disposition: CreateDisposition\n            :param write_disposition: whether to append/truncate/fail if the table exists\n            :type write_disposition: WriteDisposition\n        \"\"\"\n\n        job = {\n            \"configuration\": {\n                \"copy\": {\n                    \"sourceTable\": {\n                        \"projectId\": source_table.project_id,\n                        \"datasetId\": source_table.dataset_id,\n                        \"tableId\": source_table.table_id,\n                    },\n                    \"destinationTable\": {\n                        \"projectId\": dest_table.project_id,\n                        \"datasetId\": dest_table.dataset_id,\n                        \"tableId\": dest_table.table_id,\n                    },\n                    \"createDisposition\": create_disposition,\n                    \"writeDisposition\": write_disposition,\n                }\n            }\n        }\n\n        self.run_job(dest_table.project_id, job, dataset=dest_table.dataset)", "is_method": true, "class_name": "BigQueryClient", "function_description": "Utility method of BigQueryClient that copies data from one BigQuery table to another, with configurable options for table creation and overwrite behavior, facilitating data migration or duplication tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "from_bqtable", "line_number": 405, "body": "def from_bqtable(cls, table, client=None):\n        \"\"\"A constructor that takes a :py:class:`BQTable`.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n        return cls(table.project_id, table.dataset_id, table.table_id, client=client)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "Factory method of BigQueryTarget that creates an instance from a BQTable object, using its project, dataset, and table identifiers with an optional client parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "exists", "line_number": 413, "body": "def exists(self):\n        return self.client.table_exists(self.table)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "Checks if the specified BigQuery table exists by querying the client. This method helps validate table availability before performing operations on it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "__str__", "line_number": 416, "body": "def __str__(self):\n        return str(self.table)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "Returns the string representation of the BigQueryTarget's table, providing a simple way to identify or display the target table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "bulk_complete", "line_number": 430, "body": "def bulk_complete(cls, parameter_tuples):\n        # Instantiate the tasks to inspect them\n        tasks_with_params = [(cls(p), p) for p in parameter_tuples]\n        if not tasks_with_params:\n            return\n\n        # Grab the set of BigQuery datasets we are interested in\n        datasets = {t.output().table.dataset for t, p in tasks_with_params}\n        logger.info('Checking datasets %s for available tables', datasets)\n\n        # Query the available tables for all datasets\n        client = tasks_with_params[0][0].output().client\n        available_datasets = filter(client.dataset_exists, datasets)\n        available_tables = {d: set(client.list_tables(d)) for d in available_datasets}\n\n        # Return parameter_tuples belonging to available tables\n        for t, p in tasks_with_params:\n            table = t.output().table\n            if table.table_id in available_tables.get(table.dataset, []):\n                yield p", "is_method": true, "class_name": "MixinBigQueryBulkComplete", "function_description": "Service method in MixinBigQueryBulkComplete that filters input parameters to yield only those linked to existing BigQuery tables, enabling batch processing based on dataset availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "source_format", "line_number": 455, "body": "def source_format(self):\n        \"\"\"The source format to use (see :py:class:`SourceFormat`).\"\"\"\n        return SourceFormat.NEWLINE_DELIMITED_JSON", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the source format used for loading data, specifically indicating newline-delimited JSON format. This function informs other components about the expected input data format for processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "encoding", "line_number": 460, "body": "def encoding(self):\n        \"\"\"The encoding of the data that is going to be loaded (see :py:class:`Encoding`).\"\"\"\n        return Encoding.UTF_8", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the character encoding used for data loading, which is fixed to UTF-8. This method provides the encoding specification for the BigQueryLoadTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "write_disposition", "line_number": 465, "body": "def write_disposition(self):\n        \"\"\"What to do if the table already exists. By default this will fail the job.\n\n           See :py:class:`WriteDisposition`\"\"\"\n        return WriteDisposition.WRITE_EMPTY", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Provides the default policy for handling existing tables during a BigQuery load, specifying that the operation should fail if the target table is not empty."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "schema", "line_number": 472, "body": "def schema(self):\n        \"\"\"Schema in the format defined at https://cloud.google.com/bigquery/docs/reference/v2/jobs#configuration.load.schema.\n\n        If the value is falsy, it is omitted and inferred by BigQuery.\"\"\"\n        return []", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the BigQuery load job schema configuration or omits it to allow automatic schema inference during data loading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "max_bad_records", "line_number": 479, "body": "def max_bad_records(self):\n        \"\"\" The maximum number of bad records that BigQuery can ignore when reading data.\n\n        If the number of bad records exceeds this value, an invalid error is returned in the job result.\"\"\"\n        return 0", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the maximum allowable number of bad records BigQuery can ignore before failing a data load job. It indicates the tolerance threshold for data quality errors during ingestion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "field_delimiter", "line_number": 486, "body": "def field_delimiter(self):\n        \"\"\"The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.\"\"\"\n        return FieldDelimiter.COMMA", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Provides the default field delimiter character (comma) used to separate fields in CSV files for BigQuery loading operations. This ensures consistent parsing of CSV input data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "source_uris", "line_number": 490, "body": "def source_uris(self):\n        \"\"\"The fully-qualified URIs that point to your data in Google Cloud Storage.\n\n        Each URI can contain one '*' wildcard character and it must come after the 'bucket' name.\"\"\"\n        return [x.path for x in luigi.task.flatten(self.input())]", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns a list of fully-qualified Google Cloud Storage URIs pointing to the data sources for the load task, supporting wildcard usage after the bucket name."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "skip_leading_rows", "line_number": 497, "body": "def skip_leading_rows(self):\n        \"\"\"The number of rows at the top of a CSV file that BigQuery will skip when loading the data.\n\n        The default value is 0. This property is useful if you have header rows in the file that should be skipped.\"\"\"\n        return 0", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the number of leading rows in a CSV file that BigQuery should skip during data loading, typically used to bypass header rows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "allow_jagged_rows", "line_number": 504, "body": "def allow_jagged_rows(self):\n        \"\"\"Accept rows that are missing trailing optional columns. The missing values are treated as nulls.\n\n        If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,\n\n        an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Utility method in BigQueryLoadTask indicating if CSV rows with missing trailing columns are accepted, allowing incomplete rows to be loaded by treating missing values as nulls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "ignore_unknown_values", "line_number": 513, "body": "def ignore_unknown_values(self):\n        \"\"\"Indicates if BigQuery should allow extra values that are not represented in the table schema.\n\n        If true, the extra values are ignored. If false, records with extra columns are treated as bad records,\n\n        and if there are too many bad records, an invalid error is returned in the job result. The default value is false.\n\n        The sourceFormat property determines what BigQuery treats as an extra value:\n\n        CSV: Trailing columns JSON: Named values that don't match any column names\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Indicates whether BigQuery should ignore extra values not defined in the table schema during data loading, helping control error handling for records with unexpected columns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "allow_quoted_new_lines", "line_number": 526, "body": "def allow_quoted_new_lines(self):\n        \"\"\"\tIndicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file. The default value is false.\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns whether BigQuery permits quoted data fields with newline characters in CSV files, signaling CSV parsing behavior. It informs if multiline quoted fields are supported, defaulting to disallowing them."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 530, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        bq_client = output.client\n\n        source_uris = self.source_uris()\n        assert all(x.startswith('gs://') for x in source_uris)\n\n        job = {\n            'configuration': {\n                'load': {\n                    'destinationTable': {\n                        'projectId': output.table.project_id,\n                        'datasetId': output.table.dataset_id,\n                        'tableId': output.table.table_id,\n                    },\n                    'encoding': self.encoding,\n                    'sourceFormat': self.source_format,\n                    'writeDisposition': self.write_disposition,\n                    'sourceUris': source_uris,\n                    'maxBadRecords': self.max_bad_records,\n                    'ignoreUnknownValues': self.ignore_unknown_values\n                }\n            }\n        }\n\n        if self.source_format == SourceFormat.CSV:\n            job['configuration']['load']['fieldDelimiter'] = self.field_delimiter\n            job['configuration']['load']['skipLeadingRows'] = self.skip_leading_rows\n            job['configuration']['load']['allowJaggedRows'] = self.allow_jagged_rows\n            job['configuration']['load']['allowQuotedNewlines'] = self.allow_quoted_new_lines\n\n        if self.schema:\n            job['configuration']['load']['schema'] = {'fields': self.schema}\n        else:\n            job['configuration']['load']['autodetect'] = True\n\n        bq_client.run_job(output.table.project_id, job, dataset=output.table.dataset)", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Executes a BigQuery load job that imports data from Google Cloud Storage URIs into a specified BigQuery table, handling format-specific settings and schema definitions to automate or customize the data loading process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "write_disposition", "line_number": 574, "body": "def write_disposition(self):\n        \"\"\"What to do if the table already exists. By default this will fail the job.\n\n           See :py:class:`WriteDisposition`\"\"\"\n        return WriteDisposition.WRITE_TRUNCATE", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Returns the default policy for handling existing tables when writing query results, specifying that the table should be overwritten. This guides how query output is managed in BigQuery jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "create_disposition", "line_number": 581, "body": "def create_disposition(self):\n        \"\"\"Whether to create the table or not. See :py:class:`CreateDisposition`\"\"\"\n        return CreateDisposition.CREATE_IF_NEEDED", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Returns the table creation policy indicating if a table should be created when running a BigQuery query. This function specifies whether to create a table if it doesn't already exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "flatten_results", "line_number": 586, "body": "def flatten_results(self):\n        \"\"\"Flattens all nested and repeated fields in the query results.\n        allowLargeResults must be true if this is set to False.\"\"\"\n        return True", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "This method indicates whether nested and repeated fields in query results should be flattened, affecting BigQuery result structure and required settings. It provides a configuration flag for query result formatting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "query_mode", "line_number": 597, "body": "def query_mode(self):\n        \"\"\"The query mode. See :py:class:`QueryMode`.\"\"\"\n        return QueryMode.INTERACTIVE", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Returns the query execution mode, specifying that the query runs in interactive mode. This method provides the current query mode setting used during query execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "udf_resource_uris", "line_number": 602, "body": "def udf_resource_uris(self):\n        \"\"\"Iterator of code resource to load from a Google Cloud Storage URI (gs://bucket/path).\n        \"\"\"\n        return []", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Returns an iterator over Google Cloud Storage URIs for code resources required by the task; currently, this function provides an empty list indicating no resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "use_legacy_sql", "line_number": 608, "body": "def use_legacy_sql(self):\n        \"\"\"Whether to use legacy SQL\n        \"\"\"\n        return True", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Indicates that the task uses legacy SQL syntax for BigQuery queries. This flag helps determine the SQL dialect to apply when running queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 613, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        query = self.query\n        assert query, 'No query was provided'\n\n        bq_client = output.client\n\n        logger.info('Launching Query')\n        logger.info('Query destination: %s (%s)', output, self.write_disposition)\n        logger.info('Query SQL: %s', query)\n\n        job = {\n            'configuration': {\n                'query': {\n                    'query': query,\n                    'priority': self.query_mode,\n                    'destinationTable': {\n                        'projectId': output.table.project_id,\n                        'datasetId': output.table.dataset_id,\n                        'tableId': output.table.table_id,\n                    },\n                    'allowLargeResults': True,\n                    'createDisposition': self.create_disposition,\n                    'writeDisposition': self.write_disposition,\n                    'flattenResults': self.flatten_results,\n                    'userDefinedFunctionResources': [{\"resourceUri\": v} for v in self.udf_resource_uris],\n                    'useLegacySql': self.use_legacy_sql,\n                }\n            }\n        }\n\n        bq_client.run_job(output.table.project_id, job, dataset=output.table.dataset)", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Executes a configured SQL query in BigQuery, writing results to a specified destination table with defined job and write settings. It serves to run and manage query execution tasks within a BigQuery workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "complete", "line_number": 664, "body": "def complete(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        if not output.exists():\n            return False\n\n        existing_view = output.client.get_view(output.table)\n        return existing_view == self.view", "is_method": true, "class_name": "BigQueryCreateViewTask", "function_description": "Checks if the BigQuery view defined by the task exists and matches the expected view definition, confirming completion status. This ensures the view is present and up-to-date before proceeding."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 674, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        view = self.view\n        assert view, 'No view was provided'\n\n        logger.info('Create view')\n        logger.info('Destination: %s', output)\n        logger.info('View SQL: %s', view)\n\n        output.client.update_view(output.table, view)", "is_method": true, "class_name": "BigQueryCreateViewTask", "function_description": "Creates or updates a BigQuery view using the provided SQL query, facilitating automated management of view definitions in a data pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "destination_uris", "line_number": 704, "body": "def destination_uris(self):\n        \"\"\"\n        The fully-qualified URIs that point to your data in Google Cloud\n        Storage. Each URI can contain one '*' wildcard character and it must\n        come after the 'bucket' name.\n\n        Wildcarded destinationUris in GCSQueryTarget might not be resolved\n        correctly and result in incomplete data. If a GCSQueryTarget is used to\n        pass wildcarded destinationUris be sure to overwrite this property to\n        suppress the warning.\n        \"\"\"\n        return [x.path for x in luigi.task.flatten(self.output())]", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Returns a list of fully-qualified Google Cloud Storage URIs pointing to the task's output data, supporting wildcard usage after the bucket name for flexible data referencing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "print_header", "line_number": 718, "body": "def print_header(self):\n        \"\"\"Whether to print the header or not.\"\"\"\n        return PrintHeader.TRUE", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Returns the setting indicating that the header should always be printed. This function provides a fixed configuration for controlling header display in BigQuery extraction tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "field_delimiter", "line_number": 723, "body": "def field_delimiter(self):\n        \"\"\"\n        The separator for fields in a CSV file. The separator can be any\n        ISO-8859-1 single-byte character.\n        \"\"\"\n        return FieldDelimiter.COMMA", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Provides the default field delimiter character for CSV files, specifying how fields are separated during BigQuery data extraction tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "destination_format", "line_number": 731, "body": "def destination_format(self):\n        \"\"\"\n        The destination format to use (see :py:class:`DestinationFormat`).\n        \"\"\"\n        return DestinationFormat.CSV", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Returns the destination format used for exporting data, specifying CSV as the output format for BigQuery extraction tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "compression", "line_number": 738, "body": "def compression(self):\n        \"\"\"Whether to use compression.\"\"\"\n        return Compression.NONE", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Returns the compression setting used by the BigQueryExtractTask, indicating whether data compression is applied during extraction. This informs downstream processes about compression usage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 742, "body": "def run(self):\n        input = luigi.task.flatten(self.input())[0]\n        assert (\n            isinstance(input, BigQueryTarget) or\n            (len(input) == 1 and isinstance(input[0], BigQueryTarget))), \\\n            'Input must be exactly one BigQueryTarget, not %s' % (input)\n        bq_client = input.client\n\n        destination_uris = self.destination_uris\n        assert all(x.startswith('gs://') for x in destination_uris)\n\n        logger.info('Launching Extract Job')\n        logger.info('Extract source: %s', input)\n        logger.info('Extract destination: %s', destination_uris)\n\n        job = {\n            'configuration': {\n                'extract': {\n                    'sourceTable': {\n                        'projectId': input.table.project_id,\n                        'datasetId': input.table.dataset_id,\n                        'tableId': input.table.table_id\n                    },\n                    'destinationUris': destination_uris,\n                    'destinationFormat': self.destination_format,\n                    'compression': self.compression\n                }\n            }\n        }\n\n        if self.destination_format == 'CSV':\n            # \"Only exports to CSV may specify a field delimiter.\"\n            job['configuration']['extract']['printHeader'] = self.print_header\n            job['configuration']['extract']['fieldDelimiter'] = \\\n                self.field_delimiter\n\n        bq_client.run_job(\n            input.table.project_id,\n            job,\n            dataset=input.table.dataset)", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Executes a BigQuery extract job that exports data from a specified BigQuery table to one or more Google Cloud Storage URIs in a chosen format and compression. It supports configuration of CSV-specific options like headers and field delimiters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "generate_latest", "line_number": 41, "body": "def generate_latest(self):\n        return generate_latest(self.registry)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Returns the most recent metrics data collected in the registry in a format suitable for Prometheus to scrape. This function enables exporting current monitoring metrics for external analysis or alerting systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_started", "line_number": 44, "body": "def handle_task_started(self, task):\n        self.task_started_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Method in PrometheusMetricsCollector that increments the count of started tasks grouped by task family, supporting monitoring of task executions for performance tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_failed", "line_number": 48, "body": "def handle_task_failed(self, task):\n        self.task_failed_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Method of PrometheusMetricsCollector that updates metrics when a task fails, incrementing failure counts and recording the task execution duration by task family. It supports monitoring task reliability and performance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_disabled", "line_number": 52, "body": "def handle_task_disabled(self, task, config):\n        self.task_disabled_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Tracks and updates Prometheus metrics when a task is disabled, incrementing a counter and recording the task's execution duration by its family category."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_done", "line_number": 56, "body": "def handle_task_done(self, task):\n        self.task_done_counter.labels(family=task.family).inc()\n        # time_running can be `None` if task was already complete\n        if task.time_running is not None:\n            self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Function within PrometheusMetricsCollector that updates metrics when a task completes, incrementing the completion count and recording execution time to enable monitoring of task performance by task family."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "configure_http_handler", "line_number": 62, "body": "def configure_http_handler(self, http_handler):\n        http_handler.set_header('Content-Type', CONTENT_TYPE_LATEST)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Sets the HTTP handler's content type header to serve Prometheus metrics in the latest compatible format. This enables correct content negotiation for Prometheus scraping endpoints."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "track_job", "line_number": 74, "body": "def track_job(job_id):\n    \"\"\"\n    Tracking is done by requesting each job and then searching for whether the job\n    has one of the following states:\n    - \"RUN\",\n    - \"PEND\",\n    - \"SSUSP\",\n    - \"EXIT\"\n    based on the LSF documentation\n    \"\"\"\n    cmd = \"bjobs -noheader -o stat {}\".format(job_id)\n    track_job_proc = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, shell=True)\n    status = track_job_proc.communicate()[0].strip('\\n')\n    return status", "is_method": false, "function_description": "Function that checks and returns the current status of a job by querying its state from a job scheduler, supporting monitoring of job execution progress or completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "kill_job", "line_number": 91, "body": "def kill_job(job_id):\n    \"\"\"\n    Kill a running LSF job\n    \"\"\"\n    subprocess.call(['bkill', job_id])", "is_method": false, "function_description": "Function that terminates a running job in an LSF job scheduling system using the job's identifier, useful for managing and controlling computational tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "fetch_task_failures", "line_number": 119, "body": "def fetch_task_failures(self):\n        \"\"\"\n        Read in the error file from bsub\n        \"\"\"\n        error_file = os.path.join(self.tmp_dir, \"job.err\")\n        if os.path.isfile(error_file):\n            with open(error_file, \"r\") as f_err:\n                errors = f_err.readlines()\n        else:\n            errors = ''\n        return errors", "is_method": true, "class_name": "LSFJobTask", "function_description": "Retrieves and returns the contents of the error file generated by a job submission, providing access to any failure messages for an LSF job task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "fetch_task_output", "line_number": 131, "body": "def fetch_task_output(self):\n        \"\"\"\n        Read in the output file\n        \"\"\"\n        # Read in the output file\n        if os.path.isfile(os.path.join(self.tmp_dir, \"job.out\")):\n            with open(os.path.join(self.tmp_dir, \"job.out\"), \"r\") as f_out:\n                outputs = f_out.readlines()\n        else:\n            outputs = ''\n        return outputs", "is_method": true, "class_name": "LSFJobTask", "function_description": "Method of LSFJobTask that reads and returns the contents of a job output file, providing access to the execution results stored in a temporary directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_init_local", "line_number": 143, "body": "def _init_local(self):\n\n        base_tmp_dir = self.shared_tmp_dir\n\n        random_id = '%016x' % random.getrandbits(64)\n        task_name = random_id + self.task_id\n        # If any parameters are directories, if we don't\n        # replace the separators on *nix, it'll create a weird nested directory\n        task_name = task_name.replace(\"/\", \"::\")\n\n        # Max filename length\n        max_filename_length = os.fstatvfs(0).f_namemax\n        self.tmp_dir = os.path.join(base_tmp_dir, task_name[:max_filename_length])\n\n        LOGGER.info(\"Tmp dir: %s\", self.tmp_dir)\n        os.makedirs(self.tmp_dir)\n\n        # Dump the code to be run into a pickle file\n        LOGGER.debug(\"Dumping pickled class\")\n        self._dump(self.tmp_dir)\n\n        # Make sure that all the class's dependencies are tarred and available\n        LOGGER.debug(\"Tarballing dependencies\")\n        # Grab luigi and the module containing the code to be run\n        packages = [luigi, __import__(self.__module__, None, None, 'dummy')]\n        create_packages_archive(packages, os.path.join(self.tmp_dir, \"packages.tar\"))\n\n        # Now, pass onto the class's specified init_local() method.\n        self.init_local()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Initializes a local working directory with a unique name, sets up necessary files and dependencies for the LSFJobTask, and prepares the environment for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "run", "line_number": 181, "body": "def run(self):\n        \"\"\"\n        The procedure:\n        - Pickle the class\n        - Tarball the dependencies\n        - Construct a bsub argument that runs a generic runner function with the path to the pickled class\n        - Runner function loads the class from pickle\n        - Runner class untars the dependencies\n        - Runner function hits the button on the class's work() method\n        \"\"\"\n        self._init_local()\n        self._run_job()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Core method of LSFJobTask that initiates the job execution by preparing the task environment and submitting it for processing through an external batch system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_dump", "line_number": 205, "body": "def _dump(self, out_dir=''):\n        \"\"\"\n        Dump instance to file.\n        \"\"\"\n        self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n        if self.__module__ == '__main__':\n            dump_inst = pickle.dumps(self)\n            module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n            dump_inst = dump_inst.replace('(c__main__', \"(c\" + module_name)\n            open(self.job_file, \"w\").write(dump_inst)\n\n        else:\n            pickle.dump(self, open(self.job_file, \"w\"))", "is_method": true, "class_name": "LSFJobTask", "function_description": "Private method of LSFJobTask that serializes the job instance to a file, enabling saving and later retrieval of the job's state for persistence or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_run_job", "line_number": 219, "body": "def _run_job(self):\n        \"\"\"\n        Build a bsub argument that will run lsf_runner.py on the directory we've specified.\n        \"\"\"\n\n        args = []\n\n        if isinstance(self.output(), list):\n            log_output = os.path.split(self.output()[0].path)\n        else:\n            log_output = os.path.split(self.output().path)\n\n        args += [\"bsub\", \"-q\", self.queue_flag]\n        args += [\"-n\", str(self.n_cpu_flag)]\n        args += [\"-M\", str(self.memory_flag)]\n        args += [\"-R\", \"rusage[%s]\" % self.resource_flag]\n        args += [\"-W\", str(self.runtime_flag)]\n        if self.job_name_flag:\n            args += [\"-J\", str(self.job_name_flag)]\n        args += [\"-o\", os.path.join(log_output[0], \"job.out\")]\n        args += [\"-e\", os.path.join(log_output[0], \"job.err\")]\n        if self.extra_bsub_args:\n            args += self.extra_bsub_args.split()\n\n        # Find where the runner file is\n        runner_path = os.path.abspath(lsf_runner.__file__)\n\n        args += [runner_path]\n        args += [self.tmp_dir]\n\n        # That should do it. Let the world know what we're doing.\n        LOGGER.info(\"### LSF SUBMISSION ARGS: %s\",\n                    \" \".join([str(a) for a in args]))\n\n        # Submit the job\n        run_job_proc = subprocess.Popen(\n            [str(a) for a in args],\n            stdin=subprocess.PIPE, stdout=subprocess.PIPE, cwd=self.tmp_dir)\n        output = run_job_proc.communicate()[0]\n\n        # ASSUMPTION\n        # The result will be of the format\n        # Job <123> is submitted ot queue <myqueue>\n        # So get the number in those first brackets.\n        # I cannot think of a better workaround that leaves logic on the Task side of things.\n        LOGGER.info(\"### JOB SUBMISSION OUTPUT: %s\", str(output))\n        self.job_id = int(output.split(\"<\")[1].split(\">\")[0])\n        LOGGER.info(\n            \"Job %ssubmitted as job %s\",\n            self.job_name_flag + ' ',\n            str(self.job_id)\n        )\n\n        self._track_job()\n\n        # If we want to save the job temporaries, then do so\n        # We'll move them to be next to the job output\n        if self.save_job_info:\n            LOGGER.info(\"Saving up temporary bits\")\n\n            # dest_dir = self.output().path\n            shutil.move(self.tmp_dir, \"/\".join(log_output[0:-1]))\n\n        # Now delete the temporaries, if they're there.\n        self._finish()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Builds and submits a job to the LSF scheduler running a specified script, then tracks and manages its execution including logging and cleanup of temporary files. It automates job submission with resource and runtime configurations for batch processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_track_job", "line_number": 285, "body": "def _track_job(self):\n        time0 = 0\n        while True:\n            # Sleep for a little bit\n            time.sleep(self.poll_time)\n\n            # See what the job's up to\n            # ASSUMPTION\n            lsf_status = track_job(self.job_id)\n            if lsf_status == \"RUN\":\n                self.job_status = RUNNING\n                LOGGER.info(\"Job is running...\")\n                if time0 == 0:\n                    time0 = int(round(time.time()))\n            elif lsf_status == \"PEND\":\n                self.job_status = PENDING\n                LOGGER.info(\"Job is pending...\")\n            elif lsf_status == \"DONE\" or lsf_status == \"EXIT\":\n                # Then the job could either be failed or done.\n                errors = self.fetch_task_failures()\n                if not errors:\n                    self.job_status = DONE\n                    LOGGER.info(\"Job is done\")\n                    time1 = int(round(time.time()))\n\n                    # Return a near estimate of the run time to with +/- the\n                    # self.poll_time\n                    job_name = str(self.job_id)\n                    if self.job_name_flag:\n                        job_name = \"%s %s\" % (self.job_name_flag, job_name)\n                    LOGGER.info(\n                        \"### JOB COMPLETED: %s in %s seconds\",\n                        job_name,\n                        str(time1-time0)\n                    )\n                else:\n                    self.job_status = FAILED\n                    LOGGER.error(\"Job has FAILED\")\n                    LOGGER.error(\"\\n\\n\")\n                    LOGGER.error(\"Traceback: \")\n                    for error in errors:\n                        LOGGER.error(error)\n                break\n            elif lsf_status == \"SSUSP\":\n                self.job_status = PENDING\n                LOGGER.info(\"Job is suspended (basically, pending)...\")\n\n            else:\n                self.job_status = UNKNOWN\n                LOGGER.info(\"Job status is UNKNOWN!\")\n                LOGGER.info(\"Status is : %s\", lsf_status)\n                break", "is_method": true, "class_name": "LSFJobTask", "function_description": "Monitors the status of a batch job by periodically checking its state, updating internal status, and logging progress until the job completes, fails, or encounters an unknown condition."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_finish", "line_number": 338, "body": "def _finish(self):\n        LOGGER.info(\"Cleaning up temporary bits\")\n        if self.tmp_dir and os.path.exists(self.tmp_dir):\n            LOGGER.info('Removing directory %s', self.tmp_dir)\n            shutil.rmtree(self.tmp_dir)", "is_method": true, "class_name": "LSFJobTask", "function_description": "Private method in LSFJobTask that cleans up temporary files by removing the task's temporary directory if it exists, ensuring proper resource cleanup after job completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "run", "line_number": 354, "body": "def run(self):\n        self.init_local()\n        self.work()", "is_method": true, "class_name": "LocalLSFJobTask", "function_description": "Utility method of LocalLSFJobTask that initializes local settings and executes the primary task workflow, serving as the main entry point for running the job's operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/target.py", "function": "_make_method", "line_number": 55, "body": "def _make_method(cls, method_name):\n        def new_method(self, *args, **kwargs):\n            return self._chained_call(method_name, *args, **kwargs)\n        return new_method", "is_method": true, "class_name": "CascadingClient", "function_description": "Private utility in CascadingClient that dynamically creates a method invoking a chained call with given arguments, facilitating flexible delegation of method calls within chained client operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/target.py", "function": "_chained_call", "line_number": 60, "body": "def _chained_call(self, method_name, *args, **kwargs):\n        for i in range(len(self.clients)):\n            client = self.clients[i]\n            try:\n                result = getattr(client, method_name)(*args, **kwargs)\n                return result\n            except luigi.target.FileSystemException:\n                # For exceptions that are semantical, we must throw along\n                raise\n            except BaseException:\n                is_last_iteration = (i + 1) >= len(self.clients)\n                if is_last_iteration:\n                    raise\n                else:\n                    logger.warning('The %s failed to %s, using fallback class %s',\n                                   client.__class__.__name__, method_name, self.clients[i + 1].__class__.__name__)", "is_method": true, "class_name": "CascadingClient", "function_description": "Core method of CascadingClient that attempts to invoke a specified method sequentially across multiple clients, gracefully handling failures by falling back to the next client until successful or all clients fail."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "get_dataproc_client", "line_number": 26, "body": "def get_dataproc_client():\n    return _dataproc_client", "is_method": false, "function_description": "Returns a pre-initialized client instance for interacting with the Dataproc service, facilitating management and operation of cloud-based data processing clusters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "set_dataproc_client", "line_number": 30, "body": "def set_dataproc_client(client):\n    global _dataproc_client\n    _dataproc_client = client", "is_method": false, "function_description": "Utility function that sets a global client instance for interacting with Dataproc services, enabling other functions to access and use the configured client."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_job", "line_number": 53, "body": "def submit_job(self, job_config):\n        self._job = self.dataproc_client.projects().regions().jobs()\\\n            .submit(projectId=self.gcloud_project_id, region=self.dataproc_region, body=job_config).execute()\n        self._job_id = self._job['reference']['jobId']\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Method of DataprocBaseTask that submits a job configuration to Google Cloud Dataproc, initiating its execution and returning job details including the assigned job ID. It enables task automation by interacting with Dataproc service APIs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_spark_job", "line_number": 59, "body": "def submit_spark_job(self, jars, main_class, job_args=None):\n\n        if job_args is None:\n            job_args = []\n\n        job_config = {\"job\": {\n            \"placement\": {\n                \"clusterName\": self.dataproc_cluster_name\n            },\n            \"sparkJob\": {\n                \"args\": job_args,\n                \"mainClass\": main_class,\n                \"jarFileUris\": jars\n            }\n        }}\n        self.submit_job(job_config)\n        self._job_name = os.path.basename(self._job['sparkJob']['mainClass'])\n        logger.info(\"Submitted new dataproc job:{} id:{}\".format(self._job_name, self._job_id))\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Submits a Spark job to a specified Dataproc cluster using provided JAR files, main class, and optional arguments, facilitating automated job deployment and execution within a managed big data environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_pyspark_job", "line_number": 79, "body": "def submit_pyspark_job(self, job_file, extra_files=list(), job_args=None):\n\n        if job_args is None:\n            job_args = []\n\n        job_config = {\"job\": {\n            \"placement\": {\n                \"clusterName\": self.dataproc_cluster_name\n            },\n            \"pysparkJob\": {\n                \"mainPythonFileUri\": job_file,\n                \"pythonFileUris\": extra_files,\n                \"args\": job_args\n            }\n        }}\n        self.submit_job(job_config)\n        self._job_name = os.path.basename(self._job['pysparkJob']['mainPythonFileUri'])\n        logger.info(\"Submitted new dataproc job:{} id:{}\".format(self._job_name, self._job_id))\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Method of DataprocBaseTask that configures and submits a PySpark job to a specified Dataproc cluster, allowing customization with additional files and job arguments for distributed data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "wait_for_job", "line_number": 99, "body": "def wait_for_job(self):\n        if self._job is None:\n            raise Exception(\"You must submit a job before you can wait for it\")\n        while True:\n            job_result = self.dataproc_client.projects().regions().jobs()\\\n                .get(projectId=self.gcloud_project_id, region=self.dataproc_region, jobId=self._job_id).execute()\n            status = job_result['status']['state']\n            logger.info(\"Current dataproc status: {} job:{} id:{}\".format(status, self._job_name, self._job_id))\n            if status == 'DONE':\n                break\n            if status == 'ERROR':\n                raise Exception(job_result['status']['details'])\n            time.sleep(5)", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Method of the DataprocBaseTask class that monitors and waits for a submitted Dataproc job to complete, raising exceptions for errors or if no job was submitted. Useful for synchronous job execution workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 122, "body": "def run(self):\n        self.submit_spark_job(main_class=self.main_class,\n                              jars=self.jars.split(\",\") if self.jars else [],\n                              job_args=self.job_args.split(\",\") if self.job_args else [])\n        self.wait_for_job()", "is_method": true, "class_name": "DataprocSparkTask", "function_description": "Core method of DataprocSparkTask that submits a Spark job with specified classes, jars, and arguments, then waits for its completion. It orchestrates the execution and monitoring of distributed Spark tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 137, "body": "def run(self):\n        self.submit_pyspark_job(job_file=self.job_file,\n                                extra_files=self.extra_files.split(\",\") if self.extra_files else [],\n                                job_args=self.job_args.split(\",\") if self.job_args else [])\n        self.wait_for_job()", "is_method": true, "class_name": "DataprocPysparkTask", "function_description": "Core method of DataprocPysparkTask that submits a PySpark job with specified files and arguments, then waits for its completion, enabling automated execution and monitoring of Spark tasks on a Dataproc cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "_get_cluster_status", "line_number": 158, "body": "def _get_cluster_status(self):\n        return self.dataproc_client.projects().regions().clusters()\\\n            .get(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name)\\\n            .execute()", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "Utility method in CreateDataprocClusterTask that retrieves the current status of a specific Dataproc cluster using Google Cloud's API. It enables monitoring and managing cluster state in data processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "complete", "line_number": 163, "body": "def complete(self):\n        try:\n            self._get_cluster_status()\n            return True  # No (404) error so the cluster already exists\n        except HttpError as e:\n            if e.resp.status == 404:\n                return False  # We got a 404 so the cluster doesn't exist yet\n            else:\n                raise e", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "Method of CreateDataprocClusterTask that checks if a Dataproc cluster exists by verifying its status, returning True if it does and False if it receives a not found error."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 173, "body": "def run(self):\n        base_uri = \"https://www.googleapis.com/compute/v1/projects/{}\".format(self.gcloud_project_id)\n        software_config = {\"imageVersion\": self.image_version} if self.image_version else {}\n\n        cluster_conf = {\n            \"clusterName\": self.dataproc_cluster_name,\n            \"projectId\": self.gcloud_project_id,\n            \"config\": {\n                \"configBucket\": \"\",\n                \"gceClusterConfig\": {\n                    \"networkUri\": base_uri + \"/global/networks/\" + self.gcloud_network,\n                    \"zoneUri\": base_uri + \"/zones/\" + self.gcloud_zone,\n                    \"serviceAccountScopes\": [\n                        \"https://www.googleapis.com/auth/cloud-platform\"\n                    ]\n                },\n                \"masterConfig\": {\n                    \"numInstances\": 1,\n                    \"machineTypeUri\": base_uri + \"/zones/\" + self.gcloud_zone + \"/machineTypes/\" + self.master_node_type,\n                    \"diskConfig\": {\n                        \"bootDiskSizeGb\": self.master_disk_size,\n                        \"numLocalSsds\": 0\n                    }\n                },\n                \"workerConfig\": {\n                    \"numInstances\": self.worker_normal_count,\n                    \"machineTypeUri\": base_uri + \"/zones/\" + self.gcloud_zone + \"/machineTypes/\" + self.worker_node_type,\n                    \"diskConfig\": {\n                        \"bootDiskSizeGb\": self.worker_disk_size,\n                        \"numLocalSsds\": 0\n                    }\n                },\n                \"secondaryWorkerConfig\": {\n                    \"numInstances\": self.worker_preemptible_count,\n                    \"isPreemptible\": True\n                },\n                \"softwareConfig\": software_config\n            }\n        }\n\n        self.dataproc_client.projects().regions().clusters()\\\n            .create(projectId=self.gcloud_project_id, region=self.dataproc_region, body=cluster_conf).execute()\n\n        while True:\n            time.sleep(10)\n            cluster_status = self._get_cluster_status()\n            status = cluster_status['status']['state']\n            logger.info(\"Creating new dataproc cluster: {} status: {}\".format(self.dataproc_cluster_name, status))\n            if status == 'RUNNING':\n                break\n            if status == 'ERROR':\n                raise Exception(cluster_status['status']['details'])", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "Creates and monitors a Google Cloud Dataproc cluster until it becomes active or errors out. This method configures cluster parameters, initiates creation, and waits for the cluster to be ready for use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "_get_cluster_status", "line_number": 236, "body": "def _get_cluster_status(self):\n        try:\n            return self.dataproc_client.projects().regions().clusters()\\\n                .get(projectId=self.gcloud_project_id, region=self.dataproc_region,\n                     clusterName=self.dataproc_cluster_name, fields=\"status\")\\\n                .execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                return None  # We got a 404 so the cluster doesn't exist\n            else:\n                raise e", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Core utility method of DeleteDataprocClusterTask that retrieves the current status of a specified Dataproc cluster, returning None if the cluster does not exist, facilitating cluster existence checks and status monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "complete", "line_number": 248, "body": "def complete(self): return self._get_cluster_status() is None", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Checks if the Dataproc cluster has been successfully deleted by verifying its status is no longer available. This method indicates completion of the delete cluster task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 250, "body": "def run(self):\n        self.dataproc_client.projects().regions().clusters()\\\n            .delete(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name).execute()\n\n        while True:\n            time.sleep(10)\n            status = self._get_cluster_status()\n            if status is None:\n                logger.info(\"Finished shutting down cluster: {}\".format(self.dataproc_cluster_name))\n                break\n            logger.info(\"Shutting down cluster: {} current status: {}\".format(self.dataproc_cluster_name, status['status']['state']))", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Deletes a specified Dataproc cluster and continuously monitors its shutdown status until the cluster is fully terminated, providing progress updates. This supports automated cluster lifecycle management in cloud environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "marker_key", "line_number": 76, "body": "def marker_key(self):\n        \"\"\"\n        Generate a key for the indicator hash.\n        \"\"\"\n        return '%s:%s' % (self.marker_prefix, self.update_id)", "is_method": true, "class_name": "RedisTarget", "function_description": "Generates a unique key combining a marker prefix and update identifier, used to identify or access a specific indicator hash within Redis storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "touch", "line_number": 82, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n\n        We index the parameters `update_id` and `date`.\n        \"\"\"\n        marker_key = self.marker_key()\n        self.redis_client.hset(marker_key, 'update_id', self.update_id)\n        self.redis_client.hset(marker_key, 'date', datetime.datetime.now().isoformat())\n\n        if self.expire is not None:\n            self.redis_client.expire(marker_key, self.expire)", "is_method": true, "class_name": "RedisTarget", "function_description": "Marks the update as completed by recording its ID and timestamp in Redis, optionally setting an expiration. This function helps track update status for synchronization or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "exists", "line_number": 95, "body": "def exists(self):\n        \"\"\"\n        Test, if this task has been run.\n        \"\"\"\n        return self.redis_client.exists(self.marker_key()) == 1", "is_method": true, "class_name": "RedisTarget", "function_description": "Checks if the task represented by this RedisTarget has already been executed by verifying the presence of a specific marker key in Redis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__str__", "line_number": 60, "body": "def __str__(self):\n        return \"Command '%s' on host %s returned non-zero exit status %d\" % (\n            self.cmd, self.host, self.returncode)", "is_method": true, "class_name": "RemoteCalledProcessError", "function_description": "Overrides the string representation of RemoteCalledProcessError to provide a concise error message detailing the failed command, host, and exit status. It facilitates clear and informative error reporting for remote process executions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__repr__", "line_number": 77, "body": "def __repr__(self):\n        return '%s(%r, %r, %r, %r, %r)' % (\n            type(self).__name__, self.host, self.username, self.key_file, self.connect_timeout, self.port)", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides a string representation of the RemoteContext instance, displaying its key connection attributes for easier debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__eq__", "line_number": 81, "body": "def __eq__(self, other):\n        return repr(self) == repr(other)", "is_method": true, "class_name": "RemoteContext", "function_description": "Core method of the RemoteContext class that defines equality by comparing the string representations of two instances, enabling intuitive instance comparison based on their descriptive state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__hash__", "line_number": 84, "body": "def __hash__(self):\n        return hash(repr(self))", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides a hash value for RemoteContext instances based on their string representation, enabling their use as keys in hash-based collections like sets and dictionaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_host_ref", "line_number": 87, "body": "def _host_ref(self):\n        if self.username:\n            return \"{0}@{1}\".format(self.username, self.host)\n        else:\n            return self.host", "is_method": true, "class_name": "RemoteContext", "function_description": "Private method of RemoteContext that returns the host identifier, including the username if available, for connection or display purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_prepare_cmd", "line_number": 93, "body": "def _prepare_cmd(self, cmd):\n        connection_cmd = [\"ssh\", self._host_ref(), \"-o\", \"ControlMaster=no\"]\n        if self.sshpass:\n            connection_cmd = [\"sshpass\", \"-e\"] + connection_cmd\n        else:\n            connection_cmd += [\"-o\", \"BatchMode=yes\"]  # no password prompts etc\n        if self.port:\n            connection_cmd.extend([\"-p\", self.port])\n\n        if self.connect_timeout is not None:\n            connection_cmd += ['-o', 'ConnectTimeout=%d' % self.connect_timeout]\n\n        if self.no_host_key_check:\n            connection_cmd += ['-o', 'UserKnownHostsFile=/dev/null',\n                               '-o', 'StrictHostKeyChecking=no']\n\n        if self.key_file:\n            connection_cmd.extend([\"-i\", self.key_file])\n\n        if self.tty:\n            connection_cmd.append('-t')\n        return connection_cmd + cmd", "is_method": true, "class_name": "RemoteContext", "function_description": "Constructs and returns a fully configured SSH command list with options for authentication, connection settings, and terminal allocation, preparing it for remote execution in the RemoteContext environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "Popen", "line_number": 116, "body": "def Popen(self, cmd, **kwargs):\n        \"\"\"\n        Remote Popen.\n        \"\"\"\n        prefixed_cmd = self._prepare_cmd(cmd)\n        return subprocess.Popen(prefixed_cmd, **kwargs)", "is_method": true, "class_name": "RemoteContext", "function_description": "Method of RemoteContext that executes a command remotely by preparing it and invoking it as a subprocess, enabling remote process management within the context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "check_output", "line_number": 123, "body": "def check_output(self, cmd):\n        \"\"\"\n        Execute a shell command remotely and return the output.\n\n        Simplified version of Popen when you only want the output as a string and detect any errors.\n        \"\"\"\n        p = self.Popen(cmd, stdout=subprocess.PIPE)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise RemoteCalledProcessError(p.returncode, cmd, self.host, output=output)\n        return output", "is_method": true, "class_name": "RemoteContext", "function_description": "Utility method in RemoteContext that executes a remote shell command, returning its output as a string while raising an error if the command fails. Useful for running and capturing results of remote processes safely."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "tunnel", "line_number": 136, "body": "def tunnel(self, local_port, remote_port=None, remote_host=\"localhost\"):\n        \"\"\"\n        Open a tunnel between localhost:local_port and remote_host:remote_port via the host specified by this context.\n\n        Remember to close() the returned \"tunnel\" object in order to clean up\n        after yourself when you are done with the tunnel.\n        \"\"\"\n        tunnel_host = \"{0}:{1}:{2}\".format(local_port, remote_host, remote_port)\n        proc = self.Popen(\n            # cat so we can shut down gracefully by closing stdin\n            [\"-L\", tunnel_host, \"echo -n ready && cat\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n        )\n        # make sure to get the data so we know the connection is established\n        ready = proc.stdout.read(5)\n        assert ready == b\"ready\", \"Didn't get ready from remote echo\"\n        yield  # user code executed here\n        proc.communicate()\n        assert proc.returncode == 0, \"Tunnel process did an unclean exit (returncode %s)\" % (proc.returncode,)", "is_method": true, "class_name": "RemoteContext", "function_description": "Service method of the RemoteContext class that establishes and manages an SSH tunnel between a local and remote port, facilitating secure port forwarding through the specified remote host. It ensures connection readiness and requires explicit closure to clean up resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "exists", "line_number": 163, "body": "def exists(self, path):\n        \"\"\"\n        Return `True` if file or directory at `path` exist, False otherwise.\n        \"\"\"\n        try:\n            self.remote_context.check_output([\"test\", \"-e\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Checks whether a file or directory exists at the specified path in a remote filesystem. This function enables conditional operations based on remote file presence, supporting workflows that interact with remote storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "listdir", "line_number": 176, "body": "def listdir(self, path):\n        while path.endswith('/'):\n            path = path[:-1]\n\n        path = path or '.'\n        listing = self.remote_context.check_output([\"find\", \"-L\", path, \"-type\", \"f\"]).splitlines()\n        return [v.decode('utf-8') for v in listing]", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Lists all file paths under a specified directory in a remote file system, returning a flattened list of file names for use in file management or inspection tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "isdir", "line_number": 184, "body": "def isdir(self, path):\n        \"\"\"\n        Return `True` if directory at `path` exist, False otherwise.\n        \"\"\"\n        try:\n            self.remote_context.check_output([\"test\", \"-d\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Checks if a specified path on a remote file system is a directory, returning True if it exists and False otherwise. This function helps verify directory presence before performing file system operations remotely."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "remove", "line_number": 197, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove file or directory at location `path`.\n        \"\"\"\n        if recursive:\n            cmd = [\"rm\", \"-r\", path]\n        else:\n            cmd = [\"rm\", path]\n\n        self.remote_context.check_output(cmd)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Removes a specified file or directory from the remote file system, optionally deleting directories recursively. This enables remote cleanup or file management through a simple interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "mkdir", "line_number": 208, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            elif not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            else:\n                return\n\n        if parents:\n            cmd = ['mkdir', '-p', path]\n        else:\n            cmd = ['mkdir', path, '2>&1']\n\n        try:\n            self.remote_context.check_output(cmd)\n        except subprocess.CalledProcessError as e:\n            if b'no such file' in e.output.lower():\n                raise luigi.target.MissingParentDirectory()\n            raise", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Creates a directory at the specified remote path, optionally creating parent directories and handling existing path conflicts. It ensures directory creation with error handling for invalid or missing paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_scp", "line_number": 229, "body": "def _scp(self, src, dest):\n        cmd = [\"scp\", \"-q\", \"-C\", \"-o\", \"ControlMaster=no\"]\n        if self.remote_context.sshpass:\n            cmd = [\"sshpass\", \"-e\"] + cmd\n        else:\n            cmd.append(\"-B\")\n        if self.remote_context.no_host_key_check:\n            cmd.extend(['-o', 'UserKnownHostsFile=/dev/null',\n                        '-o', 'StrictHostKeyChecking=no'])\n        if self.remote_context.key_file:\n            cmd.extend([\"-i\", self.remote_context.key_file])\n        if self.remote_context.port:\n            cmd.extend([\"-P\", self.remote_context.port])\n        if os.path.isdir(src):\n            cmd.extend([\"-r\"])\n        cmd.extend([src, dest])\n        p = subprocess.Popen(cmd)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise subprocess.CalledProcessError(p.returncode, cmd, output=output)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Internal method of RemoteFileSystem to securely copy files or directories between local and remote locations using scp, supporting various SSH options like password, key file, port, and host key checking customization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "put", "line_number": 250, "body": "def put(self, local_path, path):\n        # create parent folder if not exists\n        normpath = posixpath.normpath(path)\n        folder = os.path.dirname(normpath)\n        if folder and not self.exists(folder):\n            self.remote_context.check_output(['mkdir', '-p', folder])\n\n        tmp_path = path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n        self._scp(local_path, \"%s:%s\" % (self.remote_context._host_ref(), tmp_path))\n        self.remote_context.check_output(['mv', tmp_path, path])", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Uploads a local file to a specified remote path, ensuring the target directory exists and using a temporary file for safe transfer before finalizing the destination file. This facilitates reliable remote file storage operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "get", "line_number": 261, "body": "def get(self, path, local_path):\n        # Create folder if it does not exist\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder:\n            try:\n                os.makedirs(folder)\n            except OSError:\n                pass\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n        self._scp(\"%s:%s\" % (self.remote_context._host_ref(), path), tmp_local_path)\n        os.rename(tmp_local_path, local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Utility method of RemoteFileSystem that downloads a file from a remote path to a specified local path, ensuring the local directory exists and using a temporary file to avoid partial writes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__del__", "line_number": 292, "body": "def __del__(self):\n        super(AtomicRemoteFileWriter, self).__del__()\n\n        try:\n            if self.fs.exists(self.__tmp_path):\n                self.fs.remote_context.check_output(['rm', self.__tmp_path])\n        except Exception:\n            # Don't propagate the exception; bad things can happen.\n            logger.exception('Failed to delete in-flight file')", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Destructor method in AtomicRemoteFileWriter that attempts to remove a temporary remote file during object cleanup, ensuring no leftover partial files remain after writing operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "close", "line_number": 302, "body": "def close(self):\n        super(AtomicRemoteFileWriter, self).close()\n        self.fs.remote_context.check_output(['mv', self.__tmp_path, self.path])", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Ensures that the temporary file is properly closed and atomically moved to its final remote location, guaranteeing safe and complete file writing in remote storage operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "tmp_path", "line_number": 307, "body": "def tmp_path(self):\n        return self.__tmp_path", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Returns the temporary file path used by the AtomicRemoteFileWriter instance for intermediate or atomic write operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "fs", "line_number": 311, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Accessor method returning the underlying filesystem object used by the AtomicRemoteFileWriter for file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "fs", "line_number": 330, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "RemoteTarget", "function_description": "Returns the underlying filesystem object used by the RemoteTarget instance, providing access to file system operations within the remote target context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "open", "line_number": 333, "body": "def open(self, mode='r'):\n        if mode == 'w':\n            file_writer = AtomicRemoteFileWriter(self.fs, self.path)\n            if self.format:\n                return self.format.pipe_writer(file_writer)\n            else:\n                return file_writer\n        elif mode == 'r':\n            file_reader = luigi.format.InputPipeProcessWrapper(\n                self.fs.remote_context._prepare_cmd([\"cat\", self.path]))\n            if self.format:\n                return self.format.pipe_reader(file_reader)\n            else:\n                return file_reader\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Provides an interface to open a remote file for reading or writing, optionally applying a specified data format. Supports atomic write operations and remote read streaming for efficient file handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "put", "line_number": 350, "body": "def put(self, local_path):\n        self.fs.put(local_path, self.path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Uploads a local file to a remote filesystem path managed by the RemoteTarget. This method facilitates transferring files from local storage to the remote target location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "get", "line_number": 353, "body": "def get(self, local_path):\n        self.fs.get(self.path, local_path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Simple method in RemoteTarget that downloads content from a remote path to a specified local file path, facilitating file transfer from remote storage to the local system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "configuration_section", "line_number": 48, "body": "def configuration_section(self):\n        \"\"\"\n        Override to change the configuration section used\n        to obtain default credentials.\n        \"\"\"\n        return 'redshift'", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Returns the configuration section name used to obtain default credentials, allowing subclasses to specify which configuration applies for authentication purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_access_key_id", "line_number": 56, "body": "def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_access_key_id')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Utility method in _CredentialsMixin that retrieves the AWS access key ID from configuration for authentication purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_secret_access_key", "line_number": 63, "body": "def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return self._get_configuration_attribute('aws_secret_access_key')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Returns the AWS secret access key from the configuration, enabling secure authentication for AWS services within the credential management context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_account_id", "line_number": 70, "body": "def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_account_id')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Returns the AWS account ID from the configuration attributes, providing an easy way to access this identifier within credential management contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_arn_role_name", "line_number": 77, "body": "def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return self._get_configuration_attribute('aws_arn_role_name')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Returns the AWS ARN role name from the configuration, providing a standardized way to access this credential attribute within classes using the _CredentialsMixin."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_session_token", "line_number": 84, "body": "def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return self._get_configuration_attribute('aws_session_token')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides access to the AWS session token by retrieving it from the underlying configuration, facilitating authenticated AWS service interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "_get_configuration_attribute", "line_number": 90, "body": "def _get_configuration_attribute(self, attribute):\n        config = luigi.configuration.get_config()\n\n        value = config.get(self.configuration_section, attribute, default=None)\n\n        if not value:\n            value = os.environ.get(attribute.upper(), None)\n\n        return value", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Utility method in _CredentialsMixin that retrieves a configuration value by checking the class-specific config section and then environment variables, facilitating flexible and prioritized access to credential settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "_credentials", "line_number": 100, "body": "def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides a credential string for AWS authentication based on available credential pairs or raises an error if none are configured. Enables tasks to securely access AWS resources using either IAM role or access key credentials."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "s3_load_path", "line_number": 166, "body": "def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method in S3CopyToTable provides the load path for the S3 data source but currently returns None, indicating no path is specified or it acts as a placeholder."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy_options", "line_number": 174, "body": "def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Returns additional configuration options for copying data into a table, such as formatting rules and delimiters. Currently, it provides no extra options by default."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "prune_date", "line_number": 205, "body": "def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method provides a customizable date for pruning operations in data copying workflows, enabling control over which records are filtered based on a comparison column during table synchronization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "table_attributes", "line_number": 214, "body": "def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Provides a placeholder method to specify additional table attributes like distribution style or sort keys during a copy operation to a table. Currently returns an empty string, serving as a customizable extension point."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "table_constraints", "line_number": 225, "body": "def table_constraints(self):\n        \"\"\"\n        Add extra table constraints, for example:\n\n        PRIMARY KEY (MY_FIELD, MY_FIELD_2)\n        UNIQUE KEY (MY_FIELD_3)\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Returns additional SQL table constraints as a string, such as primary or unique keys, to enforce data integrity during table creation or modification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "do_truncate_table", "line_number": 235, "body": "def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Indicates whether the target table should be cleared before loading new data. This method provides a toggle for truncation behavior during data copying processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "do_prune", "line_number": 241, "body": "def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Determines if pruning parameters are fully implemented to control data deletion before copying new data, ensuring consistent prune configuration or raising an exception if partially configured. Useful for managing data freshness during S3 to table data copies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "table_type", "line_number": 255, "body": "def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Returns the type of the table, which is an empty string by default in this implementation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "queries", "line_number": 262, "body": "def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method provides an overridden interface intended to return a list of executable queries but currently returns an empty list, serving as a placeholder in the S3CopyToTable class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "truncate_table", "line_number": 268, "body": "def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Core method of S3CopyToTable that clears all data from the specified table in the connected database to prepare it for fresh data loading or synchronization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "prune", "line_number": 276, "body": "def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Method in S3CopyToTable that removes records from a specified table where a given column's value is on or after a cutoff date, helping maintain or clean data based on temporal criteria."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "create_schema", "line_number": 284, "body": "def create_schema(self, connection):\n        \"\"\"\n        Will create the schema in the database\n        \"\"\"\n        if '.' not in self.table:\n            return\n\n        query = 'CREATE SCHEMA IF NOT EXISTS {schema_name};'.format(schema_name=self.table.split('.')[0])\n        connection.cursor().execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Creates the database schema specified by the table name if it does not already exist, facilitating organized and structured data storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "create_table", "line_number": 294, "body": "def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ', ' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        elif len(self.columns[0]) == 3:\n            # if columns is specified as (name, type, encoding) tuples\n            # possible column encodings: https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n            coldefs = ','.join(\n                '{name} {type} ENCODE {encoding}'.format(\n                    name=name,\n                    type=type,\n                    encoding=encoding) for name, type, encoding in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ',' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Creates a database table with specified column definitions, types, and optional encodings using the given connection. It supports customized table schemas and constraints for managing data storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 360, "body": "def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        if self.enable_metadata_columns:\n            self.post_copy_metacolumns(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Core method of the S3CopyToTable class that manages copying data from an S3 path into a target table, including creating the table if missing, handling metadata, and committing the transaction. It ensures data ingestion with necessary setup and cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy", "line_number": 387, "body": "def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if self.columns and len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Core method of S3CopyToTable that executes a Redshift COPY command to load data from an S3 file into a database table, supporting both key-based and role-based AWS credentials for authentication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 411, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Returns a RedshiftTarget object representing the destination table for data insertion, encapsulating connection and table details. This function provides a standardized way to specify where copied data should be stored in Redshift."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "does_schema_exist", "line_number": 425, "body": "def does_schema_exist(self, connection):\n        \"\"\"\n        Determine whether the schema already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as schema_exists \"\n                     \"from pg_namespace \"\n                     \"where nspname = lower(%s) limit 1\")\n        else:\n            return True\n\n        cursor = connection.cursor()\n        try:\n            schema = self.table.split('.')[0]\n            cursor.execute(query, [schema])\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Checks if the database schema for the specified table exists, aiding in validation before database operations. This ensures schema presence to avoid errors during data copying processes in S3CopyToTable workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "does_table_exist", "line_number": 446, "body": "def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Method of the S3CopyToTable class that checks if a specified database table exists by querying metadata tables, assisting in conditional operations that depend on the presence of the table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "init_copy", "line_number": 467, "body": "def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_schema_exist(connection):\n            logger.info(\"Creating schema for %s\", self.table)\n            self.create_schema(connection)\n\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            self.create_table(connection)\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Prepares a database table for data copying by ensuring schema and table existence, optionally adding metadata columns, truncating the table, or pruning old data based on configured settings. Useful for maintaining clean and ready tables before bulk data loads."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "post_copy", "line_number": 490, "body": "def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Provides a post-copy operation service that executes additional SQL queries for data cleansing or transferring data to production tables after initial copying. It supports finalizing data preparation in database workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "post_copy_metacolums", "line_number": 498, "body": "def post_copy_metacolums(self, cursor):\n        \"\"\"\n        Performs post-copy to fill metadata columns.\n        \"\"\"\n        logger.info('Executing post copy metadata queries')\n        for query in self.metadata_queries:\n            cursor.execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Method of S3CopyToTable that executes predefined metadata queries after data copy to populate metadata columns in the target table, ensuring metadata integrity and completeness following data transfer operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "jsonpath", "line_number": 532, "body": "def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyJSONToTable", "function_description": "Returns an overridden JSON path schema location for the table, defaulting to an empty string. This method allows customization of the JSON schema location in the S3CopyJSONToTable context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy_json_options", "line_number": 540, "body": "def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyJSONToTable", "function_description": "Returns additional copy options for JSON data transfer, such as compression methods; currently, it provides no extra options and serves as a placeholder or override point."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy", "line_number": 549, "body": "def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))", "is_method": true, "class_name": "S3CopyJSONToTable", "function_description": "Method of S3CopyJSONToTable that copies JSON data from an S3 location into a Redshift table using specified credentials and copy options, facilitating data import into Redshift from cloud storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 591, "body": "def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()", "is_method": true, "class_name": "RedshiftManifestTask", "function_description": "Generates a Redshift manifest file listing all files in specified S3 folders, marking each as mandatory for downstream Redshift COPY operations. This facilitates bulk data loading by creating the required manifest JSON."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "password", "line_number": 645, "body": "def password(self):\n        return None", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Returns None to indicate that no password is provided or required."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "update_id", "line_number": 649, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Returns a unique identifier associated with the current task instance, typically used to track or reference specific insert operations within the KillOpenRedshiftSessions context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 656, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Returns a RedshiftTarget object representing the dataset inserted by this class, encapsulating connection and table metadata for downstream processing or tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 671, "body": "def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Service method of KillOpenRedshiftSessions that terminates all active non-system Redshift sessions for a specific database, ensuring exclusive access or maintenance operations. It handles connection resets gracefully to maintain reliable operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 725, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )", "is_method": true, "class_name": "RedshiftQuery", "function_description": "Returns a RedshiftTarget encapsulating the connection and query execution details for this RedshiftQuery instance, facilitating downstream operations on the query's result."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "s3_unload_path", "line_number": 755, "body": "def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "This method returns the unload path for S3 storage, but currently returns an empty string, indicating no specific unload path is set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "unload_options", "line_number": 762, "body": "def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Returns a string of default options for unloading data from Redshift, specifying delimiters, compression, and overwrite behavior to standardize unload operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "unload_query", "line_number": 769, "body": "def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials '{credentials}' \"\n                \"{unload_options};\")", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Returns a default Redshift UNLOAD SQL command template for exporting query results to an S3 path with specified credentials and options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 777, "body": "def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            credentials=self._credentials())\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n\n        cursor = connection.cursor()\n        cursor.execute(unload_query)\n        logger.info(cursor.statusmessage)\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Executes a Redshift UNLOAD command to export query results to S3, then marks task completion by updating output state and managing the database connection lifecycle."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 799, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Returns a RedshiftTarget object representing the destination table and connection details for the executed Redshift query. This enables downstream tasks to reference the query output location consistently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "percentage_progress", "line_number": 47, "body": "def percentage_progress(self):\n        \"\"\"\n        :return: percentage of query overall progress\n        \"\"\"\n        return self._status.get('stats', {}).get('progressPercentage', 0.1)", "is_method": true, "class_name": "PrestoClient", "function_description": "Returns the current overall progress of a query as a percentage, providing a simple way to monitor query execution status within the PrestoClient."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "info_uri", "line_number": 54, "body": "def info_uri(self):\n        \"\"\"\n        :return: query UI link\n        \"\"\"\n        return self._status.get('infoUri')", "is_method": true, "class_name": "PrestoClient", "function_description": "Returns the user interface link for the current query status, allowing users or systems to access query details via a web interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "execute", "line_number": 60, "body": "def execute(self, query, parameters=None, mode=None):\n        \"\"\"\n\n        :param query: query to run\n        :param parameters: parameters should be injected in the query\n        :param mode: \"fetch\" - yields rows, \"watch\" - yields log entries\n        :return:\n        \"\"\"\n        class Mode(Enum):\n            watch = 'watch'\n            fetch = 'fetch'\n\n        _mode = Mode(mode) if mode else Mode.watch\n\n        with closing(self._connection.cursor()) as cursor:\n            cursor.execute(query, parameters)\n            status = self._status\n            while status:\n                sleep(self.sleep_time)\n                status = cursor.poll()\n                if status:\n                    if _mode == Mode.watch:\n                        yield status\n                    self._status = status\n\n            if _mode == Mode.fetch:\n                for row in cursor.fetchall():\n                    yield row", "is_method": true, "class_name": "PrestoClient", "function_description": "Provides a flexible interface to execute SQL queries on a Presto server, either fetching result rows or streaming query status logs based on the selected mode. Useful for running parameterized queries and handling asynchronous query execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "__new__", "line_number": 97, "body": "def __new__(cls, name, bases, attrs):\n        def _client(self):\n            def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val\n\n            connection = Connection(**dict(_kwargs()))\n            return PrestoClient(connection=connection)\n\n        attrs.update({\n            '_client': property(_client)\n        })\n        return super(cls, WithPrestoClient).__new__(cls, name, bases, attrs)", "is_method": true, "class_name": "WithPrestoClient", "function_description": "Adds a lazily evaluated PrestoClient property to classes, providing seamless access to a Presto connection based on the class instance\u2019s attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_count_query", "line_number": 135, "body": "def _count_query(self):\n        partition = OrderedDict(self.partition or {1: 1})\n\n        def _clauses():\n            for k in partition.keys():\n                yield '{} = %s'.format(k)\n\n        clauses = ' AND '.join(_clauses())\n\n        query = 'SELECT COUNT(*) AS cnt FROM {}.{}.{} WHERE {} LIMIT 1'.format(\n            self.catalog,\n            self.database,\n            self.table,\n            clauses\n        )\n        params = list(partition.values())\n        return query, params", "is_method": true, "class_name": "PrestoTarget", "function_description": "Generates a parameterized SQL count query based on table partitions to count matching records efficiently. It supports dynamic partition filtering for use in data validation or metadata retrieval tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_table_doesnot_exist", "line_number": 153, "body": "def _table_doesnot_exist(self, exception):\n        pattern = re.compile(\n            r'line (\\d+):(\\d+): Table {}.{}.{} does not exist'.format(\n                self.catalog,\n                self.database,\n                self.table\n            )\n        )\n        try:\n            message = exception.message['message']\n            if pattern.match(message):\n                return True\n        finally:\n            return False", "is_method": true, "class_name": "PrestoTarget", "function_description": "Checks whether a given exception indicates that a specific table in Presto does not exist, facilitating error handling related to missing tables in database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "count", "line_number": 168, "body": "def count(self):\n        if not self._count:\n            '''\n            replace to\n            self._count, *_ = next(self._client.execute(*self.count_query, 'fetch'))\n            after py2 deprecation\n            '''\n            self._count = next(self._client.execute(*self._count_query, mode='fetch'))[0]\n        return self._count", "is_method": true, "class_name": "PrestoTarget", "function_description": "Core method of the PrestoTarget class that retrieves and caches the total count of items from a query result, providing an efficient way to access the record count without repeated database queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "exists", "line_number": 178, "body": "def exists(self):\n        \"\"\"\n\n        :return: `True` if given table exists and there are any rows in a given partition\n                 `False` if no rows in the partition exists or table is absent\n        \"\"\"\n        try:\n            return self.count() > 0\n        except DatabaseError as exception:\n            if self._table_doesnot_exist(exception):\n                return False\n        except Exception:\n            raise", "is_method": true, "class_name": "PrestoTarget", "function_description": "Checks whether a specified table exists and contains any rows in a given partition, returning True if it does and False otherwise. This function helps verify data availability before performing further operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "host", "line_number": 201, "body": "def host(self):\n        return presto().host", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the host address configured for the Presto service, providing other functions access to this connection detail."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "port", "line_number": 205, "body": "def port(self):\n        return presto().port", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the port number used by the Presto service, providing other components with access details for network communication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "user", "line_number": 209, "body": "def user(self):\n        return presto().user", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the current user associated with the Presto database connection managed by the PrestoTask class. This provides easy access to identify the executing user context in Presto operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "username", "line_number": 213, "body": "def username(self):\n        return self.user", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the username associated with the PrestoTask instance, providing access to the user information linked to the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "schema", "line_number": 217, "body": "def schema(self):\n        return self.database", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the database schema associated with the PrestoTask, providing access to its underlying database structure or metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "password", "line_number": 221, "body": "def password(self):\n        return presto().password", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the current password used by the Presto connection, providing secure access credentials for authentication in PrestoTask operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "catalog", "line_number": 225, "body": "def catalog(self):\n        return presto().catalog", "is_method": true, "class_name": "PrestoTask", "function_description": "Utility method in PrestoTask that provides access to the Presto service's catalog, enabling other functions to retrieve metadata about available databases and tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "poll_interval", "line_number": 229, "body": "def poll_interval(self):\n        return presto().poll_interval", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the polling interval configuration from the Presto service, allowing tasks to determine how frequently they should check for updates or status changes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "source", "line_number": 233, "body": "def source(self):\n        return 'pyhive'", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns the source identifier string for the PrestoTask, indicating the data connector or library it uses. This helps other components recognize the task's data source type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "protocol", "line_number": 241, "body": "def protocol(self):\n        return 'https' if self.password else presto().protocol", "is_method": true, "class_name": "PrestoTask", "function_description": "Determines the network protocol to use, returning 'https' if a password exists; otherwise, it defaults to the Presto service's protocol setting. This supports secure connection configuration based on authentication presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "requests_kwargs", "line_number": 253, "body": "def requests_kwargs(self):\n        return {\n            'verify': False\n        }", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns a dictionary with default keyword arguments for HTTP requests, disabling SSL certificate verification. This supports making requests without SSL validation, useful for environments with self-signed certificates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_maybe_set_tracking_url", "line_number": 260, "body": "def _maybe_set_tracking_url(self):\n        if not self._tracking_url_set:\n            self.set_tracking_url(self._client.info_uri)\n            self._tracking_url_set = True", "is_method": true, "class_name": "PrestoTask", "function_description": "Ensures the tracking URL is set once by retrieving it from the client and marking it as set, facilitating task monitoring within the PrestoTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_set_progress", "line_number": 265, "body": "def _set_progress(self):\n        self.set_progress_percentage(self._client.percentage_progress)", "is_method": true, "class_name": "PrestoTask", "function_description": "Internal method of the PrestoTask class that updates the task's progress percentage based on the client's current progress state. It enables consistent progress tracking within task management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "run", "line_number": 268, "body": "def run(self):\n        for _ in self._client.execute(self.query):\n            self._maybe_set_tracking_url()\n            self._set_progress()", "is_method": true, "class_name": "PrestoTask", "function_description": "Core method of PrestoTask that executes a query using a client and updates tracking and progress status during execution, facilitating task monitoring and management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "output", "line_number": 273, "body": "def output(self):\n        return PrestoTarget(\n            client=self._client,\n            catalog=self.catalog,\n            database=self.database,\n            table=self.table,\n            partition=self.partition,\n        )", "is_method": true, "class_name": "PrestoTask", "function_description": "Returns a configured PrestoTarget object representing the query destination, encapsulating client and table location details for downstream processing or task chaining."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_client", "line_number": 98, "body": "def _client(self):\n            def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val\n\n            connection = Connection(**dict(_kwargs()))\n            return PrestoClient(connection=connection)", "is_method": true, "class_name": "WithPrestoClient", "function_description": "Provides a PrestoClient instance configured with connection parameters derived from the current object's attributes, facilitating interaction with a Presto database within the WithPrestoClient context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_clauses", "line_number": 138, "body": "def _clauses():\n            for k in partition.keys():\n                yield '{} = %s'.format(k)", "is_method": true, "class_name": "PrestoTarget", "function_description": "Generates SQL equality clauses for each key in a given partition dictionary, facilitating dynamic query construction in PrestoTarget operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_kwargs", "line_number": 99, "body": "def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val", "is_method": true, "class_name": "WithPrestoClient", "function_description": "Internal generator method of WithPrestoClient that yields non-empty instance attributes matching the parameters of Cursor's initializer, facilitating dynamic argument collection for cursor construction or configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_connect", "line_number": 66, "body": "def _connect(self):\n        \"\"\"\n        Log in to ftp.\n        \"\"\"\n        if self.sftp:\n            self._sftp_connect()\n        else:\n            self._ftp_connect()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method in RemoteFileSystem that establishes a connection to a remote server using either FTP or SFTP protocol based on configuration. It facilitates authenticated access needed for subsequent file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_connect", "line_number": 75, "body": "def _sftp_connect(self):\n        try:\n            import pysftp\n        except ImportError:\n            logger.warning('Please install pysftp to use SFTP.')\n\n        self.conn = pysftp.Connection(self.host, username=self.username, password=self.password,\n                                      port=self.port, **self.pysftp_conn_kwargs)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Internal method of RemoteFileSystem that establishes an SFTP connection using stored host and authentication details, enabling secure remote file operations via the pysftp library."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_connect", "line_number": 84, "body": "def _ftp_connect(self):\n        if self.tls:\n            self.conn = ftplib.FTP_TLS()\n        else:\n            self.conn = ftplib.FTP()\n        self.conn.connect(self.host, self.port, timeout=self.timeout)\n        self.conn.login(self.username, self.password)\n        if self.tls:\n            self.conn.prot_p()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method in RemoteFileSystem that establishes and authenticates an FTP or FTPS connection based on TLS settings, enabling secure or standard file transfer sessions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_close", "line_number": 94, "body": "def _close(self):\n        \"\"\"\n        Close ftp connection.\n        \"\"\"\n        if self.sftp:\n            self._sftp_close()\n        else:\n            self._ftp_close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Utility method in RemoteFileSystem that closes the current FTP or SFTP connection, ensuring proper termination of the remote file transfer session."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_close", "line_number": 103, "body": "def _sftp_close(self):\n        self.conn.close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method in RemoteFileSystem that closes the active SFTP connection, ensuring proper termination of the remote file session."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_close", "line_number": 106, "body": "def _ftp_close(self):\n        self.conn.quit()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method in RemoteFileSystem that closes the FTP connection by terminating the session. It ensures proper release of remote resources after file operations over FTP."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "exists", "line_number": 109, "body": "def exists(self, path, mtime=None):\n        \"\"\"\n        Return `True` if file or directory at `path` exist, False otherwise.\n\n        Additional check on modified time when mtime is passed in.\n\n        Return False if the file's modified time is older mtime.\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            exists = self._sftp_exists(path, mtime)\n        else:\n            exists = self._ftp_exists(path, mtime)\n\n        self._close()\n\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Method of RemoteFileSystem that checks if a file or directory exists at a given path, optionally verifying it is newer than a specified modification time. It supports existence checks over both SFTP and FTP connections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_exists", "line_number": 128, "body": "def _sftp_exists(self, path, mtime):\n        exists = False\n        if mtime:\n            exists = self.conn.stat(path).st_mtime > mtime\n        elif self.conn.exists(path):\n            exists = True\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Utility method of RemoteFileSystem that checks if a file exists at a given path and, optionally, if it has been modified more recently than a specified timestamp. It supports verification of file presence and freshness over SFTP connections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_exists", "line_number": 136, "body": "def _ftp_exists(self, path, mtime):\n        dirname, fn = os.path.split(path)\n\n        files = self.conn.nlst(dirname)\n\n        exists = False\n        if path in files or fn in files:\n            if mtime:\n                mdtm = self.conn.sendcmd('MDTM ' + path)\n                modified = datetime.datetime.strptime(mdtm[4:], \"%Y%m%d%H%M%S\")\n                exists = modified > mtime\n            else:\n                exists = True\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Checks whether a file exists on a remote FTP server and optionally verifies if it has been modified since a given timestamp. This function helps manage remote file synchronization by confirming presence and recency of files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "remove", "line_number": 151, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove file or directory at location ``path``.\n\n        :param path: a path within the FileSystem to remove.\n        :type path: str\n        :param recursive: if the path is a directory, recursively remove the directory and\n                          all of its descendants. Defaults to ``True``.\n        :type recursive: bool\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            self._sftp_remove(path, recursive)\n        else:\n            self._ftp_remove(path, recursive)\n\n        self._close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Core method of the RemoteFileSystem class that deletes a specified file or directory, optionally removing all nested contents recursively. It supports removal via different file transfer protocols."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_remove", "line_number": 170, "body": "def _sftp_remove(self, path, recursive):\n        if self.conn.isfile(path):\n            self.conn.unlink(path)\n        else:\n            if not recursive:\n                raise RuntimeError(\"Path is not a regular file, and recursive option is not set\")\n            directories = []\n            # walk the tree, and execute call backs when files,\n            # directories and unknown types are encountered\n            # files must be removed first.  then directories can be removed\n            # after the files are gone.\n            self.conn.walktree(path, self.conn.unlink, directories.append, self.conn.unlink)\n            for directory in reversed(directories):\n                self.conn.rmdir(directory)\n            self.conn.rmdir(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method in RemoteFileSystem that deletes a file or directory over SFTP, supporting recursive removal of directory contents when specified. It ensures safe cleanup of remote paths by handling files and directories appropriately."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_remove", "line_number": 186, "body": "def _ftp_remove(self, path, recursive):\n        if recursive:\n            self._rm_recursive(self.conn, path)\n        else:\n            try:\n                # try delete file\n                self.conn.delete(path)\n            except ftplib.all_errors:\n                # it is a folder, delete it\n                self.conn.rmd(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method of RemoteFileSystem that deletes a file or directory on an FTP server, supporting recursive removal of directories when specified. It handles both file and folder deletion seamlessly based on the target path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_rm_recursive", "line_number": 197, "body": "def _rm_recursive(self, ftp, path):\n        \"\"\"\n        Recursively delete a directory tree on a remote server.\n\n        Source: https://gist.github.com/artlogic/2632647\n        \"\"\"\n        wd = ftp.pwd()\n\n        # check if it is a file first, because some FTP servers don't return\n        # correctly on ftp.nlst(file)\n        try:\n            ftp.cwd(path)\n        except ftplib.all_errors:\n            # this is a file, we will just delete the file\n            ftp.delete(path)\n            return\n\n        try:\n            names = ftp.nlst()\n        except ftplib.all_errors:\n            # some FTP servers complain when you try and list non-existent paths\n            return\n\n        for name in names:\n            if os.path.split(name)[1] in ('.', '..'):\n                continue\n\n            try:\n                ftp.cwd(name)   # if we can cwd to it, it's a folder\n                ftp.cwd(wd)   # don't try a nuke a folder we're in\n                ftp.cwd(path)  # then go back to where we were\n                self._rm_recursive(ftp, name)\n            except ftplib.all_errors:\n                ftp.delete(name)\n\n        try:\n            ftp.cwd(wd)  # do not delete the folder that we are in\n            ftp.rmd(path)\n        except ftplib.all_errors as e:\n            print('_rm_recursive: Could not remove {0}: {1}'.format(path, e))", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Recursively deletes a directory and all its contents on a remote FTP server, handling files and folders to enable complete removal of remote directory trees."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "put", "line_number": 238, "body": "def put(self, local_path, path, atomic=True):\n        \"\"\"\n        Put file from local filesystem to (s)FTP.\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            self._sftp_put(local_path, path, atomic)\n        else:\n            self._ftp_put(local_path, path, atomic)\n\n        self._close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Uploads a file from the local filesystem to a remote FTP or SFTP server, optionally using atomic operation for safer transfers. It simplifies remote file storage by handling connection and transfer protocols transparently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_put", "line_number": 251, "body": "def _sftp_put(self, local_path, path, atomic):\n        normpath = os.path.normpath(path)\n        directory = os.path.dirname(normpath)\n        self.conn.makedirs(directory)\n\n        if atomic:\n            tmp_path = os.path.join(directory, 'luigi-tmp-{:09d}'.format(random.randrange(0, 1e10)))\n        else:\n            tmp_path = normpath\n\n        self.conn.put(local_path, tmp_path)\n\n        if atomic:\n            self.conn.rename(tmp_path, normpath)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method of RemoteFileSystem that uploads a local file to a remote location via SFTP, optionally using an atomic rename to ensure safe file replacement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_put", "line_number": 266, "body": "def _ftp_put(self, local_path, path, atomic):\n        normpath = os.path.normpath(path)\n        folder = os.path.dirname(normpath)\n\n        # create paths if do not exists\n        for subfolder in folder.split(os.sep):\n            if subfolder and subfolder not in self.conn.nlst():\n                self.conn.mkd(subfolder)\n\n            self.conn.cwd(subfolder)\n\n        # go back to ftp root folder\n        self.conn.cwd(\"/\")\n\n        # random file name\n        if atomic:\n            tmp_path = folder + os.sep + 'luigi-tmp-%09d' % random.randrange(0, 1e10)\n        else:\n            tmp_path = normpath\n\n        self.conn.storbinary('STOR %s' % tmp_path, open(local_path, 'rb'))\n\n        if atomic:\n            self.conn.rename(tmp_path, normpath)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Handles uploading a local file to a specified FTP path on a remote server, creating required directories and optionally performing an atomic (temporary file + rename) write to ensure safe file replacement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "get", "line_number": 291, "body": "def get(self, path, local_path):\n        \"\"\"\n        Download file from (s)FTP to local filesystem.\n        \"\"\"\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder and not os.path.exists(folder):\n            os.makedirs(folder)\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n\n        # download file\n        self._connect()\n\n        if self.sftp:\n            self._sftp_get(path, tmp_local_path)\n        else:\n            self._ftp_get(path, tmp_local_path)\n\n        self._close()\n\n        os.rename(tmp_local_path, local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Downloads a file from a remote FTP or SFTP server to a specified local path, ensuring the local directory exists and handling temporary file naming during transfer for safe download completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_get", "line_number": 314, "body": "def _sftp_get(self, path, tmp_local_path):\n        self.conn.get(path, tmp_local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Internal method of RemoteFileSystem that downloads a remote file to a specified local temporary path via SFTP."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_get", "line_number": 317, "body": "def _ftp_get(self, path, tmp_local_path):\n        self.conn.retrbinary('RETR %s' % path, open(tmp_local_path, 'wb').write)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Core private method of the RemoteFileSystem class that downloads a file from a remote FTP server to a specified local temporary path. It supports file retrieval via FTP protocol for use in file transfer operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "listdir", "line_number": 320, "body": "def listdir(self, path='.'):\n        \"\"\"\n        Gets an list of the contents of path in (s)FTP\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            contents = self._sftp_listdir(path)\n        else:\n            contents = self._ftp_listdir(path)\n\n        self._close()\n\n        return contents", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Retrieves the list of files and directories at a specified path from a remote FTP or SFTP server, abstracting protocol differences for seamless directory listing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_listdir", "line_number": 335, "body": "def _sftp_listdir(self, path):\n        return self.conn.listdir(remotepath=path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Internal helper method of RemoteFileSystem that lists files and directories at a given remote path via an SFTP connection. It supports remote directory navigation and file management operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_listdir", "line_number": 338, "body": "def _ftp_listdir(self, path):\n        return self.conn.nlst(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Private method of the RemoteFileSystem class that retrieves a list of filenames from a specified directory via FTP connection. It provides directory listing capability over FTP."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "move_to_final_destination", "line_number": 359, "body": "def move_to_final_destination(self):\n        self._fs.put(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicFtpFile", "function_description": "Moves a file from its temporary location to its designated final path within the filesystem, finalizing the file transfer or storage process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "fs", "line_number": 363, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "AtomicFtpFile", "function_description": "Returns the underlying filesystem object associated with this AtomicFtpFile instance, enabling access to filesystem operations related to the file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "fs", "line_number": 392, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "RemoteTarget", "function_description": "Returns the underlying filesystem object associated with the RemoteTarget instance, providing access to filesystem operations relevant to the remote target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "open", "line_number": 395, "body": "def open(self, mode):\n        \"\"\"\n        Open the FileSystem target.\n\n        This method returns a file-like object which can either be read from or written to depending\n        on the specified mode.\n\n        :param mode: the mode `r` opens the FileSystemTarget in read-only mode, whereas `w` will\n                     open the FileSystemTarget in write mode. Subclasses can implement\n                     additional options.\n        :type mode: str\n        \"\"\"\n        if mode == 'w':\n            return self.format.pipe_writer(AtomicFtpFile(self._fs, self.path))\n\n        elif mode == 'r':\n            temppath = '{}-luigi-tmp-{:09d}'.format(\n                self.path.lstrip('/'), random.randrange(0, 1e10)\n            )\n            try:\n                # store reference to the TemporaryDirectory because it will be removed on GC\n                self.__temp_dir = tempfile.TemporaryDirectory(\n                    prefix=\"luigi-contrib-ftp_\"\n                )\n            except AttributeError:\n                # TemporaryDirectory only available in Python3, use old behaviour in Python2\n                # this file will not be cleaned up automatically\n                self.__tmp_path = os.path.join(\n                    tempfile.gettempdir(), 'luigi-contrib-ftp', temppath\n                )\n            else:\n                self.__tmp_path = os.path.join(self.__temp_dir.name, temppath)\n\n            # download file to local\n            self._fs.get(self.path, self.__tmp_path)\n\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(io.FileIO(self.__tmp_path, 'r')))\n            )\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Provides read or write file-like access to a remote filesystem target by opening it in the specified mode, handling temporary local download for reads and streaming writes for uploads."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "exists", "line_number": 437, "body": "def exists(self):\n        return self.fs.exists(self.path, self.mtime)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Checks whether the target file exists at the specified path with an optional modification time constraint. This function helps determine the presence and freshness of remote files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "put", "line_number": 440, "body": "def put(self, local_path, atomic=True):\n        self.fs.put(local_path, self.path, atomic)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Core method of the RemoteTarget class that uploads a file from a local path to a remote location, optionally ensuring the operation is atomic to prevent partial updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "get", "line_number": 443, "body": "def get(self, local_path):\n        self.fs.get(self.path, local_path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Utility method in RemoteTarget that downloads or copies the content from the remote path to a specified local path, facilitating local access to remote data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_avro_uri", "line_number": 33, "body": "def _avro_uri(self, target):\n        path_or_uri = target.uri if hasattr(target, 'uri') else target.path\n        return path_or_uri if path_or_uri.endswith('.avro') else path_or_uri.rstrip('/') + '/*.avro'", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Helper method in BigQueryLoadAvro that constructs or normalizes an Avro file URI or path, ensuring it points to valid Avro files for data loading operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "source_uris", "line_number": 37, "body": "def source_uris(self):\n        return [self._avro_uri(x) for x in flatten(self.input())]", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Returns a list of Avro file URIs derived from the input sources, facilitating data loading or processing tasks within the BigQueryLoadAvro class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_get_input_schema", "line_number": 40, "body": "def _get_input_schema(self):\n        \"\"\"Arbitrarily picks an object in input and reads the Avro schema from it.\"\"\"\n        assert avro, 'avro module required'\n\n        input_target = flatten(self.input())[0]\n        input_fs = input_target.fs if hasattr(input_target, 'fs') else GCSClient()\n        input_uri = self.source_uris()[0]\n        if '*' in input_uri:\n            file_uris = list(input_fs.list_wildcard(input_uri))\n            if file_uris:\n                input_uri = file_uris[0]\n            else:\n                raise RuntimeError('No match for ' + input_uri)\n\n        schema = []\n        exception_reading_schema = []\n\n        def read_schema(fp):\n            # fp contains the file part downloaded thus far. We rely on that the DataFileReader\n            # initializes itself fine as soon as the file header with schema is downloaded, without\n            # requiring the remainder of the file...\n            try:\n                reader = avro.datafile.DataFileReader(fp, avro.io.DatumReader())\n                schema[:] = [BigQueryLoadAvro._get_writer_schema(reader.datum_reader)]\n            except Exception as e:\n                # Save but assume benign unless schema reading ultimately fails. The benign\n                # exception in case of insufficiently big downloaded file part seems to be:\n                # TypeError('ord() expected a character, but string of length 0 found',).\n                exception_reading_schema[:] = [e]\n                return False\n            return True\n\n        input_fs.download(input_uri, 64 * 1024, read_schema).close()\n        if not schema:\n            raise exception_reading_schema[0]\n        return schema[0]", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Extracts and returns the Avro schema from an Avro data file specified by the first source URI, handling wildcards and partial file downloads to reliably read the schema for data loading purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_get_writer_schema", "line_number": 78, "body": "def _get_writer_schema(datum_reader):\n        \"\"\"Python-version agnostic getter for datum_reader writer(s)_schema attribute\n\n        Parameters:\n        datum_reader (avro.io.DatumReader): DatumReader\n\n        Returns:\n        Returning correct attribute name depending on Python version.\n        \"\"\"\n        return datum_reader.writer_schema", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Private helper method in BigQueryLoadAvro that abstracts access to the writer schema attribute of an Avro DatumReader, ensuring compatibility across Python versions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_set_output_doc", "line_number": 89, "body": "def _set_output_doc(self, avro_schema):\n        bq_client = self.output().client.client\n        table = self.output().table\n\n        patch = {\n            'description': avro_schema.doc,\n        }\n\n        bq_client.tables().patch(projectId=table.project_id,\n                                 datasetId=table.dataset_id,\n                                 tableId=table.table_id,\n                                 body=patch).execute()", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Sets the description metadata of a BigQuery table using the provided Avro schema's documentation. This method updates the table's description to reflect schema information for better data context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "run", "line_number": 102, "body": "def run(self):\n        super(BigQueryLoadAvro, self).run()\n\n        # We propagate documentation in one fire-and-forget attempt; the output table is\n        # left to exist without documentation if this step raises an exception.\n        try:\n            self._set_output_doc(self._get_input_schema())\n        except Exception as e:\n            logger.warning('Could not propagate Avro doc to BigQuery table description: %r', e)", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Performs the base load operation and attempts to propagate Avro schema documentation to the BigQuery output table, issuing a warning if documentation propagation fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "read_schema", "line_number": 57, "body": "def read_schema(fp):\n            # fp contains the file part downloaded thus far. We rely on that the DataFileReader\n            # initializes itself fine as soon as the file header with schema is downloaded, without\n            # requiring the remainder of the file...\n            try:\n                reader = avro.datafile.DataFileReader(fp, avro.io.DatumReader())\n                schema[:] = [BigQueryLoadAvro._get_writer_schema(reader.datum_reader)]\n            except Exception as e:\n                # Save but assume benign unless schema reading ultimately fails. The benign\n                # exception in case of insufficiently big downloaded file part seems to be:\n                # TypeError('ord() expected a character, but string of length 0 found',).\n                exception_reading_schema[:] = [e]\n                return False\n            return True", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Reads and extracts the Avro schema from a partially downloaded file, enabling schema retrieval without requiring the full file download. It supports incremental file processing for efficient schema access in Avro data loading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_home", "line_number": 42, "body": "def pig_home(self):\n        return configuration.get_config().get('pig', 'home', '/usr/share/pig')", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns the configured home directory path for Pig, defaulting to '/usr/share/pig' if not set. This function provides access to Pig's installation location for related job tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_command_path", "line_number": 45, "body": "def pig_command_path(self):\n        return os.path.join(self.pig_home(), \"bin/pig\")", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns the filesystem path to the Pig executable, enabling other components to locate and invoke Pig commands within the PigJobTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_env_vars", "line_number": 48, "body": "def pig_env_vars(self):\n        \"\"\"\n        Dictionary of environment variables that should be set when running Pig.\n\n        Ex::\n            return { 'PIG_CLASSPATH': '/your/path' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns environment variables required to configure the Pig execution environment for a PigJobTask. This allows customization of runtime settings when running Pig jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_properties", "line_number": 57, "body": "def pig_properties(self):\n        \"\"\"\n        Dictionary of properties that should be set when running Pig.\n\n        Example::\n\n            return { 'pig.additional.jars':'/path/to/your/jar' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns a dictionary of configuration properties to customize the Pig execution environment for the job, allowing callers to specify additional settings like extra JAR files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_parameters", "line_number": 67, "body": "def pig_parameters(self):\n        \"\"\"\n        Dictionary of parameters that should be set for the Pig job.\n\n        Example::\n\n            return { 'YOUR_PARAM_NAME':'Your param value' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns a dictionary of configuration parameters for the Pig job, allowing customization of job execution settings in the PigJobTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_options", "line_number": 77, "body": "def pig_options(self):\n        \"\"\"\n        List of options that will be appended to the Pig command.\n\n        Example::\n\n            return ['-x', 'local']\n        \"\"\"\n        return []", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns a list of command-line options to customize Pig job execution. This allows configuring how the Pig job runs, such as setting execution mode or other parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "_build_pig_cmd", "line_number": 97, "body": "def _build_pig_cmd(self):\n        opts = self.pig_options()\n\n        def line(k, v):\n            return ('%s=%s%s' % (k, v, os.linesep)).encode('utf-8')\n\n        with tempfile.NamedTemporaryFile() as param_file, tempfile.NamedTemporaryFile() as prop_file:\n            if self.pig_parameters():\n                items = self.pig_parameters().items()\n                param_file.writelines(line(k, v) for (k, v) in items)\n                param_file.flush()\n                opts.append('-param_file')\n                opts.append(param_file.name)\n\n            if self.pig_properties():\n                items = self.pig_properties().items()\n                prop_file.writelines(line(k, v) for k, v in items)\n                prop_file.flush()\n                opts.append('-propertyFile')\n                opts.append(prop_file.name)\n\n            cmd = [self.pig_command_path()] + opts + [\"-f\", self.pig_script_path()]\n\n            logger.info(subprocess.list2cmdline(cmd))\n            yield cmd", "is_method": true, "class_name": "PigJobTask", "function_description": "Constructs and yields the complete command line for executing a Pig script, including options, parameters, and properties managed via temporary files. Useful for preparing and running Pig jobs with dynamic configuration in a controlled environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "run", "line_number": 123, "body": "def run(self):\n        with self._build_pig_cmd() as cmd:\n            self.track_and_progress(cmd)", "is_method": true, "class_name": "PigJobTask", "function_description": "Core method of the PigJobTask class that executes a Pig command and tracks its progress, enabling automated execution and monitoring of Pig jobs within a workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "track_and_progress", "line_number": 127, "body": "def track_and_progress(self, cmd):\n        temp_stdout = tempfile.TemporaryFile('wb')\n        env = os.environ.copy()\n        env['PIG_HOME'] = self.pig_home()\n        for k, v in self.pig_env_vars().items():\n            env[k] = v\n\n        proc = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        reads = [proc.stderr.fileno(), proc.stdout.fileno()]\n        # tracking the possible problems with this job\n        err_lines = []\n        with PigRunContext():\n            while proc.poll() is None:\n                ret = select.select(reads, [], [])\n                for fd in ret[0]:\n                    if fd == proc.stderr.fileno():\n                        line = proc.stderr.readline().decode('utf8')\n                        err_lines.append(line)\n                    if fd == proc.stdout.fileno():\n                        line_bytes = proc.stdout.readline()\n                        temp_stdout.write(line_bytes)\n                        line = line_bytes.decode('utf8')\n\n                err_line = line.lower()\n                if err_line.find('More information at:') != -1:\n                    logger.info(err_line.split('more information at: ')[-1].strip())\n                if err_line.find(' - '):\n                    t = err_line.split(' - ')[-1].strip()\n                    if t != \"\":\n                        logger.info(t)\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line.decode('utf8') for an_err_line in proc.stderr])\n        if proc.returncode == 0:\n            logger.info(\"Job completed successfully!\")\n        else:\n            logger.error(\"Error when running script:\\n%s\", self.pig_script_path())\n            logger.error(err)\n            raise PigJobError(\"Pig script failed with return value: %s\" % (proc.returncode,), err=err)", "is_method": true, "class_name": "PigJobTask", "function_description": "Core method of PigJobTask that runs a Pig script subprocess, streams its output for progress tracking, logs important messages, and raises errors if the job fails. It enables real-time monitoring and error handling of Pig jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__enter__", "line_number": 172, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "PigRunContext", "function_description": "Enables the PigRunContext to set up a custom SIGTERM signal handler upon entering a runtime context, allowing controlled job termination within a with-statement block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "kill_job", "line_number": 177, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        if self.job_id:\n            logger.info('Job interrupted, killing job %s', self.job_id)\n            subprocess.call(['pig', '-e', '\"kill %s\"' % self.job_id])\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "PigRunContext", "function_description": "Terminates the currently running Pig job associated with the context and exits the program with an appropriate signal-based status code if provided. This enables controlled job interruption and cleanup in PigRunContext workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__exit__", "line_number": 185, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "PigRunContext", "function_description": "Handles cleanup on context exit by terminating the job on a keyboard interrupt and restoring the original SIGTERM signal handler. It ensures graceful shutdown within the PigRunContext."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__str__", "line_number": 198, "body": "def __str__(self):\n        info = self.message\n        if self.out:\n            info += \"\\nSTDOUT: \" + str(self.out)\n        if self.err:\n            info += \"\\nSTDERR: \" + str(self.err)\n        return info", "is_method": true, "class_name": "PigJobError", "function_description": "Returns a detailed string representation of the PigJobError instance, including its message and any captured standard output or error, to aid in error reporting and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "line", "line_number": 100, "body": "def line(k, v):\n            return ('%s=%s%s' % (k, v, os.linesep)).encode('utf-8')", "is_method": true, "class_name": "PigJobTask", "function_description": "Helper function that formats a key-value pair as a UTF-8 encoded string with a line separator, useful for preparing data output in PigJobTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "get_path", "line_number": 82, "body": "def get_path(self):\n        \"\"\"\n        Returns a temporary file path based on a MD5 hash generated with the task's name and its arguments\n        \"\"\"\n        md5_hash = hashlib.md5(self.task_id.encode()).hexdigest()\n        logger.debug('Hash %s corresponds to task %s', md5_hash, self.task_id)\n\n        return os.path.join(self.temp_dir, str(self.unique.value), md5_hash)", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Returns a unique temporary file path derived from an MD5 hash of the task identifier, supporting consistent and isolated storage for task-related data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "exists", "line_number": 91, "body": "def exists(self):\n        \"\"\"\n        Checks if the file exists\n        \"\"\"\n        return os.path.isfile(self.get_path())", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Utility method in RunAnywayTarget that checks for the existence of the target's associated file, enabling conditional operations based on file presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "done", "line_number": 97, "body": "def done(self):\n        \"\"\"\n        Creates temporary file to mark the task as `done`\n        \"\"\"\n        logger.info('Marking %s as done', self)\n\n        fn = self.get_path()\n        try:\n            os.makedirs(os.path.dirname(fn))\n        except OSError:\n            pass\n        open(fn, 'w').close()", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Marks the RunAnywayTarget task as completed by creating a temporary file as a completion indicator. This enables tracking or signaling task completion status within workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "open", "line_number": 47, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            return self.format.pipe_reader(\n                ReadableWebHdfsFile(path=self.path, client=self.fs)\n            )\n\n        return self.format.pipe_writer(\n            AtomicWebHdfsFile(path=self.path, client=self.fs)\n        )", "is_method": true, "class_name": "WebHdfsTarget", "function_description": "Provides a file-like interface to read from or write to a WebHDFS path, returning a formatted reader or writer stream based on the specified mode. Useful for interacting with WebHDFS files in a structured, format-aware manner."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "read", "line_number": 68, "body": "def read(self):\n        self.generator = self.client.read(self.path)\n        res = list(self.generator)[0]\n        return res", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Utility method of ReadableWebHdfsFile that reads and returns the first chunk of data from a file on WebHDFS using the associated client connection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "readlines", "line_number": 73, "body": "def readlines(self, char='\\n'):\n        self.generator = self.client.read(self.path, buffer_char=char)\n        return self.generator", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Provides an iterator that reads the file line-by-line using a specified character as the delimiter, enabling efficient streaming of file contents from WebHDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__enter__", "line_number": 77, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Enables the ReadableWebHdfsFile object to be used as a context manager, supporting the with statement for resource management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__exit__", "line_number": 80, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Supports the context management protocol by ensuring the file is properly closed when exiting a with statement block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__iter__", "line_number": 83, "body": "def __iter__(self):\n        self.generator = self.readlines('\\n')\n        yield from self.generator\n        self.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Core iterator method of ReadableWebHdfsFile that enables line-by-line reading of the file, allowing iteration over its contents and ensuring the file is properly closed afterward."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "close", "line_number": 88, "body": "def close(self):\n        self.generator.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Closes the underlying data generator associated with the ReadableWebHdfsFile, ensuring proper release of resources held during file reading operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "move_to_final_destination", "line_number": 101, "body": "def move_to_final_destination(self):\n        if not self.client.exists(self.path):\n            self.client.upload(self.path, self.tmp_path)", "is_method": true, "class_name": "AtomicWebHdfsFile", "function_description": "Ensures a file is permanently stored by uploading it from a temporary location if it doesn't already exist at the final destination. This supports atomic file operations in a web HDFS environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "image", "line_number": 69, "body": "def image(self):\n        return 'alpine'", "is_method": true, "class_name": "DockerTask", "function_description": "Returns the default Docker image name used by the DockerTask, providing a standard base environment for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "command", "line_number": 73, "body": "def command(self):\n        return \"echo hello world\"", "is_method": true, "class_name": "DockerTask", "function_description": "Returns a fixed command string \"echo hello world\" typically used for testing or placeholder purposes in DockerTask contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "environment", "line_number": 85, "body": "def environment(self):\n        return {}", "is_method": true, "class_name": "DockerTask", "function_description": "Returns an empty dictionary representing the environment variables for a DockerTask instance. This placeholder method can be overridden to specify task-specific environment settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "container_tmp_dir", "line_number": 89, "body": "def container_tmp_dir(self):\n        return '/tmp/luigi'", "is_method": true, "class_name": "DockerTask", "function_description": "Returns the default temporary directory path used by DockerTask containers for storing intermediate files or data during task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "binds", "line_number": 93, "body": "def binds(self):\n        '''\n        Override this to mount local volumes, in addition to the /tmp/luigi\n        which gets defined by default. This should return a list of strings.\n        e.g. ['/hostpath1:/containerpath1', '/hostpath2:/containerpath2']\n        '''\n        return None", "is_method": true, "class_name": "DockerTask", "function_description": "Returns a list of local volume bindings to mount into a Docker container, allowing customization of mounted directories alongside the default temporary volume."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "network_mode", "line_number": 102, "body": "def network_mode(self):\n        return ''", "is_method": true, "class_name": "DockerTask", "function_description": "Returns the network mode setting for the DockerTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "auto_remove", "line_number": 110, "body": "def auto_remove(self):\n        return True", "is_method": true, "class_name": "DockerTask", "function_description": "Simple accessor method in DockerTask indicating that the task should auto-remove its container after execution to prevent leftover resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "force_pull", "line_number": 114, "body": "def force_pull(self):\n        return False", "is_method": true, "class_name": "DockerTask", "function_description": "Returns a fixed value indicating that force pulling of Docker images is disabled or not supported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "run", "line_number": 169, "body": "def run(self):\n\n        # get image if missing\n        if self.force_pull or len(self._client.images(name=self._image)) == 0:\n            logger.info('Pulling docker image ' + self._image)\n            try:\n                for logline in self._client.pull(self._image, stream=True):\n                    logger.debug(logline.decode('utf-8'))\n            except APIError as e:\n                self.__logger.warning(\"Error in Docker API: \" + e.explanation)\n                raise\n\n        # remove clashing container if a container with the same name exists\n        if self.auto_remove and self.name:\n            try:\n                self._client.remove_container(self.name,\n                                              force=True)\n            except APIError as e:\n                self.__logger.warning(\"Ignored error in Docker API: \" + e.explanation)\n\n        # run the container\n        try:\n            logger.debug('Creating image: %s command: %s volumes: %s'\n                         % (self._image, self.command, self._binds))\n\n            host_config = self._client.create_host_config(binds=self._binds,\n                                                          network_mode=self.network_mode)\n\n            container = self._client.create_container(self._image,\n                                                      command=self.command,\n                                                      name=self.name,\n                                                      environment=self.environment,\n                                                      volumes=self._volumes,\n                                                      host_config=host_config,\n                                                      **self.container_options)\n            self._client.start(container['Id'])\n\n            exit_status = self._client.wait(container['Id'])\n            # docker-py>=3.0.0 returns a dict instead of the status code directly\n            if type(exit_status) is dict:\n                exit_status = exit_status['StatusCode']\n\n            if exit_status != 0:\n                stdout = False\n                stderr = True\n                error = self._client.logs(container['Id'],\n                                          stdout=stdout,\n                                          stderr=stderr)\n            if self.auto_remove:\n                try:\n                    self._client.remove_container(container['Id'])\n                except docker.errors.APIError:\n                    self.__logger.warning(\"Container \" + container['Id'] +\n                                          \" could not be removed\")\n            if exit_status != 0:\n                raise ContainerError(container, exit_status, self.command, self._image, error)\n\n        except ContainerError as e:\n            # catch non zero exti status and return it\n            container_name = ''\n            if self.name:\n                container_name = self.name\n            try:\n                message = e.message\n            except AttributeError:\n                message = str(e)\n            self.__logger.error(\"Container \" + container_name +\n                                \" exited with non zero code: \" + message)\n            raise\n        except ImageNotFound:\n            self.__logger.error(\"Image \" + self._image + \" not found\")\n            raise\n        except APIError as e:\n            self.__logger.error(\"Error in Docker API: \"+e.explanation)\n            raise\n\n        # delete temp dir\n        filesys = LocalFileSystem()\n        if self.mount_tmp and filesys.exists(self._host_tmp_dir):\n            filesys.remove(self._host_tmp_dir, recursive=True)", "is_method": true, "class_name": "DockerTask", "function_description": "Runs a Docker container by ensuring the image is available, removing conflicting containers, executing the container with specified settings, and handling errors and cleanup. This method automates container lifecycle management for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "attach", "line_number": 75, "body": "def attach(*packages):\n    \"\"\"\n    Attach a python package to hadoop map reduce tarballs to make those packages available\n    on the hadoop cluster.\n    \"\"\"\n    _attached_packages.extend(packages)", "is_method": false, "function_description": "Function that adds specified Python packages to Hadoop MapReduce tarballs, making these packages accessible across the Hadoop cluster during job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "dereference", "line_number": 83, "body": "def dereference(f):\n    if os.path.islink(f):\n        # by joining with the dirname we are certain to get the absolute path\n        return dereference(os.path.join(os.path.dirname(f), os.readlink(f)))\n    else:\n        return f", "is_method": false, "function_description": "Utility function that resolves symbolic links recursively to return the ultimate target file path, ensuring the absolute path is retrieved regardless of link nesting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "get_extra_files", "line_number": 91, "body": "def get_extra_files(extra_files):\n    result = []\n    for f in extra_files:\n        if isinstance(f, str):\n            src, dst = f, os.path.basename(f)\n        elif isinstance(f, tuple):\n            src, dst = f\n        else:\n            raise Exception()\n\n        if os.path.isdir(src):\n            src_prefix = os.path.join(src, '')\n            for base, dirs, files in os.walk(src):\n                for f in files:\n                    f_src = os.path.join(base, f)\n                    f_src_stripped = f_src[len(src_prefix):]\n                    f_dst = os.path.join(dst, f_src_stripped)\n                    result.append((f_src, f_dst))\n        else:\n            result.append((src, dst))\n\n    return result", "is_method": false, "function_description": "Function that processes a list of file paths or (source, destination) pairs, expanding directories into individual files with relative paths, returning a complete list of source-destination file mappings for packaging or copying tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "create_packages_archive", "line_number": 115, "body": "def create_packages_archive(packages, filename):\n    \"\"\"\n    Create a tar archive which will contain the files for the packages listed in packages.\n    \"\"\"\n    import tarfile\n    tar = tarfile.open(filename, \"w\")\n\n    def add(src, dst):\n        logger.debug('adding to tar: %s -> %s', src, dst)\n        tar.add(src, dst)\n\n    def add_files_for_package(sub_package_path, root_package_path, root_package_name):\n        for root, dirs, files in os.walk(sub_package_path):\n            if '.svn' in dirs:\n                dirs.remove('.svn')\n            for f in files:\n                if not f.endswith(\".pyc\") and not f.startswith(\".\"):\n                    add(dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f)\n\n    for package in packages:\n        # Put a submodule's entire package in the archive. This is the\n        # magic that usually packages everything you need without\n        # having to attach packages/modules explicitly\n        if not getattr(package, \"__path__\", None) and '.' in package.__name__:\n            package = __import__(package.__name__.rpartition('.')[0], None, None, 'non_empty')\n\n        n = package.__name__.replace(\".\", \"/\")\n\n        if getattr(package, \"__path__\", None):\n            # TODO: (BUG) picking only the first path does not\n            # properly deal with namespaced packages in different\n            # directories\n            p = package.__path__[0]\n\n            if p.endswith('.egg') and os.path.isfile(p):\n                raise 'egg files not supported!!!'\n                # Add the entire egg file\n                # p = p[:p.find('.egg') + 4]\n                # add(dereference(p), os.path.basename(p))\n\n            else:\n                # include __init__ files from parent projects\n                root = []\n                for parent in package.__name__.split('.')[0:-1]:\n                    root.append(parent)\n                    module_name = '.'.join(root)\n                    directory = '/'.join(root)\n\n                    add(dereference(__import__(module_name, None, None, 'non_empty').__path__[0] + \"/__init__.py\"),\n                        directory + \"/__init__.py\")\n\n                add_files_for_package(p, p, n)\n\n                # include egg-info directories that are parallel:\n                for egg_info_path in glob.glob(p + '*.egg-info'):\n                    logger.debug(\n                        'Adding package metadata to archive for \"%s\" found at \"%s\"',\n                        package.__name__,\n                        egg_info_path\n                    )\n                    add_files_for_package(egg_info_path, p, n)\n\n        else:\n            f = package.__file__\n            if f.endswith(\"pyc\"):\n                f = f[:-3] + \"py\"\n            if n.find(\".\") == -1:\n                add(dereference(f), os.path.basename(f))\n            else:\n                add(dereference(f), n + \".py\")\n    tar.close()", "is_method": false, "function_description": "Function that creates a tar archive containing all source files and metadata for given Python packages, facilitating distribution or deployment of package code and related resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "flatten", "line_number": 188, "body": "def flatten(sequence):\n    \"\"\"\n    A simple generator which flattens a sequence.\n\n    Only one level is flattened.\n\n    .. code-block:: python\n\n        (1, (2, 3), 4) -> (1, 2, 3, 4)\n\n    \"\"\"\n    for item in sequence:\n        if hasattr(item, \"__iter__\") and not isinstance(item, str) and not isinstance(item, bytes):\n            for i in item:\n                yield i\n        else:\n            yield item", "is_method": false, "function_description": "Function that produces a flattened version of a sequence by expanding any nested iterable elements one level deep, useful for simplifying mixed nested structures without full recursion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_and_track_hadoop_job", "line_number": 247, "body": "def run_and_track_hadoop_job(arglist, tracking_url_callback=None, env=None):\n    \"\"\"\n    Runs the job by invoking the command from the given arglist.\n    Finds tracking urls from the output and attempts to fetch errors using those urls if the job fails.\n    Throws HadoopJobError with information about the error\n    (including stdout and stderr from the process)\n    on failure and returns normally otherwise.\n\n    :param arglist:\n    :param tracking_url_callback:\n    :param env:\n    :return:\n    \"\"\"\n    logger.info('%s', subprocess.list2cmdline(arglist))\n\n    def write_luigi_history(arglist, history):\n        \"\"\"\n        Writes history to a file in the job's output directory in JSON format.\n        Currently just for tracking the job ID in a configuration where\n        no history is stored in the output directory by Hadoop.\n        \"\"\"\n        history_filename = configuration.get_config().get('core', 'history-filename', '')\n        if history_filename and '-output' in arglist:\n            output_dir = arglist[arglist.index('-output') + 1]\n            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')\n            f.write(json.dumps(history))\n            f.close()\n\n    def track_process(arglist, tracking_url_callback, env=None):\n        # Dump stdout to a temp file, poll stderr and log it\n        temp_stdout = tempfile.TemporaryFile('w+t')\n        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)\n\n        # We parse the output to try to find the tracking URL.\n        # This URL is useful for fetching the logs of the job.\n        tracking_url = None\n        job_id = None\n        application_id = None\n        err_lines = []\n\n        with HadoopRunContext() as hadoop_context:\n            while proc.poll() is None:\n                err_line = proc.stderr.readline()\n                err_lines.append(err_line)\n                err_line = err_line.strip()\n                if err_line:\n                    logger.info('%s', err_line)\n                err_line = err_line.lower()\n                tracking_url_match = TRACKING_RE.search(err_line)\n                if tracking_url_match:\n                    tracking_url = tracking_url_match.group('url')\n                    try:\n                        tracking_url_callback(tracking_url)\n                    except Exception as e:\n                        logger.error(\"Error in tracking_url_callback, disabling! %s\", e)\n\n                        def tracking_url_callback(x):\n                            return None\n                if err_line.find('running job') != -1:\n                    # hadoop jar output\n                    job_id = err_line.split('running job: ')[-1]\n                if err_line.find('submitted hadoop job:') != -1:\n                    # scalding output\n                    job_id = err_line.split('submitted hadoop job: ')[-1]\n                if err_line.find('submitted application ') != -1:\n                    application_id = err_line.split('submitted application ')[-1]\n                hadoop_context.job_id = job_id\n                hadoop_context.application_id = application_id\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])\n        temp_stdout.seek(0)\n        out = ''.join(temp_stdout.readlines())\n\n        if proc.returncode == 0:\n            write_luigi_history(arglist, {'job_id': job_id})\n            return (out, err)\n\n        # Try to fetch error logs if possible\n        message = 'Streaming job failed with exit code %d. ' % proc.returncode\n        if not tracking_url:\n            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)\n\n        try:\n            task_failures = fetch_task_failures(tracking_url)\n        except Exception as e:\n            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %\n                                 (tracking_url, e), out, err)\n\n        if not task_failures:\n            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)\n        else:\n            raise HadoopJobError(message + 'Output from tasks below:\\n%s' % task_failures, out, err)\n\n    if tracking_url_callback is None:\n        def tracking_url_callback(x): return None\n\n    return track_process(arglist, tracking_url_callback, env)", "is_method": false, "function_description": "Function that executes a Hadoop job command, monitors its output for tracking URLs, and retrieves detailed error information on failure, facilitating job tracking and robust error reporting. It supports callbacks for tracking URL updates and logs job history upon success."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "fetch_task_failures", "line_number": 347, "body": "def fetch_task_failures(tracking_url):\n    \"\"\"\n    Uses mechanize to fetch the actual task logs from the task tracker.\n\n    This is highly opportunistic, and we might not succeed.\n    So we set a low timeout and hope it works.\n    If it does not, it's not the end of the world.\n\n    TODO: Yarn has a REST API that we should probably use instead:\n    http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html\n    \"\"\"\n    import mechanize\n    timeout = 3.0\n    failures_url = tracking_url.replace('jobdetails.jsp', 'jobfailures.jsp') + '&cause=failed'\n    logger.debug('Fetching data from %s', failures_url)\n    b = mechanize.Browser()\n    b.open(failures_url, timeout=timeout)\n    links = list(b.links(text_regex='Last 4KB'))  # For some reason text_regex='All' doesn't work... no idea why\n    links = random.sample(links, min(10, len(links)))  # Fetch a random subset of all failed tasks, so not to be biased towards the early fails\n    error_text = []\n    for link in links:\n        task_url = link.url.replace('&start=-4097', '&start=-100000')  # Increase the offset\n        logger.debug('Fetching data from %s', task_url)\n        b2 = mechanize.Browser()\n        try:\n            r = b2.open(task_url, timeout=timeout)\n            data = r.read()\n        except Exception as e:\n            logger.debug('Error fetching data from %s: %s', task_url, e)\n            continue\n        # Try to get the hex-encoded traceback back from the output\n        for exc in re.findall(r'luigi-exc-hex=[0-9a-f]+', data):\n            error_text.append('---------- %s:' % task_url)\n            error_text.append(exc.split('=')[-1].decode('hex'))\n\n    return '\\n'.join(error_text)", "is_method": false, "function_description": "Function that opportunistically fetches and aggregates recent failure logs from a task tracker URL, providing insights into task errors for debugging purposes despite potential retrieval limitations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add", "line_number": 122, "body": "def add(src, dst):\n        logger.debug('adding to tar: %s -> %s', src, dst)\n        tar.add(src, dst)", "is_method": false, "function_description": "Utility function that adds a source file or directory to a tar archive under a specified destination path, facilitating file packaging and archiving."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add_files_for_package", "line_number": 126, "body": "def add_files_for_package(sub_package_path, root_package_path, root_package_name):\n        for root, dirs, files in os.walk(sub_package_path):\n            if '.svn' in dirs:\n                dirs.remove('.svn')\n            for f in files:\n                if not f.endswith(\".pyc\") and not f.startswith(\".\"):\n                    add(dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f)", "is_method": false, "function_description": "Utility function that recursively adds non-hidden, non-compiled Python files from a sub-package to a package structure, mapping file paths relative to the root package for packaging or deployment purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__enter__", "line_number": 213, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Sets up a signal handler to intercept termination signals, enabling custom job termination behavior within a context. This allows safe resource cleanup or shutdown logic during Hadoop job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "kill_job", "line_number": 218, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        if self.application_id:\n            logger.info('Job interrupted, killing application %s' % self.application_id)\n            subprocess.call(['yarn', 'application', '-kill', self.application_id])\n        elif self.job_id:\n            logger.info('Job interrupted, killing job %s', self.job_id)\n            subprocess.call(['mapred', 'job', '-kill', self.job_id])\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Terminates the running Hadoop job or application identified by its ID, handling job interruption and ensuring proper shutdown with an optional exit code based on a captured system signal."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__exit__", "line_number": 229, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Handles cleanup when exiting the HadoopRunContext, terminating the job on keyboard interruption and restoring the original SIGTERM signal handler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__str__", "line_number": 243, "body": "def __str__(self):\n        return self.message", "is_method": true, "class_name": "HadoopJobError", "function_description": "Returns the error message associated with a HadoopJobError instance as its string representation. This facilitates readable error output for logging or user display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "write_luigi_history", "line_number": 262, "body": "def write_luigi_history(arglist, history):\n        \"\"\"\n        Writes history to a file in the job's output directory in JSON format.\n        Currently just for tracking the job ID in a configuration where\n        no history is stored in the output directory by Hadoop.\n        \"\"\"\n        history_filename = configuration.get_config().get('core', 'history-filename', '')\n        if history_filename and '-output' in arglist:\n            output_dir = arglist[arglist.index('-output') + 1]\n            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')\n            f.write(json.dumps(history))\n            f.close()", "is_method": false, "function_description": "Function that saves job execution history as a JSON file in the specified output directory, useful for tracking job IDs when no history is maintained by Hadoop."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "track_process", "line_number": 275, "body": "def track_process(arglist, tracking_url_callback, env=None):\n        # Dump stdout to a temp file, poll stderr and log it\n        temp_stdout = tempfile.TemporaryFile('w+t')\n        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)\n\n        # We parse the output to try to find the tracking URL.\n        # This URL is useful for fetching the logs of the job.\n        tracking_url = None\n        job_id = None\n        application_id = None\n        err_lines = []\n\n        with HadoopRunContext() as hadoop_context:\n            while proc.poll() is None:\n                err_line = proc.stderr.readline()\n                err_lines.append(err_line)\n                err_line = err_line.strip()\n                if err_line:\n                    logger.info('%s', err_line)\n                err_line = err_line.lower()\n                tracking_url_match = TRACKING_RE.search(err_line)\n                if tracking_url_match:\n                    tracking_url = tracking_url_match.group('url')\n                    try:\n                        tracking_url_callback(tracking_url)\n                    except Exception as e:\n                        logger.error(\"Error in tracking_url_callback, disabling! %s\", e)\n\n                        def tracking_url_callback(x):\n                            return None\n                if err_line.find('running job') != -1:\n                    # hadoop jar output\n                    job_id = err_line.split('running job: ')[-1]\n                if err_line.find('submitted hadoop job:') != -1:\n                    # scalding output\n                    job_id = err_line.split('submitted hadoop job: ')[-1]\n                if err_line.find('submitted application ') != -1:\n                    application_id = err_line.split('submitted application ')[-1]\n                hadoop_context.job_id = job_id\n                hadoop_context.application_id = application_id\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])\n        temp_stdout.seek(0)\n        out = ''.join(temp_stdout.readlines())\n\n        if proc.returncode == 0:\n            write_luigi_history(arglist, {'job_id': job_id})\n            return (out, err)\n\n        # Try to fetch error logs if possible\n        message = 'Streaming job failed with exit code %d. ' % proc.returncode\n        if not tracking_url:\n            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)\n\n        try:\n            task_failures = fetch_task_failures(tracking_url)\n        except Exception as e:\n            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %\n                                 (tracking_url, e), out, err)\n\n        if not task_failures:\n            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)\n        else:\n            raise HadoopJobError(message + 'Output from tasks below:\\n%s' % task_failures, out, err)", "is_method": false, "function_description": "Function providing a way to run and monitor an external process, capturing its output and error streams, extracting tracking information, and handling job failure details, particularly in Hadoop job execution contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_job", "line_number": 414, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        packages = [luigi] + self.modules + job.extra_modules() + list(_attached_packages)\n\n        # find the module containing the job\n        packages.append(__import__(job.__module__, None, None, 'dummy'))\n\n        # find the path to out runner.py\n        runner_path = mrrunner.__file__\n        # assume source is next to compiled\n        if runner_path.endswith(\"pyc\"):\n            runner_path = runner_path[:-3] + \"py\"\n\n        base_tmp_dir = configuration.get_config().get('core', 'tmp-dir', None)\n        if base_tmp_dir:\n            warnings.warn(\"The core.tmp-dir configuration item is\"\n                          \" deprecated, please use the TMPDIR\"\n                          \" environment variable if you wish\"\n                          \" to control where luigi.contrib.hadoop may\"\n                          \" create temporary files and directories.\")\n            self.tmp_dir = os.path.join(base_tmp_dir, 'hadoop_job_%016x' % random.getrandbits(64))\n            os.makedirs(self.tmp_dir)\n        else:\n            self.tmp_dir = tempfile.mkdtemp()\n\n        logger.debug(\"Tmp dir: %s\", self.tmp_dir)\n\n        # build arguments\n        config = configuration.get_config()\n        python_executable = config.get('hadoop', 'python-executable', 'python')\n        runner_arg = 'mrrunner.pex' if job.package_binary is not None else 'mrrunner.py'\n        command = '{0} {1} {{step}}'.format(python_executable, runner_arg)\n        map_cmd = command.format(step='map')\n        cmb_cmd = command.format(step='combiner')\n        red_cmd = command.format(step='reduce')\n\n        output_final = job.output().path\n        # atomic output: replace output with a temporary work directory\n        if self.end_job_with_atomic_move_dir:\n            illegal_targets = (\n                luigi.contrib.s3.S3FlagTarget, luigi.contrib.gcs.GCSFlagTarget)\n            if isinstance(job.output(), illegal_targets):\n                raise TypeError(\"end_job_with_atomic_move_dir is not supported\"\n                                \" for {}\".format(illegal_targets))\n            output_hadoop = '{output}-temp-{time}'.format(\n                output=output_final,\n                time=datetime.datetime.now().isoformat().replace(':', '-'))\n        else:\n            output_hadoop = output_final\n\n        arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', self.streaming_jar]\n\n        # 'libjars' is a generic option, so place it first\n        libjars = [libjar for libjar in self.libjars]\n\n        for libjar in self.libjars_in_hdfs:\n            run_cmd = luigi.contrib.hdfs.load_hadoop_cmd() + ['fs', '-get', libjar, self.tmp_dir]\n            logger.debug(subprocess.list2cmdline(run_cmd))\n            subprocess.call(run_cmd)\n            libjars.append(os.path.join(self.tmp_dir, os.path.basename(libjar)))\n\n        if libjars:\n            arglist += ['-libjars', ','.join(libjars)]\n\n        # 'archives' is also a generic option\n        archives = []\n        extra_archives = job.extra_archives()\n\n        if self.archives:\n            archives = self.archives\n\n        if extra_archives:\n            archives += extra_archives\n\n        if archives:\n            arglist += ['-archives', ','.join(archives)]\n\n        # Add static files and directories\n        extra_files = get_extra_files(job.extra_files())\n\n        files = []\n        for src, dst in extra_files:\n            dst_tmp = '%s_%09d' % (dst.replace('/', '_'), random.randint(0, 999999999))\n            files += ['%s#%s' % (src, dst_tmp)]\n            # -files doesn't support subdirectories, so we need to create the dst_tmp -> dst manually\n            job.add_link(dst_tmp, dst)\n\n        if files:\n            arglist += ['-files', ','.join(files)]\n\n        jobconfs = job.jobconfs()\n\n        for k, v in self.jobconfs.items():\n            jobconfs.append('%s=%s' % (k, v))\n\n        for conf in jobconfs:\n            arglist += ['-D', conf]\n\n        arglist += self.streaming_args\n\n        # Add additional non-generic  per-job streaming args\n        extra_streaming_args = job.extra_streaming_arguments()\n        for (arg, value) in extra_streaming_args:\n            if not arg.startswith('-'):  # safety first\n                arg = '-' + arg\n            arglist += [arg, value]\n\n        arglist += ['-mapper', map_cmd]\n\n        if job.combiner != NotImplemented:\n            arglist += ['-combiner', cmb_cmd]\n        if job.reducer != NotImplemented:\n            arglist += ['-reducer', red_cmd]\n        packages_fn = 'mrrunner.pex' if job.package_binary is not None else 'packages.tar'\n        files = [\n            runner_path if job.package_binary is None else None,\n            os.path.join(self.tmp_dir, packages_fn),\n            os.path.join(self.tmp_dir, 'job-instance.pickle'),\n        ]\n\n        for f in filter(None, files):\n            arglist += ['-file', f]\n\n        if self.output_format:\n            arglist += ['-outputformat', self.output_format]\n        if self.input_format:\n            arglist += ['-inputformat', self.input_format]\n\n        allowed_input_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3Target,\n            luigi.contrib.gcs.GCSTarget)\n        for target in luigi.task.flatten(job.input_hadoop()):\n            if not isinstance(target, allowed_input_targets):\n                raise TypeError('target must one of: {}'.format(\n                    allowed_input_targets))\n            arglist += ['-input', target.path]\n\n        allowed_output_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3FlagTarget,\n            luigi.contrib.gcs.GCSFlagTarget)\n        if not isinstance(job.output(), allowed_output_targets):\n            raise TypeError('output must be one of: {}'.format(\n                allowed_output_targets))\n        arglist += ['-output', output_hadoop]\n\n        # submit job\n        if job.package_binary is not None:\n            shutil.copy(job.package_binary, os.path.join(self.tmp_dir, 'mrrunner.pex'))\n        else:\n            create_packages_archive(packages, os.path.join(self.tmp_dir, 'packages.tar'))\n\n        job.dump(self.tmp_dir)\n\n        run_and_track_hadoop_job(arglist, tracking_url_callback=job.set_tracking_url)\n\n        if self.end_job_with_atomic_move_dir:\n            luigi.contrib.hdfs.HdfsTarget(output_hadoop).move_dir(output_final)\n        self.finish()", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Runs a Hadoop streaming job by preparing all necessary resources, configuring job parameters, and managing input/output targets, enabling automated execution and tracking of distributed data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "finish", "line_number": 578, "body": "def finish(self):\n        # FIXME: check for isdir?\n        if self.tmp_dir and os.path.exists(self.tmp_dir):\n            logger.debug('Removing directory %s', self.tmp_dir)\n            shutil.rmtree(self.tmp_dir)", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Clean up temporary directories used during a Hadoop job by safely removing them when the job finishes. This ensures no residual files remain, helping maintain a clean working environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__del__", "line_number": 584, "body": "def __del__(self):\n        self.finish()", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Destructor method in HadoopJobRunner that ensures proper cleanup by calling the finish() method when the object is destroyed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "sample", "line_number": 612, "body": "def sample(self, input_stream, n, output):\n        for i, line in enumerate(input_stream):\n            if n is not None and i >= n:\n                break\n            output.write(line)", "is_method": true, "class_name": "LocalJobRunner", "function_description": "This method reads up to n lines from an input stream and writes them to an output, enabling controlled sampling of streamed data for processing or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "group", "line_number": 618, "body": "def group(self, input_stream):\n        output = StringIO()\n        lines = []\n        for i, line in enumerate(input_stream):\n            parts = line.rstrip('\\n').split('\\t')\n            blob = md5(str(i).encode('ascii')).hexdigest()  # pseudo-random blob to make sure the input isn't sorted\n            lines.append((parts[:-1], blob, line))\n        for _, _, line in sorted(lines):\n            output.write(line)\n        output.seek(0)\n        return output", "is_method": true, "class_name": "LocalJobRunner", "function_description": "Reorders input lines in a pseudo-random but stable way to prevent sorted input bias, returning the reordered content as a stream. This supports job processing scenarios needing shuffled input order reproducibly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_job", "line_number": 630, "body": "def run_job(self, job):\n        map_input = StringIO()\n\n        for i in luigi.task.flatten(job.input_hadoop()):\n            self.sample(i.open('r'), self.samplelines, map_input)\n\n        map_input.seek(0)\n\n        if job.reducer == NotImplemented:\n            # Map only job; no combiner, no reducer\n            map_output = job.output().open('w')\n            job.run_mapper(map_input, map_output)\n            map_output.close()\n            return\n\n        # run job now...\n        map_output = StringIO()\n        job.run_mapper(map_input, map_output)\n        map_output.seek(0)\n\n        if job.combiner == NotImplemented:\n            reduce_input = self.group(map_output)\n        else:\n            combine_input = self.group(map_output)\n            combine_output = StringIO()\n            job.run_combiner(combine_input, combine_output)\n            combine_output.seek(0)\n            reduce_input = self.group(combine_output)\n\n        reduce_output = job.output().open('w')\n        job.run_reducer(reduce_input, reduce_output)\n        reduce_output.close()", "is_method": true, "class_name": "LocalJobRunner", "function_description": "Executes a Luigi job by processing input data through its mapper, optional combiner, and reducer stages, managing data flow and grouping to support local job execution within the LocalJobRunner context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_get_pool", "line_number": 679, "body": "def _get_pool(self):\n        \"\"\" Protected method \"\"\"\n        if self.pool:\n            return self.pool\n        if hadoop().pool:\n            return hadoop().pool", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Protected method of BaseHadoopJobTask that returns the current pool if set, or falls back to a default pool from the Hadoop configuration, facilitating resource allocation management in Hadoop jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "jobconfs", "line_number": 690, "body": "def jobconfs(self):\n        jcs = []\n        jcs.append('mapred.job.name=%s' % self)\n        if self.mr_priority != NotImplemented:\n            jcs.append('mapred.job.priority=%s' % self.mr_priority())\n        pool = self._get_pool()\n        if pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs.append('mapred.fairscheduler.pool=%s' % pool)\n            elif scheduler_type == 'capacity':\n                jcs.append('mapred.job.queue.name=%s' % pool)\n        return jcs", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Provides the Hadoop job configuration parameters including job name, priority, and scheduling pool, supporting different scheduler types for task management in a Hadoop job execution context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run", "line_number": 721, "body": "def run(self):\n        # The best solution is to store them as lazy `cached_property`, but it\n        # has extraneous dependency. And `property` is slow (need to be\n        # calculated every time when called), so we save them as attributes\n        # directly.\n        self.serialize = DataInterchange[self.data_interchange_format]['serialize']\n        self.internal_serialize = DataInterchange[self.data_interchange_format]['internal_serialize']\n        self.deserialize = DataInterchange[self.data_interchange_format]['deserialize']\n\n        self.init_local()\n        self.job_runner().run_job(self)", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Initializes data serialization methods based on the chosen format, prepares the local environment, and executes the associated Hadoop job. This method orchestrates the setup and execution workflow for a Hadoop job task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "requires_local", "line_number": 733, "body": "def requires_local(self):\n        \"\"\"\n        Default impl - override this method if you need any local input to be accessible in init().\n        \"\"\"\n        return []", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Returns an empty list by default, indicating no local input is required during initialization; designed to be overridden to specify required local resources for task setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "requires_hadoop", "line_number": 739, "body": "def requires_hadoop(self):\n        return self.requires()", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Returns whether the Hadoop job task requires execution based on its conditions. It provides a unified way to check if the Hadoop task should run."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "input_local", "line_number": 742, "body": "def input_local(self):\n        return luigi.task.getpaths(self.requires_local())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Returns local file paths required as inputs for the task by resolving dependencies using Luigi's local scheduling. This facilitates access to prerequisite data files in a Hadoop job workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "input_hadoop", "line_number": 745, "body": "def input_hadoop(self):\n        return luigi.task.getpaths(self.requires_hadoop())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Returns the input file paths required by the Hadoop job task, facilitating dependency resolution in job workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "deps", "line_number": 748, "body": "def deps(self):\n        # Overrides the default implementation\n        return luigi.task.flatten(self.requires_hadoop()) + luigi.task.flatten(self.requires_local())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Returns a combined list of all Hadoop and local task dependencies for the job, enabling proper task scheduling and execution order within the BaseHadoopJobTask workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "on_failure", "line_number": 752, "body": "def on_failure(self, exception):\n        if isinstance(exception, HadoopJobError):\n            return \"\"\"Hadoop job failed with message: {message}\n\n    stdout:\n    {stdout}\n\n\n    stderr:\n    {stderr}\n      \"\"\".format(message=exception.message, stdout=exception.out, stderr=exception.err)\n        else:\n            return super(BaseHadoopJobTask, self).on_failure(exception)", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Provides a standardized failure handler for Hadoop jobs, formatting detailed error messages including stdout and stderr. It ensures consistent error reporting or delegates handling for non-Hadoop job exceptions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "jobconfs", "line_number": 782, "body": "def jobconfs(self):\n        jcs = super(JobTask, self).jobconfs()\n        if self.reducer == NotImplemented:\n            jcs.append('mapred.reduce.tasks=0')\n        else:\n            jcs.append('mapred.reduce.tasks=%s' % self.n_reduce_tasks)\n        if self.jobconf_truncate >= 0:\n            jcs.append('stream.jobconf.truncate.limit=%i' % self.jobconf_truncate)\n        return jcs", "is_method": true, "class_name": "JobTask", "function_description": "Provides customized job configuration settings for a JobTask, adjusting reducer tasks and truncation limits to control job execution behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_setup_remote", "line_number": 801, "body": "def _setup_remote(self):\n        self._setup_links()", "is_method": true, "class_name": "JobTask", "function_description": "Private method in JobTask that initiates remote setup by establishing necessary links for the task's remote execution environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "job_runner", "line_number": 804, "body": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        \"\"\"\n        Get the MapReduce runner for this job.\n\n        If all outputs are HdfsTargets, the DefaultHadoopJobRunner will be used.\n        Otherwise, the LocalJobRunner which streams all data through the local machine\n        will be used (great for testing).\n        \"\"\"\n        outputs = luigi.task.flatten(self.output())\n        for output in outputs:\n            if not isinstance(output, luigi.contrib.hdfs.HdfsTarget):\n                warnings.warn(\"Job is using one or more non-HdfsTarget outputs\" +\n                              \" so it will be run in local mode\")\n                return LocalJobRunner()\n        else:\n            return DefaultHadoopJobRunner()", "is_method": true, "class_name": "JobTask", "function_description": "Determines and returns the appropriate MapReduce job runner based on output target types, defaulting to Hadoop for HDFS outputs or local mode otherwise, facilitating flexible execution environments for job tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "reader", "line_number": 822, "body": "def reader(self, input_stream):\n        \"\"\"\n        Reader is a method which iterates over input lines and outputs records.\n\n        The default implementation yields one argument containing the line for each line in the input.\"\"\"\n        for line in input_stream:\n            yield line,", "is_method": true, "class_name": "JobTask", "function_description": "Provides an iterator over input lines that yields each line as a single-element tuple, facilitating standardized record processing in the JobTask workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "writer", "line_number": 830, "body": "def writer(self, outputs, stdout, stderr=sys.stderr):\n        \"\"\"\n        Writer format is a method which iterates over the output records\n        from the reducer and formats them for output.\n\n        The default implementation outputs tab separated items.\n        \"\"\"\n        for output in outputs:\n            try:\n                output = flatten(output)\n                if self.data_interchange_format == \"json\":\n                    # Only dump one json string, and skip another one, maybe key or value.\n                    output = filter(lambda x: x, output)\n                else:\n                    # JSON is already serialized, so we put `self.serialize` in a else statement.\n                    output = map(self.serialize, output)\n                print(\"\\t\".join(output), file=stdout)\n            except BaseException:\n                print(output, file=stderr)\n                raise", "is_method": true, "class_name": "JobTask", "function_description": "Formats and outputs processed records from a job task, converting and printing them as tab-separated values or JSON based on the specified data format. It enables consistent serialization of reducer outputs for downstream consumption or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "mapper", "line_number": 851, "body": "def mapper(self, item):\n        \"\"\"\n        Re-define to process an input item (usually a line of input data).\n\n        Defaults to identity mapper that sends all lines to the same reducer.\n        \"\"\"\n        yield None, item", "is_method": true, "class_name": "JobTask", "function_description": "Core method of the JobTask class that processes input items for mapping operations, serving as a customizable point to transform or route data before reduction. Defaults to passing items unchanged to a single reducer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "incr_counter", "line_number": 861, "body": "def incr_counter(self, *args, **kwargs):\n        \"\"\"\n        Increments a Hadoop counter.\n\n        Since counters can be a bit slow to update, this batches the updates.\n        \"\"\"\n        threshold = kwargs.get(\"threshold\", self.batch_counter_default)\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            key = (group_name,)\n        else:\n            group, name, count = args\n            key = (group, name)\n\n        ct = self._counter_dict.get(key, 0)\n        ct += count\n        if ct >= threshold:\n            new_arg = list(key) + [ct]\n            self._incr_counter(*new_arg)\n            ct = 0\n        self._counter_dict[key] = ct", "is_method": true, "class_name": "JobTask", "function_description": "Provides a batched increment operation for Hadoop counters to improve update efficiency by aggregating multiple increments before applying them. This helps reduce overhead in tracking job metrics during Hadoop tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_flush_batch_incr_counter", "line_number": 884, "body": "def _flush_batch_incr_counter(self):\n        \"\"\"\n        Increments any unflushed counter values.\n        \"\"\"\n        for key, count in self._counter_dict.items():\n            if count == 0:\n                continue\n            args = list(key) + [count]\n            self._incr_counter(*args)\n            self._counter_dict[key] = 0", "is_method": true, "class_name": "JobTask", "function_description": "Internal utility of JobTask that consolidates and increments accumulated counter values, ensuring all pending increments are applied and counters reset for accurate tracking across batches."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_incr_counter", "line_number": 895, "body": "def _incr_counter(self, *args):\n        \"\"\"\n        Increments a Hadoop counter.\n\n        Note that this seems to be a bit slow, ~1 ms\n\n        Don't overuse this function by updating very frequently.\n        \"\"\"\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            print('reporter:counter:%s,%s' % (group_name, count), file=sys.stderr)\n        else:\n            group, name, count = args\n            print('reporter:counter:%s,%s,%s' % (group, name, count), file=sys.stderr)", "is_method": true, "class_name": "JobTask", "function_description": "Utility method in JobTask that increments Hadoop counters to track job metrics, supporting backward compatibility and enabling performance monitoring with logging through standard error output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_modules", "line_number": 911, "body": "def extra_modules(self):\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "Returns an empty list indicating no additional modules are associated with this job task. This method can be overridden to specify extra module dependencies as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_files", "line_number": 914, "body": "def extra_files(self):\n        \"\"\"\n        Can be overriden in subclass.\n\n        Each element is either a string, or a pair of two strings (src, dst).\n\n        * `src` can be a directory (in which case everything will be copied recursively).\n        * `dst` can include subdirectories (foo/bar/baz.txt etc)\n\n        Uses Hadoop's -files option so that the same file is reused across tasks.\n        \"\"\"\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "This method provides a customizable list of additional files or directories to be shared across tasks, supporting recursive copying and destination subdirectories via Hadoop's -files option. It enables task-specific resource inclusion in distributed job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_streaming_arguments", "line_number": 927, "body": "def extra_streaming_arguments(self):\n        \"\"\"\n        Extra arguments to Hadoop command line.\n        Return here a list of (parameter, value) tuples.\n        \"\"\"\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "Returns a list of additional command-line arguments for Hadoop streaming tasks. This method enables customization of Hadoop job execution by specifying extra parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_archives", "line_number": 934, "body": "def extra_archives(self):\n        \"\"\"List of paths to archives \"\"\"\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "Returns an empty list representing additional archive paths. This placeholder method can be overridden to specify extra archive locations for a job task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add_link", "line_number": 938, "body": "def add_link(self, src, dst):\n        if not hasattr(self, '_links'):\n            self._links = []\n        self._links.append((src, dst))", "is_method": true, "class_name": "JobTask", "function_description": "Adds a directional connection from a source to a destination within the JobTask, enabling tracking or representation of relationships between elements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_setup_links", "line_number": 943, "body": "def _setup_links(self):\n        if hasattr(self, '_links'):\n            missing = []\n            for src, dst in self._links:\n                d = os.path.dirname(dst)\n                if d:\n                    try:\n                        os.makedirs(d)\n                    except OSError:\n                        pass\n                if not os.path.exists(src):\n                    missing.append(src)\n                    continue\n                if not os.path.exists(dst):\n                    # If the combiner runs, the file might already exist,\n                    # so no reason to create the link again\n                    os.link(src, dst)\n            if missing:\n                raise HadoopJobError(\n                    'Missing files for distributed cache: ' +\n                    ', '.join(missing))", "is_method": true, "class_name": "JobTask", "function_description": "Sets up filesystem hard links for specified source-destination pairs, ensuring necessary directories exist and verifying all source files are present for a distributed cache in a JobTask. Raises an error if any sources are missing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "dump", "line_number": 965, "body": "def dump(self, directory=''):\n        \"\"\"\n        Dump instance to file.\n        \"\"\"\n        with self.no_unpicklable_properties():\n            file_name = os.path.join(directory, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'(c__main__', \"(c\" + module_name)\n                open(file_name, \"wb\").write(d)\n\n            else:\n                pickle.dump(self, open(file_name, \"wb\"))", "is_method": true, "class_name": "JobTask", "function_description": "Method of JobTask that serializes and saves the current instance to a file, ensuring it can be correctly unpickled even when run as a main script. Useful for persisting the task state to disk for later retrieval or transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_map_input", "line_number": 980, "body": "def _map_input(self, input_stream):\n        \"\"\"\n        Iterate over input and call the mapper for each item.\n        If the job has a parser defined, the return values from the parser will\n        be passed as arguments to the mapper.\n\n        If the input is coded output from a previous run,\n        the arguments will be splitted in key and value.\n        \"\"\"\n        for record in self.reader(input_stream):\n            for output in self.mapper(*record):\n                yield output\n        if self.final_mapper != NotImplemented:\n            for output in self.final_mapper():\n                yield output\n        self._flush_batch_incr_counter()", "is_method": true, "class_name": "JobTask", "function_description": "Processes input data by applying a mapper (and optionally a final mapper), yielding the resulting outputs. It supports parsing inputs and handling intermediate coded output for flexible data transformation in job tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_reduce_input", "line_number": 997, "body": "def _reduce_input(self, inputs, reducer, final=NotImplemented):\n        \"\"\"\n        Iterate over input, collect values with the same key, and call the reducer for each unique key.\n        \"\"\"\n        for key, values in groupby(inputs, key=lambda x: self.internal_serialize(x[0])):\n            for output in reducer(self.deserialize(key), (v[1] for v in values)):\n                yield output\n        if final != NotImplemented:\n            for output in final():\n                yield output\n        self._flush_batch_incr_counter()", "is_method": true, "class_name": "JobTask", "function_description": "Internal method of JobTask that processes grouped input key-value pairs by applying a reducer function to each unique key, optionally yielding final results, supporting batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_mapper", "line_number": 1009, "body": "def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        Run the mapper on the hadoop node.\n        \"\"\"\n        self.init_hadoop()\n        self.init_mapper()\n        outputs = self._map_input((line[:-1] for line in stdin))\n        if self.reducer == NotImplemented:\n            self.writer(outputs, stdout)\n        else:\n            self.internal_writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Runs the mapper phase of a Hadoop job, processing input lines and directing output either to a writer or reducer depending on the job configuration. It facilitates distributed data processing within the JobTask workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_reducer", "line_number": 1021, "body": "def run_reducer(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        Run the reducer on the hadoop node.\n        \"\"\"\n        self.init_hadoop()\n        self.init_reducer()\n        outputs = self._reduce_input(self.internal_reader((line[:-1] for line in stdin)), self.reducer, self.final_reducer)\n        self.writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Method in JobTask that orchestrates running a reducer on a Hadoop node by initializing components, processing input streams, applying reducing functions, and outputting results, enabling distributed data reduction workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_combiner", "line_number": 1030, "body": "def run_combiner(self, stdin=sys.stdin, stdout=sys.stdout):\n        self.init_hadoop()\n        self.init_combiner()\n        outputs = self._reduce_input(self.internal_reader((line[:-1] for line in stdin)), self.combiner, self.final_combiner)\n        self.internal_writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Core method of JobTask that processes input data through a combiner function to produce intermediate aggregation results, facilitating efficient data reduction typically used in map-reduce workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "internal_reader", "line_number": 1036, "body": "def internal_reader(self, input_stream):\n        \"\"\"\n        Reader which uses python eval on each part of a tab separated string.\n        Yields a tuple of python objects.\n        \"\"\"\n        for input_line in input_stream:\n            yield list(map(self.deserialize, input_line.split(\"\\t\")))", "is_method": true, "class_name": "JobTask", "function_description": "Utility method of JobTask that parses tab-separated input lines, deserializing each part into Python objects and yielding them as tuples for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "internal_writer", "line_number": 1044, "body": "def internal_writer(self, outputs, stdout):\n        \"\"\"\n        Writer which outputs the python repr for each item.\n        \"\"\"\n        for output in outputs:\n            print(\"\\t\".join(map(self.internal_serialize, output)), file=stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Method of JobTask that formats and writes output items as Python representations, printing them tab-separated to a specified output stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "get", "line_number": 400, "body": "def get(x, default):\n            return x is not None and x or default", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Simple utility method that returns the input value if it is not None; otherwise, it provides a specified default value. This supports safe value retrieval with fallback inside the HadoopJobRunner class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "_replacer", "line_number": 78, "body": "def _replacer(self, match_object):\n        # this method is used as the replace function in the re.sub below\n        return self._replace_dict[match_object.group()]", "is_method": true, "class_name": "MultiReplacer", "function_description": "Helper method used internally by MultiReplacer to map regex match results to their replacement strings based on a predefined dictionary."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "__call__", "line_number": 82, "body": "def __call__(self, search_string):\n        # using function replacing for a per-result replace\n        return self._search_re.sub(self._replacer, search_string)", "is_method": true, "class_name": "MultiReplacer", "function_description": "Callable method of the MultiReplacer class that performs multiple pattern substitutions on a given string, returning the result with all specified replacements applied in a single pass."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "touch", "line_number": 137, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        Important: If the marker table doesn't exist, the connection transaction will be aborted\n        and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            # TODO: test this\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        if self.use_db_timestamps:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n                   VALUES (%s, %s)\n                \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table))\n        else:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table, inserted)\n                         VALUES (%s, %s, %s);\n                    \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table,\n                 datetime.datetime.now()))", "is_method": true, "class_name": "PostgresTarget", "function_description": "Marks an update as complete by ensuring a marker table exists and inserting a record indicating the update's completion time, supporting tracking of update status in a PostgreSQL database."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "exists", "line_number": 166, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\"\"\"SELECT 1 FROM {marker_table}\n                WHERE update_id = %s\n                LIMIT 1\"\"\".format(marker_table=self.marker_table),\n                           (self.update_id,)\n                           )\n            row = cursor.fetchone()\n        except psycopg2.ProgrammingError as e:\n            if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE:\n                row = None\n            else:\n                raise\n        return row is not None", "is_method": true, "class_name": "PostgresTarget", "function_description": "Checks if a specific update record exists in the Postgres marker table, indicating whether a particular update has been applied. This helps track update status within database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "connect", "line_number": 185, "body": "def connect(self):\n        \"\"\"\n        Get a psycopg2 connection object to the database where the table is.\n        \"\"\"\n        connection = psycopg2.connect(\n            host=self.host,\n            port=self.port,\n            database=self.database,\n            user=self.user,\n            password=self.password)\n        connection.set_client_encoding('utf-8')\n        return connection", "is_method": true, "class_name": "PostgresTarget", "function_description": "Utility method of the PostgresTarget class that establishes and returns a connection to a specified PostgreSQL database using stored credentials."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "create_marker_table", "line_number": 198, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        connection.autocommit = True\n        cursor = connection.cursor()\n        if self.use_db_timestamps:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP DEFAULT NOW())\n                  \"\"\".format(marker_table=self.marker_table)\n        else:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP);\n                  \"\"\".format(marker_table=self.marker_table)\n\n        try:\n            cursor.execute(sql)\n        except psycopg2.ProgrammingError as e:\n            if e.pgcode == psycopg2.errorcodes.DUPLICATE_TABLE:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "PostgresTarget", "function_description": "Creates a marker table in the PostgreSQL database if it does not already exist, supporting optional timestamp tracking. This setup enables tracking of update identifiers and associated target tables for synchronization or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "rows", "line_number": 245, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides an iterator over rows from the input source, yielding each as a tuple or list of values for insertion into a table. This supports processing tab-separated data line by line."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "map_column", "line_number": 253, "body": "def map_column(self, value):\n        \"\"\"\n        Applied to each column of every row returned by `rows`.\n\n        Default behaviour is to escape special characters and identify any self.null_values.\n        \"\"\"\n        if value in self.null_values:\n            return r'\\\\N'\n        else:\n            return default_escape(str(value))", "is_method": true, "class_name": "CopyToTable", "function_description": "Method of the CopyToTable class that processes each column value by escaping special characters and converting defined null values into a standard placeholder, facilitating consistent data formatting for table copying operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "output", "line_number": 266, "body": "def output(self):\n        \"\"\"\n        Returns a PostgresTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns a PostgresTarget object that represents the destination table for the inserted dataset, encapsulating connection and table details for downstream use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "copy", "line_number": 282, "body": "def copy(self, cursor, file):\n        if isinstance(self.columns[0], str):\n            column_names = self.columns\n        elif len(self.columns[0]) == 2:\n            column_names = [c[0] for c in self.columns]\n        else:\n            raise Exception('columns must consist of column strings or (column string, type string) tuples (was %r ...)' % (self.columns[0],))\n        cursor.copy_from(file, self.table, null=r'\\\\N', sep=self.column_separator, columns=column_names)", "is_method": true, "class_name": "CopyToTable", "function_description": "Method of CopyToTable that copies data from a file into a database table using a cursor, handling column specification and data formatting for efficient bulk inserts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "run", "line_number": 291, "body": "def run(self):\n        \"\"\"\n        Inserts data generated by rows() into target table.\n\n        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n\n        Normally you don't want to override this.\n        \"\"\"\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n        # transform all data generated by rows() using map_column and write data\n        # to a temporary file for import using postgres COPY\n        tmp_dir = luigi.configuration.get_config().get('postgres', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(dir=tmp_dir)\n        n = 0\n        for row in self.rows():\n            n += 1\n            if n % 100000 == 0:\n                logger.info(\"Wrote %d lines\", n)\n            rowstr = self.column_separator.join(self.map_column(val) for val in row)\n            rowstr += \"\\n\"\n            tmp_file.write(rowstr.encode('utf-8'))\n\n        logger.info(\"Done writing, importing at %s\", datetime.datetime.now())\n        tmp_file.seek(0)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, tmp_file)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except psycopg2.ProgrammingError as e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n\n        # commit and clean up\n        connection.commit()\n        connection.close()\n        tmp_file.close()", "is_method": true, "class_name": "CopyToTable", "function_description": "Service method of the CopyToTable class that inserts rows of data into a PostgreSQL table, creating the table if missing, and uses temporary file buffering with an efficient COPY operation to bulk-load data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "run", "line_number": 364, "body": "def run(self):\n        connection = self.output().connect()\n        connection.autocommit = self.autocommit\n        cursor = connection.cursor()\n        sql = self.query\n\n        logger.info('Executing query from task: {name}'.format(name=self.__class__))\n        cursor.execute(sql)\n\n        # Update marker table\n        self.output().touch(connection)\n\n        # commit and close connection\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "PostgresQuery", "function_description": "Core method of the PostgresQuery class that executes the SQL query associated with the task, updates a marker for completion, and manages database transaction commit and connection closure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "output", "line_number": 380, "body": "def output(self):\n        \"\"\"\n        Returns a PostgresTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )", "is_method": true, "class_name": "PostgresQuery", "function_description": "Returns a PostgresTarget object encapsulating the connection and query execution details of the PostgresQuery instance for downstream use. This enables easy reference to the query's target database table and credentials."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "touch", "line_number": 71, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        connection.cursor().execute(\n            \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n               VALUES (%s, %s)\n               ON DUPLICATE KEY UPDATE\n               update_id = VALUES(update_id)\n            \"\"\".format(marker_table=self.marker_table),\n            (self.update_id, self.table)\n        )\n        # make sure update is properly marked\n        assert self.exists(connection)", "is_method": true, "class_name": "MySqlTarget", "function_description": "Marks the update as complete by inserting or updating a record in a marker table, ensuring the table exists and handling connection management to track update status reliably."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "exists", "line_number": 96, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\"\"\"SELECT 1 FROM {marker_table}\n                WHERE update_id = %s\n                LIMIT 1\"\"\".format(marker_table=self.marker_table),\n                           (self.update_id,)\n                           )\n            row = cursor.fetchone()\n        except mysql.connector.Error as e:\n            if e.errno == errorcode.ER_NO_SUCH_TABLE:\n                row = None\n            else:\n                raise\n        return row is not None", "is_method": true, "class_name": "MySqlTarget", "function_description": "Checks whether a specific update record exists in the target MySQL marker table, indicating if a particular data update has been applied. This helps track update status within database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "connect", "line_number": 115, "body": "def connect(self, autocommit=False):\n        connection = mysql.connector.connect(user=self.user,\n                                             password=self.password,\n                                             host=self.host,\n                                             port=self.port,\n                                             database=self.database,\n                                             autocommit=autocommit,\n                                             **self.cnx_kwargs)\n        return connection", "is_method": true, "class_name": "MySqlTarget", "function_description": "Establishes and returns a connection to a MySQL database using the instance's credentials and configuration, optionally enabling autocommit mode. Useful for initiating database interactions within the MySqlTarget context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "create_marker_table", "line_number": 125, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect(autocommit=True)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT(20)    NOT NULL AUTO_INCREMENT,\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      TIMESTAMP DEFAULT NOW(),\n                        PRIMARY KEY (update_id),\n                        KEY id (id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except mysql.connector.Error as e:\n            if e.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "MySqlTarget", "function_description": "Creates a marker table in the database if it does not already exist, ensuring a dedicated structure for tracking update identifiers and timestamps within the MySqlTarget context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "rows", "line_number": 166, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides an iterator over rows from an input source, yielding each as a list of values parsed from tab-separated lines, for use in bulk table insertion operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "output", "line_number": 176, "body": "def output(self):\n        \"\"\"\n        Returns a MySqlTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return MySqlTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n\n        )", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns a MySqlTarget instance representing the target table where data is inserted, encapsulating connection and update details for database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "copy", "line_number": 192, "body": "def copy(self, cursor, file=None):\n        values = '({})'.format(','.join(['%s' for i in range(len(self.columns))]))\n        columns = '({})'.format(','.join([c[0] for c in self.columns]))\n        query = 'INSERT INTO {} {} VALUES {}'.format(self.table, columns, values)\n        rows = []\n\n        for idx, row in enumerate(self.rows()):\n            rows.append(row)\n\n            if (idx + 1) % self.bulk_size == 0:\n                cursor.executemany(query, rows)\n                rows = []\n\n        cursor.executemany(query, rows)", "is_method": true, "class_name": "CopyToTable", "function_description": "Core method of CopyToTable class that efficiently inserts rows into a database table in bulk using a cursor, optimizing performance by batching inserts according to a specified bulk size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "run", "line_number": 207, "body": "def run(self):\n        \"\"\"\n        Inserts data generated by rows() into target table.\n\n        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n\n        Normally you don't want to override this.\n        \"\"\"\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n\n        # attempt to copy the data into mysql\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                print(\"caling init copy...\")\n                self.init_copy(connection)\n                self.copy(cursor)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except Error as err:\n                if err.errno == errorcode.ER_NO_SUCH_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    # logger.info(\"Creating table %s\", self.table)\n                    connection.reconnect()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "CopyToTable", "function_description": "Utility method of the CopyToTable class that inserts generated rows into a target database table, creating the table if it does not exist, and ensures transactional integrity during the operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "bulk_size", "line_number": 249, "body": "def bulk_size(self):\n        return 10000", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns the fixed batch size used for bulk operations, standardizing the number of records processed at once during table copying."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "_random_id", "line_number": 84, "body": "def _random_id():\n    return 'batch-job-' + ''.join(random.sample(string.ascii_lowercase, 8))", "is_method": false, "function_description": "Generates a unique identifier string prefixed with 'batch-job-' followed by 8 random lowercase letters. Useful for creating distinct IDs for batch job tracking or identification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_active_queue", "line_number": 96, "body": "def get_active_queue(self):\n        \"\"\"Get name of first active job queue\"\"\"\n\n        # Get dict of active queues keyed by name\n        queues = {q['jobQueueName']: q for q in self._client.describe_job_queues()['jobQueues']\n                  if q['state'] == 'ENABLED' and q['status'] == 'VALID'}\n        if not queues:\n            raise Exception('No job queues with state=ENABLED and status=VALID')\n\n        # Pick the first queue as default\n        return list(queues.keys())[0]", "is_method": true, "class_name": "BatchClient", "function_description": "Provides the name of the first active job queue that is both enabled and valid, allowing other functions to select an available job queue for task submission or management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_job_id_from_name", "line_number": 108, "body": "def get_job_id_from_name(self, job_name):\n        \"\"\"Retrieve the first job ID matching the given name\"\"\"\n        jobs = self._client.list_jobs(jobQueue=self._queue, jobStatus='RUNNING')['jobSummaryList']\n        matching_jobs = [job for job in jobs if job['jobName'] == job_name]\n        if matching_jobs:\n            return matching_jobs[0]['jobId']", "is_method": true, "class_name": "BatchClient", "function_description": "Retrieves the ID of the first currently running job that matches a specified job name, facilitating job management and monitoring within the batch processing system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_job_status", "line_number": 115, "body": "def get_job_status(self, job_id):\n        \"\"\"Retrieve task statuses from ECS API\n\n        :param job_id (str): AWS Batch job uuid\n\n        Returns one of {SUBMITTED|PENDING|RUNNABLE|STARTING|RUNNING|SUCCEEDED|FAILED}\n        \"\"\"\n        response = self._client.describe_jobs(jobs=[job_id])\n\n        # Error checking\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Job status request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n\n        return response['jobs'][0]['status']", "is_method": true, "class_name": "BatchClient", "function_description": "Provides the current execution status of an AWS Batch job by retrieving it from the ECS API. Useful for monitoring job progress and handling job lifecycle events in batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_logs", "line_number": 132, "body": "def get_logs(self, log_stream_name, get_last=50):\n        \"\"\"Retrieve log stream from CloudWatch\"\"\"\n        response = self._log_client.get_log_events(\n            logGroupName='/aws/batch/job',\n            logStreamName=log_stream_name,\n            startFromHead=False)\n        events = response['events']\n        return '\\n'.join(e['message'] for e in events[-get_last:])", "is_method": true, "class_name": "BatchClient", "function_description": "Retrieves the most recent log messages from a specified CloudWatch log stream within AWS Batch, enabling monitoring and debugging of batch job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "submit_job", "line_number": 141, "body": "def submit_job(self, job_definition, parameters, job_name=None, queue=None):\n        \"\"\"Wrap submit_job with useful defaults\"\"\"\n        if job_name is None:\n            job_name = _random_id()\n        response = self._client.submit_job(\n            jobName=job_name,\n            jobQueue=queue or self.get_active_queue(),\n            jobDefinition=job_definition,\n            parameters=parameters\n        )\n        return response['jobId']", "is_method": true, "class_name": "BatchClient", "function_description": "Provides a convenient way to submit a job with default parameters like a random job name and active queue, returning the job ID. It simplifies job submission in batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "wait_on_job", "line_number": 153, "body": "def wait_on_job(self, job_id):\n        \"\"\"Poll task status until STOPPED\"\"\"\n\n        while True:\n            status = self.get_job_status(job_id)\n            if status == 'SUCCEEDED':\n                logger.info('Batch job {} SUCCEEDED'.format(job_id))\n                return True\n            elif status == 'FAILED':\n                # Raise and notify if job failed\n                jobs = self._client.describe_jobs(jobs=[job_id])['jobs']\n                job_str = json.dumps(jobs, indent=4)\n                logger.debug('Job details:\\n' + job_str)\n\n                log_stream_name = jobs[0]['attempts'][0]['container']['logStreamName']\n                logs = self.get_logs(log_stream_name)\n                raise BatchJobException('Job {} failed: {}'.format(\n                    job_id, logs))\n\n            time.sleep(self.poll_time)\n            logger.debug('Batch job status for job {0}: {1}'.format(\n                job_id, status))", "is_method": true, "class_name": "BatchClient", "function_description": "Utility method of BatchClient that continuously monitors a batch job until completion, returning success or raising an exception on failure, facilitating reliable job status tracking and error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "register_job_definition", "line_number": 176, "body": "def register_job_definition(self, json_fpath):\n        \"\"\"Register a job definition with AWS Batch, using a JSON\"\"\"\n        with open(json_fpath) as f:\n            job_def = json.load(f)\n        response = self._client.register_job_definition(**job_def)\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Register job definition request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n        return response", "is_method": true, "class_name": "BatchClient", "function_description": "Registers a job definition with AWS Batch using configuration from a JSON file, allowing batch jobs to be defined and submitted programmatically. This enables automated management of batch processing workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "run", "line_number": 207, "body": "def run(self):\n        bc = BatchClient(self.poll_time)\n        job_id = bc.submit_job(\n            self.job_definition,\n            self.parameters,\n            job_name=self.job_name,\n            queue=self.job_queue)\n        bc.wait_on_job(job_id)", "is_method": true, "class_name": "BatchTask", "function_description": "Core method of the BatchTask class that submits a batch job with specified parameters and waits for its completion, enabling synchronous execution of batch processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "parameters", "line_number": 217, "body": "def parameters(self):\n        \"\"\"Override to return a dict of parameters for the Batch Task\"\"\"\n        return {}", "is_method": true, "class_name": "BatchTask", "function_description": "Returns an empty dictionary as the default parameters for the BatchTask, intended to be overridden to provide task-specific parameter values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "_get_task_statuses", "line_number": 68, "body": "def _get_task_statuses(task_ids, cluster):\n    \"\"\"\n    Retrieve task statuses from ECS API\n\n    Returns list of {RUNNING|PENDING|STOPPED} for each id in task_ids\n    \"\"\"\n    response = client.describe_tasks(tasks=task_ids, cluster=cluster)\n\n    # Error checking\n    if response['failures'] != []:\n        raise Exception('There were some failures:\\n{0}'.format(\n            response['failures']))\n    status_code = response['ResponseMetadata']['HTTPStatusCode']\n    if status_code != 200:\n        msg = 'Task status request received status code {0}:\\n{1}'\n        raise Exception(msg.format(status_code, response))\n\n    return [t['lastStatus'] for t in response['tasks']]", "is_method": false, "function_description": "Utility function that retrieves the current statuses of specified ECS tasks within a given cluster, returning their states (RUNNING, PENDING, or STOPPED) for monitoring or management purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "_track_tasks", "line_number": 88, "body": "def _track_tasks(task_ids, cluster):\n    \"\"\"Poll task status until STOPPED\"\"\"\n    while True:\n        statuses = _get_task_statuses(task_ids, cluster)\n        if all([status == 'STOPPED' for status in statuses]):\n            logger.info('ECS tasks {0} STOPPED'.format(','.join(task_ids)))\n            break\n        time.sleep(POLL_TIME)\n        logger.debug('ECS task status for tasks {0}: {1}'.format(task_ids, statuses))", "is_method": false, "function_description": "Utility function that continuously monitors the status of given cluster tasks until all of them have stopped, providing logging updates throughout the polling process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "ecs_task_ids", "line_number": 141, "body": "def ecs_task_ids(self):\n        \"\"\"Expose the ECS task ID\"\"\"\n        if hasattr(self, '_task_ids'):\n            return self._task_ids", "is_method": true, "class_name": "ECSTask", "function_description": "Returns the ECS task IDs associated with the ECSTask instance if they are available. This method provides access to task identifiers for managing or tracking ECS tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "run", "line_number": 166, "body": "def run(self):\n        if (not self.task_def and not self.task_def_arn) or \\\n                (self.task_def and self.task_def_arn):\n            raise ValueError(('Either (but not both) a task_def (dict) or'\n                              'task_def_arn (string) must be assigned'))\n        if not self.task_def_arn:\n            # Register the task and get assigned taskDefinition ID (arn)\n            response = client.register_task_definition(**self.task_def)\n            self.task_def_arn = response['taskDefinition']['taskDefinitionArn']\n\n        # Submit the task to AWS ECS and get assigned task ID\n        # (list containing 1 string)\n        if self.command:\n            overrides = {'containerOverrides': self.command}\n        else:\n            overrides = {}\n        response = client.run_task(taskDefinition=self.task_def_arn,\n                                   overrides=overrides,\n                                   cluster=self.cluster)\n\n        if response['failures']:\n            raise Exception(\", \".join([\"fail to run task {0} reason: {1}\".format(failure['arn'], failure['reason'])\n                                       for failure in response['failures']]))\n\n        self._task_ids = [task['taskArn'] for task in response['tasks']]\n\n        # Wait on task completion\n        _track_tasks(self._task_ids, self.cluster)", "is_method": true, "class_name": "ECSTask", "function_description": "Runs an AWS ECS task by either registering a new task definition or using an existing one, then executes the task with optional command overrides and waits for its completion. It handles validation, task submission, and failure reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "metadata_columns", "line_number": 87, "body": "def metadata_columns(self):\n        \"\"\"Returns the default metadata columns.\n\n        Those columns are columns that we want each tables to have by default.\n        \"\"\"\n        return []", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Core utility method of the _MetadataColumnsMixin class that provides the default metadata columns every table should include, serving as a base for consistent table metadata management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "metadata_queries", "line_number": 95, "body": "def metadata_queries(self):\n        return []", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Returns an empty list of metadata queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "enable_metadata_columns", "line_number": 99, "body": "def enable_metadata_columns(self):\n        return False", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Method that indicates whether metadata columns are enabled; by default, it returns False, serving as a toggle point for subclasses to override if metadata columns should be supported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_add_metadata_columns", "line_number": 102, "body": "def _add_metadata_columns(self, connection):\n        cursor = connection.cursor()\n\n        for column in self.metadata_columns:\n            if len(column) == 0:\n                raise ValueError(\"_add_metadata_columns is unable to infer column information from column {column} for {table}\".format(column=column,\n                                                                                                                                       table=self.table))\n\n            column_name = column[0]\n            if not self._column_exists(cursor, column_name):\n                logger.info('Adding missing metadata column {column} to {table}'.format(column=column, table=self.table))\n                self._add_column_to_table(cursor, column)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Method of _MetadataColumnsMixin that ensures all specified metadata columns exist in a database table, adding any missing columns to maintain the table's required schema integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_column_exists", "line_number": 115, "body": "def _column_exists(self, cursor, column_name):\n        if '.' in self.table:\n            schema, table = self.table.split('.')\n            query = \"SELECT 1 AS column_exists \" \\\n                    \"FROM information_schema.columns \" \\\n                    \"WHERE table_schema = LOWER('{0}') AND table_name = LOWER('{1}') AND column_name = LOWER('{2}') LIMIT 1;\".format(schema, table, column_name)\n        else:\n            query = \"SELECT 1 AS column_exists \" \\\n                    \"FROM information_schema.columns \" \\\n                    \"WHERE table_name = LOWER('{0}') AND column_name = LOWER('{1}') LIMIT 1;\".format(self.table, column_name)\n\n        cursor.execute(query)\n        result = cursor.fetchone()\n        return bool(result)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Checks if a specific column exists in a database table, supporting optional schema-qualified table names. This method helps verify column presence to prevent errors in database operations or alter logic conditionally."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_add_column_to_table", "line_number": 130, "body": "def _add_column_to_table(self, cursor, column):\n        if len(column) == 1:\n            raise ValueError(\"_add_column_to_table() column type not specified for {column}\".format(column=column[0]))\n        elif len(column) == 2:\n            query = \"ALTER TABLE {table} ADD COLUMN {column};\".format(table=self.table, column=' '.join(column))\n        elif len(column) == 3:\n            query = \"ALTER TABLE {table} ADD COLUMN {column} ENCODE {encoding};\".format(table=self.table, column=' '.join(column[0:2]), encoding=column[2])\n        else:\n            raise ValueError(\"_add_column_to_table() found no matching behavior for {column}\".format(column=column))\n\n        cursor.execute(query)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Internal utility method of the _MetadataColumnsMixin class that adds a new column with optional encoding to a database table, validating column specification before executing the appropriate ALTER TABLE command."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "post_copy_metacolumns", "line_number": 142, "body": "def post_copy_metacolumns(self, cursor):\n        logger.info('Executing post copy metadata queries')\n        for query in self.metadata_queries:\n            cursor.execute(query)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Utility method in _MetadataColumnsMixin that executes predefined metadata-related SQL queries using the provided database cursor immediately after a data copy operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "host", "line_number": 167, "body": "def host(self):\n        return None", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns the host information for the CopyToTable instance; currently, it provides no host data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "database", "line_number": 172, "body": "def database(self):\n        return None", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns None indicating no associated database; likely a placeholder or default value within CopyToTable class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "user", "line_number": 177, "body": "def user(self):\n        return None", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns None for the user property, likely acting as a placeholder or default implementation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "password", "line_number": 182, "body": "def password(self):\n        return None", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns None as a placeholder for a password value; it currently provides no password information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "create_table", "line_number": 206, "body": "def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally) specified in columns.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(name=name, type=type) for name, type in self.columns\n            )\n            query = \"CREATE TABLE {table} ({coldefs})\".format(table=self.table, coldefs=coldefs)\n            connection.cursor().execute(query)", "is_method": true, "class_name": "CopyToTable", "function_description": "Creates a database table using specified column names and types via a given connection, facilitating table setup within the same transaction context for subsequent data insertion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "update_id", "line_number": 227, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToTable", "function_description": "Returns a unique identifier representing the current insert operation, enabling tracking or referencing specific tasks within the CopyToTable process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "init_copy", "line_number": 237, "body": "def init_copy(self, connection):\n        \"\"\"\n        Override to perform custom queries.\n\n        Any code here will be formed in the same transaction as the main copy, just prior to copying data.\n        Example use cases include truncating the table or removing all data older than X in the database\n        to keep a rolling window of data available in the table.\n        \"\"\"\n\n        # TODO: remove this after sufficient time so most people using the\n        # clear_table attribtue will have noticed it doesn't work anymore\n        if hasattr(self, \"clear_table\"):\n            raise Exception(\"The clear_table attribute has been removed. Override init_copy instead!\")\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection.cursor())", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides a customizable hook to execute queries within the same transaction before copying data, enabling operations like truncating or cleaning the target table to manage data windows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "port", "line_number": 305, "body": "def port(self):\n        \"\"\"\n        Override to specify port separately from host.\n        \"\"\"\n        return None", "is_method": true, "class_name": "Query", "function_description": "Returns the network port associated with the Query instance, allowing subclasses to specify a port separately from the host if needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "user", "line_number": 318, "body": "def user(self):\n        return None", "is_method": true, "class_name": "Query", "function_description": "Returns None for the user property, indicating no user is associated or implemented."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "password", "line_number": 323, "body": "def password(self):\n        return None", "is_method": true, "class_name": "Query", "function_description": "Returns None for the password attribute, indicating no password is set or retrievable in the Query context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "autocommit", "line_number": 337, "body": "def autocommit(self):\n        return False", "is_method": true, "class_name": "Query", "function_description": "Returns the default autocommit status for a query, indicating whether changes are automatically committed without requiring explicit instructions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "update_id", "line_number": 341, "body": "def update_id(self):\n        \"\"\"\n        Override to create a custom marker table 'update_id' signature for Query subclass task instances\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "Query", "function_description": "This method provides a unique identifier for Query subclass task instances by returning the associated task ID, supporting consistent identification across query-related operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_started", "line_number": 34, "body": "def handle_task_started(self, task):\n        title = \"Luigi: A task has been started!\"\n        text = \"A task has been started in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.started', tags=tags)\n\n        event_tags = tags + [\"task_state:STARTED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='info', priority='low')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Reports the start of a pipeline task by incrementing a corresponding metric and sending an informational event notification with task details and tags for monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_failed", "line_number": 44, "body": "def handle_task_failed(self, task):\n        title = \"Luigi: A task has failed!\"\n        text = \"A task has failed in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.failed', tags=tags)\n\n        event_tags = tags + [\"task_state:FAILED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='error', priority='normal')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Service method of DatadogMetricsCollector that tracks and reports failed tasks by incrementing failure metrics and sending an alert event with detailed task information for monitoring pipeline errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_disabled", "line_number": 54, "body": "def handle_task_disabled(self, task, config):\n        title = \"Luigi: A task has been disabled!\"\n        lines = ['A task has been disabled in the pipeline named: {name}.']\n        lines.append('The task has failed {failures} times in the last {window}')\n        lines.append('seconds, so it is being disabled for {persist} seconds.')\n\n        preformated_text = ' '.join(lines)\n\n        text = preformated_text.format(name=task.family,\n                                       persist=config.disable_persist,\n                                       failures=config.retry_count,\n                                       window=config.disable_window)\n\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.disabled', tags=tags)\n\n        event_tags = tags + [\"task_state:DISABLED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='error', priority='normal')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Service method of DatadogMetricsCollector that logs and reports when a pipeline task is disabled due to repeated failures, sending corresponding metrics and error events with task-specific tags to a monitoring system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_done", "line_number": 74, "body": "def handle_task_done(self, task):\n        # The task is already done -- Let's not re-create an event\n        if task.time_running is None:\n            return\n\n        title = \"Luigi: A task has been completed!\"\n        text = \"A task has completed in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        time_elapse = task.updated - task.time_running\n\n        self._send_increment('task.done', tags=tags)\n        self._send_gauge('task.execution_time', time_elapse, tags=tags)\n\n        event_tags = tags + [\"task_state:DONE\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='info', priority='low')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Handles completion of a task by sending metrics and an informational event about its execution time and status, enabling monitoring of pipeline task completions via Datadog."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_event", "line_number": 91, "body": "def _send_event(self, **params):\n        params['tags'] += self.default_tags\n\n        api.Event.create(**params)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Private method of DatadogMetricsCollector that sends an event to Datadog API, automatically appending default tags to the event metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_gauge", "line_number": 96, "body": "def _send_gauge(self, metric_name, value, tags=[]):\n        all_tags = tags + self.default_tags\n\n        namespaced_metric = \"{namespace}.{metric_name}\".format(namespace=self._config.metric_namespace,\n                                                               metric_name=metric_name)\n        statsd.gauge(namespaced_metric, value, tags=all_tags)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Internal method of DatadogMetricsCollector that sends a gauge metric with namespacing and combined tags for monitoring purposes. It facilitates reporting numeric values to Datadog with contextual metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_increment", "line_number": 103, "body": "def _send_increment(self, metric_name, value=1, tags=[]):\n        all_tags = tags + self.default_tags\n\n        namespaced_metric = \"{namespace}.{metric_name}\".format(namespace=self._config.metric_namespace,\n                                                               metric_name=metric_name)\n        statsd.increment(namespaced_metric, value, tags=all_tags)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Internal method of DatadogMetricsCollector that increments a specified metric with optional tags, applying a configured namespace and default tags for consistent metric reporting to Datadog."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_format_task_params_to_tags", "line_number": 110, "body": "def _format_task_params_to_tags(self, task):\n        params = []\n        for key, value in task.params.items():\n            params.append(\"{key}:{value}\".format(key=key, value=value))\n\n        return params", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Private helper method in DatadogMetricsCollector that converts a task's parameters into a list of formatted \"key:value\" tags for metric labeling or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "default_tags", "line_number": 118, "body": "def default_tags(self):\n        default_tags = []\n\n        env_tag = \"environment:{environment}\".format(environment=self._config.environment)\n        default_tags.append(env_tag)\n\n        if self._config.default_tags:\n            default_tags = default_tags + str.split(self._config.default_tags, ',')\n\n        return default_tags", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Returns a list of default tags including the environment and any additional configured tags, facilitating consistent metric labeling in Datadog monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_collection", "line_number": 38, "body": "def get_collection(self):\n        \"\"\"\n        Return targeted mongo collection to query on\n        \"\"\"\n        db_mongo = self._mongo_client[self._index]\n        return db_mongo[self._collection]", "is_method": true, "class_name": "MongoTarget", "function_description": "Provides access to a specific MongoDB collection defined by the MongoTarget instance, enabling query operations on that targeted collection within the designated database."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_index", "line_number": 45, "body": "def get_index(self):\n        \"\"\"\n        Return targeted mongo index to query on\n        \"\"\"\n        return self._mongo_client[self._index]", "is_method": true, "class_name": "MongoTarget", "function_description": "Utility method of the MongoTarget class that provides access to the specific MongoDB index configured for querying operations. It serves as the entry point for database interactions within this target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 68, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted field exists\n        \"\"\"\n        return self.read() is not None", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Checks whether the target field in the MongoCellTarget has been executed by verifying the presence of its data. This method helps determine if the target's operation or update has already occurred."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 75, "body": "def read(self):\n        \"\"\"\n        Read the target value\n        Use $project aggregate operator in order to support nested objects\n        \"\"\"\n        result = self.get_collection().aggregate([\n            {'$match': {'_id': self._document_id}},\n            {'$project': {'_value': '$' + self._path, '_id': False}}\n        ])\n\n        for doc in result:\n            if '_value' not in doc:\n                break\n\n            return doc['_value']", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Reads and returns a specific nested value from a MongoDB document using aggregation, supporting retrieval of deeply nested fields by a specified path within the document."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "write", "line_number": 91, "body": "def write(self, value):\n        \"\"\"\n        Write value to the target\n        \"\"\"\n        self.get_collection().update_one(\n            {'_id': self._document_id},\n            {'$set': {self._path: value}},\n            upsert=True\n        )", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Method of MongoCellTarget that updates or inserts a specific value at a target path within a MongoDB document, enabling seamless data modification and storage in the database."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 118, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted field exists in ALL documents\n        \"\"\"\n        return not self.get_empty_ids()", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Checks if the targeted field exists in all documents, indicating the target operation has been completed. This method helps verify the run status of a MongoRangeTarget instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 125, "body": "def read(self):\n        \"\"\"\n        Read the targets value\n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {self._field: True}\n        )\n\n        return {doc['_id']: doc[self._field] for doc in cursor}", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Core method of MongoRangeTarget that retrieves specified field values for a set of document IDs from a MongoDB collection, returning a mapping of document IDs to their corresponding target values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "write", "line_number": 139, "body": "def write(self, values):\n        \"\"\"\n        Write values to the targeted documents\n        Values need to be a dict as : {document_id: value}\n        \"\"\"\n        # Insert only for docs targeted by the target\n        filtered = {_id: value for _id, value in values.items() if _id in self._document_ids}\n\n        if not filtered:\n            return\n\n        bulk = self.get_collection().initialize_ordered_bulk_op()\n        for _id, value in filtered.items():\n            bulk.find({'_id': _id}).upsert() \\\n                    .update_one({'$set': {self._field: value}})\n\n        bulk.execute()", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Core service of MongoRangeTarget that updates specified fields for targeted documents in bulk based on given values, enabling efficient and atomic writes to multiple documents identified by their IDs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_empty_ids", "line_number": 157, "body": "def get_empty_ids(self):\n        \"\"\"\n        Get documents id with missing targeted field\n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {'_id': True}\n        )\n\n        return set(self._document_ids) - {doc['_id'] for doc in cursor}", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Method of MongoRangeTarget that identifies document IDs missing a specified targeted field within a given set, useful for filtering or processing incomplete data entries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 179, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted collection exists in the database\n        \"\"\"\n        return self.read()", "is_method": true, "class_name": "MongoCollectionTarget", "function_description": "Checks whether the MongoDB collection targeted by this instance exists, indicating if the associated operation has been executed. This function helps determine the presence of the collection before performing further actions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 186, "body": "def read(self):\n        \"\"\"\n        Return if the target collection exists in the database\n        \"\"\"\n        return self._collection in self.get_index().collection_names()", "is_method": true, "class_name": "MongoCollectionTarget", "function_description": "Checks if the target MongoDB collection exists in the database, enabling validation before performing database operations. This helps ensure that subsequent actions are executed on valid collections."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 206, "body": "def exists(self):\n        \"\"\"\n        Test if the target has been run\n        Target is considered run if the number of items in the target matches value of self._target_count\n        \"\"\"\n        return self.read() == self._target_count", "is_method": true, "class_name": "MongoCountTarget", "function_description": "Checks whether the target has been executed by verifying if its item count matches the expected target count. Useful for confirming task completion based on data presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 213, "body": "def read(self):\n        \"\"\"\n        Using the aggregate method to avoid inaccurate count if using a sharded cluster\n        https://docs.mongodb.com/manual/reference/method/db.collection.count/#behavior\n        \"\"\"\n        for res in self.get_collection().aggregate([{'$group': {'_id': None, 'count': {'$sum': 1}}}]):\n            return res.get('count', None)\n        return None", "is_method": true, "class_name": "MongoCountTarget", "function_description": "Core function of MongoCountTarget that reliably counts documents in a MongoDB collection using aggregation, ensuring accurate results even in sharded cluster environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "program_environment", "line_number": 99, "body": "def program_environment(self):\n        \"\"\"\n        Override this method to control environment variables for the program\n\n        :return: dict mapping environment variable names to values\n        \"\"\"\n        env = os.environ.copy()\n        return env", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "This method provides the execution environment variables for an external program, allowing customization of these variables when running the program through the ExternalProgramTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "always_log_stderr", "line_number": 109, "body": "def always_log_stderr(self):\n        \"\"\"\n        When True, stderr will be logged even if program execution succeeded\n\n        Override to False to log stderr only when program execution fails.\n        \"\"\"\n        return True", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Core method in ExternalProgramTask that dictates whether to always log standard error output regardless of success, enabling consistent stderr logging or conditional logging based on execution outcome."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_clean_output_file", "line_number": 117, "body": "def _clean_output_file(self, file_object):\n        file_object.seek(0)\n        return ''.join(map(lambda s: s.decode('utf-8'), file_object.readlines()))", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Core utility method of ExternalProgramTask that reads a binary file-like object from the start and decodes its contents into a single UTF-8 string. It enables consistent text extraction from output files for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "build_tracking_url", "line_number": 121, "body": "def build_tracking_url(self, logs_output):\n        \"\"\"\n        This method is intended for transforming pattern match in logs to an URL\n        :param logs_output: Found match of `self.tracking_url_pattern`\n        :return: a tracking URL for the task\n        \"\"\"\n        return logs_output", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Returns a tracking URL derived from a log pattern match, enabling task status or progress tracking based on processed log data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "run", "line_number": 129, "body": "def run(self):\n        args = list(map(str, self.program_args()))\n\n        logger.info('Running command: %s', ' '.join(args))\n        env = self.program_environment()\n        kwargs = {'env': env}\n        tmp_stdout, tmp_stderr = None, None\n        if self.capture_output:\n            tmp_stdout, tmp_stderr = tempfile.TemporaryFile(), tempfile.TemporaryFile()\n            kwargs.update({'stdout': tmp_stdout, 'stderr': tmp_stderr})\n\n        try:\n            if self.stream_for_searching_tracking_url != 'none' and self.tracking_url_pattern is not None:\n                with self._proc_with_tracking_url_context(proc_args=args, proc_kwargs=kwargs) as proc:\n                    proc.wait()\n            else:\n                proc = subprocess.Popen(args, **kwargs)\n                with ExternalProgramRunContext(proc):\n                    proc.wait()\n            success = proc.returncode == 0\n\n            if self.capture_output:\n                stdout = self._clean_output_file(tmp_stdout)\n                stderr = self._clean_output_file(tmp_stderr)\n\n                if stdout:\n                    logger.info('Program stdout:\\n{}'.format(stdout))\n                if stderr:\n                    if self.always_log_stderr or not success:\n                        logger.info('Program stderr:\\n{}'.format(stderr))\n            else:\n                stdout, stderr = None, None\n\n            if not success:\n                raise ExternalProgramRunError(\n                    'Program failed with return code={}:'.format(proc.returncode),\n                    args, env=env, stdout=stdout, stderr=stderr)\n        finally:\n            if self.capture_output:\n                tmp_stderr.close()\n                tmp_stdout.close()", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Executes an external program with specified arguments and environment, optionally capturing and logging its output, while managing process lifecycle and error handling for integration within broader workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_proc_with_tracking_url_context", "line_number": 172, "body": "def _proc_with_tracking_url_context(self, proc_args, proc_kwargs):\n        time_to_sleep = 0.5\n        file_to_write = proc_kwargs.get(self.stream_for_searching_tracking_url)\n        proc_kwargs.update({self.stream_for_searching_tracking_url: subprocess.PIPE})\n        main_proc = subprocess.Popen(proc_args, **proc_kwargs)\n        pipe_to_read = main_proc.stderr if self.stream_for_searching_tracking_url == 'stderr' else main_proc.stdout\n\n        def _track_url_by_pattern():\n            \"\"\"\n            Scans the pipe looking for a passed pattern, if the pattern is found, `set_tracking_url` callback is sent.\n            If tmp_stdout is passed, also appends lines to this file.\n            \"\"\"\n            pattern = re.compile(self.tracking_url_pattern)\n            for new_line in iter(pipe_to_read.readline, ''):\n                if new_line:\n                    if file_to_write:\n                        file_to_write.write(new_line)\n                    match = re.search(pattern, new_line.decode('utf-8'))\n                    if match:\n                        self.set_tracking_url(\n                            self.build_tracking_url(match.group(1))\n                        )\n                else:\n                    file_to_write.flush()\n                    sleep(time_to_sleep)\n\n        track_proc = Process(target=_track_url_by_pattern)\n        try:\n            track_proc.start()\n            with ExternalProgramRunContext(main_proc):\n                yield main_proc\n        finally:\n            # need to wait a bit to let the subprocess read the last lines\n            track_proc.join(time_to_sleep * 2)\n            if track_proc.is_alive():\n                track_proc.terminate()\n            pipe_to_read.close()", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Monitors a subprocess's output stream to detect URLs matching a pattern, sets a tracking URL when found, and optionally logs output while managing subprocess execution context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__enter__", "line_number": 215, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "Sets up a context to handle SIGTERM signals by replacing the default handler with a custom termination method, enabling controlled cleanup during external program execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__exit__", "line_number": 220, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "Exits the ExternalProgramRunContext, handling keyboard interrupts by terminating the job and restoring the previous SIGTERM signal handler. This ensures proper cleanup during external program execution interruptions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "kill_job", "line_number": 225, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        self.proc.kill()\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "Terminates the associated external process and exits with a code reflecting the received signal if provided, enabling controlled shutdown and cleanup in external program management contexts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__str__", "line_number": 241, "body": "def __str__(self):\n        info = self.message\n        info += '\\nCOMMAND: {}'.format(' '.join(self.args))\n        info += '\\nSTDOUT: {}'.format(self.out or '[empty]')\n        info += '\\nSTDERR: {}'.format(self.err or '[empty]')\n        env_string = None\n        if self.env:\n            env_string = ' '.join(['='.join([k, '\\'{}\\''.format(v)]) for k, v in self.env.items()])\n        info += '\\nENVIRONMENT: {}'.format(env_string or '[empty]')\n        # reset terminal color in case the ENVIRONMENT changes colors\n        info += '\\033[m'\n        return info", "is_method": true, "class_name": "ExternalProgramRunError", "function_description": "Provides a detailed string representation of an ExternalProgramRunError, including command, output, error messages, and environment variables to aid in debugging external program failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "program_environment", "line_number": 275, "body": "def program_environment(self):\n        env = super(ExternalPythonProgramTask, self).program_environment()\n\n        if self.extra_pythonpath:\n            pythonpath = ':'.join([self.extra_pythonpath, env.get('PYTHONPATH', '')])\n            env.update({'PYTHONPATH': pythonpath})\n\n        if self.virtualenv:\n            # Make the same changes to the env that a normal venv/bin/activate script would\n            path = ':'.join(['{}/bin'.format(self.virtualenv), env.get('PATH', '')])\n            env.update({\n                'PATH': path,\n                'VIRTUAL_ENV': self.virtualenv\n            })\n            # remove PYTHONHOME env variable, if it exists\n            env.pop('PYTHONHOME', None)\n\n        return env", "is_method": true, "class_name": "ExternalPythonProgramTask", "function_description": "Builds and returns the execution environment for an external Python program, incorporating custom Python paths and virtual environment settings to ensure proper dependency resolution during runtime."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_track_url_by_pattern", "line_number": 179, "body": "def _track_url_by_pattern():\n            \"\"\"\n            Scans the pipe looking for a passed pattern, if the pattern is found, `set_tracking_url` callback is sent.\n            If tmp_stdout is passed, also appends lines to this file.\n            \"\"\"\n            pattern = re.compile(self.tracking_url_pattern)\n            for new_line in iter(pipe_to_read.readline, ''):\n                if new_line:\n                    if file_to_write:\n                        file_to_write.write(new_line)\n                    match = re.search(pattern, new_line.decode('utf-8'))\n                    if match:\n                        self.set_tracking_url(\n                            self.build_tracking_url(match.group(1))\n                        )\n                else:\n                    file_to_write.flush()\n                    sleep(time_to_sleep)", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Monitors output for a specific pattern and triggers a callback with a constructed URL upon a match; optionally logs output lines to a file during scanning."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "s3", "line_number": 91, "body": "def s3(self):\n        # only import boto3 when needed to allow top-lvl s3 module import\n        import boto3\n\n        options = dict(self._options)\n\n        if self._s3:\n            return self._s3\n\n        aws_access_key_id = options.get('aws_access_key_id')\n        aws_secret_access_key = options.get('aws_secret_access_key')\n\n        # Removing key args would break backwards compatibility\n        role_arn = options.get('aws_role_arn')\n        role_session_name = options.get('aws_role_session_name')\n\n        # In case the aws_session_token is provided use it\n        aws_session_token = options.get('aws_session_token')\n\n        if role_arn and role_session_name:\n            sts_client = boto3.client('sts')\n            assumed_role = sts_client.assume_role(RoleArn=role_arn,\n                                                  RoleSessionName=role_session_name)\n            aws_secret_access_key = assumed_role['Credentials'].get(\n                'SecretAccessKey')\n            aws_access_key_id = assumed_role['Credentials'].get('AccessKeyId')\n            aws_session_token = assumed_role['Credentials'].get('SessionToken')\n            logger.debug('using aws credentials via assumed role {} as defined in luigi config'\n                         .format(role_session_name))\n\n        for key in ['aws_access_key_id', 'aws_secret_access_key',\n                    'aws_role_session_name', 'aws_role_arn', 'aws_session_token']:\n            if key in options:\n                options.pop(key)\n\n        # At this stage, if no credentials provided, boto3 would handle their resolution for us\n        # For finding out about the order in which it tries to find these credentials\n        # please see here details\n        # http://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials\n\n        if not (aws_access_key_id and aws_secret_access_key):\n            logger.debug('no credentials provided, delegating credentials resolution to boto3')\n\n        try:\n            self._s3 = boto3.resource('s3',\n                                      aws_access_key_id=aws_access_key_id,\n                                      aws_secret_access_key=aws_secret_access_key,\n                                      aws_session_token=aws_session_token,\n                                      **options)\n        except TypeError as e:\n            logger.error(e.args[0])\n            if 'got an unexpected keyword argument' in e.args[0]:\n                raise DeprecatedBotoClientException(\n                    \"Now using boto3. Check that you're passing the correct arguments\")\n            raise\n\n        return self._s3", "is_method": true, "class_name": "S3Client", "function_description": "Provides a cached boto3 S3 resource client with support for AWS credentials and optional assumed roles, enabling seamless interaction with AWS S3 services while managing credential resolution and compatibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "s3", "line_number": 150, "body": "def s3(self, value):\n        self._s3 = value", "is_method": true, "class_name": "S3Client", "function_description": "Setter method in S3Client that assigns a given value to the internal S3 attribute, enabling configuration or updating of the S3 client instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "exists", "line_number": 153, "body": "def exists(self, path):\n        \"\"\"\n        Does provided path exist on S3?\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # root always exists\n        if self._is_root(key):\n            return True\n\n        # file\n        if self._exists(bucket, key):\n            return True\n\n        if self.isdir(path):\n            return True\n\n        logger.debug('Path %s does not exist', path)\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "Checks if a specified path exists in an S3 storage, verifying bucket roots, files, or directories. It provides a simple way to confirm resource availability in S3 for other operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "remove", "line_number": 173, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove a file or directory from S3.\n        :param path: File or directory to remove\n        :param recursive: Boolean indicator to remove object and children\n        :return: Boolean indicator denoting success of the removal of 1 or more files\n        \"\"\"\n        if not self.exists(path):\n            logger.debug('Could not delete %s; path does not exist', path)\n            return False\n\n        (bucket, key) = self._path_to_bucket_and_key(path)\n        s3_bucket = self.s3.Bucket(bucket)\n        # root\n        if self._is_root(key):\n            raise InvalidDeleteException('Cannot delete root of bucket at path %s' % path)\n\n        # file\n        if self._exists(bucket, key):\n            self.s3.meta.client.delete_object(Bucket=bucket, Key=key)\n            logger.debug('Deleting %s from bucket %s', key, bucket)\n            return True\n\n        if self.isdir(path) and not recursive:\n            raise InvalidDeleteException('Path %s is a directory. Must use recursive delete' % path)\n\n        delete_key_list = [{'Key': obj.key} for obj in s3_bucket.objects.filter(Prefix=self._add_path_delimiter(key))]\n\n        # delete the directory marker file if it exists\n        if self._exists(bucket, '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)):\n            delete_key_list.append({'Key': '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)})\n\n        if len(delete_key_list) > 0:\n            n = 1000\n            for i in range(0, len(delete_key_list), n):\n                self.s3.meta.client.delete_objects(Bucket=bucket, Delete={'Objects': delete_key_list[i: i + n]})\n            return True\n\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "Provides a way to delete files or directories from an S3 bucket, supporting recursive removal of directories and ensuring safety by preventing root bucket deletion. Useful for managing and cleaning cloud storage contents programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "move", "line_number": 213, "body": "def move(self, source_path, destination_path, **kwargs):\n        \"\"\"\n        Rename/move an object from one S3 location to another.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param kwargs: Keyword arguments are passed to the boto3 function `copy`\n        \"\"\"\n        self.copy(source_path, destination_path, **kwargs)\n        self.remove(source_path)", "is_method": true, "class_name": "S3Client", "function_description": "Utility method of S3Client that moves an object by copying it to a new S3 location and deleting the original, enabling efficient renaming or relocation of S3 files or directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_key", "line_number": 223, "body": "def get_key(self, path):\n        \"\"\"\n        Returns the object summary at the path\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        if self._exists(bucket, key):\n            return self.s3.ObjectSummary(bucket, key)", "is_method": true, "class_name": "S3Client", "function_description": "Provides a summary object for an S3 object at a given path if it exists, enabling quick access to metadata without retrieving the full object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put", "line_number": 232, "body": "def put(self, local_path, destination_s3_path, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path.\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        # put the file\n        self.put_multipart(local_path, destination_s3_path, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Uploads a local file to a specified S3 location, supporting additional S3 put_object parameters. This method facilitates transferring local files into S3 storage within the S3Client class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put_string", "line_number": 244, "body": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Provides the capability to upload a string as an object to a specified S3 bucket path, supporting additional options via boto3 parameters. This function simplifies storing text data directly to AWS S3 locations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put_multipart", "line_number": 258, "body": "def put_multipart(self, local_path, destination_s3_path, part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path\n        using S3 multi-part upload (for files > 8Mb).\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param part_size: Part size in bytes. Default: 8388608 (8MB)\n        :param kwargs: Keyword arguments are passed to the boto function `upload_fileobj` as ExtraArgs\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        from boto3.s3.transfer import TransferConfig\n        # default part size for boto3 is 8Mb, changing it to fit part_size\n        # provided as a parameter\n        transfer_config = TransferConfig(multipart_chunksize=part_size)\n\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        self.s3.meta.client.upload_fileobj(\n            Fileobj=open(local_path, 'rb'), Bucket=bucket, Key=key, Config=transfer_config, ExtraArgs=kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Uploads large local files to an S3 location using multipart upload, allowing efficient transfer of files larger than 8MB with configurable part sizes and additional upload options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "copy", "line_number": 279, "body": "def copy(self, source_path, destination_path, threads=DEFAULT_THREADS, start_time=None, end_time=None,\n             part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Copy object(s) from one S3 location to another. Works for individual keys or entire directories.\n        When files are larger than `part_size`, multipart uploading will be used.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param threads: Optional argument to define the number of threads to use when copying (min: 3 threads)\n        :param start_time: Optional argument to copy files with modified dates after start_time\n        :param end_time: Optional argument to copy files with modified dates before end_time\n        :param part_size: Part size in bytes\n        :param kwargs: Keyword arguments are passed to the boto function `copy` as ExtraArgs\n        :returns tuple (number_of_files_copied, total_size_copied_in_bytes)\n        \"\"\"\n\n        # don't allow threads to be less than 3\n        threads = 3 if threads < 3 else threads\n\n        if self.isdir(source_path):\n            return self._copy_dir(source_path, destination_path, threads=threads,\n                                  start_time=start_time, end_time=end_time, part_size=part_size, **kwargs)\n\n        # If the file isn't a directory just perform a simple copy\n        else:\n            return self._copy_file(source_path, destination_path, threads=threads, part_size=part_size, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Utility method of the S3Client class that copies files or directories between S3 locations, supporting optional multithreading, multipart uploads for large files, and filtering by modification date range. It returns the count and total size of files copied."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_copy_file", "line_number": 305, "body": "def _copy_file(self, source_path, destination_path, threads=DEFAULT_THREADS, part_size=DEFAULT_PART_SIZE, **kwargs):\n        src_bucket, src_key = self._path_to_bucket_and_key(source_path)\n        dst_bucket, dst_key = self._path_to_bucket_and_key(destination_path)\n        transfer_config = TransferConfig(max_concurrency=threads, multipart_chunksize=part_size)\n        item = self.get_key(source_path)\n        copy_source = {\n            'Bucket': src_bucket,\n            'Key': src_key\n        }\n\n        self.s3.meta.client.copy(copy_source, dst_bucket, dst_key, Config=transfer_config, ExtraArgs=kwargs)\n\n        return 1, item.size", "is_method": true, "class_name": "S3Client", "function_description": "Utility method of the S3Client class that copies a file between S3 locations using configurable concurrency and multipart upload settings for efficient data transfer. It returns the copy operation status and size of the source file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_copy_dir", "line_number": 319, "body": "def _copy_dir(self, source_path, destination_path, threads=DEFAULT_THREADS,\n                  start_time=None, end_time=None, part_size=DEFAULT_PART_SIZE, **kwargs):\n        start = datetime.datetime.now()\n        copy_jobs = []\n        management_pool = ThreadPool(processes=threads)\n        transfer_config = TransferConfig(max_concurrency=threads, multipart_chunksize=part_size)\n        src_bucket, src_key = self._path_to_bucket_and_key(source_path)\n        dst_bucket, dst_key = self._path_to_bucket_and_key(destination_path)\n        src_prefix = self._add_path_delimiter(src_key)\n        dst_prefix = self._add_path_delimiter(dst_key)\n        key_path_len = len(src_prefix)\n        total_size_bytes = 0\n        total_keys = 0\n        for item in self.list(source_path, start_time=start_time, end_time=end_time, return_key=True):\n            path = item.key[key_path_len:]\n            # prevents copy attempt of empty key in folder\n            if path != '' and path != '/':\n                total_keys += 1\n                total_size_bytes += item.size\n                copy_source = {\n                    'Bucket': src_bucket,\n                    'Key': src_prefix + path\n                }\n                the_kwargs = {'Config': transfer_config, 'ExtraArgs': kwargs}\n                job = management_pool.apply_async(self.s3.meta.client.copy,\n                                                  args=(copy_source, dst_bucket, dst_prefix + path),\n                                                  kwds=the_kwargs)\n                copy_jobs.append(job)\n        # Wait for the pools to finish scheduling all the copies\n        management_pool.close()\n        management_pool.join()\n        # Raise any errors encountered in any of the copy processes\n        for result in copy_jobs:\n            result.get()\n        end = datetime.datetime.now()\n        duration = end - start\n        logger.info('%s : Complete : %s total keys copied in %s' %\n                    (datetime.datetime.now(), total_keys, duration))\n        return total_keys, total_size_bytes", "is_method": true, "class_name": "S3Client", "function_description": "Core utility method of the S3Client class that copies all objects from one S3 directory path to another, supporting multi-threaded transfers with optional filtering by modification time and customizable part size for efficient large file handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get", "line_number": 359, "body": "def get(self, s3_path, destination_local_path):\n        \"\"\"\n        Get an object stored in S3 and write it to a local path.\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        # download the file\n        self.s3.meta.client.download_file(bucket, key, destination_local_path)", "is_method": true, "class_name": "S3Client", "function_description": "Method of S3Client that downloads a file from a specified S3 path and saves it to a given local filesystem path, facilitating data retrieval from cloud storage to local environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_as_bytes", "line_number": 367, "body": "def get_as_bytes(self, s3_path):\n        \"\"\"\n        Get the contents of an object stored in S3 as bytes\n\n        :param s3_path: URL for target S3 location\n        :return: File contents as pure bytes\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        obj = self.s3.Object(bucket, key)\n        contents = obj.get()['Body'].read()\n        return contents", "is_method": true, "class_name": "S3Client", "function_description": "Utility method of the S3Client class that retrieves the raw byte content of a file stored at a given S3 path, enabling direct access to S3 objects' data for further processing or storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_as_string", "line_number": 379, "body": "def get_as_string(self, s3_path, encoding='utf-8'):\n        \"\"\"\n        Get the contents of an object stored in S3 as string.\n\n        :param s3_path: URL for target S3 location\n        :param encoding: Encoding to decode bytes to string\n        :return: File contents as a string\n        \"\"\"\n        content = self.get_as_bytes(s3_path)\n        return content.decode(encoding)", "is_method": true, "class_name": "S3Client", "function_description": "This function provides a convenient way to fetch and decode the contents of an S3 object into a string, supporting customizable text encoding. It simplifies reading S3-stored files as text for further processing or display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "isdir", "line_number": 390, "body": "def isdir(self, path):\n        \"\"\"\n        Is the parameter S3 path a directory?\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        s3_bucket = self.s3.Bucket(bucket)\n\n        # root is a directory\n        if self._is_root(key):\n            return True\n\n        for suffix in (S3_DIRECTORY_MARKER_SUFFIX_0,\n                       S3_DIRECTORY_MARKER_SUFFIX_1):\n            try:\n                self.s3.meta.client.get_object(\n                    Bucket=bucket, Key=key + suffix)\n            except botocore.exceptions.ClientError as e:\n                if not e.response['Error']['Code'] in ['NoSuchKey', '404']:\n                    raise\n            else:\n                return True\n\n        # files with this prefix\n        key_path = self._add_path_delimiter(key)\n        s3_bucket_list_result = list(itertools.islice(\n            s3_bucket.objects.filter(Prefix=key_path), 1))\n        if s3_bucket_list_result:\n            return True\n\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "Determines whether a given S3 path represents a directory by checking for specific directory markers or existence of objects under that path prefix. This helps clients differentiate between files and folders in S3's flat namespace."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "mkdir", "line_number": 424, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if raise_if_exists and self.isdir(path):\n            raise FileAlreadyExists()\n\n        bucket, key = self._path_to_bucket_and_key(path)\n        if self._is_root(key):\n            # isdir raises if the bucket doesn't exist; nothing to do here.\n            return\n\n        path = self._add_path_delimiter(path)\n\n        if not parents and not self.isdir(os.path.dirname(path)):\n            raise MissingParentDirectory()\n\n        return self.put_string(\"\", path)", "is_method": true, "class_name": "S3Client", "function_description": "Method of S3Client that creates a directory-like path in an S3 bucket, optionally ensuring parent directories exist or raising errors if the path already exists. It provides directory management functionality in S3 storage context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "listdir", "line_number": 440, "body": "def listdir(self, path, start_time=None, end_time=None, return_key=False):\n        \"\"\"\n        Get an iterable with S3 folder contents.\n        Iterable contains paths relative to queried path.\n        :param path: URL for target S3 location\n        :param start_time: Optional argument to list files with modified (offset aware) datetime after start_time\n        :param end_time: Optional argument to list files with modified (offset aware) datetime before end_time\n        :param return_key: Optional argument, when set to True will return boto3's ObjectSummary (instead of the filename)\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # grab and validate the bucket\n        s3_bucket = self.s3.Bucket(bucket)\n\n        key_path = self._add_path_delimiter(key)\n        key_path_len = len(key_path)\n        for item in s3_bucket.objects.filter(Prefix=key_path):\n            last_modified_date = item.last_modified\n            if (\n                # neither are defined, list all\n                (not start_time and not end_time) or\n                # start defined, after start\n                (start_time and not end_time and start_time < last_modified_date) or\n                # end defined, prior to end\n                (not start_time and end_time and last_modified_date < end_time) or\n                (start_time and end_time and start_time <\n                 last_modified_date < end_time)  # both defined, between\n            ):\n                if return_key:\n                    yield item\n                else:\n                    yield self._add_path_delimiter(path) + item.key[key_path_len:]", "is_method": true, "class_name": "S3Client", "function_description": "Provides an iterable listing of files in an S3 bucket path, optionally filtered by modification time and able to return either filenames or full S3 object summaries. Useful for programmatically browsing and filtering S3 folder contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "list", "line_number": 473, "body": "def list(self, path, start_time=None, end_time=None, return_key=False):  # backwards compat\n        key_path_len = len(self._add_path_delimiter(path))\n        for item in self.listdir(path, start_time=start_time, end_time=end_time, return_key=return_key):\n            if return_key:\n                yield item\n            else:\n                yield item[key_path_len:]", "is_method": true, "class_name": "S3Client", "function_description": "Provides an iterable of items at a given S3 path, optionally filtered by time range and returning full keys or relative paths for flexible listing of stored objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_get_s3_config", "line_number": 482, "body": "def _get_s3_config(key=None):\n        defaults = dict(configuration.get_config().defaults())\n        try:\n            config = dict(configuration.get_config().items('s3'))\n        except (NoSectionError, KeyError):\n            return {}\n        # So what ports etc can be read without us having to specify all dtypes\n        for k, v in config.items():\n            try:\n                config[k] = int(v)\n            except ValueError:\n                pass\n        if key:\n            return config.get(key)\n        section_only = {k: v for k, v in config.items() if k not in defaults or v != defaults[k]}\n\n        return section_only", "is_method": true, "class_name": "S3Client", "function_description": "Utility method in S3Client that retrieves S3-related configuration settings, optionally returning a specific key's value or all settings differing from defaults. It aids in managing customized S3 configurations for client operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_path_to_bucket_and_key", "line_number": 501, "body": "def _path_to_bucket_and_key(path):\n        (scheme, netloc, path, query, fragment) = urlsplit(path,\n                                                           allow_fragments=False)\n        question_mark_plus_query = '?' + query if query else ''\n        path_without_initial_slash = path[1:] + question_mark_plus_query\n        return netloc, path_without_initial_slash", "is_method": true, "class_name": "S3Client", "function_description": "Private utility method within the S3Client class that extracts the bucket name and object key from an S3 URL path. It facilitates parsing S3 resource identifiers for use in other operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_is_root", "line_number": 509, "body": "def _is_root(key):\n        return (len(key) == 0) or (key == '/')", "is_method": true, "class_name": "S3Client", "function_description": "Private helper function of the S3Client class that checks whether a given key represents the root directory. It helps determine if a key is an empty string or a single slash."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_add_path_delimiter", "line_number": 513, "body": "def _add_path_delimiter(key):\n        return key if key[-1:] == '/' or key == '' else key + '/'", "is_method": true, "class_name": "S3Client", "function_description": "Helper method in S3Client that ensures a given key string ends with a forward slash, standardizing path delimiters for consistent S3 key handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_check_deprecated_argument", "line_number": 517, "body": "def _check_deprecated_argument(**kwargs):\n        \"\"\"\n        If `encrypt_key` or `host` is part of the arguments raise an exception\n        :return: None\n        \"\"\"\n        if 'encrypt_key' in kwargs:\n            raise DeprecatedBotoClientException(\n                'encrypt_key deprecated in boto3. Please refer to boto3 documentation for encryption details.')\n        if 'host' in kwargs:\n            raise DeprecatedBotoClientException(\n                'host keyword deprecated and is replaced by region_name in boto3.\\n'\n                'example: region_name=us-west-1\\n'\n                'For region names, refer to the amazon S3 region documentation\\n'\n                'https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region')", "is_method": true, "class_name": "S3Client", "function_description": "Utility method in S3Client that validates arguments by raising exceptions if deprecated parameters like 'encrypt_key' or 'host' are used, helping enforce up-to-date boto3 usage conventions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_exists", "line_number": 532, "body": "def _exists(self, bucket, key):\n        try:\n            self.s3.Object(bucket, key).load()\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] in ['NoSuchKey', '404']:\n                return False\n            else:\n                raise\n\n        return True", "is_method": true, "class_name": "S3Client", "function_description": "Internal method of S3Client that checks whether a specified object key exists within a given S3 bucket, returning True if found and False if not."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "move_to_final_destination", "line_number": 556, "body": "def move_to_final_destination(self):\n        self.s3_client.put_multipart(\n            self.tmp_path, self.path, **self.s3_options)", "is_method": true, "class_name": "AtomicS3File", "function_description": "Moves a multipart-uploaded file from a temporary S3 location to its final destination path, finalizing the upload process within the AtomicS3File context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "read", "line_number": 568, "body": "def read(self, size=None):\n        f = self.s3_key.read(size)\n        return f", "is_method": true, "class_name": "ReadableS3File", "function_description": "Utility method of ReadableS3File that reads and returns data from an S3 object, optionally limited by size, facilitating direct access to remote file contents stored in S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "close", "line_number": 572, "body": "def close(self):\n        self.s3_key.close()\n        self.closed = True", "is_method": true, "class_name": "ReadableS3File", "function_description": "Method of the ReadableS3File class that closes the underlying S3 file resource and marks the file as closed to prevent further operations. It manages resource cleanup for S3-based file access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__del__", "line_number": 576, "body": "def __del__(self):\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Destructor method for ReadableS3File that ensures the file is properly closed when the object is deleted to release resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__exit__", "line_number": 579, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Ensures the ReadableS3File resource is properly closed when exiting a context manager block, supporting safe and automatic cleanup of file resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__enter__", "line_number": 582, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableS3File", "function_description": "Enables the ReadableS3File instance to be used within a context manager, ensuring proper resource management during file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_add_to_buffer", "line_number": 585, "body": "def _add_to_buffer(self, line):\n        self.buffer.append(line)", "is_method": true, "class_name": "ReadableS3File", "function_description": "Adds a line of text to the internal buffer of the ReadableS3File instance. This supports accumulating data for subsequent processing or retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_flush_buffer", "line_number": 588, "body": "def _flush_buffer(self):\n        output = b''.join(self.buffer)\n        self.buffer = []\n        return output", "is_method": true, "class_name": "ReadableS3File", "function_description": "Internal utility of ReadableS3File that combines buffered byte chunks into a single byte string and clears the buffer, supporting efficient sequential reading from an S3 file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "readable", "line_number": 593, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableS3File", "function_description": "This method indicates if the S3 file is readable. It provides a simple boolean status to check read permissions or accessibility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "writable", "line_number": 596, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableS3File", "function_description": "Indicates that the ReadableS3File instance does not support write operations, confirming it is read-only."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "seekable", "line_number": 599, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableS3File", "function_description": "Indicates whether the file represented by ReadableS3File supports random access. This function always returns False, signaling that seeking is not supported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__iter__", "line_number": 602, "body": "def __iter__(self):\n        key_iter = self.s3_key.__iter__()\n\n        has_next = True\n        while has_next:\n            try:\n                # grab the next chunk\n                chunk = next(key_iter)\n\n                # split on newlines, preserving the newline\n                for line in chunk.splitlines(True):\n\n                    if not line.endswith(os.linesep):\n                        # no newline, so store in buffer\n                        self._add_to_buffer(line)\n                    else:\n                        # newline found, send it out\n                        if self.buffer:\n                            self._add_to_buffer(line)\n                            yield self._flush_buffer()\n                        else:\n                            yield line\n            except StopIteration:\n                # send out anything we have left in the buffer\n                output = self._flush_buffer()\n                if output:\n                    yield output\n                has_next = False\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Provides an iterator over lines of an S3 file, yielding complete lines including buffered partial lines across chunks. This enables efficient line-by-line reading of large remote files from S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "open", "line_number": 652, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            s3_key = self.fs.get_key(self.path)\n            if not s3_key:\n                raise FileNotFoundException(\n                    \"Could not find file at %s\" % self.path)\n\n            fileobj = ReadableS3File(s3_key)\n            return self.format.pipe_reader(fileobj)\n        else:\n            return self.format.pipe_writer(AtomicS3File(self.path, self.fs, **self.s3_options))", "is_method": true, "class_name": "S3Target", "function_description": "Provides a file-like interface to read from or write to a file stored in S3, handling access modes and returning appropriate stream objects for integration with data processing formats."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "exists", "line_number": 710, "body": "def exists(self):\n        hadoopSemaphore = self.path + self.flag\n        return self.fs.exists(hadoopSemaphore)", "is_method": true, "class_name": "S3FlagTarget", "function_description": "Checks if a specific flag file exists at a given path in the filesystem. This function is useful for verifying the presence of semaphore or marker files in S3 or Hadoop-based storage systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 731, "body": "def output(self):\n        return S3Target(self.path)", "is_method": true, "class_name": "S3PathTask", "function_description": "Returns an S3Target object representing the S3 path associated with the task, enabling downstream tasks to access or manipulate data at that location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 741, "body": "def output(self):\n        return S3EmrTarget(self.path)", "is_method": true, "class_name": "S3EmrTask", "function_description": "This method provides an S3EmrTarget object representing the output destination associated with the task's path. It serves as a convenient way to access the task's designated output location in S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 752, "body": "def output(self):\n        return S3FlagTarget(self.path, flag=self.flag)", "is_method": true, "class_name": "S3FlagTask", "function_description": "Creates and returns an S3FlagTarget object configured with the instance's path and flag, encapsulating the task's target information for further processing or identification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "fix_paths", "line_number": 33, "body": "def fix_paths(job):\n    \"\"\"\n    Coerce input arguments to use temporary files when used for output.\n\n    Return a list of temporary file pairs (tmpfile, destination path) and\n    a list of arguments.\n\n    Converts each HdfsTarget to a string for the path.\n    \"\"\"\n    tmp_files = []\n    args = []\n    for x in job.args():\n        if isinstance(x, luigi.contrib.hdfs.HdfsTarget):  # input/output\n            if x.exists() or not job.atomic_output():  # input\n                args.append(x.path)\n            else:  # output\n                x_path_no_slash = x.path[:-1] if x.path[-1] == '/' else x.path\n                y = luigi.contrib.hdfs.HdfsTarget(x_path_no_slash + '-luigi-tmp-%09d' % random.randrange(0, 1e10))\n                tmp_files.append((y, x_path_no_slash))\n                logger.info('Using temp path: %s for path %s', y.path, x.path)\n                args.append(y.path)\n        else:\n            try:\n                # hopefully the target has a path to use\n                args.append(x.path)\n            except AttributeError:\n                # if there's no path then hope converting it to a string will work\n                args.append(str(x))\n\n    return (tmp_files, args)", "is_method": false, "function_description": "Utility function that prepares job arguments by replacing output paths with temporary file paths for safe atomic writes, returning the temp-to-final file mappings alongside adjusted argument paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "run_job", "line_number": 77, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        # TODO(jcrobak): libjars, files, etc. Can refactor out of\n        # hadoop.HadoopJobRunner\n        if not job.jar():\n            raise HadoopJarJobError(\"Jar not defined\")\n\n        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', job.jar()]\n        if job.main():\n            hadoop_arglist.append(job.main())\n\n        jobconfs = job.jobconfs()\n\n        for jc in jobconfs:\n            hadoop_arglist += ['-D' + jc]\n\n        (tmp_files, job_args) = fix_paths(job)\n\n        hadoop_arglist += job_args\n\n        ssh_config = job.ssh()\n        if ssh_config:\n            host = ssh_config.get(\"host\", None)\n            key_file = ssh_config.get(\"key_file\", None)\n            username = ssh_config.get(\"username\", None)\n            if not host or not key_file or not username:\n                raise HadoopJarJobError(\"missing some config for HadoopRemoteJarJobRunner\")\n            arglist = ['ssh', '-i', key_file,\n                       '-o', 'BatchMode=yes']  # no password prompts etc\n            if ssh_config.get(\"no_host_key_check\", False):\n                arglist += ['-o', 'UserKnownHostsFile=/dev/null',\n                            '-o', 'StrictHostKeyChecking=no']\n            arglist.append('{}@{}'.format(username, host))\n            hadoop_arglist = [pipes.quote(arg) for arg in hadoop_arglist]\n            arglist.append(' '.join(hadoop_arglist))\n        else:\n            if not os.path.exists(job.jar()):\n                logger.error(\"Can't find jar: %s, full path %s\", job.jar(),\n                             os.path.abspath(job.jar()))\n                raise HadoopJarJobError(\"job jar does not exist\")\n            arglist = hadoop_arglist\n\n        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url)\n\n        for a, b in tmp_files:\n            a.move(b)", "is_method": true, "class_name": "HadoopJarJobRunner", "function_description": "Executes a Hadoop jar job by constructing and running the appropriate command, supporting local or remote execution with SSH, and handles job configuration and tracking URL integration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "jar", "line_number": 133, "body": "def jar(self):\n        \"\"\"\n        Path to the jar for this Hadoop Job.\n        \"\"\"\n        return None", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Returns the file path to the jar associated with a Hadoop job task. This method provides the location of the executable jar needed to run the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "job_runner", "line_number": 145, "body": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        return HadoopJarJobRunner()", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Returns a new instance of HadoopJarJobRunner, providing a customizable entry point for running Hadoop jar jobs within the HadoopJarJobTask framework."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "atomic_output", "line_number": 149, "body": "def atomic_output(self):\n        \"\"\"\n        If True, then rewrite output arguments to be temp locations and\n        atomically move them into place after the job finishes.\n        \"\"\"\n        return True", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Returns a boolean indicating that output files should be written atomically by using temporary locations before moving them to final destinations, ensuring safe job output handling in HadoopJarJobTask."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "args", "line_number": 163, "body": "def args(self):\n        \"\"\"\n        Returns an array of args to pass to the job (after hadoop jar <jar> <main>).\n        \"\"\"\n        return []", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Returns the list of command-line arguments for a Hadoop jar job invocation after specifying the jar and main class. Typically used to customize job execution parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "run", "line_number": 137, "body": "def run(cmd, task=None):\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            close_fds=True\n        )\n        output_lines = []\n        while True:\n            line = process.stdout.readline()\n            if not line:\n                break\n            line = line.decode(\"utf-8\")\n            output_lines += [line]\n            logger.info(line.rstrip(\"\\n\"))\n        process.stdout.close()\n        exit_code = process.wait()\n        if exit_code:\n            output = \"\".join(output_lines)\n            raise subprocess.CalledProcessError(exit_code, cmd, output=output)", "is_method": true, "class_name": "_CmdLineRunner", "function_description": "Provides functionality to execute a shell command, capture and log its output in real-time, and raise an error if the command fails. Useful for running external processes with output monitoring and error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "args", "line_number": 248, "body": "def args(self):\n        \"\"\"\n        Extra String arguments that will be passed to your Dataflow job.\n        For example:\n\n        return ['--setup_file=setup.py']\n        \"\"\"\n        return []", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Returns a list of extra string arguments to be passed to a Dataflow job, allowing customization of job execution parameters. Defaults to an empty list if no additional arguments are specified."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "validate_output", "line_number": 271, "body": "def validate_output(self):\n        \"\"\"\n        Callback that can be used to validate your output before it is moved to\n        its final location. Returning false here will cause the job to fail, and\n        output to be removed instead of published.\n        \"\"\"\n        return True", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Method of the BeamDataflowJobTask class that allows validation of job output before finalization, enabling conditional job failure to prevent publishing invalid results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "file_pattern", "line_number": 279, "body": "def file_pattern(self):\n        \"\"\"\n        If one/some of the input target files are not in the pattern of part-*,\n        we can add the key of the required target and the correct file pattern\n        that should be appended in the command line here. If the input target key is not found\n        in this dict, the file pattern will be assumed to be part-* for that target.\n\n        :return A dictionary of overridden file pattern that is not part-* for the inputs\n        \"\"\"\n        return {}", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Returns a dictionary specifying custom file patterns for input targets that deviate from the default \"part-*\" naming convention, supporting flexible file matching in Beam dataflow tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "run", "line_number": 304, "body": "def run(self):\n        cmd_line = self._mk_cmd_line()\n        logger.info(' '.join(cmd_line))\n\n        self.before_run()\n\n        try:\n            self.cmd_line_runner.run(cmd_line, self)\n        except subprocess.CalledProcessError as e:\n            logger.error(e, exc_info=True)\n            self.cleanup_on_error(e)\n            os._exit(e.returncode)\n\n        self.on_successful_run()\n\n        if self.validate_output():\n            self.on_successful_output_validation()\n        else:\n            error = ValueError(\"Output validation failed\")\n            self.cleanup_on_error(error)\n            raise error", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Executes a Beam Dataflow job by constructing and running its command line, managing pre- and post-run steps, handling errors, and validating the job's output to ensure successful completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_mk_cmd_line", "line_number": 326, "body": "def _mk_cmd_line(self):\n        cmd_line = self.dataflow_executable()\n\n        cmd_line.extend(self._get_dataflow_args())\n        cmd_line.extend(self.args())\n        cmd_line.extend(self._format_input_args())\n        cmd_line.extend(self._format_output_args())\n        return cmd_line", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Constructs the complete command-line argument list to execute a Dataflow job, combining executable path and all input, output, and configuration parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_get_runner", "line_number": 335, "body": "def _get_runner(self):\n        if not self.runner:\n            logger.warning(\"Runner not supplied to BeamDataflowJobTask. \" +\n                           \"Defaulting to DirectRunner.\")\n            return \"DirectRunner\"\n        elif self.runner in [\n            \"DataflowRunner\",\n            \"DirectRunner\"\n        ]:\n            return self.runner\n        else:\n            raise ValueError(\"Runner %s is unsupported.\" % self.runner)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Internal method of BeamDataflowJobTask that validates and returns the current runner type, defaulting to \"DirectRunner\" if none is provided, ensuring compatibility with supported runners."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_get_dataflow_args", "line_number": 348, "body": "def _get_dataflow_args(self):\n        def f(key, value):\n            return '--{}={}'.format(key, value)\n\n        output = []\n\n        output.append(f(self.dataflow_params.runner, self._get_runner()))\n\n        if self.project:\n            output.append(f(self.dataflow_params.project, self.project))\n        if self.zone:\n            output.append(f(self.dataflow_params.zone, self.zone))\n        if self.region:\n            output.append(f(self.dataflow_params.region, self.region))\n        if self.staging_location:\n            output.append(f(self.dataflow_params.staging_location, self.staging_location))\n        if self.temp_location:\n            output.append(f(self.dataflow_params.temp_location, self.temp_location))\n        if self.gcp_temp_location:\n            output.append(f(self.dataflow_params.gcp_temp_location, self.gcp_temp_location))\n        if self.num_workers:\n            output.append(f(self.dataflow_params.num_workers, self.num_workers))\n        if self.autoscaling_algorithm:\n            output.append(f(self.dataflow_params.autoscaling_algorithm, self.autoscaling_algorithm))\n        if self.max_num_workers:\n            output.append(f(self.dataflow_params.max_num_workers, self.max_num_workers))\n        if self.disk_size_gb:\n            output.append(f(self.dataflow_params.disk_size_gb, self.disk_size_gb))\n        if self.worker_machine_type:\n            output.append(f(self.dataflow_params.worker_machine_type, self.worker_machine_type))\n        if self.worker_disk_type:\n            output.append(f(self.dataflow_params.worker_disk_type, self.worker_disk_type))\n        if self.network:\n            output.append(f(self.dataflow_params.network, self.network))\n        if self.subnetwork:\n            output.append(f(self.dataflow_params.subnetwork, self.subnetwork))\n        if self.job_name:\n            output.append(f(self.dataflow_params.job_name, self.job_name))\n        if self.service_account:\n            output.append(f(self.dataflow_params.service_account, self.service_account))\n        if self.labels:\n            output.append(f(self.dataflow_params.labels, json.dumps(self.labels)))\n\n        return output", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Constructs a list of formatted command-line arguments from the BeamDataflowJobTask's configuration parameters for launching a Dataflow job. This facilitates dynamic assembly of job settings to customize execution environments and resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_format_input_args", "line_number": 393, "body": "def _format_input_args(self):\n        \"\"\"\n            Parses the result(s) of self.input() into a string-serialized\n            key-value list passed to the Dataflow job. Valid inputs include:\n\n            return FooTarget()\n\n            return {\"input1\": FooTarget(), \"input2\": FooTarget2())\n\n            return (\"input\", FooTarget())\n\n            return [(\"input1\", FooTarget()), (\"input2\": FooTarget2())]\n\n            return [FooTarget(), FooTarget2()]\n\n            Unlabeled input are passed in with under the default key \"input\".\n        \"\"\"\n        job_input = self.input()\n\n        if isinstance(job_input, luigi.Target):\n            job_input = {\"input\": job_input}\n\n        elif isinstance(job_input, tuple):\n            job_input = {job_input[0]: job_input[1]}\n\n        elif isinstance(job_input, list):\n            if all(isinstance(item, tuple) for item in job_input):\n                job_input = dict(job_input)\n            else:\n                job_input = {\"input\": job_input}\n\n        elif not isinstance(job_input, dict):\n            raise ValueError(\"Invalid job input requires(). Supported types: [\"\n                             \"Target, tuple of (name, Target), \"\n                             \"dict of (name: Target), list of Targets]\")\n\n        if not isinstance(self.file_pattern(), dict):\n            raise ValueError('file_pattern() must return a dict type')\n\n        input_args = []\n\n        for (name, targets) in job_input.items():\n            uris = [\n              self.get_target_path(uri_target) for uri_target in luigi.task.flatten(targets)\n            ]\n            if isinstance(targets, dict):\n                \"\"\"\n                If targets is a dict that means it had multiple outputs.\n                Make the input args in that case \"<input key>-<task output key>\"\n                \"\"\"\n                names = [\"%s-%s\" % (name, key) for key in targets.keys()]\n\n            else:\n                names = [name] * len(uris)\n\n            input_dict = {}\n\n            for (arg_name, uri) in zip(names, uris):\n                pattern = self.file_pattern().get(name, 'part-*')\n                input_value = input_dict.get(arg_name, [])\n                input_value.append(uri.rstrip('/') + '/' + pattern)\n                input_dict[arg_name] = input_value\n\n            for (key, paths) in input_dict.items():\n                input_args.append(\"--%s=%s\" % (key, ','.join(paths)))\n\n        return input_args", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Core method of the BeamDataflowJobTask class that processes various input formats into a standardized list of string arguments specifying Dataflow job input paths, enabling flexible task input specification and integration with downstream workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_format_output_args", "line_number": 461, "body": "def _format_output_args(self):\n        \"\"\"\n            Parses the result(s) of self.output() into a string-serialized\n            key-value list passed to the Dataflow job. Valid outputs include:\n\n            return FooTarget()\n\n            return {\"output1\": FooTarget(), \"output2\": FooTarget2()}\n\n            Unlabeled outputs are passed in with under the default key \"output\".\n        \"\"\"\n        job_output = self.output()\n        if isinstance(job_output, luigi.Target):\n            job_output = {\"output\": job_output}\n        elif not isinstance(job_output, dict):\n            raise ValueError(\n                \"Task output must be a Target or a dict from String to Target\")\n\n        output_args = []\n\n        for (name, target) in job_output.items():\n            uri = self.get_target_path(target)\n            output_args.append(\"--%s=%s\" % (name, uri))\n\n        return output_args", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Formats the outputs of a BeamDataflowJobTask into command-line arguments, converting task results into string key-value pairs for passing to a Dataflow job. It supports single or multiple labeled outputs as Luigi Targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "get_target_path", "line_number": 488, "body": "def get_target_path(target):\n        \"\"\"\n            Given a luigi Target, determine a stringly typed path to pass as a\n            Dataflow job argument.\n        \"\"\"\n        if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n            return target.path\n        elif isinstance(target, bigquery.BigQueryTarget):\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n        else:\n            raise ValueError(\"Target %s not supported\" % target)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Converts various Luigi target types into string paths suitable as Dataflow job arguments, supporting local files, GCS buckets, and BigQuery tables. This enables seamless integration of different data sources in Dataflow workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "f", "line_number": 349, "body": "def f(key, value):\n            return '--{}={}'.format(key, value)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Returns a formatted command-line argument string from a key-value pair, useful for constructing parameter options dynamically in data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sparkey.py", "function": "run", "line_number": 41, "body": "def run(self):\n        self._write_sparkey_file()", "is_method": true, "class_name": "SparkeyExportTask", "function_description": "Method of SparkeyExportTask that initiates the process of writing data to a Sparkey file, enabling data export in the Sparkey format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sparkey.py", "function": "_write_sparkey_file", "line_number": 44, "body": "def _write_sparkey_file(self):\n        import sparkey\n\n        infile = self.input()\n        outfile = self.output()\n        if not isinstance(outfile, luigi.LocalTarget):\n            raise TypeError(\"output must be a LocalTarget\")\n\n        # write job output to temporary sparkey file\n        temp_output = luigi.LocalTarget(is_tmp=True)\n        w = sparkey.LogWriter(temp_output.path)\n        for line in infile.open('r'):\n            k, v = line.strip().split(self.separator, 1)\n            w[k] = v\n        w.close()\n\n        # move finished sparkey file to final destination\n        temp_output.move(outfile.path)", "is_method": true, "class_name": "SparkeyExportTask", "function_description": "Private method of SparkeyExportTask that converts key-value pairs from the input into a Sparkey log file, writing to a temporary location before finalizing the output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "slot_to_dict", "line_number": 51, "body": "def slot_to_dict(o):\n    o_dict = {}\n    for key in o.__slots__:\n        if not key.startswith('__'):\n            value = getattr(o, key, None)\n            if value is not None:\n                o_dict[key] = value\n    return o_dict", "is_method": false, "function_description": "Utility function that converts an object with __slots__ into a dictionary of its non-private, non-None attributes for easy access or serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "name", "line_number": 186, "body": "def name(self):\n        \"\"\"Name for the job, need to be unique, required\"\"\"\n        return 'SklearnExample'", "is_method": true, "class_name": "PaiTask", "function_description": "Provides a unique, fixed name identifier for the PaiTask job, which is essential for distinguishing this task in job management or scheduling systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "image", "line_number": 192, "body": "def image(self):\n        \"\"\"URL pointing to the Docker image for all tasks in the job, required\"\"\"\n        return 'openpai/pai.example.sklearn'", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the fixed Docker image URL used for all tasks within a job, providing a consistent execution environment reference in the PaiTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "tasks", "line_number": 198, "body": "def tasks(self):\n        \"\"\"List of taskRole, one task role at least, required\"\"\"\n        return []", "is_method": true, "class_name": "PaiTask", "function_description": "Returns a list of defined task roles associated with the PaiTask instance, ensuring at least one task role is present."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "auth_file_path", "line_number": 203, "body": "def auth_file_path(self):\n        \"\"\"Docker registry authentication file existing on HDFS, optional\"\"\"\n        return None", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the optional file path of the Docker registry authentication file stored on HDFS. This provides access information for authenticated Docker registry interactions if available."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "data_dir", "line_number": 208, "body": "def data_dir(self):\n        \"\"\"Data directory existing on HDFS, optional\"\"\"\n        return None", "is_method": true, "class_name": "PaiTask", "function_description": "Returns None to indicate an optional HDFS data directory location; currently, this function does not provide a data directory path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "output_dir", "line_number": 218, "body": "def output_dir(self):\n        \"\"\"Output directory on HDFS, $PAI_DEFAULT_FS_URI/$jobName/output will be used if not specified, optional\"\"\"\n        return '$PAI_DEFAULT_FS_URI/{0}/output'.format(self.name)", "is_method": true, "class_name": "PaiTask", "function_description": "Provides the default HDFS output directory path for a job, using the job's name as part of the directory structure. This helps locate or set output locations in PaiTask workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "virtual_cluster", "line_number": 223, "body": "def virtual_cluster(self):\n        \"\"\"The virtual cluster job runs on. If omitted, the job will run on default virtual cluster, optional\"\"\"\n        return 'default'", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the name of the virtual cluster on which the job runs, defaulting to 'default' if unspecified. This enables job scheduling on specific or default virtual clusters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "gpu_type", "line_number": 228, "body": "def gpu_type(self):\n        \"\"\"Specify the GPU type to be used in the tasks. If omitted, the job will run on any gpu type, optional\"\"\"\n        return None", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the specified GPU type for a task or None if no preference is set, allowing flexible GPU resource allocation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "retry_count", "line_number": 233, "body": "def retry_count(self):\n        \"\"\"Job retry count, no less than 0, optional\"\"\"\n        return 0", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the number of times a job has been retried, ensuring the count is never negative. This method provides retry tracking for task management in the PaiTask class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "__init_token", "line_number": 237, "body": "def __init_token(self):\n        self.__openpai = OpenPai()\n\n        request_json = json.dumps({'username': self.__openpai.username, 'password': self.__openpai.password,\n                                   'expiration': self.__openpai.expiration})\n        logger.debug('Get token request {0}'.format(request_json))\n        response = rs.post(urljoin(self.__openpai.pai_url, '/api/v1/token'),\n                           headers={'Content-Type': 'application/json'}, data=request_json)\n        logger.debug('Get token response {0}'.format(response.text))\n        if response.status_code != 200:\n            msg = 'Get token request failed, response is {}'.format(response.text)\n            logger.error(msg)\n            raise Exception(msg)\n        else:\n            self.__token = response.json()['token']", "is_method": true, "class_name": "PaiTask", "function_description": "Private initialization method in PaiTask that authenticates with OpenPai and obtains an access token for subsequent API requests by submitting user credentials."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "__check_job_status", "line_number": 261, "body": "def __check_job_status(self):\n        response = rs.get(urljoin(self.__openpai.pai_url, '/api/v1/jobs/{0}'.format(self.name)))\n        logger.debug('Check job response {0}'.format(response.text))\n        if response.status_code == 404:\n            msg = 'Job {0} is not found'.format(self.name)\n            logger.debug(msg)\n            raise HTTPError(msg, response=response)\n        elif response.status_code != 200:\n            msg = 'Get job request failed, response is {}'.format(response.text)\n            logger.error(msg)\n            raise HTTPError(msg, response=response)\n        job_state = response.json()['jobStatus']['state']\n        if job_state in ['UNKNOWN', 'WAITING', 'RUNNING']:\n            logger.debug('Job {0} is running in state {1}'.format(self.name, job_state))\n            return False\n        else:\n            msg = 'Job {0} finished in state {1}'.format(self.name, job_state)\n            logger.info(msg)\n            if job_state == 'SUCCEED':\n                return True\n            else:\n                raise RuntimeError(msg)", "is_method": true, "class_name": "PaiTask", "function_description": "Checks the current status of a job in the PaiTask system, indicating whether it is still running or has finished successfully. It raises errors for job not found or failure states, supporting job monitoring and error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "run", "line_number": 284, "body": "def run(self):\n        job = PaiJob(self.name, self.image, self.tasks)\n        job.virtualCluster = self.virtual_cluster\n        job.authFile = self.auth_file_path\n        job.codeDir = self.code_dir\n        job.dataDir = self.data_dir\n        job.outputDir = self.output_dir\n        job.retryCount = self.retry_count\n        job.gpuType = self.gpu_type\n        request_json = json.dumps(job,  default=slot_to_dict)\n        logger.debug('Submit job request {0}'.format(request_json))\n        response = rs.post(urljoin(self.__openpai.pai_url, '/api/v1/jobs'),\n                           headers={'Content-Type': 'application/json',\n                                    'Authorization': 'Bearer {}'.format(self.__token)}, data=request_json)\n        logger.debug('Submit job response {0}'.format(response.text))\n        # 202 is success for job submission, see https://github.com/Microsoft/pai/blob/master/docs/rest-server/API.md\n        if response.status_code != 202:\n            msg = 'Submit job failed, response code is {0}, body is {1}'.format(response.status_code, response.text)\n            logger.error(msg)\n            raise HTTPError(msg, response=response)\n        while not self.__check_job_status():\n            time.sleep(self.__POLL_TIME)", "is_method": true, "class_name": "PaiTask", "function_description": "Class PaiTask method that submits a configured job to a remote job management system and continuously polls until the job completes, ensuring reliable execution and monitoring of distributed tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "complete", "line_number": 310, "body": "def complete(self):\n        try:\n            return self.__check_job_status()\n        except HTTPError:\n            return False\n        except RuntimeError:\n            return False", "is_method": true, "class_name": "PaiTask", "function_description": "Checks the current status of a PaiTask job, returning its completion status or False if an error occurs during the status check."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "accept_trailing_slash_in_existing_dirpaths", "line_number": 41, "body": "def accept_trailing_slash_in_existing_dirpaths(func):\n    @wraps(func)\n    def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            logger.warning(\"Dropbox paths should NOT have trailing slashes. This causes additional API calls\")\n            logger.warning(\"Consider modifying your calls to {}, so that they don't use paths than end with '/'\".format(func.__name__))\n\n            if self._exists_and_is_dir(path[:-1]):\n                path = path[:-1]\n\n        return func(self, path, *args, **kwargs)\n\n    return wrapped", "is_method": false, "function_description": "Decorator that normalizes existing directory paths by removing trailing slashes to prevent redundant API calls, while warning users to avoid paths ending with '/'. It enhances functions dealing with Dropbox path handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "accept_trailing_slash", "line_number": 56, "body": "def accept_trailing_slash(func):\n    @wraps(func)\n    def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            path = path[:-1]\n        return func(self, path, *args, **kwargs)\n\n    return wrapped", "is_method": false, "function_description": "Decorator function that normalizes URL paths by removing trailing slashes, ensuring consistent path handling for wrapped route-handling functions in web applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "exists", "line_number": 87, "body": "def exists(self, path):\n        if path == '/':\n            return True\n        if path.endswith('/'):\n            path = path[:-1]\n            return self._exists_and_is_dir(path)\n\n        try:\n            self.conn.files_get_metadata(path)\n            return True\n        except dropbox.exceptions.ApiError as e:\n            if isinstance(e.error.get_path(), dropbox.files.LookupError):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "DropboxClient", "function_description": "Checks if a specified path exists in Dropbox, distinguishing between files and directories. It provides a straightforward way to verify the presence of a file or folder in the Dropbox storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "remove", "line_number": 104, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        if not self.exists(path):\n            return False\n        self.conn.files_delete_v2(path)\n        return True", "is_method": true, "class_name": "DropboxClient", "function_description": "Utility method of DropboxClient that deletes a specified file or folder path from Dropbox, optionally handling recursive deletion and skipping trash, ensuring removal only if the path exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "mkdir", "line_number": 111, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            elif raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            else:\n                return\n\n        self.conn.files_create_folder_v2(path)", "is_method": true, "class_name": "DropboxClient", "function_description": "Creates a directory at the specified path in Dropbox, optionally creating parent directories. It handles existing paths by raising errors or skipping creation based on parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "isdir", "line_number": 123, "body": "def isdir(self, path):\n        if path == '/':\n            return True\n        try:\n            md = self.conn.files_get_metadata(path)\n            return isinstance(md, dropbox.files.FolderMetadata)\n        except dropbox.exceptions.ApiError as e:\n            if isinstance(e.error.get_path(), dropbox.files.LookupError):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "DropboxClient", "function_description": "Method of DropboxClient that checks if a given path corresponds to an existing directory in Dropbox, enabling validation of folder presence and type through metadata retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "listdir", "line_number": 136, "body": "def listdir(self, path, **kwargs):\n        dirs = []\n        lister = self.conn.files_list_folder(path, recursive=True, **kwargs)\n        dirs.extend(lister.entries)\n        while lister.has_more:\n            lister = self.conn.files_list_folder_continue(lister.cursor)\n            dirs.extend(lister.entries)\n        return [d.path_display for d in dirs]", "is_method": true, "class_name": "DropboxClient", "function_description": "Provides a complete list of all file and folder paths under a specified Dropbox directory, supporting recursive retrieval and pagination for large directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "move", "line_number": 146, "body": "def move(self, path, dest):\n        self.conn.files_move_v2(from_path=path, to_path=dest)", "is_method": true, "class_name": "DropboxClient", "function_description": "Utility method of DropboxClient that moves a file or folder from one path to another within Dropbox storage. It enables programmatic relocation of Dropbox items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "copy", "line_number": 150, "body": "def copy(self, path, dest):\n        self.conn.files_copy_v2(from_path=path, to_path=dest)", "is_method": true, "class_name": "DropboxClient", "function_description": "Utility method of DropboxClient that copies a file or folder from one path to another within Dropbox storage, facilitating file management and organization operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "download_as_bytes", "line_number": 153, "body": "def download_as_bytes(self, path):\n        metadata, response = self.conn.files_download(path)\n        return response.content", "is_method": true, "class_name": "DropboxClient", "function_description": "Downloads the content of a file from Dropbox at the given path and returns it as a byte sequence. This enables direct binary access to file data for further processing or storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "upload", "line_number": 157, "body": "def upload(self, tmp_path, dest_path):\n        with open(tmp_path, 'rb') as f:\n            file_size = os.path.getsize(tmp_path)\n\n            CHUNK_SIZE = 4 * 1000 * 1000\n            upload_session_start_result = self.conn.files_upload_session_start(f.read(CHUNK_SIZE))\n            commit = dropbox.files.CommitInfo(path=dest_path)\n            cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id,\n                                                       offset=f.tell())\n\n            if f.tell() >= file_size:\n                self.conn.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)\n                return\n\n            while f.tell() < file_size:\n                if (file_size - f.tell()) <= CHUNK_SIZE:\n                    self.conn.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)\n                else:\n                    self.conn.files_upload_session_append_v2(f.read(CHUNK_SIZE), cursor)\n                    cursor.offset = f.tell()", "is_method": true, "class_name": "DropboxClient", "function_description": "Uploads a file from a local temporary path to a specified Dropbox destination, handling large files by uploading them in chunks via Dropbox's upload session functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "_exists_and_is_dir", "line_number": 178, "body": "def _exists_and_is_dir(self, path):\n        \"\"\"\n        Auxiliary method, used by the 'accept_trailing_slash' and 'accept_trailing_slash_in_existing_dirpaths' decorators\n        :param path: a Dropbox path that does NOT ends with a '/' (even if it is a directory)\n        \"\"\"\n        if path == '/':\n            return True\n        try:\n            md = self.conn.files_get_metadata(path)\n            is_dir = isinstance(md, dropbox.files.FolderMetadata)\n            return is_dir\n        except dropbox.exceptions.ApiError:\n            return False", "is_method": true, "class_name": "DropboxClient", "function_description": "Helper method that checks whether a given Dropbox path exists and is a directory, supporting decorators that handle trailing slashes in directory paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "read", "line_number": 209, "body": "def read(self):\n        return self.client.download_as_bytes(self.path)", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Returns the file's content as bytes by downloading it from Dropbox. This method provides direct access to the file data for further processing or reading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__enter__", "line_number": 212, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Enables use of ReadableDropboxFile instances as context managers, supporting automatic resource management with the 'with' statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__exit__", "line_number": 215, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Special method that ensures the ReadableDropboxFile resource is properly closed when exiting a context manager block. It supports safe and automatic cleanup of file resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__del__", "line_number": 218, "body": "def __del__(self):\n        self.close()\n        if os.path.exists(self.download_file_location):\n            os.remove(self.download_file_location)", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Destructor method that ensures the file is closed and its local downloaded copy is deleted when a ReadableDropboxFile instance is destroyed, helping to manage resource cleanup automatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "close", "line_number": 223, "body": "def close(self):\n        self.closed = True", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Marks the ReadableDropboxFile instance as closed, signaling that no further operations should be performed on the file. This method is useful for managing file lifecycle and resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "readable", "line_number": 226, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Indicates that the ReadableDropboxFile instance supports reading operations. This method confirms the file's readability status for other components interacting with it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "writable", "line_number": 229, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Indicates that the ReadableDropboxFile instance does not support writing operations. This method clarifies the file's read-only status for other components."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "seekable", "line_number": 232, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Indicates whether the ReadableDropboxFile supports seeking within the file. This method returns False, showing that seeking operations are not supported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "move_to_final_destination", "line_number": 248, "body": "def move_to_final_destination(self):\n        \"\"\"\n        After editing the file locally, this function uploads it to the Dropbox cloud\n        \"\"\"\n        self.client.upload(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicWritableDropboxFile", "function_description": "Method of the AtomicWritableDropboxFile class that uploads a locally edited temporary file to its final location in Dropbox, ensuring changes are saved to the cloud."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "fs", "line_number": 302, "body": "def fs(self):\n        return self.client", "is_method": true, "class_name": "DropboxTarget", "function_description": "Returns the underlying client object used for filesystem operations, providing direct access to Dropbox API functions within the DropboxTarget class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "temporary_path", "line_number": 306, "body": "def temporary_path(self):\n        tmp_dir = tempfile.mkdtemp()\n        num = random.randrange(0, 1e10)\n        temp_path = '{}{}luigi-tmp-{:010}{}'.format(\n            tmp_dir, os.sep,\n            num, ntpath.basename(self.path))\n\n        yield temp_path\n        # We won't reach here if there was an user exception.\n        self.fs.upload(temp_path, self.path)", "is_method": true, "class_name": "DropboxTarget", "function_description": "Provides a temporary local file path for staging data before uploading it to Dropbox, ensuring atomic file operations in the DropboxTarget class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "open", "line_number": 317, "body": "def open(self, mode):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableDropboxFile(self.path, self.client))\n        else:\n            return self.format.pipe_writer(AtomicWritableDropboxFile(self.path, self.client))", "is_method": true, "class_name": "DropboxTarget", "function_description": "Provides a file-like interface to Dropbox storage, allowing reading or writing files with controlled modes. It abstracts Dropbox file operations into standard read/write streams for seamless integration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "tracking_url_pattern", "line_number": 60, "body": "def tracking_url_pattern(self):\n        if self.deploy_mode == \"cluster\":\n            # in cluster mode client only receives application status once a period of time\n            return r\"tracking URL: (https?://.*)\\s\"\n        else:\n            return r\"Bound (?:.*) to (?:.*), and started at (https?://.*)\\s\"", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Provides the appropriate regular expression pattern to extract the Spark job tracking URL based on the deployment mode, facilitating monitoring and status retrieval of Spark applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "hadoop_user_name", "line_number": 83, "body": "def hadoop_user_name(self):\n        return None", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the Hadoop user name associated with the task, or None if not set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "spark_version", "line_number": 87, "body": "def spark_version(self):\n        return \"spark\"", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns a fixed string indicating the Spark version identifier."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "spark_submit", "line_number": 91, "body": "def spark_submit(self):\n        return configuration.get_config().get(self.spark_version, 'spark-submit', 'spark-submit')", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured executable path or command for submitting Spark jobs based on the specified Spark version, facilitating consistent Spark job submission across environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "master", "line_number": 95, "body": "def master(self):\n        return configuration.get_config().get(self.spark_version, \"master\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the master URL for the configured Spark version, enabling tasks to connect to the appropriate Spark cluster manager."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "deploy_mode", "line_number": 99, "body": "def deploy_mode(self):\n        return configuration.get_config().get(self.spark_version, \"deploy-mode\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the deployment mode configured for the specified Spark version, enabling tasks to know how they should be submitted or executed in a Spark cluster environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "jars", "line_number": 103, "body": "def jars(self):\n        return self._list_config(configuration.get_config().get(self.spark_version, \"jars\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the list of jar dependencies configured for the specific Spark version, enabling Spark job submission tasks to include necessary external libraries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "packages", "line_number": 107, "body": "def packages(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"packages\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves and returns the list of configured package dependencies for the current Spark version, facilitating dynamic inclusion of necessary packages during Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "py_files", "line_number": 112, "body": "def py_files(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"py-files\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured Python file paths linked to the Spark version, facilitating the submission of dependent Python files in Spark job executions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "files", "line_number": 117, "body": "def files(self):\n        return self._list_config(configuration.get_config().get(self.spark_version, \"files\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns a list of file paths configured for the current Spark version, facilitating access to Spark job dependencies or resources specified in the configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_conf", "line_number": 121, "body": "def _conf(self):\n        conf = collections.OrderedDict(self.conf or {})\n        if self.pyspark_python:\n            conf['spark.pyspark.python'] = self.pyspark_python\n        if self.pyspark_driver_python:\n            conf['spark.pyspark.driver.python'] = self.pyspark_driver_python\n        return conf", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Internal helper method of SparkSubmitTask that assembles Spark configuration settings, including optional Python interpreter paths for executor and driver environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "conf", "line_number": 130, "body": "def conf(self):\n        return self._dict_config(configuration.get_config().get(self.spark_version, \"conf\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the Spark configuration dictionary for the current Spark version, providing access to relevant settings needed for task submission and execution in a Spark environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "properties_file", "line_number": 134, "body": "def properties_file(self):\n        return configuration.get_config().get(self.spark_version, \"properties-file\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the path of the properties file associated with the current Spark version from the configuration settings. This enables tasks to access version-specific configurations for Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_memory", "line_number": 138, "body": "def driver_memory(self):\n        return configuration.get_config().get(self.spark_version, \"driver-memory\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured driver memory setting for the specified Spark version. This allows retrieval of memory allocation details used when submitting Spark jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_java_options", "line_number": 142, "body": "def driver_java_options(self):\n        return configuration.get_config().get(self.spark_version, \"driver-java-options\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the Java options configured for the Spark driver based on the current Spark version, enabling customized driver JVM settings during Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_library_path", "line_number": 146, "body": "def driver_library_path(self):\n        return configuration.get_config().get(self.spark_version, \"driver-library-path\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the driver library path configured for the specified Spark version, enabling tasks to locate necessary Spark driver dependencies during execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_class_path", "line_number": 150, "body": "def driver_class_path(self):\n        return configuration.get_config().get(self.spark_version, \"driver-class-path\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured driver class path for the current Spark version, enabling tasks to access necessary classes during Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "executor_memory", "line_number": 154, "body": "def executor_memory(self):\n        return configuration.get_config().get(self.spark_version, \"executor-memory\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured executor memory setting for the specified Spark version, facilitating resource allocation during Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_cores", "line_number": 158, "body": "def driver_cores(self):\n        return configuration.get_config().get(self.spark_version, \"driver-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured number of CPU cores allocated to the Spark driver for the specified Spark version, supporting resource management in Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "supervise", "line_number": 162, "body": "def supervise(self):\n        return bool(configuration.get_config().get(self.spark_version, \"supervise\", False))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method checks if the Spark job supervision feature is enabled for the specified Spark version in the configuration. It provides a boolean status indicating whether the application should run with Spark's supervise mode active."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "total_executor_cores", "line_number": 166, "body": "def total_executor_cores(self):\n        return configuration.get_config().get(self.spark_version, \"total-executor-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured total number of executor cores for the specified Spark version, supporting resource management in Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "executor_cores", "line_number": 170, "body": "def executor_cores(self):\n        return configuration.get_config().get(self.spark_version, \"executor-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the number of executor cores configured for the current Spark version, enabling tasks to query resource allocation settings dynamically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "queue", "line_number": 174, "body": "def queue(self):\n        return configuration.get_config().get(self.spark_version, \"queue\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured cluster queue identifier for the Spark version associated with this task, enabling task scheduling on the appropriate resource queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "num_executors", "line_number": 178, "body": "def num_executors(self):\n        return configuration.get_config().get(self.spark_version, \"num-executors\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the configured number of executors for the specified Spark version, providing SparkSubmitTask with dynamic resource allocation details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "archives", "line_number": 182, "body": "def archives(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"archives\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns a list of archive file paths configured for the current Spark version, facilitating access to necessary resources during Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "hadoop_conf_dir", "line_number": 187, "body": "def hadoop_conf_dir(self):\n        return configuration.get_config().get(self.spark_version, \"hadoop-conf-dir\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the Hadoop configuration directory path associated with the Spark version used by the SparkSubmitTask instance. This allows access to Hadoop settings relevant for Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "get_environment", "line_number": 190, "body": "def get_environment(self):\n        env = os.environ.copy()\n        for prop in ('HADOOP_CONF_DIR', 'HADOOP_USER_NAME'):\n            var = getattr(self, prop.lower(), None)\n            if var:\n                env[prop] = var\n        return env", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Provides environment variables for Spark submission by including specified Hadoop configuration and user settings, ensuring the task runs with the necessary system context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "program_environment", "line_number": 198, "body": "def program_environment(self):\n        return self.get_environment()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Returns the execution environment configuration for the SparkSubmitTask, allowing tasks to access or use specific environment settings during submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "program_args", "line_number": 201, "body": "def program_args(self):\n        return self.spark_command() + self.app_command()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Concatenates and returns the full list of command-line arguments needed to submit a Spark application, combining Spark-specific and application-specific parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "spark_command", "line_number": 204, "body": "def spark_command(self):\n        command = [self.spark_submit]\n        command += self._text_arg('--master', self.master)\n        command += self._text_arg('--deploy-mode', self.deploy_mode)\n        command += self._text_arg('--name', self.name)\n        command += self._text_arg('--class', self.entry_class)\n        command += self._list_arg('--jars', self.jars)\n        command += self._list_arg('--packages', self.packages)\n        command += self._list_arg('--py-files', self.py_files)\n        command += self._list_arg('--files', self.files)\n        command += self._list_arg('--archives', self.archives)\n        command += self._dict_arg('--conf', self._conf)\n        command += self._text_arg('--properties-file', self.properties_file)\n        command += self._text_arg('--driver-memory', self.driver_memory)\n        command += self._text_arg('--driver-java-options', self.driver_java_options)\n        command += self._text_arg('--driver-library-path', self.driver_library_path)\n        command += self._text_arg('--driver-class-path', self.driver_class_path)\n        command += self._text_arg('--executor-memory', self.executor_memory)\n        command += self._text_arg('--driver-cores', self.driver_cores)\n        command += self._flag_arg('--supervise', self.supervise)\n        command += self._text_arg('--total-executor-cores', self.total_executor_cores)\n        command += self._text_arg('--executor-cores', self.executor_cores)\n        command += self._text_arg('--queue', self.queue)\n        command += self._text_arg('--num-executors', self.num_executors)\n        return command", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Constructs and returns the full command-line argument list to submit a Spark job, incorporating all configured options like master, deploy mode, resources, dependencies, and runtime settings. This enables automated and customizable Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "app_command", "line_number": 230, "body": "def app_command(self):\n        if not self.app:\n            raise NotImplementedError(\"subclass should define an app (.jar or .py file)\")\n        return [self.app] + self.app_options()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Constructs the command-line argument list to run the application, combining the app file with its specific options. It ensures the app file is defined for execution within the SparkSubmitTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_list_config", "line_number": 235, "body": "def _list_config(self, config):\n        if config and isinstance(config, str):\n            return list(map(lambda x: x.strip(), config.split(',')))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Utility method in SparkSubmitTask that converts a comma-separated string into a list of trimmed strings, facilitating configurable parameter handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dict_config", "line_number": 239, "body": "def _dict_config(self, config):\n        if config and isinstance(config, str):\n            return dict(map(lambda i: i.split('=', 1), config.split('|')))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Converts a configuration string formatted as key=value pairs separated by '|' into a dictionary. This enables easy access to configuration parameters within the SparkSubmitTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_text_arg", "line_number": 243, "body": "def _text_arg(self, name, value):\n        if value:\n            return [name, value]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Helper method in SparkSubmitTask that formats a command-line argument as a name-value pair list if the value is provided, otherwise returns an empty list for conditional argument inclusion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_list_arg", "line_number": 248, "body": "def _list_arg(self, name, value):\n        if value and isinstance(value, (list, tuple)):\n            return [name, ','.join(value)]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Utility method within SparkSubmitTask that formats a list or tuple argument into a name-value pair suitable for command-line submission, returning an empty list if input is invalid or absent."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dict_arg", "line_number": 253, "body": "def _dict_arg(self, name, value):\n        command = []\n        if value and isinstance(value, dict):\n            for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n        return command", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Constructs a list of command-line arguments from a dictionary, formatting each key-value pair as separate arguments with a specified flag. This supports building dynamic commands for Spark submission tasks requiring multiple parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_flag_arg", "line_number": 260, "body": "def _flag_arg(self, name, value):\n        if value:\n            return [name]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Private helper method of SparkSubmitTask that returns a flag argument as a single-item list if its value is truthy, otherwise returns an empty list. It assists in building command-line argument lists conditionally."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "name", "line_number": 281, "body": "def name(self):\n        return self.__class__.__name__", "is_method": true, "class_name": "PySparkTask", "function_description": "Returns the class name of the PySparkTask instance, providing a simple way to identify the task type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "py_packages", "line_number": 285, "body": "def py_packages(self):\n        packages = configuration.get_config().get('spark', 'py-packages', None)\n        if packages:\n            return map(lambda s: s.strip(), packages.split(','))", "is_method": true, "class_name": "PySparkTask", "function_description": "Method of PySparkTask that retrieves and returns a list of Python package names specified in the Spark configuration, enabling dynamic access to required PySpark dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "files", "line_number": 291, "body": "def files(self):\n        if self.deploy_mode == \"cluster\":\n            return [self.run_pickle]", "is_method": true, "class_name": "PySparkTask", "function_description": "Returns a list containing the run_pickle file path when the deployment mode is set to cluster, supporting file management in cluster deployments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "setup_remote", "line_number": 302, "body": "def setup_remote(self, sc):\n        self._setup_packages(sc)", "is_method": true, "class_name": "PySparkTask", "function_description": "Sets up the remote execution environment by configuring necessary packages on the given SparkContext. It prepares the PySparkTask for distributed processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "app_command", "line_number": 314, "body": "def app_command(self):\n        if self.deploy_mode == \"cluster\":\n            pickle_loc = os.path.basename(self.run_pickle)\n        else:\n            pickle_loc = self.run_pickle\n        return [self.app, pickle_loc] + self.app_options()", "is_method": true, "class_name": "PySparkTask", "function_description": "Constructs the command to run a PySpark application, adjusting the application pickle location based on deployment mode and appending additional options for execution. This enables flexible command generation for submitting Spark tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "run", "line_number": 321, "body": "def run(self):\n        path_name_fragment = re.sub(r'[^\\w]', '_', self.name)\n        self.run_path = tempfile.mkdtemp(prefix=path_name_fragment)\n        self.run_pickle = os.path.join(self.run_path, '.'.join([path_name_fragment, 'pickle']))\n        with open(self.run_pickle, 'wb') as fd:\n            # Copy module file to run path.\n            module_path = os.path.abspath(inspect.getfile(self.__class__))\n            shutil.copy(module_path, os.path.join(self.run_path, '.'))\n            self._dump(fd)\n        try:\n            super(PySparkTask, self).run()\n        finally:\n            shutil.rmtree(self.run_path)", "is_method": true, "class_name": "PySparkTask", "function_description": "Sets up a temporary runtime environment for a PySparkTask, serializes its state, executes the task, and cleans up the temporary files afterward. This enables isolated task execution with automatic resource management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dump", "line_number": 335, "body": "def _dump(self, fd):\n        with self.no_unpicklable_properties():\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'c__main__', b'c' + module_name.encode('ascii'))\n                fd.write(d)\n            else:\n                pickle.dump(self, fd)", "is_method": true, "class_name": "PySparkTask", "function_description": "Internal method of PySparkTask that serializes the task object to a file descriptor, handling special cases for main module pickling to ensure correct module naming during serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_setup_packages", "line_number": 345, "body": "def _setup_packages(self, sc):\n        \"\"\"\n        This method compresses and uploads packages to the cluster\n\n        \"\"\"\n        packages = self.py_packages\n        if not packages:\n            return\n        for package in packages:\n            mod = importlib.import_module(package)\n            try:\n                mod_path = mod.__path__[0]\n            except AttributeError:\n                mod_path = mod.__file__\n            tar_path = os.path.join(self.run_path, package + '.tar.gz')\n            tar = tarfile.open(tar_path, \"w:gz\")\n            tar.add(mod_path, os.path.basename(mod_path))\n            tar.close()\n            sc.addPyFile(tar_path)", "is_method": true, "class_name": "PySparkTask", "function_description": "Prepares and uploads specified Python packages as compressed archives to a Spark cluster, ensuring required dependencies are available for distributed PySpark tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_opener", "line_number": 89, "body": "def get_opener(self, name):\n        \"\"\"Retrieve an opener for the given protocol\n\n        :param name: name of the opener to open\n        :type name: string\n        :raises NoOpenerError: if no opener has been registered of that name\n\n        \"\"\"\n        if name not in self.registry:\n            raise NoOpenerError(\"No opener for %s\" % name)\n        index = self.registry[name]\n        return self.openers[index]", "is_method": true, "class_name": "OpenerRegistry", "function_description": "Utility method of the OpenerRegistry class that retrieves a registered opener object by its protocol name, raising an error if no matching opener exists. It enables access to protocol-specific openers for resource handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "add", "line_number": 102, "body": "def add(self, opener):\n        \"\"\"Adds an opener to the registry\n\n        :param opener: Opener object\n        :type opener: Opener inherited object\n\n        \"\"\"\n\n        index = len(self.openers)\n        self.openers[index] = opener\n        for name in opener.names:\n            self.registry[name] = index", "is_method": true, "class_name": "OpenerRegistry", "function_description": "Adds a new opener object to the registry, associating its identifying names for quick lookup. This enables managing and accessing different opener instances by their names within the OpenerRegistry."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "open", "line_number": 115, "body": "def open(self, target_uri, **kwargs):\n        \"\"\"Open target uri.\n\n        :param target_uri: Uri to open\n        :type target_uri: string\n\n        :returns: Target object\n\n        \"\"\"\n        target = urlsplit(target_uri, scheme=self.default_opener)\n\n        opener = self.get_opener(target.scheme)\n        query = opener.conform_query(target.query)\n\n        target = opener.get_target(\n            target.scheme,\n            target.path,\n            target.fragment,\n            target.username,\n            target.password,\n            target.hostname,\n            target.port,\n            query,\n            **kwargs\n        )\n        target.opener_path = target_uri\n\n        return target", "is_method": true, "class_name": "OpenerRegistry", "function_description": "Service method of OpenerRegistry that opens a resource from a given URI using a suitable registered opener, returning the corresponding target object for further interaction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "conform_query", "line_number": 157, "body": "def conform_query(cls, query):\n        \"\"\"Converts the query string from a target uri, uses\n        cls.allowed_kwargs, and cls.filter_kwargs to drive logic.\n\n        :param query: Unparsed query string\n        :type query: urllib.parse.unsplit(uri).query\n        :returns: Dictionary of parsed values, everything in cls.allowed_kwargs\n            with values set to True will be parsed as json strings.\n\n        \"\"\"\n        query = parse_qs(query, keep_blank_values=True)\n\n        # Remove any unexpected keywords from the query string.\n        if cls.filter_kwargs:\n            query = {x: y for x, y in query.items() if x in cls.allowed_kwargs}\n\n        for key, vals in query.items():\n            # Multiple values of the same name could be passed use first\n            # Also params without strings will be treated as true values\n            if cls.allowed_kwargs.get(key, False):\n                val = json.loads(vals[0] or 'true')\n            else:\n                val = vals[0] or 'true'\n\n            query[key] = val\n\n        return query", "is_method": true, "class_name": "Opener", "function_description": "Core method of the Opener class that parses and filters a query string into a dictionary, converting designated parameters into JSON values while enforcing allowed keys for controlled query parameter handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 212, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        full_path = (hostname or '') + path\n        query.update(kwargs)\n        return MockTarget(full_path, **query)", "is_method": true, "class_name": "MockOpener", "function_description": "Utility method in MockOpener that constructs and returns a MockTarget object combining URL components and additional query parameters, facilitating mocked resource access or testing scenarios."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 237, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        full_path = (hostname or '') + path\n        query.update(kwargs)\n        return LocalTarget(full_path, **query)", "is_method": true, "class_name": "LocalOpener", "function_description": "Constructs and returns a LocalTarget object representing a local resource identified by URL components and additional parameters. It centralizes local path resolution and target creation for use in file or resource handling operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 261, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        query.update(kwargs)\n        return S3Target('{scheme}://{hostname}{path}'.format(\n            scheme=scheme, hostname=hostname, path=path), **query)", "is_method": true, "class_name": "S3Opener", "function_description": "Constructs and returns an S3Target object representing a specific S3 resource identified by the given URL components and additional query parameters. It enables standardized access to S3 locations within the S3Opener context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "connection", "line_number": 69, "body": "def connection(self):\n        return blockblobservice.BlockBlobService(account_name=self.options.get(\"account_name\"),\n                                                 account_key=self.options.get(\"account_key\"),\n                                                 sas_token=self.options.get(\"sas_token\"),\n                                                 protocol=self.kwargs.get(\"protocol\"),\n                                                 connection_string=self.kwargs.get(\"connection_string\"),\n                                                 endpoint_suffix=self.kwargs.get(\"endpoint_suffix\"),\n                                                 custom_domain=self.kwargs.get(\"custom_domain\"),\n                                                 is_emulated=self.kwargs.get(\"is_emulated\") or False)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides a configured connection to Azure Blob Storage using available authentication and connection parameters, enabling further operations on blobs within the AzureBlobClient context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "upload", "line_number": 79, "body": "def upload(self, tmp_path, container, blob, **kwargs):\n        logging.debug(\"Uploading file '{tmp_path}' to container '{container}' and blob '{blob}'\".format(\n            tmp_path=tmp_path, container=container, blob=blob))\n        self.create_container(container)\n        lease_id = self.connection.acquire_blob_lease(container, blob)\\\n            if self.exists(\"{container}/{blob}\".format(container=container, blob=blob)) else None\n        try:\n            self.connection.create_blob_from_path(container, blob, tmp_path, lease_id=lease_id, progress_callback=kwargs.get(\"progress_callback\"))\n        finally:\n            if lease_id is not None:\n                self.connection.release_blob_lease(container, blob, lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Uploads a file from a local path to a specified Azure Blob Storage container and blob, handling container creation and blob leasing to ensure safe and consistent upload operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "download_as_bytes", "line_number": 91, "body": "def download_as_bytes(self, container, blob, bytes_to_read=None):\n        start_range, end_range = (0, bytes_to_read-1) if bytes_to_read is not None else (None, None)\n        logging.debug(\"Downloading from container '{container}' and blob '{blob}' as bytes\".format(\n            container=container, blob=blob))\n        return self.connection.get_blob_to_bytes(container, blob, start_range=start_range, end_range=end_range).content", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Utility method of AzureBlobClient that downloads specified bytes or entire content of a blob from a storage container and returns it as bytes for processing or storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "download_as_file", "line_number": 97, "body": "def download_as_file(self, container, blob, location):\n        logging.debug(\"Downloading from container '{container}' and blob '{blob}' to {location}\".format(\n            container=container, blob=blob, location=location))\n        return self.connection.get_blob_to_path(container, blob, location)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides a method to download a blob from a specified Azure Blob Storage container directly to a local file path. This enables saving remote blob data as a file for further local use or processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "create_container", "line_number": 102, "body": "def create_container(self, container_name):\n        return self.connection.create_container(container_name)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Utility method of AzureBlobClient that creates a new storage container with the specified name to organize and manage blobs within Azure Blob Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "delete_container", "line_number": 105, "body": "def delete_container(self, container_name):\n        lease_id = self.connection.acquire_container_lease(container_name)\n        self.connection.delete_container(container_name, lease_id=lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Service method of AzureBlobClient that deletes a specified blob container by securely acquiring a lease before removal, ensuring authorized and safe container deletion operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "exists", "line_number": 109, "body": "def exists(self, path):\n        container, blob = self.splitfilepath(path)\n        return self.connection.exists(container, blob)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Utility method in AzureBlobClient that checks if a specified blob exists in the given Azure storage container, facilitating verification of resource availability before operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "remove", "line_number": 113, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        container, blob = self.splitfilepath(path)\n        if not self.exists(path):\n            return False\n        lease_id = self.connection.acquire_blob_lease(container, blob)\n        self.connection.delete_blob(container, blob, lease_id=lease_id)\n        return True", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Method of AzureBlobClient that deletes a specified blob from Azure storage, optionally handling recursive deletion. It ensures exclusive access by acquiring a lease before removal and returns success status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "mkdir", "line_number": 121, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        container, blob = self.splitfilepath(path)\n        if raise_if_exists and self.exists(path):\n            raise FileAlreadyExists(\"The Azure blob path '{blob}' already exists under container '{container}'\".format(\n                blob=blob, container=container))", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Creates a directory-like path in Azure Blob storage, optionally creating parent directories and raising an error if the path already exists. This assists in organizing blob data hierarchically within a container."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "isdir", "line_number": 127, "body": "def isdir(self, path):\n        \"\"\"\n        Azure Blob Storage has no concept of directories. It always returns False\n        :param str path: Path of the Azure blob storage\n        :return: False\n        \"\"\"\n        return False", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Method in AzureBlobClient that always returns False for directory checks, reflecting Azure Blob Storage's flat namespace without real directory structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "move", "line_number": 135, "body": "def move(self, path, dest):\n        try:\n            return self.copy(path, dest) and self.remove(path)\n        except IOError:\n            self.remove(dest)\n            return False", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Method of AzureBlobClient that moves a blob from a source path to a destination by copying then deleting it, providing a way to relocate blobs within Azure Blob storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "copy", "line_number": 142, "body": "def copy(self, path, dest):\n        source_container, source_blob = self.splitfilepath(path)\n        dest_container, dest_blob = self.splitfilepath(dest)\n        if source_container != dest_container:\n            raise Exception(\n                \"Can't copy blob from '{source_container}' to '{dest_container}'. File can be moved within container\".format(\n                    source_container=source_container, dest_container=dest_container\n                ))\n\n        source_lease_id = self.connection.acquire_blob_lease(source_container, source_blob)\n        destination_lease_id = self.connection.acquire_blob_lease(dest_container, dest_blob) if self.exists(dest) else None\n        try:\n            return self.connection.copy_blob(source_container, dest_blob, self.connection.make_blob_url(\n                source_container, source_blob),\n                destination_lease_id=destination_lease_id, source_lease_id=source_lease_id)\n        finally:\n            self.connection.release_blob_lease(source_container, source_blob, source_lease_id)\n            if destination_lease_id is not None:\n                self.connection.release_blob_lease(dest_container, dest_blob, destination_lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Method of AzureBlobClient that copies a blob within the same Azure storage container, ensuring lease acquisition for safe concurrent access during the operation. It enables controlled duplication of blobs without cross-container transfers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "rename_dont_move", "line_number": 162, "body": "def rename_dont_move(self, path, dest):\n        self.move(path, dest)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Calls the move method to rename a blob without changing its location. It provides a simplified interface for renaming blobs within the same storage path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "splitfilepath", "line_number": 166, "body": "def splitfilepath(filepath):\n        splitpath = filepath.split(\"/\")\n        container = splitpath[0]\n        blobsplit = splitpath[1:]\n        blob = None if not blobsplit else \"/\".join(blobsplit)\n        return container, blob", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Utility function in AzureBlobClient that splits a blob storage file path into its container and blob name components for easier blob identification and access within Azure storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "read", "line_number": 185, "body": "def read(self, n=None):\n        return self.client.download_as_bytes(self.container, self.blob, n)", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Reads up to n bytes from an Azure Blob storage file, providing a convenient way to access blob content in byte form."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__enter__", "line_number": 188, "body": "def __enter__(self):\n        if self.download_when_reading:\n            self.client.download_as_file(self.container, self.blob, self.download_file_location)\n            self.fid = open(self.download_file_location)\n            return self.fid\n        else:\n            return self", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Enables using ReadableAzureBlobFile in a with-statement, optionally downloading the blob to a local file for reading. Provides seamless access to the blob's content either via a local file or the object itself."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__exit__", "line_number": 196, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Ensures the Azure Blob file resource is properly closed when exiting a context manager block, supporting safe and automatic resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__del__", "line_number": 199, "body": "def __del__(self):\n        self.close()\n        if os._exists(self.download_file_location):\n            os.remove(self.download_file_location)", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Destructor method of the ReadableAzureBlobFile class that ensures the file is closed and its downloaded temporary file is deleted when the object is destroyed, helping manage resource cleanup automatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "close", "line_number": 204, "body": "def close(self):\n        if self.download_when_reading:\n            if self.fid is not None and not self.fid.closed:\n                self.fid.close()\n                self.fid = None", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Closes the file stream if it was opened during download, ensuring proper resource cleanup when reading from an Azure Blob storage file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "readable", "line_number": 210, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "This method simply indicates that the ReadableAzureBlobFile instance supports reading operations. It provides a boolean status confirming the file's readability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "writable", "line_number": 213, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Indicates that files accessed through ReadableAzureBlobFile are read-only and cannot be written to. This method signals the immutability of the file stream to other components."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "seekable", "line_number": 216, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Indicates whether the Azure Blob file supports random access; always returns False to show the file is not seekable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "move_to_final_destination", "line_number": 231, "body": "def move_to_final_destination(self):\n        self.client.upload(self.tmp_path, self.container, self.blob, **self.azure_blob_options)", "is_method": true, "class_name": "AtomicAzureBlobFile", "function_description": "Uploads a temporary file to its final location in Azure Blob Storage, completing the transfer process within the AtomicAzureBlobFile context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "fs", "line_number": 269, "body": "def fs(self):\n        \"\"\"\n        The :py:class:`FileSystem` associated with :class:`.AzureBlobTarget`\n        \"\"\"\n        return self.client", "is_method": true, "class_name": "AzureBlobTarget", "function_description": "Returns the filesystem client associated with the AzureBlobTarget, providing access to Azure Blob Storage operations. It enables other functions to interact with the underlying storage system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "open", "line_number": 275, "body": "def open(self, mode):\n        \"\"\"\n        Open the target for reading or writing\n\n        :param char mode:\n            'r' for reading and 'w' for writing.\n\n            'b' is not supported and will be stripped if used. For binary mode, use `format`\n        :return:\n            * :class:`.ReadableAzureBlobFile` if 'r'\n            * :class:`.AtomicAzureBlobFile` if 'w'\n        \"\"\"\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableAzureBlobFile(self.container, self.blob, self.client, self.download_when_reading, **self.azure_blob_options))\n        else:\n            return self.format.pipe_writer(AtomicAzureBlobFile(self.container, self.blob, self.client, **self.azure_blob_options))", "is_method": true, "class_name": "AzureBlobTarget", "function_description": "Provides a way to open an Azure Blob target for reading or writing, returning appropriate file-like objects that handle data streaming based on the specified mode. Facilitates file operations on Azure Blob storage by abstracting mode-specific behaviors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_pyspark_runner_with", "line_number": 115, "body": "def _pyspark_runner_with(name, entry_point_class):\n    return type(name, (AbstractPySparkRunner,), {'_entry_point_class': entry_point_class})", "is_method": false, "function_description": "Factory function that dynamically creates a subclass of AbstractPySparkRunner with a specified entry point class for running PySpark jobs. It enables flexible definition of runner classes with customized entry points."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_use_spark_session", "line_number": 123, "body": "def _use_spark_session():\n    return bool(configuration.get_config().get('pyspark_runner', \"use_spark_session\", False))", "is_method": false, "function_description": "Utility function that checks configuration to determine if a Spark session should be used for PySpark operations. It enables conditional execution paths based on Spark session availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_get_runner_class", "line_number": 127, "body": "def _get_runner_class():\n    if _use_spark_session():\n        return PySparkSessionRunner\n    return PySparkRunner", "is_method": false, "function_description": "Returns the appropriate PySpark runner class based on whether a Spark session is in use, enabling dynamic selection of execution environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__enter__", "line_number": 57, "body": "def __enter__(self):\n        from pyspark import SparkContext\n        self.sc = SparkContext(conf=self.conf)\n        return self.sc, self.sc", "is_method": true, "class_name": "SparkContextEntryPoint", "function_description": "Provides context management to initialize and return a SparkContext instance, simplifying Spark setup within a with-statement in the SparkContextEntryPoint class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__exit__", "line_number": 62, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.sc.stop()", "is_method": true, "class_name": "SparkContextEntryPoint", "function_description": "Cleans up the SparkContext by stopping it when exiting a runtime context, ensuring proper resource release and application shutdown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_check_major_spark_version", "line_number": 69, "body": "def _check_major_spark_version(self):\n        from pyspark import __version__ as spark_version\n        major_version = int(spark_version.split('.')[0])\n        if major_version < 2:\n            raise RuntimeError(\n                '''\n                Apache Spark {} does not support SparkSession entrypoint.\n                Try to set 'pyspark_runner.use_spark_session' to 'False' and switch to old-style syntax\n                '''.format(spark_version)\n            )", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "Checks if the installed Apache Spark version supports SparkSession and raises an error if it is below version 2, ensuring compatibility with SparkSession-based functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__enter__", "line_number": 80, "body": "def __enter__(self):\n        self._check_major_spark_version()\n        from pyspark.sql import SparkSession\n        self.spark = SparkSession \\\n            .builder \\\n            .config(conf=self.conf) \\\n            .enableHiveSupport() \\\n            .getOrCreate()\n\n        return self.spark, self.spark.sparkContext", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "Context manager method that initializes and returns a SparkSession with Hive support and its SparkContext, ensuring compatibility with the required Spark version."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__exit__", "line_number": 91, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.spark.stop()", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "Terminates the Spark session, ensuring resources are properly released when exiting a context managed block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "run", "line_number": 106, "body": "def run(self):\n        from pyspark import SparkConf\n        conf = SparkConf()\n        self.job.setup(conf)\n        with self._entry_point_class(conf=conf) as (entry_point, sc):\n            self.job.setup_remote(sc)\n            self.job.main(entry_point, *self.args)", "is_method": true, "class_name": "AbstractPySparkRunner", "function_description": "Core method of AbstractPySparkRunner that configures and executes a PySpark job by initializing the Spark context and running the job\u2019s main logic with provided arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "create_hadoopcli_client", "line_number": 38, "body": "def create_hadoopcli_client():\n    \"\"\"\n    Given that we want one of the hadoop cli clients,\n    this one will return the right one.\n    \"\"\"\n    version = hdfs_config.get_configured_hadoop_version()\n    if version == \"cdh4\":\n        return HdfsClient()\n    elif version == \"cdh3\":\n        return HdfsClientCdh3()\n    elif version == \"apache1\":\n        return HdfsClientApache1()\n    else:\n        raise ValueError(\"Error: Unknown version specified in Hadoop version\"\n                         \"configuration parameter\")", "is_method": false, "function_description": "Function that returns the appropriate Hadoop CLI client instance based on the configured Hadoop version, enabling version-specific interactions with Hadoop file systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "call_check", "line_number": 63, "body": "def call_check(command):\n        p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode != 0:\n            raise hdfs_error.HDFSCliError(command, p.returncode, stdout, stderr)\n        return stdout", "is_method": true, "class_name": "HdfsClient", "function_description": "Class HdfsClient's call_check function executes a system command, checks for errors, and returns the command's output. It provides a reliable interface for running HDFS-related shell commands with automatic error handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "exists", "line_number": 70, "body": "def exists(self, path):\n        \"\"\"\n        Use ``hadoop fs -stat`` to check file existence.\n        \"\"\"\n\n        cmd = load_hadoop_cmd() + ['fs', '-stat', path]\n        logger.debug('Running file existence check: %s', subprocess.list2cmdline(cmd))\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        else:\n            not_found_pattern = \"^.*No such file or directory$\"\n            not_found_re = re.compile(not_found_pattern)\n            for line in stderr.split('\\n'):\n                if not_found_re.match(line):\n                    return False\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "is_method": true, "class_name": "HdfsClient", "function_description": "Checks if a specified file or directory exists in HDFS by executing a Hadoop file system status command. This method helps verify file presence before performing further HDFS operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "move", "line_number": 89, "body": "def move(self, path, dest):\n        parent_dir = os.path.dirname(dest)\n        if parent_dir != '' and not self.exists(parent_dir):\n            self.mkdir(parent_dir)\n        if not isinstance(path, (list, tuple)):\n            path = [path]\n        else:\n            warnings.warn(\"Renaming multiple files at once is not atomic.\", stacklevel=2)\n        self.call_check(load_hadoop_cmd() + ['fs', '-mv'] + path + [dest])", "is_method": true, "class_name": "HdfsClient", "function_description": "Provides functionality to move or rename one or multiple files within HDFS, creating the destination directory if needed; supports batch moves with a warning about non-atomic operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "remove", "line_number": 99, "body": "def remove(self, path, recursive=True, skip_trash=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-rm', '-r']\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-rm']\n\n        if skip_trash:\n            cmd = cmd + ['-skipTrash']\n\n        cmd = cmd + [path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Utility method of HdfsClient that deletes files or directories from HDFS, optionally recursively and with an option to bypass the trash, facilitating file system cleanup operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "chmod", "line_number": 111, "body": "def chmod(self, path, permissions, recursive=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-chmod', '-R', permissions, path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-chmod', permissions, path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Method of HdfsClient that changes file or directory permissions at a given path, optionally applying changes recursively to all nested files and directories. It facilitates managing access controls in Hadoop Distributed File System environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "chown", "line_number": 118, "body": "def chown(self, path, owner, group, recursive=False):\n        if owner is None:\n            owner = ''\n        if group is None:\n            group = ''\n        ownership = \"%s:%s\" % (owner, group)\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-chown', '-R', ownership, path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-chown', ownership, path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Method of HdfsClient that changes the owner and group of a specified HDFS path, optionally applying the change recursively to all contained files and directories. It facilitates permission management on Hadoop file systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "count", "line_number": 130, "body": "def count(self, path):\n        cmd = load_hadoop_cmd() + ['fs', '-count', path]\n        stdout = self.call_check(cmd)\n        lines = stdout.split('\\n')\n        for line in stdout.split('\\n'):\n            if line.startswith(\"OpenJDK 64-Bit Server VM warning\") or line.startswith(\"It's highly recommended\") or not line:\n                lines.pop(lines.index(line))\n            else:\n                (dir_count, file_count, content_size, ppath) = stdout.split()\n        results = {'content_size': content_size, 'dir_count': dir_count, 'file_count': file_count}\n        return results", "is_method": true, "class_name": "HdfsClient", "function_description": "Service method of HdfsClient that returns the count of directories, files, and total content size at a specified HDFS path. It provides quick summary statistics for file system management or monitoring tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "copy", "line_number": 142, "body": "def copy(self, path, destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-cp', path, destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "Utility method in HdfsClient that copies a file or directory from one HDFS path to another, enabling file management within the Hadoop filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "put", "line_number": 145, "body": "def put(self, local_path, destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-put', local_path, destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "Uploads a local file to a specified destination in the Hadoop Distributed File System (HDFS), facilitating data transfer from the local environment to HDFS storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "get", "line_number": 148, "body": "def get(self, path, local_destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-get', path, local_destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "Provides a way to copy a file from a Hadoop filesystem path to a local destination. This function enables local access to files stored in HDFS for further processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "getmerge", "line_number": 151, "body": "def getmerge(self, path, local_destination, new_line=False):\n        if new_line:\n            cmd = load_hadoop_cmd() + ['fs', '-getmerge', '-nl', path, local_destination]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-getmerge', path, local_destination]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Utility method of HdfsClient that merges files from a specified HDFS path into a single local file, optionally adding new lines between merged files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "mkdir", "line_number": 158, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if parents and raise_if_exists:\n            raise NotImplementedError(\"HdfsClient.mkdir can't raise with -p\")\n        try:\n            cmd = (load_hadoop_cmd() + ['fs', '-mkdir'] +\n                   (['-p'] if parents else []) +\n                   [path])\n            self.call_check(cmd)\n        except hdfs_error.HDFSCliError as ex:\n            if \"File exists\" in ex.stderr:\n                if raise_if_exists:\n                    raise FileAlreadyExists(ex.stderr)\n            else:\n                raise", "is_method": true, "class_name": "HdfsClient", "function_description": "Creates a directory at the specified HDFS path, optionally including parent directories, with control over error raising if the directory already exists. This supports managed directory setup in HDFS environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "listdir", "line_number": 173, "body": "def listdir(self, path, ignore_directories=False, ignore_files=False,\n                include_size=False, include_type=False, include_time=False, recursive=False):\n        if not path:\n            path = \".\"  # default to current/home catalog\n\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs'] + self.recursive_listdir_cmd + [path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-ls', path]\n        lines = self.call_check(cmd).split('\\n')\n\n        for line in lines:\n            if not line:\n                continue\n            elif line.startswith('OpenJDK 64-Bit Server VM warning') or line.startswith('It\\'s highly recommended') or line.startswith('Found'):\n                continue  # \"hadoop fs -ls\" outputs \"Found %d items\" as its first line\n            elif ignore_directories and line[0] == 'd':\n                continue\n            elif ignore_files and line[0] == '-':\n                continue\n            data = line.split(' ')\n\n            file = data[-1]\n            size = int(data[-4])\n            line_type = line[0]\n            extra_data = ()\n\n            if include_size:\n                extra_data += (size,)\n            if include_type:\n                extra_data += (line_type,)\n            if include_time:\n                time_str = '%sT%s' % (data[-3], data[-2])\n                modification_time = datetime.datetime.strptime(time_str,\n                                                               '%Y-%m-%dT%H:%M')\n                extra_data += (modification_time,)\n\n            if len(extra_data) > 0:\n                yield (file,) + extra_data\n            else:\n                yield file", "is_method": true, "class_name": "HdfsClient", "function_description": "Provides flexible listing of files and directories in HDFS with optional filters and metadata like size, type, and modification time; supports recursive traversal and can exclude files or directories as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "touchz", "line_number": 215, "body": "def touchz(self, path):\n        self.call_check(load_hadoop_cmd() + ['fs', '-touchz', path])", "is_method": true, "class_name": "HdfsClient", "function_description": "Method of HdfsClient that creates an empty file at the specified HDFS path or updates its timestamp if the file exists, commonly used to initialize or mark files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "mkdir", "line_number": 224, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        \"\"\"\n        No explicit -p switch, this version of Hadoop always creates parent directories.\n        \"\"\"\n        try:\n            self.call_check(load_hadoop_cmd() + ['fs', '-mkdir', path])\n        except hdfs_error.HDFSCliError as ex:\n            if \"File exists\" in ex.stderr:\n                if raise_if_exists:\n                    raise FileAlreadyExists(ex.stderr)\n            else:\n                raise", "is_method": true, "class_name": "HdfsClientCdh3", "function_description": "Creates a directory at the specified path in HDFS, automatically creating parent directories. It can optionally raise an error if the directory already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "remove", "line_number": 237, "body": "def remove(self, path, recursive=True, skip_trash=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-rmr']\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-rm']\n\n        if skip_trash:\n            cmd = cmd + ['-skipTrash']\n\n        cmd = cmd + [path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClientCdh3", "function_description": "Method of HdfsClientCdh3 that deletes a file or directory in HDFS, supporting recursive removal and optional bypassing of the trash for immediate deletion. It provides a controlled way to remove HDFS paths programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "exists", "line_number": 258, "body": "def exists(self, path):\n        cmd = load_hadoop_cmd() + ['fs', '-test', '-e', path]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        elif p.returncode == 1:\n            return False\n        else:\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "is_method": true, "class_name": "HdfsClientApache1", "function_description": "Utility method of HdfsClientApache1 that checks whether a specified path exists in the Hadoop file system, returning a boolean result or raising an error for unexpected conditions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "url", "line_number": 62, "body": "def url(self):\n        # the hdfs package allows it to specify multiple namenodes by passing a string containing\n        # multiple namenodes separated by ';'\n        hosts = self.host.split(\";\")\n        urls = ['http://' + host + ':' + str(self.port) for host in hosts]\n        return \";\".join(urls)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Constructs and returns a semicolon-separated string of HTTP URLs for all configured HDFS namenodes, facilitating connection setup to multiple hosts through the WebHdfsClient."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "client", "line_number": 70, "body": "def client(self):\n        # A naive benchmark showed that 1000 existence checks took 2.5 secs\n        # when not recreating the client, and 4.0 secs when recreating it. So\n        # not urgent to memoize it. Note that it *might* be issues with process\n        # forking and whatnot (as the one in the snakebite client) if we\n        # memoize it too trivially.\n        if self.client_type == 'kerberos':\n            from hdfs.ext.kerberos import KerberosClient\n            return KerberosClient(url=self.url)\n        else:\n            import hdfs\n            return hdfs.InsecureClient(url=self.url, user=self.user)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides a WebHDFS client instance configured for either Kerberos-secured or insecure connections based on the client's authentication type. This enables interaction with HDFS using appropriate security settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "walk", "line_number": 83, "body": "def walk(self, path, depth=1):\n        return self.client.walk(path, depth=depth)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Utility method of the WebHdfsClient class that traverses directories up to a specified depth, enabling exploration of the filesystem hierarchy on a WebHDFS service."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "exists", "line_number": 86, "body": "def exists(self, path):\n        \"\"\"\n        Returns true if the path exists and false otherwise.\n        \"\"\"\n        import hdfs\n        try:\n            self.client.status(path)\n            return True\n        except hdfs.util.HdfsError as e:\n            if str(e).startswith('File does not exist: '):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Checks whether a specified path exists in the HDFS filesystem, returning a boolean that indicates presence or absence of the path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "upload", "line_number": 100, "body": "def upload(self, hdfs_path, local_path, overwrite=False):\n        return self.client.upload(hdfs_path, local_path, overwrite=overwrite)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Utility method of the WebHdfsClient class that uploads a local file to a specified HDFS path, with an option to overwrite existing files. It simplifies file transfer from local storage to HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "download", "line_number": 103, "body": "def download(self, hdfs_path, local_path, overwrite=False, n_threads=-1):\n        return self.client.download(hdfs_path, local_path, overwrite=overwrite,\n                                    n_threads=n_threads)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Utility method of WebHdfsClient that downloads files from HDFS to a local path, supporting optional overwriting and multithreading for efficient data transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "remove", "line_number": 107, "body": "def remove(self, hdfs_path, recursive=True, skip_trash=False):\n        assert skip_trash  # Yes, you need to explicitly say skip_trash=True\n        return self.client.delete(hdfs_path, recursive=recursive)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Method of WebHdfsClient that deletes a file or directory from HDFS, requiring explicit confirmation to bypass trash and optionally supports recursive removal of directory contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "read", "line_number": 111, "body": "def read(self, hdfs_path, offset=0, length=None, buffer_size=None,\n             chunk_size=1024, buffer_char=None):\n        return self.client.read(hdfs_path, offset=offset, length=length,\n                                buffer_size=buffer_size, chunk_size=chunk_size,\n                                buffer_char=buffer_char)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Utility method of the WebHdfsClient class that reads data from a specified HDFS path, supporting optional offsets, length limits, and buffer configurations for flexible and efficient data retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "move", "line_number": 117, "body": "def move(self, path, dest):\n        parts = dest.rstrip('/').split('/')\n        if len(parts) > 1:\n            dir_path = '/'.join(parts[0:-1])\n            if not self.exists(dir_path):\n                self.mkdir(dir_path, parents=True)\n        self.client.rename(path, dest)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Method of WebHdfsClient that moves a file or directory from one path to another, creating any necessary destination directories beforehand to ensure the move operation succeeds."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "mkdir", "line_number": 125, "body": "def mkdir(self, path, parents=True, mode=0o755, raise_if_exists=False):\n        \"\"\"\n        Has no returnvalue (just like WebHDFS)\n        \"\"\"\n        if not parents or raise_if_exists:\n            warnings.warn('webhdfs mkdir: parents/raise_if_exists not implemented')\n        permission = int(oct(mode)[2:])  # Convert from int(decimal) to int(octal)\n        self.client.makedirs(path, permission=permission)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Creates a directory at the specified path on the WebHDFS server with given permissions. Supports basic directory creation but does not handle parent directories or existence checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "put", "line_number": 158, "body": "def put(self, local_path, destination):\n        \"\"\"\n        Restricted version of upload\n        \"\"\"\n        self.upload(local_path, destination)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Wraps the upload functionality to provide a simplified, restricted interface for uploading local files to a specified destination in the WebHDFS system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "get", "line_number": 164, "body": "def get(self, path, local_destination):\n        \"\"\"\n        Restricted version of download\n        \"\"\"\n        self.download(path, local_destination)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "This method provides a simplified interface to download a file from a given path on the HDFS to a local destination, acting as a restricted or limited download operation within WebHdfsClient."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "listdir", "line_number": 170, "body": "def listdir(self, path, ignore_directories=False, ignore_files=False,\n                include_size=False, include_type=False, include_time=False,\n                recursive=False):\n        assert not recursive\n        return self.client.list(path, status=False)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides a listing of entries in the specified HDFS directory path, with options to filter and include metadata (currently unsupported). It enables directory content retrieval for further processing or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "touchz", "line_number": 176, "body": "def touchz(self, path):\n        \"\"\"\n        To touchz using the web hdfs \"write\" cmd.\n        \"\"\"\n        self.client.write(path, data='', overwrite=False)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Creates an empty file at the specified path on HDFS without overwriting existing files, providing a way to ensure the file's presence via the WebHDFS client."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "__del__", "line_number": 93, "body": "def __del__(self):\n        # TODO: not sure is_tmp belongs in Targets construction arguments\n        if self.is_tmp and self.exists():\n            self.remove(skip_trash=True)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Destructor method of HdfsTarget that ensures temporary targets are deleted upon object destruction, preventing leftover temporary files in the Hadoop filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "fs", "line_number": 99, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "HdfsTarget", "function_description": "Accessor method in HdfsTarget that returns the underlying filesystem object, enabling other components to interact with the HDFS storage system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "glob_exists", "line_number": 102, "body": "def glob_exists(self, expected_files):\n        ls = list(self.fs.listdir(self.path))\n        if len(ls) == expected_files:\n            return True\n        return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Checks if the number of files at the target HDFS path matches the expected count, confirming the existence of a complete set of files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "open", "line_number": 108, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            return self.format.pipe_reader(self.path)\n        else:\n            return self.format.pipe_writer(self.path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Service method of HdfsTarget that opens a file path in read or write mode using the associated format's pipe handlers, enabling data streaming to or from HDFS storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "remove", "line_number": 117, "body": "def remove(self, skip_trash=False):\n        self.fs.remove(self.path, skip_trash=skip_trash)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Method of the HdfsTarget class that deletes the file or directory at the specified path, optionally bypassing the trash for immediate removal."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "rename", "line_number": 120, "body": "def rename(self, path, raise_if_exists=False):\n        \"\"\"\n        Does not change self.path.\n\n        Unlike ``move_dir()``, ``rename()`` might cause nested directories.\n        See spotify/luigi#522\n        \"\"\"\n        if isinstance(path, HdfsTarget):\n            path = path.path\n        if raise_if_exists and self.fs.exists(path):\n            raise RuntimeError('Destination exists: %s' % path)\n        self.fs.rename(self.path, path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Renames the current HDFS target to a new path, optionally raising an error if the destination already exists. It supports renaming that may create nested directories unlike the move_dir method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "move", "line_number": 133, "body": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        Alias for ``rename()``\n        \"\"\"\n        self.rename(path, raise_if_exists=raise_if_exists)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Alias method in HdfsTarget that relocates a file or directory to a new path, optionally raising an error if the destination exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "move_dir", "line_number": 139, "body": "def move_dir(self, path):\n        \"\"\"\n        Move using :py:class:`~luigi.contrib.hdfs.abstract_client.HdfsFileSystem.rename_dont_move`\n\n        New since after luigi v2.1: Does not change self.path\n\n        One could argue that the implementation should use the\n        mkdir+raise_if_exists approach, but we at Spotify have had more trouble\n        with that over just using plain mv.  See spotify/luigi#557\n        \"\"\"\n        self.fs.rename_dont_move(self.path, path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Provides a method to rename (move) a directory within HDFS without altering the internal path state, facilitating atomic directory moves in data pipeline workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "copy", "line_number": 151, "body": "def copy(self, dst_dir):\n        \"\"\"\n        Copy to destination directory.\n        \"\"\"\n        self.fs.copy(self.path, dst_dir)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Utility method of HdfsTarget that copies the current file or directory to a specified destination directory in the Hadoop filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "is_writable", "line_number": 157, "body": "def is_writable(self):\n        \"\"\"\n        Currently only works with hadoopcli\n        \"\"\"\n        if \"/\" in self.path:\n            # example path: /log/ap/2013-01-17/00\n            parts = self.path.split(\"/\")\n            # start with the full path and then up the tree until we can check\n            length = len(parts)\n            for part in range(length):\n                path = \"/\".join(parts[0:length - part]) + \"/\"\n                if self.fs.exists(path):\n                    # if the path exists and we can write there, great!\n                    if self._is_writable(path):\n                        return True\n                    # if it exists and we can't =( sad panda\n                    else:\n                        return False\n            # We went through all parts of the path and we still couldn't find\n            # one that exists.\n            return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Determines if the HDFS path or any of its parent directories is writable, enabling validation before write operations in a Hadoop filesystem. This helps ensure data can be safely written to the target location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "_is_writable", "line_number": 179, "body": "def _is_writable(self, path):\n        test_path = path + '.test_write_access-%09d' % random.randrange(1e10)\n        try:\n            self.fs.touchz(test_path)\n            self.fs.remove(test_path, recursive=False)\n            return True\n        except hdfs_clients.HDFSCliError:\n            return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Utility method of the HdfsTarget class that checks if a given HDFS path is writable by attempting to create and delete a test file, facilitating validation of write permissions before data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "exists", "line_number": 219, "body": "def exists(self):\n        hadoopSemaphore = self.path + self.flag\n        return self.fs.exists(hadoopSemaphore)", "is_method": true, "class_name": "HdfsFlagTarget", "function_description": "Checks if a specific flag file exists in the Hadoop filesystem path tied to this target, enabling presence validation for coordination or signaling purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/clients.py", "function": "get_autoconfig_client", "line_number": 33, "body": "def get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):\n    \"\"\"\n    Creates the client as specified in the `luigi.cfg` configuration.\n    \"\"\"\n    try:\n        return client_cache.client\n    except AttributeError:\n        configured_client = hdfs_config.get_configured_hdfs_client()\n        if configured_client == \"webhdfs\":\n            client_cache.client = hdfs_webhdfs_client.WebHdfsClient()\n        elif configured_client == \"hadoopcli\":\n            client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()\n        else:\n            raise Exception(\"Unknown hdfs client \" + configured_client)\n        return client_cache.client", "is_method": false, "function_description": "Function that provides a configured HDFS client instance based on external settings, caching the client for reuse. It enables seamless interaction with different HDFS client types as specified in the configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/clients.py", "function": "_with_ac", "line_number": 50, "body": "def _with_ac(method_name):\n    def result(*args, **kwargs):\n        return getattr(get_autoconfig_client(), method_name)(*args, **kwargs)\n    return result", "is_method": false, "function_description": "Utility function that creates dynamic proxies to methods of an autoconfig client, enabling transparent delegation of calls to the client's corresponding methods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "load_hadoop_cmd", "line_number": 53, "body": "def load_hadoop_cmd():\n    return hadoopcli().command.split()", "is_method": false, "function_description": "This function returns the Hadoop command split into a list of arguments, facilitating its use in subprocess calls or command execution workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "get_configured_hadoop_version", "line_number": 57, "body": "def get_configured_hadoop_version():\n    \"\"\"\n    CDH4 (hadoop 2+) has a slightly different syntax for interacting with hdfs\n    via the command line.\n\n    The default version is CDH4, but one can override\n    this setting with \"cdh3\" or \"apache1\" in the hadoop section of the config\n    in order to use the old syntax.\n    \"\"\"\n    return hadoopcli().version.lower()", "is_method": false, "function_description": "Function that returns the configured Hadoop version string, reflecting the syntax style used for command-line HDFS interactions based on user or default settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "get_configured_hdfs_client", "line_number": 69, "body": "def get_configured_hdfs_client():\n    \"\"\"\n    This is a helper that fetches the configuration value for 'client' in\n    the [hdfs] section. It will return the client that retains backwards\n    compatibility when 'client' isn't configured.\n    \"\"\"\n    return hdfs().client", "is_method": false, "function_description": "Utility function that returns the configured HDFS client, ensuring backward compatibility when no specific client is set in the configuration. It facilitates interaction with HDFS in various contexts requiring file system access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "tmppath", "line_number": 78, "body": "def tmppath(path=None, include_unix_username=True):\n    \"\"\"\n    @param path: target path for which it is needed to generate temporary location\n    @type path: str\n    @type include_unix_username: bool\n    @rtype: str\n\n    Note that include_unix_username might work on windows too.\n    \"\"\"\n    addon = \"luigitemp-%08d\" % random.randrange(1e9)\n    temp_dir = '/tmp'  # default tmp dir if none is specified in config\n\n    # 1. Figure out to which temporary directory to place\n    configured_hdfs_tmp_dir = hdfs().tmp_dir\n    if configured_hdfs_tmp_dir is not None:\n        # config is superior\n        base_dir = configured_hdfs_tmp_dir\n    elif path is not None:\n        # need to copy correct schema and network location\n        parsed = urlparse(path)\n        base_dir = urlunparse((parsed.scheme, parsed.netloc, temp_dir, '', '', ''))\n    else:\n        # just system temporary directory\n        base_dir = temp_dir\n\n    # 2. Figure out what to place\n    if path is not None:\n        if path.startswith(temp_dir + '/'):\n            # Not 100%, but some protection from directories like /tmp/tmp/file\n            subdir = path[len(temp_dir):]\n        else:\n            # Protection from /tmp/hdfs:/dir/file\n            parsed = urlparse(path)\n            subdir = parsed.path\n        subdir = subdir.lstrip('/') + '-'\n    else:\n        # just return any random temporary location\n        subdir = ''\n\n    if include_unix_username:\n        subdir = os.path.join(getpass.getuser(), subdir)\n\n    return os.path.join(base_dir, subdir + addon)", "is_method": false, "function_description": "Generates a unique temporary file path based on an optional target path and Unix username inclusion, supporting local and configured HDFS directories. Useful for safely creating temp locations aligned with existing storage schemas."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "abort", "line_number": 43, "body": "def abort(self):\n        logger.info(\"Aborting %s('%s'). Removing temporary file '%s'\",\n                    self.__class__.__name__, self.path, self.tmppath)\n        super(HdfsAtomicWritePipe, self).abort()\n        remove(self.tmppath, skip_trash=True)", "is_method": true, "class_name": "HdfsAtomicWritePipe", "function_description": "Method of HdfsAtomicWritePipe that cancels an ongoing write operation and removes the associated temporary file to ensure no partial data remains."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "close", "line_number": 49, "body": "def close(self):\n        super(HdfsAtomicWritePipe, self).close()\n        try:\n            if exists(self.path):\n                remove(self.path)\n        except Exception as ex:\n            if isinstance(ex, HDFSCliError) or ex.args[0].contains(\"FileNotFoundException\"):\n                pass\n            else:\n                raise ex\n        if not all(result['result'] for result in rename(self.tmppath, self.path) or []):\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))", "is_method": true, "class_name": "HdfsAtomicWritePipe", "function_description": "Finalizer method of HdfsAtomicWritePipe that ensures atomic file write by safely removing any existing target file and renaming the temporary file to the final destination, raising an error if the operation fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "abort", "line_number": 74, "body": "def abort(self):\n        logger.info(\"Aborting %s('%s'). Removing temporary dir '%s'\",\n                    self.__class__.__name__, self.path, self.tmppath)\n        super(HdfsAtomicWriteDirPipe, self).abort()\n        remove(self.tmppath, skip_trash=True)", "is_method": true, "class_name": "HdfsAtomicWriteDirPipe", "function_description": "Method of HdfsAtomicWriteDirPipe that aborts the current write operation by cleaning up the temporary directory to ensure no partial data persists. It provides fail-safe cancellation for atomic directory write processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "close", "line_number": 80, "body": "def close(self):\n        super(HdfsAtomicWriteDirPipe, self).close()\n        try:\n            if exists(self.path):\n                remove(self.path)\n        except Exception as ex:\n            if isinstance(ex, HDFSCliError) or ex.args[0].contains(\"FileNotFoundException\"):\n                pass\n            else:\n                raise ex\n\n        # it's unlikely to fail in this way but better safe than sorry\n        if not all(result['result'] for result in rename(self.tmppath, self.path) or []):\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))\n\n        if os.path.basename(self.tmppath) in map(os.path.basename, listdir(self.path)):\n            remove(self.path)\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))", "is_method": true, "class_name": "HdfsAtomicWriteDirPipe", "function_description": "Handles closing an atomic write operation to an HDFS directory by ensuring temporary files are safely renamed to the target path, guaranteeing atomicity and cleaning up partial writes to prevent inconsistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 105, "body": "def hdfs_writer(self, path):\n        return self.pipe_writer(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "Proxy method in PlainFormat that delegates writing data to HDFS by invoking the pipe_writer function with the given path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 108, "body": "def hdfs_reader(self, path):\n        return self.pipe_reader(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "This method provides a way to read data from a given HDFS path by delegating the operation to an existing pipe_reader function. It enables simplified access to HDFS files within the PlainFormat class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 111, "body": "def pipe_reader(self, path):\n        return HdfsReadPipe(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "Returns an HDFS read pipe for the specified path, enabling streaming data access from Hadoop file system locations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 114, "body": "def pipe_writer(self, output_pipe):\n        return HdfsAtomicWritePipe(output_pipe)", "is_method": true, "class_name": "PlainFormat", "function_description": "Returns an atomic write pipe wrapping the given output pipe to ensure safe, consistent writing to HDFS within the PlainFormat context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 123, "body": "def hdfs_writer(self, path):\n        return self.pipe_writer(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Simple wrapper method in PlainDirFormat that provides an interface for writing data to a given path using the existing pipe_writer functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 126, "body": "def hdfs_reader(self, path):\n        return self.pipe_reader(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Delegates reading data from an HDFS path by invoking a general pipe-based reader, providing a unified interface for accessing file contents within the PlainDirFormat context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 129, "body": "def pipe_reader(self, path):\n        # exclude underscore-prefixedfiles/folders (created by MapReduce)\n        return HdfsReadPipe(\"%s/[^_]*\" % path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Utility method of PlainDirFormat that creates a pipe to read files from a specified directory, excluding those with names starting with an underscore (commonly MapReduce system files)."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 133, "body": "def pipe_writer(self, path):\n        return HdfsAtomicWriteDirPipe(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Returns an atomic write pipe for writing data to a specified directory path, ensuring safe and atomic file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 152, "body": "def pipe_writer(self, output):\n        return self.writer(output)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "This method provides a unified interface to obtain a writer object for output streams, delegating the actual writing process to an underlying writer function. It simplifies writing data in a format compatible with HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 155, "body": "def pipe_reader(self, input):\n        return self.reader(input)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Simple delegate method in CompatibleHdfsFormat that forwards the input to an internal reader function, facilitating data reading from HDFS-compatible sources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 158, "body": "def hdfs_writer(self, output):\n        return self.writer(output)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Returns a writable output stream compatible with the HDFS format, facilitating data writing operations in distributed file systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 161, "body": "def hdfs_reader(self, input):\n        return self.reader(input)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Simple wrapper method in CompatibleHdfsFormat that delegates reading HDFS input to an underlying reader function. It provides unified access to HDFS data reading functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "__getstate__", "line_number": 168, "body": "def __getstate__(self):\n        d = self.__dict__.copy()\n        for attr in ('reader', 'writer'):\n            method = getattr(self, attr)\n            try:\n                # if instance method, pickle instance and method name\n                d[attr] = method.__self__, method.__func__.__name__\n            except AttributeError:\n                pass  # not an instance method\n        return d", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Method in CompatibleHdfsFormat that customizes object serialization by converting instance methods to a pickle-friendly form, ensuring compatibility when saving the object's state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "__setstate__", "line_number": 179, "body": "def __setstate__(self, d):\n        self.__dict__ = d\n        for attr in ('reader', 'writer'):\n            try:\n                method_self, method_name = d[attr]\n            except ValueError:\n                continue\n            method = getattr(method_self, method_name)\n            setattr(self, attr, method)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Reconstructs an instance's state by restoring its attributes and converting stored method references back into callable methods, ensuring compatibility when deserializing objects with reader and writer methods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/abstract_client.py", "function": "rename", "line_number": 31, "body": "def rename(self, path, dest):\n        \"\"\"\n        Rename or move a file.\n\n        In hdfs land, \"mv\" is often called rename. So we add an alias for\n        ``move()`` called ``rename()``. This is also to keep backward\n        compatibility since ``move()`` became standardized in luigi's\n        filesystem interface.\n        \"\"\"\n        return self.move(path, dest)", "is_method": true, "class_name": "HdfsFileSystem", "function_description": "Renames or moves a file within the HDFS file system, providing backward compatibility by aliasing the move operation under the rename method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/abstract_client.py", "function": "rename_dont_move", "line_number": 42, "body": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Override this method with an implementation that uses rename2,\n        which is a rename operation that never moves.\n\n        rename2 -\n        https://github.com/apache/hadoop/blob/ae91b13/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java\n        (lines 483-523)\n        \"\"\"\n        # We only override this method to be able to provide a more specific\n        # docstring.\n        return super(HdfsFileSystem, self).rename_dont_move(path, dest)", "is_method": true, "class_name": "HdfsFileSystem", "function_description": "This method provides a specialized rename operation in HdfsFileSystem that changes filenames without moving data locations, ensuring efficient renaming within the same storage directory."}]