[{"file": "./dataset/RepoExec/test-apps/luigi/setup.py", "function": "get_static_files", "line_number": 21, "body": "def get_static_files(path):\n    return [os.path.join(dirpath.replace(\"luigi/\", \"\"), ext)\n            for (dirpath, dirnames, filenames) in os.walk(path)\n            for ext in [\"*.html\", \"*.js\", \"*.css\", \"*.png\",\n                        \"*.eot\", \"*.svg\", \"*.ttf\", \"*.woff\", \"*.woff2\"]]", "is_method": false, "function_description": "Generates a list of potential file paths for common web static assets (e.g., HTML, CSS, JS, images, fonts) within a specified directory tree. This facilitates the identification or management of web application resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "_warn_node", "line_number": 56, "body": "def _warn_node(self, msg, node, *args, **kwargs):\n    \"\"\"\n    Mute warnings that are like ``WARNING: nonlocal image URI found: https://img. ...``\n\n    Solution was found by googling, copied it from SO:\n\n    http://stackoverflow.com/questions/12772927/specifying-an-online-image-in-sphinx-restructuredtext-format\n    \"\"\"\n    if not msg.startswith('nonlocal image URI found:'):\n        self._warnfunc(msg, '%s:%s' % get_source_line(node), *args, **kwargs)", "is_method": false, "function_description": "This function filters and suppresses specific warnings related to \"nonlocal image URI found\", preventing them from being displayed. It allows all other warning types to be processed normally."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "parameter_repr", "line_number": 27, "body": "def parameter_repr(self):\n        \"\"\"\n        When building documentation, we want Parameter objects to show their\n        description in a nice way\n        \"\"\"\n        significance = 'Insignificant ' if not self.significant else ''\n        class_name = self.__class__.__name__\n        has_default = self._default != luigi.parameter._no_value\n        default = ' (defaults to {})'.format(self._default) if has_default else ''\n        description = (': ' + self.description if self.description else '')\n        return significance + class_name + default + description", "is_method": false, "function_description": "Generates a human-readable string representation of a parameter object. This formatted output includes its significance, class, default value, and description, primarily for documentation purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/doc/conf.py", "function": "assertIn", "line_number": 41, "body": "def assertIn(needle, haystack):\n        \"\"\"\n        We test repr of Parameter objects, since it'll be used for readthedocs\n        \"\"\"\n        assert needle in haystack", "is_method": false, "function_description": "Asserts that a given 'needle' element is present within a 'haystack' collection or sequence. Raises an AssertionError if the 'needle' is not found.\n```text\nAsserts that a given 'needle' element is present within a 'haystack' collection or sequence. Raises an AssertionError if the 'needle' is not found.\n```"}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "requires", "line_number": 66, "body": "def requires(self):\n        return [ErrorTask1(), ErrorTask2(), SuccessTask1(), DynamicErrorTaskSubmitter()]", "is_method": true, "class_name": "PerTaskRetryPolicy", "function_description": "This method declares the specific tasks that this `PerTaskRetryPolicy` governs. It defines the set of tasks to which this retry policy's logic will be applied."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 69, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "PerTaskRetryPolicy", "function_description": "Defines the unique local file target for a Luigi task's output or status. It provides a `luigi.LocalTarget` object, typically for tracking task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 82, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "ErrorTask1", "function_description": "Simulates a task failure by always raising an exception, incrementing a retry counter. Useful for testing error handling and retry mechanisms in a controlled environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 86, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "ErrorTask1", "function_description": "Defines the local file target where the output of this Luigi task instance will be stored."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 99, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "ErrorTask2", "function_description": "The method simulates a failing task by incrementing a retry counter and raising an exception. It serves as a utility for testing error handling and retry mechanisms."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 103, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "ErrorTask2", "function_description": "Defines the task's output target as a unique temporary local file, crucial for Luigi's dependency management."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 110, "body": "def run(self):\n        target = yield DynamicErrorTask1()\n\n        if target.exists():\n            with self.output().open('w') as output:\n                output.write('SUCCESS DynamicErrorTaskSubmitter\\n')", "is_method": true, "class_name": "DynamicErrorTaskSubmitter", "function_description": "Orchestrates the execution of a dynamic error task. It confirms the dependent task's successful completion, signaling its own success accordingly."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 117, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "DynamicErrorTaskSubmitter", "function_description": "Defines the local file target where the Luigi task's output will be stored. It specifies a temporary path based on the task's ID for results."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 130, "body": "def run(self):\n        self.retry += 1\n        raise Exception('Test Exception. Retry Index %s for %s' % (self.retry, self.task_family))", "is_method": true, "class_name": "DynamicErrorTask1", "function_description": "Simulates a task failure by raising an exception, incrementing a retry counter. Useful for testing error handling and retry mechanisms."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 134, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "DynamicErrorTask1", "function_description": "Defines and returns the output target for the Luigi task. It points to a unique local file in the temporary directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "requires", "line_number": 139, "body": "def requires(self):\n        return [SuccessSubTask1()]", "is_method": true, "class_name": "SuccessTask1", "function_description": "Declares the specific sub-tasks that must be successfully completed as prerequisites for `SuccessTask1`. It defines the immediate dependencies for this task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 142, "body": "def run(self):\n        with self.output().open('w') as output:\n            output.write('SUCCESS Test Task 4\\n')", "is_method": true, "class_name": "SuccessTask1", "function_description": "This method records the successful completion of 'Test Task 4' by writing a specific success string to its designated output file. It serves to signal a task's successful execution within a workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 146, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "SuccessTask1", "function_description": "Defines the local file system target where the task's output will be stored. This is a standard Luigi method for tasks to declare their completion artifact."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "run", "line_number": 155, "body": "def run(self):\n        with self.output().open('w') as output:\n            output.write('SUCCESS Test Task 4.1\\n')", "is_method": true, "class_name": "SuccessSubTask1", "function_description": "Writes a predefined success message to its output. This method indicates the successful completion of `SuccessSubTask1`."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/per_task_retry_policy.py", "function": "output", "line_number": 159, "body": "def output(self):\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.task_id)", "is_method": true, "class_name": "SuccessSubTask1", "function_description": "This method specifies the local file target where the Luigi task stores its output. It provides a unique path for the task's results, essential for pipeline management."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "run", "line_number": 31, "body": "def run(self):\n        \"\"\"\n        Generates :py:attr:`~.UserItemMatrix.data_size` elements.\n        Writes this data in \\\\ separated value format into the target :py:func:`~/.UserItemMatrix.output`.\n\n        The data has the following elements:\n\n        * `user` is the default Elasticsearch id field,\n        * `track`: the text,\n        * `rating`: the day when the data was created.\n\n        \"\"\"\n        w = self.output().open('w')\n        for user in range(self.data_size):\n            track = int(random.random() * self.data_size)\n            w.write('%d\\\\%d\\\\%f' % (user, track, 1.0))\n        w.close()", "is_method": true, "class_name": "UserItemMatrix", "function_description": "Generates and writes a specified amount of synthetic user-item interaction data. Each entry includes a user ID, a random track ID, and a fixed rating, outputting to a file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "output", "line_number": 49, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('data-matrix', format=luigi.format.Gzip)", "is_method": true, "class_name": "UserItemMatrix", "function_description": "Defines the output target for this Luigi task. It specifies an HDFS file named 'data-matrix' as the gzipped output."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "app_options", "line_number": 84, "body": "def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]", "is_method": true, "class_name": "SparkALS", "function_description": "Provides the input and output file paths that are passed as main arguments to the Spark ALS application."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "requires", "line_number": 88, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.UserItemMatrix`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return UserItemMatrix(self.data_size)", "is_method": true, "class_name": "SparkALS", "function_description": "Specifies that the SparkALS task requires the `UserItemMatrix` task to be completed. This defines a crucial dependency in the task workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/spark_als.py", "function": "output", "line_number": 98, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        # The corresponding Spark job outputs as GZip format.\n        return luigi.contrib.hdfs.HdfsTarget('als-output/', format=luigi.format.Gzip)", "is_method": true, "class_name": "SparkALS", "function_description": "This method specifies the target output for the Spark ALS task, defining that its successful execution will create a gzipped file in HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "output", "line_number": 33, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on a remote server using SSH.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return RemoteTarget(\n            \"/tmp/stuff\",\n            SSH_HOST\n        )", "is_method": true, "class_name": "CreateRemoteData", "function_description": "Defines the remote file target that this task will produce upon successful execution. This target represents a file on a remote server accessible via SSH."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "run", "line_number": 46, "body": "def run(self):\n        remote = RemoteContext(SSH_HOST)\n        print(remote.check_output([\n            \"ps aux > {0}\".format(self.output().path)\n        ]))", "is_method": true, "class_name": "CreateRemoteData", "function_description": "This method connects to a remote server via SSH to execute a shell command. It saves the command's output to a specified file on the remote host, thus creating remote data."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "requires", "line_number": 60, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.CreateRemoteData`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return CreateRemoteData()", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "This method declares that the `ProcessRemoteData` task depends on the `CreateRemoteData` task. It specifies the necessary prerequisite for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "run", "line_number": 70, "body": "def run(self):\n        processes_per_user = defaultdict(int)\n        with self.input().open('r') as infile:\n            for line in infile:\n                username = line.split()[0]\n                processes_per_user[username] += 1\n\n        toplist = sorted(\n            processes_per_user.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n\n        with self.output().open('w') as outfile:\n            for user, n_processes in toplist:\n                print(n_processes, user, file=outfile)", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "This method processes remote data to count and rank user activity from an input stream. It then outputs the sorted list of users by their process counts."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ssh_remote_execution.py", "function": "output", "line_number": 87, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will simulate the creation of a file in a filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return MockTarget(\"output\", mirror_on_stderr=True)", "is_method": true, "class_name": "ProcessRemoteData", "function_description": "This method defines the expected output target for the `ProcessRemoteData` task. It simulates a file creation, crucial for dependency tracking in workflow systems like Luigi."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "output", "line_number": 27, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in the local file system.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(self.date.strftime('/var/tmp/text/%Y-%m-%d.txt'))", "is_method": true, "class_name": "InputText", "function_description": "Returns a Luigi target that defines the expected local file system path for the task's dated output, specifically a daily text file. This specifies where the task's results will be stored.\nReturns a Luigi target that defines the expected local file system path for the task's dated output, specifically a daily text file. This specifies where the task's results will be stored."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "requires", "line_number": 41, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.InputText`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [InputText(date) for date in self.date_interval.dates()]", "is_method": true, "class_name": "WordCount", "function_description": "Defines the prerequisite tasks required for the WordCount operation. It specifies a list of `InputText` tasks, ensuring correct execution order within the workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "output", "line_number": 51, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/var/tmp/text-count/%s' % self.date_interval)", "is_method": true, "class_name": "WordCount", "function_description": "Specifies the local file path where the WordCount task's output will be stored upon successful completion, defining the target for this data processing task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount.py", "function": "run", "line_number": 61, "body": "def run(self):\n        \"\"\"\n        1. count the words for each of the :py:meth:`~.InputText.output` targets created by :py:class:`~.InputText`\n        2. write the count into the :py:meth:`~.WordCount.output` target\n        \"\"\"\n        count = {}\n\n        # NOTE: self.input() actually returns an element for the InputText.output() target\n        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in count.items():\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close()", "is_method": true, "class_name": "WordCount", "function_description": "This method of the `WordCount` class processes input text to count the occurrences of each word. It then writes these aggregated word counts to a designated output target."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "run", "line_number": 54, "body": "def run(self):\n        time.sleep(1)\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "Creates an empty output file after a one-second delay, potentially signaling a completed or ready state for a subsequent process."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "requires", "line_number": 58, "body": "def requires(self):\n        global current_nodes\n\n        if max_total_nodes > current_nodes:\n            valor = int(random.uniform(1, 30))\n            for i in range(valor // max_depth):\n                current_nodes += 1\n                yield Bar(current_nodes)", "is_method": true, "class_name": "Bar", "function_description": "This generator method creates and yields new `Bar` instances (nodes) as long as a global limit is not met. It controls the dynamic expansion of a node-based structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo_complex.py", "function": "output", "line_number": 67, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        time.sleep(1)\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "Defines the local file target representing the output of this Bar task. This serves as a dependency for other tasks in a Luigi pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 27, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Config_%d.txt' % self.seed)", "is_method": true, "class_name": "Configuration", "function_description": "This method specifies the local file path that serves as the output target for the associated task. It defines where the task's results will be stored on the filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 37, "body": "def run(self):\n        time.sleep(5)\n        rnd.seed(self.seed)\n\n        result = ','.join(\n            [str(x) for x in rnd.sample(list(range(300)), rnd.randint(7, 25))])\n        with self.output().open('w') as f:\n            f.write(result)", "is_method": true, "class_name": "Configuration", "function_description": "The `run` method generates a reproducible, comma-separated string of unique random integers and writes it to a file. It provides a source of controlled random data for testing or configuration generation."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Data_%d.txt' % self.magic_number)", "is_method": true, "class_name": "Data", "function_description": "This method defines and returns the local filesystem target where a Luigi task's output will be stored upon successful completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 60, "body": "def run(self):\n        time.sleep(1)\n        with self.output().open('w') as f:\n            f.write('%s' % self.magic_number)", "is_method": true, "class_name": "Data", "function_description": "This method writes a predefined \"magic number\" to a specified output target. It serves to generate a simple data file containing this value."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "output", "line_number": 69, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/Dynamic_%d.txt' % self.seed)", "is_method": true, "class_name": "Dynamic", "function_description": "This method specifies the local file system target where the task's output will be stored, enabling other tasks to locate its expected results."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/dynamic_requirements.py", "function": "run", "line_number": 79, "body": "def run(self):\n        # This could be done using regular requires method\n        config = self.clone(Configuration)\n        yield config\n\n        with config.output().open() as f:\n            data = [int(x) for x in f.read().split(',')]\n\n        # ... but not this\n        data_dependent_deps = [Data(magic_number=x) for x in data]\n        yield data_dependent_deps\n\n        with self.output().open('w') as f:\n            f.write('Tada!')", "is_method": true, "class_name": "Dynamic", "function_description": "This method enables dynamic workflow execution by first processing a configuration, then reading its output to determine and yield data-dependent downstream tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/hello_world.py", "function": "run", "line_number": 16, "body": "def run(self):\n        print(\"{task} says: Hello world!\".format(task=self.__class__.__name__))", "is_method": true, "class_name": "HelloWorldTask", "function_description": "This method executes a basic 'Hello world!' task. It prints a greeting message to the console, identifying itself by its class name."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "output", "line_number": 34, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file that will be created in a FTP server.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return RemoteTarget('/experiment/output1.txt', HOST, username=USER, password=PWD)", "is_method": true, "class_name": "ExperimentTask", "function_description": "Defines the expected output for the experiment task. This method specifies that a successful task execution will create a file on an FTP server."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "run", "line_number": 44, "body": "def run(self):\n        \"\"\"\n        The execution of this task will write 4 lines of data on this task's target output.\n        \"\"\"\n        with self.output().open('w') as outfile:\n            print(\"data 0 200 10 50 60\", file=outfile)\n            print(\"data 1 190 9 52 60\", file=outfile)\n            print(\"data 2 200 10 52 60\", file=outfile)\n            print(\"data 3 195 1 52 60\", file=outfile)", "is_method": true, "class_name": "ExperimentTask", "function_description": "This method executes the experiment task. It writes four predefined lines of data to the task's designated output target."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "requires", "line_number": 61, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.ExperimentTask`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return ExperimentTask()", "is_method": true, "class_name": "ProcessingTask", "function_description": "This method declares that `ProcessingTask` depends on `ExperimentTask`. It defines the prerequisite task required for `ProcessingTask` to run, facilitating task orchestration."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "output", "line_number": 71, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget('/tmp/processeddata.txt')", "is_method": true, "class_name": "ProcessingTask", "function_description": "This method specifies the expected output of the processing task, defining the target location of the local file that a successful execution will generate."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/ftp_experiment_outputs.py", "function": "run", "line_number": 81, "body": "def run(self):\n        avg = 0.0\n        elements = 0\n        sumval = 0.0\n\n        # Target objects are a file system/format abstraction and this will return a file stream object\n        # NOTE: self.input() actually returns the ExperimentTask.output() target\n        for line in self.input().open('r'):\n            values = line.split(\" \")\n            avg += float(values[2])\n            sumval += float(values[3])\n            elements = elements + 1\n\n        # average\n        avg = avg / elements\n\n        # save calculated values\n        with self.output().open('w') as outfile:\n            print(avg, sumval, file=outfile)", "is_method": true, "class_name": "ProcessingTask", "function_description": "Processes input data streams by parsing space-separated values. It calculates the average of values in one column and the sum of values in another, then writes these aggregate statistics to an output."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists_spark.py", "function": "main", "line_number": 9, "body": "def main(argv):\n    input_paths = argv[1].split(',')\n    output_path = argv[2]\n\n    spark = SparkSession.builder.getOrCreate()\n\n    streams = spark.read.option('sep', '\\t').csv(input_paths[0])\n    for stream_path in input_paths[1:]:\n        streams.union(spark.read.option('sep', '\\t').csv(stream_path))\n\n    # The second field is the artist\n    counts = streams \\\n        .map(lambda row: (row[1], 1)) \\\n        .reduceByKey(operator.add)\n\n    counts.write.option('sep', '\\t').csv(output_path)", "is_method": false, "function_description": "Processes a tab-separated CSV file using Apache Spark. It counts occurrences of the artist field (second column) from the input data and writes the aggregated counts to a specified output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "run", "line_number": 32, "body": "def run(self):\n        \"\"\"\n        Writes data in JSON format into the task's output target.\n\n        The data objects have the following attributes:\n\n        * `_id` is the default Elasticsearch id field,\n        * `text`: the text,\n        * `date`: the day when the data was created.\n\n        \"\"\"\n        today = datetime.date.today()\n        with self.output().open('w') as output:\n            for i in range(5):\n                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n                                         'date': str(today)}))\n                output.write('\\n')", "is_method": true, "class_name": "FakeDocuments", "function_description": "Generates a small set of synthetic JSON documents with IDs, text, and creation dates. It writes this structured, fake data to the task's specified output target."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.date)", "is_method": true, "class_name": "FakeDocuments", "function_description": "This method specifies the local file system path where the `FakeDocuments` task's output will be stored. It provides a Luigi target representing the expected output file for the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/elasticsearch_index.py", "function": "requires", "line_number": 103, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.FakeDocuments`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return FakeDocuments()", "is_method": true, "class_name": "IndexDocuments", "function_description": "This method declares the `IndexDocuments` task's dependencies. It specifies that the `FakeDocuments` task must complete before `IndexDocuments` can run."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 65, "body": "def requires(self):\n        for i in range(5, 200):\n            yield Bar(i)", "is_method": true, "class_name": "Boom", "function_description": "This method provides a generator that yields an iterative sequence of `Bar` objects. It supplies a set of required components or resources for the `Boom` class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 78, "body": "def requires(self):\n        yield MyExternal()\n        yield Boom(0)", "is_method": true, "class_name": "Foo", "function_description": "This method declares the external components or dependencies that the `Foo` class requires to function correctly. It provides a list of prerequisites for the class's operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "run", "line_number": 87, "body": "def run(self):\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "Creates an empty output file or resource. This often serves as a marker to signal task completion within a workflow or dependency management system."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "output", "line_number": 90, "body": "def output(self):\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "This method defines the local file target where this Luigi task's output will be stored. It enables Luigi to manage task dependencies and locate results."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 102, "body": "def requires(self):\n        yield MyExternal()\n        yield Boom(0)", "is_method": true, "class_name": "DateTask", "function_description": "This method declares the dependencies required for the DateTask to execute. It yields specific external and internal task objects that must be completed first."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/execution_summary_example.py", "function": "requires", "line_number": 113, "body": "def requires(self):\n        for i in range(10):\n            yield Foo(100, 2 * i)\n        for i in range(10):\n            yield DateTask(datetime.date(1998, 3, 23) + datetime.timedelta(days=i), 5)", "is_method": true, "class_name": "EntryPoint", "function_description": "Provides a stream of required tasks or dependencies, yielding instances of `Foo` and `DateTask` objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "requires", "line_number": 37, "body": "def requires(self):\n        for i in range(10):\n            yield Bar(i)", "is_method": true, "class_name": "Foo", "function_description": "This generator method provides a sequence of ten `Bar` objects, likely representing components or dependencies needed by the `Foo` class."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "run", "line_number": 46, "body": "def run(self):\n        time.sleep(1)\n        self.output().open('w').close()", "is_method": true, "class_name": "Bar", "function_description": "Completes a task by creating or emptying its designated output file after a one-second delay, typically signaling successful execution or readiness for subsequent steps."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/foo.py", "function": "output", "line_number": 50, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        time.sleep(1)\n        return luigi.LocalTarget('/tmp/bar/%d' % self.num)", "is_method": true, "class_name": "Bar", "function_description": "Provides a Luigi LocalTarget object specifying the task's output file path. This method defines where the task's results will be stored for other tasks to consume."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "input", "line_number": 41, "body": "def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "Specifies the S3 location for the input data source of the word count process."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "output", "line_number": 44, "body": "def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "This method specifies the S3 location where the word count results from the PySpark job will be stored. It defines the output target for the processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "main", "line_number": 47, "body": "def main(self, sc, *args):\n        sc.textFile(self.input().path) \\\n          .flatMap(lambda line: line.split()) \\\n          .map(lambda word: (word, 1)) \\\n          .reduceByKey(lambda a, b: a + b) \\\n          .saveAsTextFile(self.output().path)", "is_method": true, "class_name": "InlinePySparkWordCount", "function_description": "Provides a PySpark implementation to count word frequencies from an input text file. It processes the data and saves the aggregated counts to an specified output path."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "app_options", "line_number": 81, "body": "def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]", "is_method": true, "class_name": "PySparkWordCount", "function_description": "This method of `PySparkWordCount` provides the input and output file paths needed as arguments for the Spark application. It configures the necessary I/O for the Spark job."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "input", "line_number": 85, "body": "def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")", "is_method": true, "class_name": "PySparkWordCount", "function_description": "Provides the S3 target path for the word count input data. It specifies the source location for the text to be processed by the PySpark word count application."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/pyspark_wc.py", "function": "output", "line_number": 88, "body": "def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')", "is_method": true, "class_name": "PySparkWordCount", "function_description": "This method specifies the Amazon S3 location where the word count results will be stored, providing the target for the PySpark operation's output."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "hadoop_examples_jar", "line_number": 28, "body": "def hadoop_examples_jar():\n    config = luigi.configuration.get_config()\n    examples_jar = config.get('hadoop', 'examples-jar')\n    if not examples_jar:\n        logger.error(\"You must specify hadoop:examples-jar in luigi.cfg\")\n        raise\n    if not os.path.exists(examples_jar):\n        logger.error(\"Can't find example jar: \" + examples_jar)\n        raise\n    return examples_jar", "is_method": false, "function_description": "Retrieves and validates the configured path to the Hadoop examples JAR file. It ensures the specified JAR exists for use by Luigi tasks requiring Hadoop examples."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "output", "line_number": 54, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.terasort_in)", "is_method": true, "class_name": "TeraGen", "function_description": "This method defines the target output for the TeraGen task. It specifies that the task's successful execution will create a file in HDFS, providing the location for its result."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "jar", "line_number": 64, "body": "def jar(self):\n        return hadoop_examples_jar()", "is_method": true, "class_name": "TeraGen", "function_description": "Returns the Hadoop examples JAR. This enables TeraGen to access necessary resources for Hadoop-related operations or benchmarks."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "args", "line_number": 70, "body": "def args(self):\n        # First arg is 10B -- each record is 100bytes\n        return [self.records, self.output()]", "is_method": true, "class_name": "TeraGen", "function_description": "Provides the necessary arguments for the TeraGen process, specifying the number of records to generate and the designated output location."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "requires", "line_number": 85, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.TeraGen`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return TeraGen(terasort_in=self.terasort_in)", "is_method": true, "class_name": "TeraSort", "function_description": "This method declares that the `TeraSort` task depends on the `TeraGen` task. It ensures `TeraGen` completes before `TeraSort` can execute within the workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "output", "line_number": 95, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.terasort_out)", "is_method": true, "class_name": "TeraSort", "function_description": "Specifies the HDFS file where the TeraSort task's output will be written. It defines the target location for the successful completion of this distributed sorting operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "jar", "line_number": 105, "body": "def jar(self):\n        return hadoop_examples_jar()", "is_method": true, "class_name": "TeraSort", "function_description": "Provides the Hadoop examples JAR file, typically used for submitting distributed jobs like TeraSort."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/terasort.py", "function": "args", "line_number": 111, "body": "def args(self):\n        return [self.input(), self.output()]", "is_method": true, "class_name": "TeraSort", "function_description": "This method defines and provides the input and output paths required for the TeraSort operation. It specifies where the sorting process should read data from and write sorted data to."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "output", "line_number": 38, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('/tmp/text/%Y-%m-%d.txt'))", "is_method": true, "class_name": "InputText", "function_description": "Provides the Luigi HDFS target representing the expected output file for the task, dynamically determined by date."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "requires", "line_number": 60, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.InputText`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [InputText(date) for date in self.date_interval.dates()]", "is_method": true, "class_name": "WordCount", "function_description": "Declares the upstream `InputText` tasks that the `WordCount` task depends on. It identifies these prerequisites based on a specified date interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "output", "line_number": 70, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('/tmp/text-count/%s' % self.date_interval)", "is_method": true, "class_name": "WordCount", "function_description": "Specifies the HDFS file target that represents the successful output of the WordCount task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "mapper", "line_number": 80, "body": "def mapper(self, line):\n        for word in line.strip().split():\n            yield word, 1", "is_method": true, "class_name": "WordCount", "function_description": "Prepares a line of text for word counting by yielding each word paired with an initial count of one. It serves as the mapping step in a MapReduce process."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/wordcount_hadoop.py", "function": "reducer", "line_number": 84, "body": "def reducer(self, key, values):\n        yield key, sum(values)", "is_method": true, "class_name": "WordCount", "function_description": "This method aggregates numerical values associated with a specific key. It sums all occurrences to provide a final total count for that key, typical for MapReduce-style operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 37, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, it expects a file to be present in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('data/streams_%Y-%m-%d.tsv'))", "is_method": true, "class_name": "ExternalStreams", "function_description": "This method provides the HDFS file target for this external streams task's output, specifying where its processed data is expected to reside for subsequent tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 54, "body": "def run(self):\n        \"\"\"\n        Generates bogus data and writes it into the :py:meth:`~.Streams.output` target.\n        \"\"\"\n        with self.output().open('w') as output:\n            for _ in range(1000):\n                output.write('{} {} {}\\n'.format(\n                    random.randint(0, 999),\n                    random.randint(0, 999),\n                    random.randint(0, 999)))", "is_method": true, "class_name": "Streams", "function_description": "This method generates 1000 lines of random \"bogus\" numerical data. It writes this synthetic data to the class's configured output target for testing or simulation purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 65, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in the local file system.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(self.date.strftime('data/streams_%Y_%m_%d_faked.tsv'))", "is_method": true, "class_name": "Streams", "function_description": "Defines the local file system target where the Streams task expects to write its generated data upon successful completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 84, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(self.date.strftime('data/streams_%Y_%m_%d_faked.tsv'))", "is_method": true, "class_name": "StreamsHdfs", "function_description": "Defines the HDFS target for this task's output. It returns an `HdfsTarget` representing the specific dated file expected upon successful execution, crucial for Luigi workflow management."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 103, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(\"data/artist_streams_{}.tsv\".format(self.date_interval))", "is_method": true, "class_name": "AggregateArtists", "function_description": "Defines the expected local file path where this Luigi task will store its output, allowing other tasks to declare dependencies on it."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 113, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.Streams`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [Streams(date) for date in self.date_interval]", "is_method": true, "class_name": "AggregateArtists", "function_description": "Specifies `Streams` tasks as dependencies for `AggregateArtists` across its date interval. It defines the necessary prerequisites for workflow orchestration."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 123, "body": "def run(self):\n        artist_count = defaultdict(int)\n\n        for t in self.input():\n            with t.open('r') as in_file:\n                for line in in_file:\n                    _, artist, track = line.strip().split()\n                    artist_count[artist] += 1\n\n        with self.output().open('w') as out_file:\n            for artist, count in artist_count.items():\n                out_file.write('{}\\t{}\\n'.format(artist, count))", "is_method": true, "class_name": "AggregateArtists", "function_description": "Aggregates and counts the total occurrences of each unique artist found in input data. It provides a summarized list of artists and their activity for reporting or further analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 159, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget(\"data/artist_streams_%s.tsv\" % self.date_interval)", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "This method defines the HDFS file path where the task's output data will be stored. It represents the target output for a Luigi workflow task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 169, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.StreamsHdfs`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return [StreamsHdfs(date) for date in self.date_interval]", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "This method declares the upstream tasks that must be completed before the `AggregateArtistsSpark` task can execute, ensuring its data dependencies are met."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "app_options", "line_number": 179, "body": "def app_options(self):\n        # :func:`~luigi.task.Task.input` returns the targets produced by the tasks in\n        # `~luigi.task.Task.requires`.\n        return [','.join([p.path for p in self.input()]),\n                self.output().path]", "is_method": true, "class_name": "AggregateArtistsSpark", "function_description": "This method compiles and returns a list containing the comma-separated input file paths and the output file path. It provides essential I/O configuration arguments for an external application, likely a Spark job."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 196, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.AggregateArtists` or\n        * :py:class:`~.AggregateArtistsSpark` if :py:attr:`~/.Top10Artists.use_spark` is set.\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        if self.use_spark:\n            return AggregateArtistsSpark(self.date_interval)\n        else:\n            return AggregateArtists(self.date_interval)", "is_method": true, "class_name": "Top10Artists", "function_description": "Determines the prerequisite tasks that the Top10Artists job depends on. It conditionally selects either a Spark or a non-Spark aggregation task."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "output", "line_number": 210, "body": "def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file on the local filesystem.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`luigi.target.Target`)\n        \"\"\"\n        return luigi.LocalTarget(\"data/top_artists_%s.tsv\" % self.date_interval)", "is_method": true, "class_name": "Top10Artists", "function_description": "This method specifies the output target for the Top10Artists task. It provides a Luigi LocalTarget object representing the TSV file where the task's results will be written."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "run", "line_number": 220, "body": "def run(self):\n        top_10 = nlargest(10, self._input_iterator())\n        with self.output().open('w') as out_file:\n            for streams, artist in top_10:\n                out_line = '\\t'.join([\n                    str(self.date_interval.date_a),\n                    str(self.date_interval.date_b),\n                    artist,\n                    str(streams)\n                ])\n                out_file.write((out_line + '\\n'))", "is_method": true, "class_name": "Top10Artists", "function_description": "The `run` method in `Top10Artists` determines the top 10 artists by stream count from its input. It then writes this ranked list, including a date interval, to an output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "_input_iterator", "line_number": 232, "body": "def _input_iterator(self):\n        with self.input().open('r') as in_file:\n            for line in in_file:\n                artist, streams = line.strip().split()\n                yield int(streams), artist", "is_method": true, "class_name": "Top10Artists", "function_description": "This internal method iterates over an input file, parsing each line to extract and yield artist stream data. It provides structured (streams, artist) pairs for further processing, like finding top artists."}, {"file": "./dataset/RepoExec/test-apps/luigi/examples/top_artists.py", "function": "requires", "line_number": 264, "body": "def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.Top10Artists`\n\n        :return: list of object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return Top10Artists(self.date_interval, self.use_spark)", "is_method": true, "class_name": "ArtistToplistToDatabase", "function_description": "Declares `Top10Artists` as a required dependency for this task, ensuring it runs only after its prerequisite is successfully completed. This defines the execution order within a data pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_partition_tasks", "line_number": 91, "body": "def _partition_tasks(worker):\n    \"\"\"\n    Takes a worker and sorts out tasks based on their status.\n    Still_pending_not_ext is only used to get upstream_failure, upstream_missing_dependency and run_by_other_worker\n    \"\"\"\n    task_history = worker._add_task_history\n    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}\n    set_tasks = {}\n    set_tasks[\"completed\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}\n    set_tasks[\"already_done\"] = {task for (task, status, ext) in task_history\n                                 if status == 'DONE' and task not in pending_tasks and task not in set_tasks[\"completed\"]}\n    set_tasks[\"ever_failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"failed\"] = set_tasks[\"ever_failed\"] - set_tasks[\"completed\"]\n    set_tasks[\"scheduling_error\"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}\n    set_tasks[\"still_pending_ext\"] = {task for (task, status, ext) in task_history\n                                      if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and not ext}\n    set_tasks[\"still_pending_not_ext\"] = {task for (task, status, ext) in task_history\n                                          if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and ext}\n    set_tasks[\"run_by_other_worker\"] = set()\n    set_tasks[\"upstream_failure\"] = set()\n    set_tasks[\"upstream_missing_dependency\"] = set()\n    set_tasks[\"upstream_run_by_other_worker\"] = set()\n    set_tasks[\"upstream_scheduling_error\"] = set()\n    set_tasks[\"not_run\"] = set()\n    return set_tasks", "is_method": false, "function_description": "Based on a worker's task history, this function categorizes tasks into sets according to their status. It provides a structured overview for managing task states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_root_task", "line_number": 118, "body": "def _root_task(worker):\n    \"\"\"\n    Return the first task scheduled by the worker, corresponding to the root task\n    \"\"\"\n    return worker._add_task_history[0][0]", "is_method": false, "function_description": "Retrieves the initial task a worker was assigned, serving as the root of its task execution history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_populate_unknown_statuses", "line_number": 125, "body": "def _populate_unknown_statuses(set_tasks):\n    \"\"\"\n    Add the \"upstream_*\" and \"not_run\" statuses my mutating set_tasks.\n    \"\"\"\n    visited = set()\n    for task in set_tasks[\"still_pending_not_ext\"]:\n        _depth_first_search(set_tasks, task, visited)", "is_method": false, "function_description": "This function updates the `set_tasks` structure, assigning 'upstream_*' and 'not_run' statuses to tasks. It identifies and populates unknown task states for dependency tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_depth_first_search", "line_number": 134, "body": "def _depth_first_search(set_tasks, current_task, visited):\n    \"\"\"\n    This dfs checks why tasks are still pending.\n    \"\"\"\n    visited.add(current_task)\n    if current_task in set_tasks[\"still_pending_not_ext\"]:\n        upstream_failure = False\n        upstream_missing_dependency = False\n        upstream_run_by_other_worker = False\n        upstream_scheduling_error = False\n        for task in current_task._requires():\n            if task not in visited:\n                _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n                set_tasks[\"upstream_failure\"].add(current_task)\n                upstream_failure = True\n            if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:\n                set_tasks[\"upstream_missing_dependency\"].add(current_task)\n                upstream_missing_dependency = True\n            if task in set_tasks[\"run_by_other_worker\"] or task in set_tasks[\"upstream_run_by_other_worker\"]:\n                set_tasks[\"upstream_run_by_other_worker\"].add(current_task)\n                upstream_run_by_other_worker = True\n            if task in set_tasks[\"scheduling_error\"]:\n                set_tasks[\"upstream_scheduling_error\"].add(current_task)\n                upstream_scheduling_error = True\n        if not upstream_failure and not upstream_missing_dependency and \\\n                not upstream_run_by_other_worker and not upstream_scheduling_error and \\\n                current_task not in set_tasks[\"run_by_other_worker\"]:\n            set_tasks[\"not_run\"].add(current_task)", "is_method": false, "function_description": "Performs a depth-first search to determine why tasks are pending. It categorizes pending tasks by tracing upstream failures, missing dependencies, or other execution statuses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str", "line_number": 165, "body": "def _get_str(task_dict, extra_indent):\n    \"\"\"\n    This returns a string for each status\n    \"\"\"\n    summary_length = execution_summary().summary_length\n\n    lines = []\n    task_names = sorted(task_dict.keys())\n    for task_family in task_names:\n        tasks = task_dict[task_family]\n        tasks = sorted(tasks, key=lambda x: str(x))\n        prefix_size = 8 if extra_indent else 4\n        prefix = ' ' * prefix_size\n\n        line = None\n\n        if summary_length > 0 and len(lines) >= summary_length:\n            line = prefix + \"...\"\n            lines.append(line)\n            break\n        if len(tasks[0].get_params()) == 0:\n            line = prefix + '- {0} {1}()'.format(len(tasks), str(task_family))\n        elif _get_len_of_params(tasks[0]) > 60 or len(str(tasks[0])) > 200 or \\\n                (len(tasks) == 2 and len(tasks[0].get_params()) > 1 and (_get_len_of_params(tasks[0]) > 40 or len(str(tasks[0])) > 100)):\n            \"\"\"\n            This is to make sure that there is no really long task in the output\n            \"\"\"\n            line = prefix + '- {0} {1}(...)'.format(len(tasks), task_family)\n        elif len((tasks[0].get_params())) == 1:\n            attributes = {getattr(task, tasks[0].get_params()[0][0]) for task in tasks}\n            param_class = tasks[0].get_params()[0][1]\n            first, last = _ranging_attributes(attributes, param_class)\n            if first is not None and last is not None and len(attributes) > 3:\n                param_str = '{0}...{1}'.format(param_class.serialize(first), param_class.serialize(last))\n            else:\n                param_str = '{0}'.format(_get_str_one_parameter(tasks))\n            line = prefix + '- {0} {1}({2}={3})'.format(len(tasks), task_family, tasks[0].get_params()[0][0], param_str)\n        else:\n            ranging = False\n            params = _get_set_of_params(tasks)\n            unique_param_keys = list(_get_unique_param_keys(params))\n            if len(unique_param_keys) == 1:\n                unique_param, = unique_param_keys\n                attributes = params[unique_param]\n                param_class = unique_param[1]\n                first, last = _ranging_attributes(attributes, param_class)\n                if first is not None and last is not None and len(attributes) > 2:\n                    ranging = True\n                    line = prefix + '- {0} {1}({2}'.format(len(tasks), task_family, _get_str_ranging_multiple_parameters(first, last, tasks, unique_param))\n            if not ranging:\n                if len(tasks) == 1:\n                    line = prefix + '- {0} {1}'.format(len(tasks), tasks[0])\n                if len(tasks) == 2:\n                    line = prefix + '- {0} {1} and {2}'.format(len(tasks), tasks[0], tasks[1])\n                if len(tasks) > 2:\n                    line = prefix + '- {0} {1} ...'.format(len(tasks), tasks[0])\n        lines.append(line)\n    return '\\n'.join(lines)", "is_method": false, "function_description": "Generates a concise, formatted string summarizing tasks grouped by family. It provides a human-readable overview of task execution, abbreviating details for clarity and brevity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_len_of_params", "line_number": 225, "body": "def _get_len_of_params(task):\n    return sum(len(param[0]) for param in task.get_params())", "is_method": false, "function_description": "Calculates the sum of the lengths of the first element for each parameter associated with a given task. This function provides a cumulative size metric for the task's parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str_ranging_multiple_parameters", "line_number": 229, "body": "def _get_str_ranging_multiple_parameters(first, last, tasks, unique_param):\n    row = ''\n    str_unique_param = '{0}...{1}'.format(unique_param[1].serialize(first), unique_param[1].serialize(last))\n    for param in tasks[0].get_params():\n        row += '{0}='.format(param[0])\n        if param[0] == unique_param[0]:\n            row += '{0}'.format(str_unique_param)\n        else:\n            row += '{0}'.format(param[1].serialize(getattr(tasks[0], param[0])))\n        if param != tasks[0].get_params()[-1]:\n            row += \", \"\n    row += ')'\n    return row", "is_method": false, "function_description": "Generates a formatted string representing a task's parameters, where one specified parameter is shown as a range. Other parameters are serialized from the task object itself."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_set_of_params", "line_number": 244, "body": "def _get_set_of_params(tasks):\n    params = {}\n    for param in tasks[0].get_params():\n        params[param] = {getattr(task, param[0]) for task in tasks}\n    return params", "is_method": false, "function_description": "Collects unique values for specified parameters across a list of task objects. It returns a dictionary where each parameter's descriptor maps to a set of its distinct values found among all tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_unique_param_keys", "line_number": 251, "body": "def _get_unique_param_keys(params):\n    for param_key, param_values in params.items():\n        if len(param_values) > 1:\n            yield param_key", "is_method": false, "function_description": "Provides keys for parameters that have multiple distinct values in an input dictionary. Useful for identifying variable configuration options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_ranging_attributes", "line_number": 257, "body": "def _ranging_attributes(attributes, param_class):\n    \"\"\"\n    Checks if there is a continuous range\n    \"\"\"\n    next_attributes = {param_class.next_in_enumeration(attribute) for attribute in attributes}\n    in_first = attributes.difference(next_attributes)\n    in_second = next_attributes.difference(attributes)\n    if len(in_first) == 1 and len(in_second) == 1:\n        for x in attributes:\n            if {param_class.next_in_enumeration(x)} == in_second:\n                return next(iter(in_first)), x\n    return None, None", "is_method": false, "function_description": "Checks if a set of attributes forms a continuous range according to a provided enumeration logic. If so, it identifies and returns the start and end points of this continuous sequence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_str_one_parameter", "line_number": 271, "body": "def _get_str_one_parameter(tasks):\n    row = ''\n    count = 0\n    for task in tasks:\n        if (len(row) >= 30 and count > 2 and count != len(tasks) - 1) or len(row) > 200:\n            row += '...'\n            break\n        param = task.get_params()[0]\n        row += '{0}'.format(param[1].serialize(getattr(task, param[0])))\n        if count < len(tasks) - 1:\n            row += ','\n        count += 1\n    return row", "is_method": false, "function_description": "This function serializes the first parameter of each task into a comma-separated string. It truncates the result for brevity when it exceeds a certain length."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_serialize_first_param", "line_number": 286, "body": "def _serialize_first_param(task):\n    return task.get_params()[0][1].serialize(getattr(task, task.get_params()[0][0]))", "is_method": false, "function_description": "Serializes an attribute of the `task` object. It dynamically determines the attribute to serialize and the serializer to use from the task's first parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_number_of_tasks_for", "line_number": 290, "body": "def _get_number_of_tasks_for(status, group_tasks):\n    if status == \"still_pending\":\n        return (_get_number_of_tasks(group_tasks[\"still_pending_ext\"]) +\n                _get_number_of_tasks(group_tasks[\"still_pending_not_ext\"]))\n    return _get_number_of_tasks(group_tasks[status])", "is_method": false, "function_description": "Computes the total number of tasks for a specified status. It aggregates counts from two specific categories when the status is \"still_pending.\""}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_number_of_tasks", "line_number": 297, "body": "def _get_number_of_tasks(task_dict):\n    return sum(len(tasks) for tasks in task_dict.values())", "is_method": false, "function_description": "Calculates the total count of individual tasks stored across all categories within a given dictionary. It sums the number of tasks contained in each category's list or collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_comments", "line_number": 301, "body": "def _get_comments(group_tasks):\n    \"\"\"\n    Get the human readable comments and quantities for the task types.\n    \"\"\"\n    comments = {}\n    for status, human in _COMMENTS:\n        num_tasks = _get_number_of_tasks_for(status, group_tasks)\n        if num_tasks:\n            space = \"    \" if status in _PENDING_SUB_STATUSES else \"\"\n            comments[status] = '{space}* {num_tasks} {human}:\\n'.format(\n                space=space,\n                num_tasks=num_tasks,\n                human=human)\n    return comments", "is_method": false, "function_description": "This function generates a dictionary of human-readable summaries for different task types. It quantifies the number of tasks for each status, preparing formatted strings suitable for display or reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_run_by_other_worker", "line_number": 350, "body": "def _get_run_by_other_worker(worker):\n    \"\"\"\n    This returns a set of the tasks that are being run by other worker\n    \"\"\"\n    task_sets = _get_external_workers(worker).values()\n    return functools.reduce(lambda a, b: a | b, task_sets, set())", "is_method": false, "function_description": "This function aggregates and returns a single set of all tasks currently being processed by other workers. It provides an overview of tasks managed externally to a given worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_get_external_workers", "line_number": 358, "body": "def _get_external_workers(worker):\n    \"\"\"\n    This returns a dict with a set of tasks for all of the other workers\n    \"\"\"\n    worker_that_blocked_task = collections.defaultdict(set)\n    get_work_response_history = worker._get_work_response_history\n    for get_work_response in get_work_response_history:\n        if get_work_response['task_id'] is None:\n            for running_task in get_work_response['running_tasks']:\n                other_worker_id = running_task['worker']\n                other_task_id = running_task['task_id']\n                other_task = worker._scheduled_tasks.get(other_task_id)\n                if other_worker_id == worker._id or not other_task:\n                    continue\n                worker_that_blocked_task[other_worker_id].add(other_task)\n    return worker_that_blocked_task", "is_method": false, "function_description": "Retrieves a mapping of other workers to tasks they are currently running. This helps identify external tasks potentially affecting or blocking the current worker's operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_group_tasks_by_name_and_status", "line_number": 376, "body": "def _group_tasks_by_name_and_status(task_dict):\n    \"\"\"\n    Takes a dictionary with sets of tasks grouped by their status and\n    returns a dictionary with dictionaries with an array of tasks grouped by\n    their status and task name\n    \"\"\"\n    group_status = {}\n    for task in task_dict:\n        if task.task_family not in group_status:\n            group_status[task.task_family] = []\n        group_status[task.task_family].append(task)\n    return group_status", "is_method": false, "function_description": "This function organizes a collection of task objects. It groups them into a dictionary where keys are task family names and values are lists of corresponding tasks, simplifying access by task type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_summary_dict", "line_number": 390, "body": "def _summary_dict(worker):\n    set_tasks = _partition_tasks(worker)\n    set_tasks[\"run_by_other_worker\"] = _get_run_by_other_worker(worker)\n    _populate_unknown_statuses(set_tasks)\n    return set_tasks", "is_method": false, "function_description": "Generates a comprehensive summary of tasks for a given worker, categorizing them and identifying those handled by other workers. It populates task statuses for a complete view."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_summary_format", "line_number": 397, "body": "def _summary_format(set_tasks, worker):\n    group_tasks = {}\n    for status, task_dict in set_tasks.items():\n        group_tasks[status] = _group_tasks_by_name_and_status(task_dict)\n    comments = _get_comments(group_tasks)\n    num_all_tasks = sum([len(set_tasks[\"already_done\"]),\n                         len(set_tasks[\"completed\"]), len(set_tasks[\"failed\"]),\n                         len(set_tasks[\"scheduling_error\"]),\n                         len(set_tasks[\"still_pending_ext\"]),\n                         len(set_tasks[\"still_pending_not_ext\"])])\n    str_output = ''\n    str_output += 'Scheduled {0} tasks of which:\\n'.format(num_all_tasks)\n    for status in _ORDERED_STATUSES:\n        if status not in comments:\n            continue\n        str_output += '{0}'.format(comments[status])\n        if status != 'still_pending':\n            str_output += '{0}\\n'.format(_get_str(group_tasks[status], status in _PENDING_SUB_STATUSES))\n    ext_workers = _get_external_workers(worker)\n    group_tasks_ext_workers = {}\n    for ext_worker, task_dict in ext_workers.items():\n        group_tasks_ext_workers[ext_worker] = _group_tasks_by_name_and_status(task_dict)\n    if len(ext_workers) > 0:\n        str_output += \"\\nThe other workers were:\\n\"\n        count = 0\n        for ext_worker, task_dict in ext_workers.items():\n            if count > 3 and count < len(ext_workers) - 1:\n                str_output += \"    and {0} other workers\".format(len(ext_workers) - count)\n                break\n            str_output += \"    - {0} ran {1} tasks\\n\".format(ext_worker, len(task_dict))\n            count += 1\n        str_output += '\\n'\n    if num_all_tasks == sum([len(set_tasks[\"already_done\"]),\n                             len(set_tasks[\"scheduling_error\"]),\n                             len(set_tasks[\"still_pending_ext\"]),\n                             len(set_tasks[\"still_pending_not_ext\"])]):\n        if len(ext_workers) == 0:\n            str_output += '\\n'\n        str_output += 'Did not run any tasks'\n    one_line_summary = _create_one_line_summary(_tasks_status(set_tasks))\n    str_output += \"\\n{0}\".format(one_line_summary)\n    if num_all_tasks == 0:\n        str_output = 'Did not schedule any tasks'\n    return str_output", "is_method": false, "function_description": "Generates a detailed text summary of scheduled task statuses. It reports task counts by category and includes information on tasks processed by external workers, providing a comprehensive operational overview."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_create_one_line_summary", "line_number": 443, "body": "def _create_one_line_summary(status_code):\n    \"\"\"\n    Given a status_code of type LuigiStatusCode which has a tuple value, returns a one line summary\n    \"\"\"\n    return \"This progress looks {0} because {1}\".format(*status_code.value)", "is_method": false, "function_description": "Converts a `LuigiStatusCode` object into a concise, human-readable one-line summary string. This function provides a simple way to get a descriptive status message for display or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_tasks_status", "line_number": 450, "body": "def _tasks_status(set_tasks):\n    \"\"\"\n    Given a grouped set of tasks, returns a LuigiStatusCode\n    \"\"\"\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            return LuigiStatusCode.SUCCESS_WITH_RETRY\n        else:\n            if set_tasks[\"scheduling_error\"]:\n                return LuigiStatusCode.FAILED_AND_SCHEDULING_FAILED\n            return LuigiStatusCode.FAILED\n    elif set_tasks[\"scheduling_error\"]:\n        return LuigiStatusCode.SCHEDULING_FAILED\n    elif set_tasks[\"not_run\"]:\n        return LuigiStatusCode.NOT_RUN\n    elif set_tasks[\"still_pending_ext\"]:\n        return LuigiStatusCode.MISSING_EXT\n    else:\n        return LuigiStatusCode.SUCCESS", "is_method": false, "function_description": "Determines the consolidated status of a group of tasks by evaluating various flags like failure, scheduling errors, or pending states. It provides a single Luigi status code for monitoring and reporting task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "_summary_wrap", "line_number": 471, "body": "def _summary_wrap(str_output):\n    return textwrap.dedent(\"\"\"\n    ===== Luigi Execution Summary =====\n\n    {str_output}\n\n    ===== Luigi Execution Summary =====\n    \"\"\").format(str_output=str_output)", "is_method": false, "function_description": "This utility wraps a given string with specific \"Luigi Execution Summary\" banners, providing a consistent visual presentation for execution outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "summary", "line_number": 481, "body": "def summary(worker):\n    \"\"\"\n    Given a worker, return a human readable summary of what the worker have\n    done.\n    \"\"\"\n    return _summary_wrap(_summary_format(_summary_dict(worker), worker))", "is_method": false, "function_description": "This function generates a human-readable summary of a given worker's completed activities. It provides a concise overview of the worker's actions for reporting or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "__str__", "line_number": 84, "body": "def __str__(self):\n        return \"LuigiRunResult with status {0}\".format(self.status)", "is_method": true, "class_name": "LuigiRunResult", "function_description": "Provides a concise string representation of a Luigi run result object, indicating its current status. Useful for logging and debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/execution_summary.py", "function": "__repr__", "line_number": 87, "body": "def __repr__(self):\n        return \"LuigiRunResult(status={0!r},worker={1!r},scheduling_succeeded={2!r})\".format(self.status, self.worker, self.scheduling_succeeded)", "is_method": true, "class_name": "LuigiRunResult", "function_description": "Provides a concise, developer-friendly string representation of a Luigi task run's outcome. It aids in debugging and logging by displaying key status information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_is_external", "line_number": 80, "body": "def _is_external(task):\n    return task.run is None or task.run == NotImplemented", "is_method": false, "function_description": "Determines if a task is considered \"external\" by checking if its run method is undefined or explicitly marked as not implemented. This function helps classify tasks not executed directly by the system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_retry_policy_dict", "line_number": 84, "body": "def _get_retry_policy_dict(task):\n    return RetryPolicy(task.retry_count, task.disable_hard_timeout, task.disable_window)._asdict()", "is_method": false, "function_description": "This internal helper extracts specific retry policy parameters from a task object. It provides these parameters as a dictionary, suitable for configuration or serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "check_complete", "line_number": 395, "body": "def check_complete(task, out_queue):\n    \"\"\"\n    Checks if task is complete, puts the result to out_queue.\n    \"\"\"\n    logger.debug(\"Checking if %s is complete\", task)\n    try:\n        is_complete = task.complete()\n    except Exception:\n        is_complete = TracebackWrapper(traceback.format_exc())\n    out_queue.put((task, is_complete))", "is_method": false, "function_description": "Checks a given task's completion status, handling exceptions during the check. It then places the task and its completion result into an output queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "rpc_message_callback", "line_number": 500, "body": "def rpc_message_callback(fn):\n    fn.is_rpc_message_callback = True\n    return fn", "is_method": false, "function_description": "Marks a function to designate it as an RPC message callback. This enables frameworks to identify and utilize it for handling remote procedure calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_run_get_new_deps", "line_number": 132, "body": "def _run_get_new_deps(self):\n        task_gen = self.task.run()\n\n        if not isinstance(task_gen, types.GeneratorType):\n            return None\n\n        next_send = None\n        while True:\n            try:\n                if next_send is None:\n                    requires = next(task_gen)\n                else:\n                    requires = task_gen.send(next_send)\n            except StopIteration:\n                return None\n\n            new_req = flatten(requires)\n            if all(t.complete() for t in new_req):\n                next_send = getpaths(requires)\n            else:\n                new_deps = [(t.task_module, t.task_family, t.to_str_params())\n                            for t in new_req]\n                return new_deps", "is_method": true, "class_name": "TaskProcess", "function_description": "Advances a task's internal execution state, retrieving its immediate dependencies. It returns a list of any newly identified, incomplete sub-tasks that require external processing to proceed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 156, "body": "def run(self):\n        logger.info('[pid %s] Worker %s running   %s', os.getpid(), self.worker_id, self.task)\n\n        if self.use_multiprocessing:\n            # Need to have different random seeds if running in separate processes\n            random.seed((os.getpid(), time.time()))\n\n        status = FAILED\n        expl = ''\n        missing = []\n        new_deps = []\n        try:\n            # Verify that all the tasks are fulfilled! For external tasks we\n            # don't care about unfulfilled dependencies, because we are just\n            # checking completeness of self.task so outputs of dependencies are\n            # irrelevant.\n            if self.check_unfulfilled_deps and not _is_external(self.task):\n                missing = [dep.task_id for dep in self.task.deps() if not dep.complete()]\n                if missing:\n                    deps = 'dependency' if len(missing) == 1 else 'dependencies'\n                    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n            self.task.trigger_event(Event.START, self.task)\n            t0 = time.time()\n            status = None\n\n            if _is_external(self.task):\n                # External task\n                if self.task.complete():\n                    status = DONE\n                else:\n                    status = FAILED\n                    expl = 'Task is an external data dependency ' \\\n                        'and data does not exist (yet?).'\n            else:\n                with self._forward_attributes():\n                    new_deps = self._run_get_new_deps()\n                if not new_deps:\n                    if not self.check_complete_on_run or self.task.complete():\n                        status = DONE\n                    else:\n                        raise TaskException(\"Task finished running, but complete() is still returning false.\")\n                else:\n                    status = PENDING\n\n            if new_deps:\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task)\n            elif status == DONE:\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                expl = self.task.on_success()\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n\n        except KeyboardInterrupt:\n            raise\n        except BaseException as ex:\n            status = FAILED\n            expl = self._handle_run_exception(ex)\n\n        finally:\n            self.result_queue.put(\n                (self.task.task_id, status, expl, missing, new_deps))", "is_method": true, "class_name": "TaskProcess", "function_description": "This method orchestrates the execution of a single task, managing its dependencies and lifecycle events. It determines the task's completion status, handles exceptions, and reports the final result."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_run_exception", "line_number": 222, "body": "def _handle_run_exception(self, ex):\n        logger.exception(\"[pid %s] Worker %s failed    %s\", os.getpid(), self.worker_id, self.task)\n        self.task.trigger_event(Event.FAILURE, self.task, ex)\n        return self.task.on_failure(ex)", "is_method": true, "class_name": "TaskProcess", "function_description": "Manages exceptions that occur during a task's execution by logging the error, triggering a failure event, and invoking the task's custom failure handler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_recursive_terminate", "line_number": 227, "body": "def _recursive_terminate(self):\n        import psutil\n\n        try:\n            parent = psutil.Process(self.pid)\n            children = parent.children(recursive=True)\n\n            # terminate parent. Give it a chance to clean up\n            super(TaskProcess, self).terminate()\n            parent.wait()\n\n            # terminate children\n            for child in children:\n                try:\n                    child.terminate()\n                except psutil.NoSuchProcess:\n                    continue\n        except psutil.NoSuchProcess:\n            return", "is_method": true, "class_name": "TaskProcess", "function_description": "This method terminates the `TaskProcess` instance and recursively terminates all its child processes. It ensures a clean shutdown by preventing orphaned processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "terminate", "line_number": 247, "body": "def terminate(self):\n        \"\"\"Terminate this process and its subprocesses.\"\"\"\n        # default terminate() doesn't cleanup child processes, it orphans them.\n        try:\n            return self._recursive_terminate()\n        except ImportError:\n            return super(TaskProcess, self).terminate()", "is_method": true, "class_name": "TaskProcess", "function_description": "As a method of TaskProcess, it ensures a complete shutdown by terminating the process and all its subprocesses. This prevents orphaned processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_forward_attributes", "line_number": 256, "body": "def _forward_attributes(self):\n        # forward configured attributes to the task\n        for reporter_attr, task_attr in self.forward_reporter_attributes.items():\n            setattr(self.task, task_attr, getattr(self.status_reporter, reporter_attr))\n        try:\n            yield self\n        finally:\n            # reset attributes again\n            for reporter_attr, task_attr in self.forward_reporter_attributes.items():\n                setattr(self.task, task_attr, None)", "is_method": true, "class_name": "TaskProcess", "function_description": "Temporarily transfers configured attributes from a status reporter to the task object. It ensures these attributes are reset on the task after an operation, facilitating clean state management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 275, "body": "def run(self):\n        if self.context:\n            logger.debug('Importing module and instantiating ' + self.context)\n            module_path, class_name = self.context.rsplit('.', 1)\n            module = importlib.import_module(module_path)\n            cls = getattr(module, class_name)\n\n            with cls(self):\n                super(ContextManagedTaskProcess, self).run()\n        else:\n            super(ContextManagedTaskProcess, self).run()", "is_method": true, "class_name": "ContextManagedTaskProcess", "function_description": "This method executes the process's main task, optionally wrapping its execution within a dynamically instantiated context manager. This enables external resources or environments to be set up and torn down around the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_tracking_url", "line_number": 301, "body": "def update_tracking_url(self, tracking_url):\n        self._scheduler.add_task(\n            task_id=self._task_id,\n            worker=self._worker_id,\n            status=RUNNING,\n            tracking_url=tracking_url\n        )", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Updates the tracking URL for the current task, registering its status as running with the scheduler. This enables external systems to monitor the task's progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_status_message", "line_number": 309, "body": "def update_status_message(self, message):\n        self._scheduler.set_task_status_message(self._task_id, message)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Updates the status message for a specific task. This allows other components to report the current state of a running task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "update_progress_percentage", "line_number": 312, "body": "def update_progress_percentage(self, percentage):\n        self._scheduler.set_task_progress_percentage(self._task_id, percentage)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Notifies the underlying scheduler about the current completion percentage of the associated task. This updates the task's progress status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "decrease_running_resources", "line_number": 315, "body": "def decrease_running_resources(self, decrease_resources):\n        self._scheduler.decrease_running_task_resources(self._task_id, decrease_resources)", "is_method": true, "class_name": "TaskStatusReporter", "function_description": "Decreases the running resources allocated to the task by instructing the scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__str__", "line_number": 335, "body": "def __str__(self):\n        return str(self.content)", "is_method": true, "class_name": "SchedulerMessage", "function_description": "Provides a user-friendly string representation of the SchedulerMessage object. It returns the string value of its `content` attribute for easy display or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__eq__", "line_number": 338, "body": "def __eq__(self, other):\n        return self.content == other", "is_method": true, "class_name": "SchedulerMessage", "function_description": "This method defines equality for SchedulerMessage objects by comparing their content. It enables straightforward checks for identical message payloads."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "respond", "line_number": 341, "body": "def respond(self, response):\n        self._scheduler.add_scheduler_message_response(self._task_id, self._message_id, response)", "is_method": true, "class_name": "SchedulerMessage", "function_description": "This method allows a `SchedulerMessage` to send a response back to its associated scheduler. It records the response for a specific task and message ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "apply_async", "line_number": 352, "body": "def apply_async(self, function, args):\n        return function(*args)", "is_method": true, "class_name": "SingleProcessPool", "function_description": "This method immediately executes a specified function with its arguments. It provides a synchronous implementation mimicking the `apply_async` interface for a single-process context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "put", "line_number": 367, "body": "def put(self, obj, block=None, timeout=None):\n        return self.append(obj)", "is_method": true, "class_name": "DequeQueue", "function_description": "Adds an item to the end of the DequeQueue, effectively enqueuing it for later retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "get", "line_number": 370, "body": "def get(self, block=None, timeout=None):\n        try:\n            return self.pop()\n        except IndexError:\n            raise Queue.Empty", "is_method": true, "class_name": "DequeQueue", "function_description": "Retrieves the next item from the queue, providing a way to consume queued data. It signals when the queue is empty."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "stop", "line_number": 478, "body": "def stop(self):\n        self._should_stop.set()", "is_method": true, "class_name": "KeepAliveThread", "function_description": "This method signals the KeepAliveThread to stop its ongoing operations and terminate its execution gracefully."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 481, "body": "def run(self):\n        while True:\n            self._should_stop.wait(self._ping_interval)\n            if self._should_stop.is_set():\n                logger.info(\"Worker %s was stopped. Shutting down Keep-Alive thread\" % self._worker_id)\n                break\n            with fork_lock:\n                response = None\n                try:\n                    response = self._scheduler.ping(worker=self._worker_id)\n                except BaseException:  # httplib.BadStatusLine:\n                    logger.warning('Failed pinging scheduler')\n\n                # handle rpc messages\n                if response:\n                    for message in response[\"rpc_messages\"]:\n                        self._rpc_message_callback(message)", "is_method": true, "class_name": "KeepAliveThread", "function_description": "This method continuously pings a scheduler to maintain a worker's active status and receive remote procedure call (RPC) messages. It ensures continuous communication and liveness in a distributed system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_task", "line_number": 565, "body": "def _add_task(self, *args, **kwargs):\n        \"\"\"\n        Call ``self._scheduler.add_task``, but store the values too so we can\n        implement :py:func:`luigi.execution_summary.summary`.\n        \"\"\"\n        task_id = kwargs['task_id']\n        status = kwargs['status']\n        runnable = kwargs['runnable']\n        task = self._scheduled_tasks.get(task_id)\n        if task:\n            self._add_task_history.append((task, status, runnable))\n            kwargs['owners'] = task._owner_list()\n\n        if task_id in self._batch_running_tasks:\n            for batch_task in self._batch_running_tasks.pop(task_id):\n                self._add_task_history.append((batch_task, status, True))\n\n        if task and kwargs.get('params'):\n            kwargs['param_visibilities'] = task._get_param_visibilities()\n\n        self._scheduler.add_task(*args, **kwargs)\n\n        logger.info('Informed scheduler that task   %s   has status   %s', task_id, status)", "is_method": true, "class_name": "Worker", "function_description": "Informs the scheduler about a task's status while also recording detailed task information. This supports historical tracking and the generation of execution summaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__enter__", "line_number": 589, "body": "def __enter__(self):\n        \"\"\"\n        Start the KeepAliveThread.\n        \"\"\"\n        self._keep_alive_thread = KeepAliveThread(self._scheduler, self._id,\n                                                  self._config.ping_interval,\n                                                  self._handle_rpc_message)\n        self._keep_alive_thread.daemon = True\n        self._keep_alive_thread.start()\n        return self", "is_method": true, "class_name": "Worker", "function_description": "Establishes and starts a background thread that ensures the worker's continuous active status and communication with its scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "__exit__", "line_number": 600, "body": "def __exit__(self, type, value, traceback):\n        \"\"\"\n        Stop the KeepAliveThread and kill still running tasks.\n        \"\"\"\n        self._keep_alive_thread.stop()\n        self._keep_alive_thread.join()\n        for task in self._running_tasks.values():\n            if task.is_alive():\n                task.terminate()\n        self._task_result_queue.close()\n        return False", "is_method": true, "class_name": "Worker", "function_description": "As part of the context manager protocol, this method ensures a clean shutdown of the Worker by stopping its keep-alive thread, terminating active tasks, and closing the result queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_generate_worker_info", "line_number": 612, "body": "def _generate_worker_info(self):\n        # Generate as much info as possible about the worker\n        # Some of these calls might not be available on all OS's\n        args = [('salt', '%09d' % random.randrange(0, 999999999)),\n                ('workers', self.worker_processes)]\n        try:\n            args += [('host', socket.gethostname())]\n        except BaseException:\n            pass\n        try:\n            args += [('username', getpass.getuser())]\n        except BaseException:\n            pass\n        try:\n            args += [('pid', os.getpid())]\n        except BaseException:\n            pass\n        try:\n            sudo_user = os.getenv(\"SUDO_USER\")\n            if sudo_user:\n                args.append(('sudo_user', sudo_user))\n        except BaseException:\n            pass\n        return args", "is_method": true, "class_name": "Worker", "function_description": "Generates a list of identifying attributes for the worker, including host, user, and process details. This provides essential contextual information about the running worker instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_generate_worker_id", "line_number": 637, "body": "def _generate_worker_id(self, worker_info):\n        worker_info_str = ', '.join(['{}={}'.format(k, v) for k, v in worker_info])\n        return 'Worker({})'.format(worker_info_str)", "is_method": true, "class_name": "Worker", "function_description": "Generates a unique and formatted string identifier for a worker from its key information. This ID facilitates easy identification and tracking of individual workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_validate_task", "line_number": 641, "body": "def _validate_task(self, task):\n        if not isinstance(task, Task):\n            raise TaskException('Can not schedule non-task %s' % task)\n\n        if not task.initialized():\n            # we can't get the repr of it since it's not initialized...\n            raise TaskException('Task of class %s not initialized. Did you override __init__ and forget to call super(...).__init__?' % task.__class__.__name__)", "is_method": true, "class_name": "Worker", "function_description": "Ensures that a given object is a valid and initialized Task instance. This prevents the worker from attempting to process malformed or unready tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_complete_error", "line_number": 649, "body": "def _log_complete_error(self, task, tb):\n        log_msg = \"Will not run {task} or any dependencies due to error in complete() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)", "is_method": true, "class_name": "Worker", "function_description": "Logs a warning when a critical error in a task's `complete()` method prevents the task and its dependencies from executing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_dependency_error", "line_number": 653, "body": "def _log_dependency_error(self, task, tb):\n        log_msg = \"Will not run {task} or any dependencies due to error in deps() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)", "is_method": true, "class_name": "Worker", "function_description": "Logs a warning when a task's dependencies fail to resolve, preventing the task and its dependents from executing. It provides critical insight into operational failures within a worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_unexpected_error", "line_number": 657, "body": "def _log_unexpected_error(self, task):\n        logger.exception(\"Luigi unexpected framework error while scheduling %s\", task)", "is_method": true, "class_name": "Worker", "function_description": "Logs an unexpected framework error encountered during task scheduling within the Luigi system. This helps in identifying and debugging critical issues in the task management process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_announce_scheduling_failure", "line_number": 660, "body": "def _announce_scheduling_failure(self, task, expl):\n        try:\n            self._scheduler.announce_scheduling_failure(\n                worker=self._id,\n                task_name=str(task),\n                family=task.task_family,\n                params=task.to_str_params(only_significant=True),\n                expl=expl,\n                owners=task._owner_list(),\n            )\n        except Exception:\n            formatted_traceback = traceback.format_exc()\n            self._email_unexpected_error(task, formatted_traceback)\n            raise", "is_method": true, "class_name": "Worker", "function_description": "Informs the scheduler about a task's scheduling failure, providing detailed context for the specific task and worker. It ensures this critical announcement is robustly reported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_complete_error", "line_number": 675, "body": "def _email_complete_error(self, task, formatted_traceback):\n        self._announce_scheduling_failure(task, formatted_traceback)\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} failed scheduling. Host: {host}\",\n                              headline=\"Will not run {task} or any dependencies due to error in complete() method\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "Notifies about task scheduling failures caused by errors in the `complete()` method. It can optionally send an email alert regarding these critical issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_dependency_error", "line_number": 683, "body": "def _email_dependency_error(self, task, formatted_traceback):\n        self._announce_scheduling_failure(task, formatted_traceback)\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} failed scheduling. Host: {host}\",\n                              headline=\"Will not run {task} or any dependencies due to error in deps() method\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "This method notifies stakeholders about a task's scheduling failure due to an error in its dependency resolution. It announces the failure and can optionally send an email alert."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_unexpected_error", "line_number": 691, "body": "def _email_unexpected_error(self, task, formatted_traceback):\n        # this sends even if failure e-mails are disabled, as they may indicate\n        # a more severe failure that may not reach other alerting methods such\n        # as scheduler batch notification\n        self._email_error(task, formatted_traceback,\n                          subject=\"Luigi: Framework error while scheduling {task}. Host: {host}\",\n                          headline=\"Luigi framework error\",\n                          )", "is_method": true, "class_name": "Worker", "function_description": "This method sends an urgent email notification for unexpected framework errors during task scheduling, bypassing regular error email settings to ensure critical alerts are sent."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_task_failure", "line_number": 700, "body": "def _email_task_failure(self, task, formatted_traceback):\n        if self._config.send_failure_email:\n            self._email_error(task, formatted_traceback,\n                              subject=\"Luigi: {task} FAILED. Host: {host}\",\n                              headline=\"A task failed when running. Most likely run() raised an exception.\",\n                              )", "is_method": true, "class_name": "Worker", "function_description": "This Worker method sends an email notification for a failed task, if configured to do so. It provides an alert for monitoring and debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_email_error", "line_number": 707, "body": "def _email_error(self, task, formatted_traceback, subject, headline):\n        formatted_subject = subject.format(task=task, host=self.host)\n        formatted_headline = headline.format(task=task, host=self.host)\n        command = subprocess.list2cmdline(sys.argv)\n        message = notifications.format_task_error(\n            formatted_headline, task, command, formatted_traceback)\n        notifications.send_error_email(formatted_subject, message, task.owner_email)", "is_method": true, "class_name": "Worker", "function_description": "This method sends a formatted error email to the task owner when an error occurs. It includes task details, hostname, and traceback information in the subject and message."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_task_load_error", "line_number": 715, "body": "def _handle_task_load_error(self, exception, task_ids):\n        msg = 'Cannot find task(s) sent by scheduler: {}'.format(','.join(task_ids))\n        logger.exception(msg)\n        subject = 'Luigi: {}'.format(msg)\n        error_message = notifications.wrap_traceback(exception)\n        for task_id in task_ids:\n            self._add_task(\n                worker=self._id,\n                task_id=task_id,\n                status=FAILED,\n                runnable=False,\n                expl=error_message,\n            )\n        notifications.send_error_email(subject, error_message)", "is_method": true, "class_name": "Worker", "function_description": "Manages and reports failures when the worker cannot load tasks sent by the scheduler. It logs the error, marks tasks as failed, and sends a notification email.\nManages and reports failures when the worker cannot load tasks sent by the scheduler. It logs the error, marks tasks as failed, and sends a notification email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "add", "line_number": 730, "body": "def add(self, task, multiprocess=False, processes=0):\n        \"\"\"\n        Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.\n        \"\"\"\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool(processes=processes if processes > 0 else None)\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = {task.task_id}\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n            raise\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded", "is_method": true, "class_name": "Worker", "function_description": "Adds a task and its dependencies to the Worker for processing. It orchestrates their completion checks and scheduling, managing the entire task graph's execution with optional multiprocessing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_task_batcher", "line_number": 776, "body": "def _add_task_batcher(self, task):\n        family = task.task_family\n        if family not in self._batch_families_sent:\n            task_class = type(task)\n            batch_param_names = task_class.batch_param_names()\n            if batch_param_names:\n                self._scheduler.add_task_batcher(\n                    worker=self._id,\n                    task_family=family,\n                    batched_args=batch_param_names,\n                    max_batch_size=task.max_batch_size,\n                )\n            self._batch_families_sent.add(family)", "is_method": true, "class_name": "Worker", "function_description": "Registers a task family's batching configuration with the scheduler. This enables efficient grouping and processing of similar tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add", "line_number": 790, "body": "def _add(self, task, is_complete):\n        if self._config.task_limit is not None and len(self._scheduled_tasks) >= self._config.task_limit:\n            logger.warning('Will not run %s or any dependencies due to exceeded task-limit of %d', task, self._config.task_limit)\n            deps = None\n            status = UNKNOWN\n            runnable = False\n\n        else:\n            formatted_traceback = None\n            try:\n                self._check_complete_value(is_complete)\n            except KeyboardInterrupt:\n                raise\n            except AsyncCompletionException as ex:\n                formatted_traceback = ex.trace\n            except BaseException:\n                formatted_traceback = traceback.format_exc()\n\n            if formatted_traceback is not None:\n                self.add_succeeded = False\n                self._log_complete_error(task, formatted_traceback)\n                task.trigger_event(Event.DEPENDENCY_MISSING, task)\n                self._email_complete_error(task, formatted_traceback)\n                deps = None\n                status = UNKNOWN\n                runnable = False\n\n            elif is_complete:\n                deps = None\n                status = DONE\n                runnable = False\n                task.trigger_event(Event.DEPENDENCY_PRESENT, task)\n\n            elif _is_external(task):\n                deps = None\n                status = PENDING\n                runnable = self._config.retry_external_tasks\n                task.trigger_event(Event.DEPENDENCY_MISSING, task)\n                logger.warning('Data for %s does not exist (yet?). The task is an '\n                               'external data dependency, so it cannot be run from'\n                               ' this luigi process.', task)\n\n            else:\n                try:\n                    deps = task.deps()\n                    self._add_task_batcher(task)\n                except Exception as ex:\n                    formatted_traceback = traceback.format_exc()\n                    self.add_succeeded = False\n                    self._log_dependency_error(task, formatted_traceback)\n                    task.trigger_event(Event.BROKEN_TASK, task, ex)\n                    self._email_dependency_error(task, formatted_traceback)\n                    deps = None\n                    status = UNKNOWN\n                    runnable = False\n                else:\n                    status = PENDING\n                    runnable = True\n\n            if task.disabled:\n                status = DISABLED\n\n            if deps:\n                for d in deps:\n                    self._validate_dependency(d)\n                    task.trigger_event(Event.DEPENDENCY_DISCOVERED, task, d)\n                    yield d  # return additional tasks to add\n\n                deps = [d.task_id for d in deps]\n\n        self._scheduled_tasks[task.task_id] = task\n        self._add_task(\n            worker=self._id,\n            task_id=task.task_id,\n            status=status,\n            deps=deps,\n            runnable=runnable,\n            priority=task.priority,\n            resources=task.process_resources(),\n            params=task.to_str_params(),\n            family=task.task_family,\n            module=task.task_module,\n            batchable=task.batchable,\n            retry_policy_dict=_get_retry_policy_dict(task),\n            accepts_messages=task.accepts_messages,\n        )", "is_method": true, "class_name": "Worker", "function_description": "Registers a task with the worker, determining its initial status and runnability by checking completion, external status, and resolving dependencies. It manages task limits and potential errors during this process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_validate_dependency", "line_number": 877, "body": "def _validate_dependency(self, dependency):\n        if isinstance(dependency, Target):\n            raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')\n        elif not isinstance(dependency, Task):\n            raise Exception('requires() must return Task objects but {} is a {}'.format(dependency, type(dependency)))", "is_method": true, "class_name": "Worker", "function_description": "This private method validates a dependency, ensuring it is a `Task` object and not a `Target`. It enforces proper dependency type declaration within the worker system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_check_complete_value", "line_number": 883, "body": "def _check_complete_value(self, is_complete):\n        if is_complete not in (True, False):\n            if isinstance(is_complete, TracebackWrapper):\n                raise AsyncCompletionException(is_complete.trace)\n            raise Exception(\"Return value of Task.complete() must be boolean (was %r)\" % is_complete)", "is_method": true, "class_name": "Worker", "function_description": "Validates a task's completion status, ensuring it is a boolean or an exception wrapper. It raises specific errors if the provided value is invalid or represents an asynchronous failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_add_worker", "line_number": 889, "body": "def _add_worker(self):\n        self._worker_info.append(('first_task', self._first_task))\n        self._scheduler.add_worker(self._id, self._worker_info)", "is_method": true, "class_name": "Worker", "function_description": "Registers the current worker instance with the system's scheduler, after preparing its initial task information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_log_remote_tasks", "line_number": 893, "body": "def _log_remote_tasks(self, get_work_response):\n        logger.debug(\"Done\")\n        logger.debug(\"There are no more tasks to run at this time\")\n        if get_work_response.running_tasks:\n            for r in get_work_response.running_tasks:\n                logger.debug('%s is currently run by worker %s', r['task_id'], r['worker'])\n        elif get_work_response.n_pending_tasks:\n            logger.debug(\n                \"There are %s pending tasks possibly being run by other workers\",\n                get_work_response.n_pending_tasks)\n            if get_work_response.n_unique_pending:\n                logger.debug(\n                    \"There are %i pending tasks unique to this worker\",\n                    get_work_response.n_unique_pending)\n            if get_work_response.n_pending_last_scheduled:\n                logger.debug(\n                    \"There are %i pending tasks last scheduled by this worker\",\n                    get_work_response.n_pending_last_scheduled)", "is_method": true, "class_name": "Worker", "function_description": "Logs detailed debug information about the state of remote tasks based on a work response, showing running and pending tasks relevant to the worker's operational status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_work_task_id", "line_number": 912, "body": "def _get_work_task_id(self, get_work_response):\n        if get_work_response.get('task_id') is not None:\n            return get_work_response['task_id']\n        elif 'batch_id' in get_work_response:\n            try:\n                task = load_task(\n                    module=get_work_response.get('task_module'),\n                    task_name=get_work_response['task_family'],\n                    params_str=get_work_response['task_params'],\n                )\n            except Exception as ex:\n                self._handle_task_load_error(ex, get_work_response['batch_task_ids'])\n                self.run_succeeded = False\n                return None\n\n            self._scheduler.add_task(\n                worker=self._id,\n                task_id=task.task_id,\n                module=get_work_response.get('task_module'),\n                family=get_work_response['task_family'],\n                params=task.to_str_params(),\n                status=RUNNING,\n                batch_id=get_work_response['batch_id'],\n            )\n            return task.task_id\n        else:\n            return None", "is_method": true, "class_name": "Worker", "function_description": "Provides the unique task ID for a worker to process. It extracts an existing ID or instantiates and registers a new task from batch work details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_get_work", "line_number": 940, "body": "def _get_work(self):\n        if self._stop_requesting_work:\n            return GetWorkResponse(None, 0, 0, 0, 0, WORKER_STATE_DISABLED)\n\n        if self.worker_processes > 0:\n            logger.debug(\"Asking scheduler for work...\")\n            r = self._scheduler.get_work(\n                worker=self._id,\n                host=self.host,\n                assistant=self._assistant,\n                current_tasks=list(self._running_tasks.keys()),\n            )\n        else:\n            logger.debug(\"Checking if tasks are still pending\")\n            r = self._scheduler.count_pending(worker=self._id)\n\n        running_tasks = r['running_tasks']\n        task_id = self._get_work_task_id(r)\n\n        self._get_work_response_history.append({\n            'task_id': task_id,\n            'running_tasks': running_tasks,\n        })\n\n        if task_id is not None and task_id not in self._scheduled_tasks:\n            logger.info('Did not schedule %s, will load it dynamically', task_id)\n\n            try:\n                # TODO: we should obtain the module name from the server!\n                self._scheduled_tasks[task_id] = \\\n                    load_task(module=r.get('task_module'),\n                              task_name=r['task_family'],\n                              params_str=r['task_params'])\n            except TaskClassException as ex:\n                self._handle_task_load_error(ex, [task_id])\n                task_id = None\n                self.run_succeeded = False\n\n        if task_id is not None and 'batch_task_ids' in r:\n            batch_tasks = filter(None, [\n                self._scheduled_tasks.get(batch_id) for batch_id in r['batch_task_ids']])\n            self._batch_running_tasks[task_id] = batch_tasks\n\n        return GetWorkResponse(\n            task_id=task_id,\n            running_tasks=running_tasks,\n            n_pending_tasks=r['n_pending_tasks'],\n            n_unique_pending=r['n_unique_pending'],\n\n            # TODO: For a tiny amount of time (a month?) we'll keep forwards compatibility\n            #  That is you can user a newer client than server (Sep 2016)\n            n_pending_last_scheduled=r.get('n_pending_last_scheduled', 0),\n            worker_state=r.get('worker_state', WORKER_STATE_ACTIVE),\n        )", "is_method": true, "class_name": "Worker", "function_description": "A Worker method that requests and dynamically loads tasks from a scheduler. It provides the worker with information on running and pending tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_run_task", "line_number": 995, "body": "def _run_task(self, task_id):\n        if task_id in self._running_tasks:\n            logger.debug('Got already running task id {} from scheduler, taking a break'.format(task_id))\n            next(self._sleeper())\n            return\n\n        task = self._scheduled_tasks[task_id]\n\n        task_process = self._create_task_process(task)\n\n        self._running_tasks[task_id] = task_process\n\n        if task_process.use_multiprocessing:\n            with fork_lock:\n                task_process.start()\n        else:\n            # Run in the same process\n            task_process.run()", "is_method": true, "class_name": "Worker", "function_description": "Initiates the execution of a specific scheduled task. It ensures task uniqueness and dispatches it for processing, either in a new process or within the current one."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_create_task_process", "line_number": 1014, "body": "def _create_task_process(self, task):\n        message_queue = multiprocessing.Queue() if task.accepts_messages else None\n        reporter = TaskStatusReporter(self._scheduler, task.task_id, self._id, message_queue)\n        use_multiprocessing = self._config.force_multiprocessing or bool(self.worker_processes > 1)\n        return ContextManagedTaskProcess(\n            self._config.task_process_context,\n            task, self._id, self._task_result_queue, reporter,\n            use_multiprocessing=use_multiprocessing,\n            worker_timeout=self._config.timeout,\n            check_unfulfilled_deps=self._config.check_unfulfilled_deps,\n            check_complete_on_run=self._config.check_complete_on_run,\n        )", "is_method": true, "class_name": "Worker", "function_description": "Creates and configures a context-managed process for a given task, enabling its independent execution. It sets up communication and reporting mechanisms for task monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_purge_children", "line_number": 1027, "body": "def _purge_children(self):\n        \"\"\"\n        Find dead children and put a response on the result queue.\n\n        :return:\n        \"\"\"\n        for task_id, p in self._running_tasks.items():\n            if not p.is_alive() and p.exitcode:\n                error_msg = 'Task {} died unexpectedly with exit code {}'.format(task_id, p.exitcode)\n                p.task.trigger_event(Event.PROCESS_FAILURE, p.task, error_msg)\n            elif p.timeout_time is not None and time.time() > float(p.timeout_time) and p.is_alive():\n                p.terminate()\n                error_msg = 'Task {} timed out after {} seconds and was terminated.'.format(task_id, p.worker_timeout)\n                p.task.trigger_event(Event.TIMEOUT, p.task, error_msg)\n            else:\n                continue\n\n            logger.info(error_msg)\n            self._task_result_queue.put((task_id, FAILED, error_msg, [], []))", "is_method": true, "class_name": "Worker", "function_description": "Monitors and cleans up child processes that have died unexpectedly or timed out. It reports their failure status and associated errors to a results queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_next_task", "line_number": 1047, "body": "def _handle_next_task(self):\n        \"\"\"\n        We have to catch three ways a task can be \"done\":\n\n        1. normal execution: the task runs/fails and puts a result back on the queue,\n        2. new dependencies: the task yielded new deps that were not complete and\n           will be rescheduled and dependencies added,\n        3. child process dies: we need to catch this separately.\n        \"\"\"\n        self._idle_since = None\n        while True:\n            self._purge_children()  # Deal with subprocess failures\n\n            try:\n                task_id, status, expl, missing, new_requirements = (\n                    self._task_result_queue.get(\n                        timeout=self._config.wait_interval))\n            except Queue.Empty:\n                return\n\n            task = self._scheduled_tasks[task_id]\n            if not task or task_id not in self._running_tasks:\n                continue\n                # Not a running task. Probably already removed.\n                # Maybe it yielded something?\n\n            # external task if run not implemented, retry-able if config option is enabled.\n            external_task_retryable = _is_external(task) and self._config.retry_external_tasks\n            if status == FAILED and not external_task_retryable:\n                self._email_task_failure(task, expl)\n\n            new_deps = []\n            if new_requirements:\n                new_req = [load_task(module, name, params)\n                           for module, name, params in new_requirements]\n                for t in new_req:\n                    self.add(t)\n                new_deps = [t.task_id for t in new_req]\n\n            self._add_task(worker=self._id,\n                           task_id=task_id,\n                           status=status,\n                           expl=json.dumps(expl),\n                           resources=task.process_resources(),\n                           runnable=None,\n                           params=task.to_str_params(),\n                           family=task.task_family,\n                           module=task.task_module,\n                           new_deps=new_deps,\n                           assistant=self._assistant,\n                           retry_policy_dict=_get_retry_policy_dict(task))\n\n            self._running_tasks.pop(task_id)\n\n            # re-add task to reschedule missing dependencies\n            if missing:\n                reschedule = True\n\n                # keep out of infinite loops by not rescheduling too many times\n                for task_id in missing:\n                    self.unfulfilled_counts[task_id] += 1\n                    if (self.unfulfilled_counts[task_id] >\n                            self._config.max_reschedules):\n                        reschedule = False\n                if reschedule:\n                    self.add(task)\n\n            self.run_succeeded &= (status == DONE) or (len(new_deps) > 0)\n            return", "is_method": true, "class_name": "Worker", "function_description": "Manages the outcome of a single worker task, handling completion, failure, new or missing dependencies, and child process issues. It updates the task's status and coordinates with the scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_sleeper", "line_number": 1117, "body": "def _sleeper(self):\n        # TODO is exponential backoff necessary?\n        while True:\n            jitter = self._config.wait_jitter\n            wait_interval = self._config.wait_interval + random.uniform(0, jitter)\n            logger.debug('Sleeping for %f seconds', wait_interval)\n            time.sleep(wait_interval)\n            yield", "is_method": true, "class_name": "Worker", "function_description": "This generator method controls the Worker's operational frequency. It introduces periodic, configurable delays with random jitter to regulate task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_keep_alive", "line_number": 1126, "body": "def _keep_alive(self, get_work_response):\n        \"\"\"\n        Returns true if a worker should stay alive given.\n\n        If worker-keep-alive is not set, this will always return false.\n        For an assistant, it will always return the value of worker-keep-alive.\n        Otherwise, it will return true for nonzero n_pending_tasks.\n\n        If worker-count-uniques is true, it will also\n        require that one of the tasks is unique to this worker.\n        \"\"\"\n        if not self._config.keep_alive:\n            return False\n        elif self._assistant:\n            return True\n        elif self._config.count_last_scheduled:\n            return get_work_response.n_pending_last_scheduled > 0\n        elif self._config.count_uniques:\n            return get_work_response.n_unique_pending > 0\n        elif get_work_response.n_pending_tasks == 0:\n            return False\n        elif not self._config.max_keep_alive_idle_duration:\n            return True\n        elif not self._idle_since:\n            return True\n        else:\n            time_to_shutdown = self._idle_since + self._config.max_keep_alive_idle_duration - datetime.datetime.now()\n            logger.debug(\"[%s] %s until shutdown\", self._id, time_to_shutdown)\n            return time_to_shutdown > datetime.timedelta(0)", "is_method": true, "class_name": "Worker", "function_description": "Determines if a worker should remain active based on its configuration, role, pending tasks, and idle duration. It provides a decision for worker lifecycle management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "handle_interrupt", "line_number": 1156, "body": "def handle_interrupt(self, signum, _):\n        \"\"\"\n        Stops the assistant from asking for more work on SIGUSR1\n        \"\"\"\n        if signum == signal.SIGUSR1:\n            self._start_phasing_out()", "is_method": true, "class_name": "Worker", "function_description": "This method handles the SIGUSR1 signal for a Worker, initiating a graceful shutdown process. It prevents the worker from requesting new tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_start_phasing_out", "line_number": 1163, "body": "def _start_phasing_out(self):\n        \"\"\"\n        Go into a mode where we dont ask for more work and quit once existing\n        tasks are done.\n        \"\"\"\n        self._config.keep_alive = False\n        self._stop_requesting_work = True", "is_method": true, "class_name": "Worker", "function_description": "This method initiates a graceful shutdown for the Worker. It stops the worker from requesting new tasks, allowing it to complete existing assignments before terminating."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "run", "line_number": 1171, "body": "def run(self):\n        \"\"\"\n        Returns True if all scheduled tasks were executed successfully.\n        \"\"\"\n        logger.info('Running Worker with %d processes', self.worker_processes)\n\n        sleeper = self._sleeper()\n        self.run_succeeded = True\n\n        self._add_worker()\n\n        while True:\n            while len(self._running_tasks) >= self.worker_processes > 0:\n                logger.debug('%d running tasks, waiting for next task to finish', len(self._running_tasks))\n                self._handle_next_task()\n\n            get_work_response = self._get_work()\n\n            if get_work_response.worker_state == WORKER_STATE_DISABLED:\n                self._start_phasing_out()\n\n            if get_work_response.task_id is None:\n                if not self._stop_requesting_work:\n                    self._log_remote_tasks(get_work_response)\n                if len(self._running_tasks) == 0:\n                    self._idle_since = self._idle_since or datetime.datetime.now()\n                    if self._keep_alive(get_work_response):\n                        next(sleeper)\n                        continue\n                    else:\n                        break\n                else:\n                    self._handle_next_task()\n                    continue\n\n            # task_id is not None:\n            logger.debug(\"Pending tasks: %s\", get_work_response.n_pending_tasks)\n            self._run_task(get_work_response.task_id)\n\n        while len(self._running_tasks):\n            logger.debug('Shut down Worker, %d more tasks to go', len(self._running_tasks))\n            self._handle_next_task()\n\n        return self.run_succeeded", "is_method": true, "class_name": "Worker", "function_description": "This method serves as the main execution loop for the Worker, continuously fetching and executing tasks concurrently. It manages the worker's operational lifecycle and reports if all scheduled tasks completed successfully."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "_handle_rpc_message", "line_number": 1216, "body": "def _handle_rpc_message(self, message):\n        logger.info(\"Worker %s got message %s\" % (self._id, message))\n\n        # the message is a dict {'name': <function_name>, 'kwargs': <function_kwargs>}\n        name = message['name']\n        kwargs = message['kwargs']\n\n        # find the function and check if it's callable and configured to work\n        # as a message callback\n        func = getattr(self, name, None)\n        tpl = (self._id, name)\n        if not callable(func):\n            logger.error(\"Worker %s has no function '%s'\" % tpl)\n        elif not getattr(func, \"is_rpc_message_callback\", False):\n            logger.error(\"Worker %s function '%s' is not available as rpc message callback\" % tpl)\n        else:\n            logger.info(\"Worker %s successfully dispatched rpc message to function '%s'\" % tpl)\n            func(**kwargs)", "is_method": true, "class_name": "Worker", "function_description": "Routes incoming RPC messages to a corresponding method within the Worker. It ensures the method is callable and designated for RPC callbacks before execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "set_worker_processes", "line_number": 1236, "body": "def set_worker_processes(self, n):\n        # set the new value\n        self.worker_processes = max(1, n)\n\n        # tell the scheduler\n        self._scheduler.add_worker(self._id, {'workers': self.worker_processes})", "is_method": true, "class_name": "Worker", "function_description": "Configures the number of active processes for this worker. It updates the worker's process count and informs the scheduler about the new allocation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/worker.py", "function": "dispatch_scheduler_message", "line_number": 1244, "body": "def dispatch_scheduler_message(self, task_id, message_id, content, **kwargs):\n        task_id = str(task_id)\n        if task_id in self._running_tasks:\n            task_process = self._running_tasks[task_id]\n            if task_process.status_reporter.scheduler_messages:\n                message = SchedulerMessage(self._scheduler, task_id, message_id, content, **kwargs)\n                task_process.status_reporter.scheduler_messages.put(message)", "is_method": true, "class_name": "Worker", "function_description": "Dispatches a message from the scheduler to a specific running task managed by this worker, provided the task is active and able to receive it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "generate_email", "line_number": 134, "body": "def generate_email(sender, subject, message, recipients, image_png):\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    from email.mime.image import MIMEImage\n\n    msg_root = MIMEMultipart('related')\n\n    msg_text = MIMEText(message, email().format, 'utf-8')\n    msg_root.attach(msg_text)\n\n    if image_png:\n        with open(image_png, 'rb') as fp:\n            msg_image = MIMEImage(fp.read(), 'png')\n        msg_root.attach(msg_image)\n\n    msg_root['Subject'] = subject\n    msg_root['From'] = sender\n    msg_root['To'] = ','.join(recipients)\n\n    return msg_root", "is_method": false, "function_description": "Constructs a complete email message with specified sender, subject, text, and recipients. It can also embed a PNG image directly into the email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "wrap_traceback", "line_number": 156, "body": "def wrap_traceback(traceback):\n    \"\"\"\n    For internal use only (until further notice)\n    \"\"\"\n    if email().format == 'html':\n        try:\n            from pygments import highlight\n            from pygments.lexers import PythonTracebackLexer\n            from pygments.formatters import HtmlFormatter\n            with_pygments = True\n        except ImportError:\n            with_pygments = False\n\n        if with_pygments:\n            formatter = HtmlFormatter(noclasses=True)\n            wrapped = highlight(traceback, PythonTracebackLexer(), formatter)\n        else:\n            wrapped = '<pre>%s</pre>' % traceback\n    else:\n        wrapped = traceback\n\n    return wrapped", "is_method": false, "function_description": "Formats a Python traceback for display, applying HTML syntax highlighting with Pygments if the output is HTML. Otherwise, it returns the traceback as plain text."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_smtp", "line_number": 180, "body": "def send_email_smtp(sender, subject, message, recipients, image_png):\n    import smtplib\n\n    smtp_config = smtp()\n    kwargs = dict(\n        host=smtp_config.host,\n        port=smtp_config.port,\n        local_hostname=smtp_config.local_hostname,\n    )\n    if smtp_config.timeout:\n        kwargs['timeout'] = smtp_config.timeout\n\n    try:\n        smtp_conn = smtplib.SMTP_SSL(**kwargs) if smtp_config.ssl else smtplib.SMTP(**kwargs)\n        smtp_conn.ehlo_or_helo_if_needed()\n        if smtp_conn.has_extn('starttls') and not smtp_config.no_tls:\n            smtp_conn.starttls()\n        if smtp_config.username and smtp_config.password:\n            smtp_conn.login(smtp_config.username, smtp_config.password)\n\n        msg_root = generate_email(sender, subject, message, recipients, image_png)\n\n        smtp_conn.sendmail(sender, recipients, msg_root.as_string())\n    except socket.error as exception:\n        logger.error(\"Not able to connect to smtp server: %s\", exception)", "is_method": false, "function_description": "This function connects to an SMTP server to send an email. It handles connection setup, authentication, and sends a composed message, optionally including an image, to specified recipients."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_ses", "line_number": 207, "body": "def send_email_ses(sender, subject, message, recipients, image_png):\n    \"\"\"\n    Sends notification through AWS SES.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import client as boto3_client\n\n    client = boto3_client('ses')\n\n    msg_root = generate_email(sender, subject, message, recipients, image_png)\n    response = client.send_raw_email(Source=sender,\n                                     Destinations=recipients,\n                                     RawMessage={'Data': msg_root.as_string()})\n\n    logger.debug((\"Message sent to SES.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))", "is_method": false, "function_description": "This function sends emails using AWS Simple Email Service (SES). It prepares and delivers raw email content, optionally including a PNG image, for notification purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_sendgrid", "line_number": 232, "body": "def send_email_sendgrid(sender, subject, message, recipients, image_png):\n    import sendgrid as sendgrid_lib\n    client = sendgrid_lib.SendGridAPIClient(sendgrid().apikey)\n\n    to_send = sendgrid_lib.Mail(\n            from_email=sender,\n            to_emails=recipients,\n            subject=subject)\n\n    if email().format == 'html':\n        to_send.add_content(message, 'text/html')\n    else:\n        to_send.add_content(message, 'text/plain')\n\n    if image_png:\n        to_send.add_attachment(image_png)\n\n    client.send(to_send)", "is_method": false, "function_description": "This function sends an email using the SendGrid API. It supports plain text or HTML content and can include a PNG image attachment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_email_disabled_reason", "line_number": 252, "body": "def _email_disabled_reason():\n    if email().format == 'none':\n        return \"email format is 'none'\"\n    elif email().force_send:\n        return None\n    elif sys.stdout.isatty():\n        return \"running from a tty\"\n    else:\n        return None", "is_method": false, "function_description": "Provides a reason string if email sending is suppressed due to configuration (e.g., format 'none') or environment (e.g., running in a TTY). It returns None if email sending is permitted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email_sns", "line_number": 263, "body": "def send_email_sns(sender, subject, message, topic_ARN, image_png):\n    \"\"\"\n    Sends notification through AWS SNS. Takes Topic ARN from recipients.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import resource as boto3_resource\n\n    sns = boto3_resource('sns')\n    topic = sns.Topic(topic_ARN[0])\n\n    # Subject is max 100 chars\n    if len(subject) > 100:\n        subject = subject[0:48] + '...' + subject[-49:]\n\n    response = topic.publish(Subject=subject, Message=message)\n\n    logger.debug((\"Message sent to SNS.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))", "is_method": false, "function_description": "Publishes a text message to a specified AWS SNS topic. This provides a programmatic way to send notifications or alerts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_email", "line_number": 290, "body": "def send_email(subject, message, sender, recipients, image_png=None):\n    \"\"\"\n    Decides whether to send notification. Notification is cancelled if there are\n    no recipients or if stdout is onto tty or if in debug mode.\n\n    Dispatches on config value email.method.  Default is 'smtp'.\n    \"\"\"\n    notifiers = {\n        'ses': send_email_ses,\n        'sendgrid': send_email_sendgrid,\n        'smtp': send_email_smtp,\n        'sns': send_email_sns,\n    }\n\n    subject = _prefix(subject)\n    if not recipients or recipients == (None,):\n        return\n\n    if _email_disabled_reason():\n        logger.info(\"Not sending email to %r because %s\",\n                    recipients, _email_disabled_reason())\n        return\n\n    # Clean the recipients lists to allow multiple email addresses, comma\n    # separated in luigi.cfg\n    recipients_tmp = []\n    for r in recipients:\n        recipients_tmp.extend([a.strip() for a in r.split(',') if a.strip()])\n\n    # Replace original recipients with the clean list\n    recipients = recipients_tmp\n\n    logger.info(\"Sending email to %r\", recipients)\n\n    # Get appropriate sender and call it to send the notification\n    email_sender = notifiers[email().method]\n    email_sender(sender, subject, message, recipients, image_png)", "is_method": false, "function_description": "Provides a configurable email sending service that dispatches emails using various methods (e.g., SMTP, SES). It includes logic to prevent sending under specific conditions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_email_recipients", "line_number": 329, "body": "def _email_recipients(additional_recipients=None):\n    receiver = email().receiver\n    recipients = [receiver] if receiver else []\n    if additional_recipients:\n        if isinstance(additional_recipients, str):\n            recipients.append(additional_recipients)\n        else:\n            recipients.extend(additional_recipients)\n    return recipients", "is_method": false, "function_description": "This function compiles a complete list of email recipients, including a primary receiver and any optional additional addresses. It provides a unified list for sending emails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "send_error_email", "line_number": 340, "body": "def send_error_email(subject, message, additional_recipients=None):\n    \"\"\"\n    Sends an email to the configured error email, if it's configured.\n    \"\"\"\n    recipients = _email_recipients(additional_recipients)\n    sender = email().sender\n    send_email(\n        subject=subject,\n        message=message,\n        sender=sender,\n        recipients=recipients\n    )", "is_method": false, "function_description": "Dispatches error notifications via email to configured recipients, facilitating prompt alerts for system issues."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "_prefix", "line_number": 354, "body": "def _prefix(subject):\n    \"\"\"\n    If the config has a special prefix for emails then this function adds\n    this prefix.\n    \"\"\"\n    if email().prefix:\n        return \"{} {}\".format(email().prefix, subject)\n    else:\n        return subject", "is_method": false, "function_description": "Adds a configured prefix to an email subject string if the email settings specify one, otherwise returns the original subject."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "format_task_error", "line_number": 365, "body": "def format_task_error(headline, task, command, formatted_exception=None):\n    \"\"\"\n    Format a message body for an error email related to a luigi.task.Task\n\n    :param headline: Summary line for the message\n    :param task: `luigi.task.Task` instance where this error occurred\n    :param formatted_exception: optional string showing traceback\n\n    :return: message body\n    \"\"\"\n\n    if formatted_exception:\n        formatted_exception = wrap_traceback(formatted_exception)\n    else:\n        formatted_exception = \"\"\n\n    if email().format == 'html':\n        msg_template = textwrap.dedent('''\n        <html>\n        <body>\n        <h2>{headline}</h2>\n\n        <table style=\"border-top: 1px solid black; border-bottom: 1px solid black\">\n        <thead>\n        <tr><th>name</th><td>{name}</td></tr>\n        </thead>\n        <tbody>\n        {param_rows}\n        </tbody>\n        </table>\n        </pre>\n\n        <h2>Command line</h2>\n        <pre>\n        {command}\n        </pre>\n\n        <h2>Traceback</h2>\n        {traceback}\n        </body>\n        </html>\n        ''')\n\n        str_params = task.to_str_params()\n        params = '\\n'.join('<tr><th>{}</th><td>{}</td></tr>'.format(*items) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, param_rows=params,\n                                   command=command, traceback=formatted_exception)\n    else:\n        msg_template = textwrap.dedent('''\\\n        {headline}\n\n        Name: {name}\n\n        Parameters:\n        {params}\n\n        Command line:\n          {command}\n\n        {traceback}\n        ''')\n\n        str_params = task.to_str_params()\n        max_width = max([0] + [len(x) for x in str_params.keys()])\n        params = '\\n'.join('  {:{width}}: {}'.format(*items, width=max_width) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, params=params,\n                                   command=command, traceback=formatted_exception)\n\n    return body", "is_method": false, "function_description": "Formats a comprehensive error message body for a Luigi task failure, including task details, command line, and an optional traceback, for display or email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "run", "line_number": 56, "body": "def run(self):\n        raise ValueError('Testing notifications triggering')", "is_method": true, "class_name": "TestNotificationsTask", "function_description": "This method of TestNotificationsTask intentionally raises an error to simulate a failure, specifically for testing how the application handles errors and triggers notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/notifications.py", "function": "complete", "line_number": 59, "body": "def complete(self):\n        if self.raise_in_complete:\n            raise ValueError('Testing notifications triggering')\n        return False", "is_method": true, "class_name": "TestNotificationsTask", "function_description": "The `complete` method of `TestNotificationsTask` is designed to simulate task completion failures. It conditionally raises a `ValueError` to test error notification and handling mechanisms."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/retcodes.py", "function": "run_with_retcodes", "line_number": 61, "body": "def run_with_retcodes(argv):\n    \"\"\"\n    Run luigi with command line parsing, but raise ``SystemExit`` with the configured exit code.\n\n    Note: Usually you use the luigi binary directly and don't call this function yourself.\n\n    :param argv: Should (conceptually) be ``sys.argv[1:]``\n    \"\"\"\n    logger = logging.getLogger('luigi-interface')\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        retcodes = retcode()\n\n    worker = None\n    try:\n        worker = luigi.interface._run(argv).worker\n    except luigi.interface.PidLockAlreadyTakenExit:\n        sys.exit(retcodes.already_running)\n    except Exception:\n        # Some errors occur before logging is set up, we set it up now\n        env_params = luigi.interface.core()\n        InterfaceLogging.setup(env_params)\n        logger.exception(\"Uncaught exception in luigi\")\n        sys.exit(retcodes.unhandled_exception)\n\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        task_sets = luigi.execution_summary._summary_dict(worker)\n        root_task = luigi.execution_summary._root_task(worker)\n        non_empty_categories = {k: v for k, v in task_sets.items() if v}.keys()\n\n    def has(status):\n        assert status in luigi.execution_summary._ORDERED_STATUSES\n        return status in non_empty_categories\n\n    codes_and_conds = (\n        (retcodes.missing_data, has('still_pending_ext')),\n        (retcodes.task_failed, has('failed')),\n        (retcodes.already_running, has('run_by_other_worker')),\n        (retcodes.scheduling_error, has('scheduling_error')),\n        (retcodes.not_run, has('not_run')),\n    )\n    expected_ret_code = max(code * (1 if cond else 0) for code, cond in codes_and_conds)\n\n    if expected_ret_code == 0 and \\\n       root_task not in task_sets[\"completed\"] and \\\n       root_task not in task_sets[\"already_done\"]:\n        sys.exit(retcodes.not_run)\n    else:\n        sys.exit(expected_ret_code)", "is_method": false, "function_description": "Runs a Luigi workflow using command-line arguments and exits the process with a system exit code. This code reflects the workflow's overall outcome, including success, failures, or other conditions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/retcodes.py", "function": "has", "line_number": 90, "body": "def has(status):\n        assert status in luigi.execution_summary._ORDERED_STATUSES\n        return status in non_empty_categories", "is_method": false, "function_description": "Determines if a specified Luigi task status category has associated tasks. This utility verifies the presence of tasks within a particular execution state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "namespace", "line_number": 50, "body": "def namespace(namespace=None, scope=''):\n    \"\"\"\n    Call to set namespace of tasks declared after the call.\n\n    It is often desired to call this function with the keyword argument\n    ``scope=__name__``.\n\n    The ``scope`` keyword makes it so that this call is only effective for task\n    classes with a matching [*]_ ``__module__``. The default value for\n    ``scope`` is the empty string, which means all classes. Multiple calls with\n    the same scope simply replace each other.\n\n    The namespace of a :py:class:`Task` can also be changed by specifying the property\n    ``task_namespace``.\n\n    .. code-block:: python\n\n        class Task2(luigi.Task):\n            task_namespace = 'namespace2'\n\n    This explicit setting takes priority over whatever is set in the\n    ``namespace()`` method, and it's also inherited through normal python\n    inheritence.\n\n    There's no equivalent way to set the ``task_family``.\n\n    *New since Luigi 2.6.0:* ``scope`` keyword argument.\n\n    .. [*] When there are multiple levels of matching module scopes like\n           ``a.b`` vs ``a.b.c``, the more specific one (``a.b.c``) wins.\n    .. seealso:: The new and better scaling :py:func:`auto_namespace`\n    \"\"\"\n    Register._default_namespace_dict[scope] = namespace or ''", "is_method": false, "function_description": "Sets a default namespace for Luigi tasks, affecting tasks declared after the call. This namespace can be scoped to specific modules, aiding in task organization and management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "auto_namespace", "line_number": 85, "body": "def auto_namespace(scope=''):\n    \"\"\"\n    Same as :py:func:`namespace`, but instead of a constant namespace, it will\n    be set to the ``__module__`` of the task class. This is desirable for these\n    reasons:\n\n     * Two tasks with the same name will not have conflicting task families\n     * It's more pythonic, as modules are Python's recommended way to\n       do namespacing.\n     * It's traceable. When you see the full name of a task, you can immediately\n       identify where it is defined.\n\n    We recommend calling this function from your package's outermost\n    ``__init__.py`` file. The file contents could look like this:\n\n    .. code-block:: python\n\n        import luigi\n\n        luigi.auto_namespace(scope=__name__)\n\n    To reset an ``auto_namespace()`` call, you can use\n    ``namespace(scope='my_scope')``.  But this will not be\n    needed (and is also discouraged) if you use the ``scope`` kwarg.\n\n    *New since Luigi 2.6.0.*\n    \"\"\"\n    namespace(namespace=_SAME_AS_PYTHON_MODULE, scope=scope)", "is_method": false, "function_description": "Automatically sets the namespace for Luigi tasks based on their Python module. This prevents naming conflicts between tasks and enhances traceability and organization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_id_str", "line_number": 115, "body": "def task_id_str(task_family, params):\n    \"\"\"\n    Returns a canonical string used to identify a particular task\n\n    :param task_family: The task family (class name) of the task\n    :param params: a dict mapping parameter names to their serialized values\n    :return: A unique, shortened identifier corresponding to the family and params\n    \"\"\"\n    # task_id is a concatenation of task family, the first values of the first 3 parameters\n    # sorted by parameter name and a md5hash of the family/parameters as a cananocalised json.\n    param_str = json.dumps(params, separators=(',', ':'), sort_keys=True)\n    param_hash = hashlib.md5(param_str.encode('utf-8')).hexdigest()\n\n    param_summary = '_'.join(p[:TASK_ID_TRUNCATE_PARAMS]\n                             for p in (params[p] for p in sorted(params)[:TASK_ID_INCLUDE_PARAMS]))\n    param_summary = TASK_ID_INVALID_CHAR_REGEX.sub('_', param_summary)\n\n    return '{}_{}_{}'.format(task_family, param_summary, param_hash[:TASK_ID_TRUNCATE_HASH])", "is_method": false, "function_description": "Generates a unique, canonical string identifier for a specific task based on its family and parameters. This ID is useful for consistent task tracking or resource naming."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "externalize", "line_number": 756, "body": "def externalize(taskclass_or_taskobject):\n    \"\"\"\n    Returns an externalized version of a Task. You may both pass an\n    instantiated task object or a task class. Some examples:\n\n    .. code-block:: python\n\n        class RequiringTask(luigi.Task):\n            def requires(self):\n                task_object = self.clone(MyTask)\n                return externalize(task_object)\n\n            ...\n\n    Here's mostly equivalent code, but ``externalize`` is applied to a task\n    class instead.\n\n    .. code-block:: python\n\n        @luigi.util.requires(externalize(MyTask))\n        class RequiringTask(luigi.Task):\n            pass\n            ...\n\n    Of course, it may also be used directly on classes and objects (for example\n    for reexporting or other usage).\n\n    .. code-block:: python\n\n        MyTask = externalize(MyTask)\n        my_task_2 = externalize(MyTask2(param='foo'))\n\n    If you however want a task class to be external from the beginning, you're\n    better off inheriting :py:class:`ExternalTask` rather than :py:class:`Task`.\n\n    This function tries to be side-effect free by creating a copy of the class\n    or the object passed in and then modify that object. In particular this\n    code shouldn't do anything.\n\n    .. code-block:: python\n\n        externalize(MyTask)  # BAD: This does nothing (as after luigi 2.4.0)\n    \"\"\"\n    copied_value = copy.copy(taskclass_or_taskobject)\n    if copied_value is taskclass_or_taskobject:\n        # Assume it's a class\n        clazz = taskclass_or_taskobject\n\n        @_task_wraps(clazz)\n        class _CopyOfClass(clazz):\n            # How to copy a class: http://stackoverflow.com/a/9541120/621449\n            _visible_in_registry = False\n        _CopyOfClass.run = None\n        return _CopyOfClass\n    else:\n        # We assume it's an object\n        copied_value.run = None\n        return copied_value", "is_method": false, "function_description": "This function transforms a Luigi Task (class or object) into an \"external\" version. It indicates the task's `run` method should not be executed by the current worker, as its completion is assumed to be handled elsewhere."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "getpaths", "line_number": 834, "body": "def getpaths(struct):\n    \"\"\"\n    Maps all Tasks in a structured data object to their .output().\n    \"\"\"\n    if isinstance(struct, Task):\n        return struct.output()\n    elif isinstance(struct, dict):\n        return struct.__class__((k, getpaths(v)) for k, v in struct.items())\n    elif isinstance(struct, (list, tuple)):\n        return struct.__class__(getpaths(r) for r in struct)\n    else:\n        # Remaining case: assume struct is iterable...\n        try:\n            return [getpaths(r) for r in struct]\n        except TypeError:\n            raise Exception('Cannot map %s to Task/dict/list' % str(struct))", "is_method": false, "function_description": "Recursively traverses a structured data object to extract the output of all embedded Task instances. It transforms a task-oriented structure into a results-oriented one."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "flatten", "line_number": 852, "body": "def flatten(struct):\n    \"\"\"\n    Creates a flat list of all all items in structured output (dicts, lists, items):\n\n    .. code-block:: python\n\n        >>> sorted(flatten({'a': 'foo', 'b': 'bar'}))\n        ['bar', 'foo']\n        >>> sorted(flatten(['foo', ['bar', 'troll']]))\n        ['bar', 'foo', 'troll']\n        >>> flatten('foo')\n        ['foo']\n        >>> flatten(42)\n        [42]\n    \"\"\"\n    if struct is None:\n        return []\n    flat = []\n    if isinstance(struct, dict):\n        for _, result in struct.items():\n            flat += flatten(result)\n        return flat\n    if isinstance(struct, str):\n        return [struct]\n\n    try:\n        # if iterable\n        iterator = iter(struct)\n    except TypeError:\n        return [struct]\n\n    for result in iterator:\n        flat += flatten(result)\n    return flat", "is_method": false, "function_description": "Transforms nested dictionaries, lists, and atomic elements into a single flat list. This provides a unified view of all individual items within any structured input."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "flatten_output", "line_number": 888, "body": "def flatten_output(task):\n    \"\"\"\n    Lists all output targets by recursively walking output-less (wrapper) tasks.\n\n    FIXME order consistently.\n    \"\"\"\n    r = flatten(task.output())\n    if not r:\n        for dep in flatten(task.requires()):\n            r += flatten_output(dep)\n    return r", "is_method": false, "function_description": "This function recursively collects all output targets associated with a given task. It navigates task dependencies to find outputs even from \"wrapper\" tasks that do not produce direct outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_task_wraps", "line_number": 901, "body": "def _task_wraps(task_class):\n    # In order to make the behavior of a wrapper class nicer, we set the name of the\n    # new class to the wrapped class, and copy over the docstring and module as well.\n    # This makes it possible to pickle the wrapped class etc.\n    # Btw, this is a slight abuse of functools.wraps. It's meant to be used only for\n    # functions, but it works for classes too, if you pass updated=[]\n    assigned = functools.WRAPPER_ASSIGNMENTS + ('_namespace_at_class_time',)\n    return functools.wraps(task_class, assigned=assigned, updated=[])", "is_method": false, "function_description": "This function serves as a utility to properly register a wrapper class by copying essential metadata (like name and docstring) from the wrapped `task_class`, improving its behavior and allowing proper pickling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "batchable", "line_number": 192, "body": "def batchable(self):\n        \"\"\"\n        True if this instance can be run as part of a batch. By default, True\n        if it has any batched parameters\n        \"\"\"\n        return bool(self.batch_param_names())", "is_method": true, "class_name": "Task", "function_description": "Checks if a Task instance is configured for batch processing, allowing it to be run efficiently with other tasks that share batched parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "disable_window", "line_number": 216, "body": "def disable_window(self):\n        \"\"\"\n        Override this positive integer to have different ``disable_window`` at task level.\n        Check :ref:`scheduler-config`\n        \"\"\"\n        return self.disable_window_seconds", "is_method": true, "class_name": "Task", "function_description": "This method returns the task's specific disable window duration. It allows individual tasks to override the default or global disable period."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "owner_email", "line_number": 229, "body": "def owner_email(self):\n        '''\n        Override this to send out additional error emails to task owner, in addition to the one\n        defined in the global configuration. This should return a string or a list of strings. e.g.\n        'test@exmaple.com' or ['test1@example.com', 'test2@example.com']\n        '''\n        return None", "is_method": true, "class_name": "Task", "function_description": "Allows defining additional email addresses for task-specific error notifications. This supplements globally configured recipients for customized error reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_owner_list", "line_number": 237, "body": "def _owner_list(self):\n        \"\"\"\n        Turns the owner_email property into a list. This should not be overridden.\n        \"\"\"\n        owner_email = self.owner_email\n        if owner_email is None:\n            return []\n        elif isinstance(owner_email, str):\n            return owner_email.split(',')\n        else:\n            return owner_email", "is_method": true, "class_name": "Task", "function_description": "Normalizes the `Task` object's owner email property into a list format. This method ensures owner emails are always iterable, handling single strings, comma-separated strings, or no email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "use_cmdline_section", "line_number": 250, "body": "def use_cmdline_section(self):\n        ''' Property used by core config such as `--workers` etc.\n        These will be exposed without the class as prefix.'''\n        return True", "is_method": true, "class_name": "Task", "function_description": "Returns a flag indicating whether core command-line options associated with this task, such as `--workers`, should be accessible globally without a class prefix."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "event_handler", "line_number": 256, "body": "def event_handler(cls, event):\n        \"\"\"\n        Decorator for adding event handlers.\n        \"\"\"\n        def wrapped(callback):\n            cls._event_callbacks.setdefault(cls, {}).setdefault(event, set()).add(callback)\n            return callback\n        return wrapped", "is_method": true, "class_name": "Task", "function_description": "This class method serves as a decorator to register callback functions that are executed when a specific event occurs. It enables an event-driven architecture for `Task` classes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "trigger_event", "line_number": 265, "body": "def trigger_event(self, event, *args, **kwargs):\n        \"\"\"\n        Trigger that calls all of the specified events associated with this class.\n        \"\"\"\n        for event_class, event_callbacks in self._event_callbacks.items():\n            if not isinstance(self, event_class):\n                continue\n            for callback in event_callbacks.get(event, []):\n                try:\n                    # callbacks are protected\n                    callback(*args, **kwargs)\n                except KeyboardInterrupt:\n                    return\n                except BaseException:\n                    logger.exception(\"Error in event callback for %r\", event)", "is_method": true, "class_name": "Task", "function_description": "This method dispatches a specified event by invoking all relevant callbacks registered for the current task instance's class. It provides an extensible way for tasks to trigger actions based on events."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "accepts_messages", "line_number": 282, "body": "def accepts_messages(self):\n        \"\"\"\n        For configuring which scheduler messages can be received. When falsy, this tasks does not\n        accept any message. When True, all messages are accepted.\n        \"\"\"\n        return False", "is_method": true, "class_name": "Task", "function_description": "This method configures whether the task can receive scheduler messages. It currently prevents the task from accepting any incoming messages."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_module", "line_number": 290, "body": "def task_module(self):\n        ''' Returns what Python module to import to get access to this class. '''\n        # TODO(erikbern): we should think about a language-agnostic mechanism\n        return self.__class__.__module__", "is_method": true, "class_name": "Task", "function_description": "This method returns the Python module name where the `Task` class is defined. It provides a way to programmatically determine the class's origin for introspection or dynamic operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_task_namespace", "line_number": 315, "body": "def get_task_namespace(cls):\n        \"\"\"\n        The task family for the given class.\n\n        Note: You normally don't want to override this.\n        \"\"\"\n        if cls.task_namespace != cls.__not_user_specified:\n            return cls.task_namespace\n        elif cls._namespace_at_class_time == _SAME_AS_PYTHON_MODULE:\n            return cls.__module__\n        return cls._namespace_at_class_time", "is_method": true, "class_name": "Task", "function_description": "This class method determines the hierarchical namespace or \"task family\" for a given task class. It typically defaults to the class's Python module if not explicitly set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "task_family", "line_number": 328, "body": "def task_family(self):\n        \"\"\"\n        DEPRECATED since after 2.4.0. See :py:meth:`get_task_family` instead.\n        Hopefully there will be less meta magic in Luigi.\n\n        Convenience method since a property on the metaclass isn't directly\n        accessible through the class instances.\n        \"\"\"\n        return self.__class__.task_family", "is_method": true, "class_name": "Task", "function_description": "Retrieves the task family identifier associated with a Task instance. This method is deprecated; use `get_task_family` instead for this capability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_task_family", "line_number": 339, "body": "def get_task_family(cls):\n        \"\"\"\n        The task family for the given class.\n\n        If ``task_namespace`` is not set, then it's simply the name of the\n        class.  Otherwise, ``<task_namespace>.`` is prefixed to the class name.\n\n        Note: You normally don't want to override this.\n        \"\"\"\n        if not cls.get_task_namespace():\n            return cls.__name__\n        else:\n            return \"{}.{}\".format(cls.get_task_namespace(), cls.__name__)", "is_method": true, "class_name": "Task", "function_description": "This method provides a unique, namespaced identifier for a task class. It generates this by optionally prefixing the class name with its configured task namespace."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_params", "line_number": 354, "body": "def get_params(cls):\n        \"\"\"\n        Returns all of the Parameters for this Task.\n        \"\"\"\n        # We want to do this here and not at class instantiation, or else there is no room to extend classes dynamically\n        params = []\n        for param_name in dir(cls):\n            param_obj = getattr(cls, param_name)\n            if not isinstance(param_obj, Parameter):\n                continue\n\n            params.append((param_name, param_obj))\n\n        # The order the parameters are created matters. See Parameter class\n        params.sort(key=lambda t: t[1]._counter)\n        return params", "is_method": true, "class_name": "Task", "function_description": "Provides a sorted list of all `Parameter` objects defined directly on the `Task` class. This facilitates dynamic discovery of a task's configurable attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "batch_param_names", "line_number": 372, "body": "def batch_param_names(cls):\n        return [name for name, p in cls.get_params() if p._is_batchable()]", "is_method": true, "class_name": "Task", "function_description": "Retrieves the names of parameters within a Task class that are designed to support batch processing. This enables systems to efficiently group and execute operations on these parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_param_names", "line_number": 376, "body": "def get_param_names(cls, include_significant=False):\n        return [name for name, p in cls.get_params() if include_significant or p.significant]", "is_method": true, "class_name": "Task", "function_description": "Retrieves a list of parameter names defined for the class, with an option to include only those marked as significant. This facilitates inspecting or interacting with task configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "get_param_values", "line_number": 380, "body": "def get_param_values(cls, params, args, kwargs):\n        \"\"\"\n        Get the values of the parameters from the args and kwargs.\n\n        :param params: list of (param_name, Parameter).\n        :param args: positional arguments\n        :param kwargs: keyword arguments.\n        :returns: list of `(name, value)` tuples, one for each parameter.\n        \"\"\"\n        result = {}\n\n        params_dict = dict(params)\n\n        task_family = cls.get_task_family()\n\n        # In case any exceptions are thrown, create a helpful description of how the Task was invoked\n        # TODO: should we detect non-reprable arguments? These will lead to mysterious errors\n        exc_desc = '%s[args=%s, kwargs=%s]' % (task_family, args, kwargs)\n\n        # Fill in the positional arguments\n        positional_params = [(n, p) for n, p in params if p.positional]\n        for i, arg in enumerate(args):\n            if i >= len(positional_params):\n                raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))\n            param_name, param_obj = positional_params[i]\n            result[param_name] = param_obj.normalize(arg)\n\n        # Then the keyword arguments\n        for param_name, arg in kwargs.items():\n            if param_name in result:\n                raise parameter.DuplicateParameterException('%s: parameter %s was already set as a positional parameter' % (exc_desc, param_name))\n            if param_name not in params_dict:\n                raise parameter.UnknownParameterException('%s: unknown parameter %s' % (exc_desc, param_name))\n            result[param_name] = params_dict[param_name].normalize(arg)\n\n        # Then use the defaults for anything not filled in\n        for param_name, param_obj in params:\n            if param_name not in result:\n                if not param_obj.has_task_value(task_family, param_name):\n                    raise parameter.MissingParameterException(\"%s: requires the '%s' parameter to be set\" % (exc_desc, param_name))\n                result[param_name] = param_obj.task_value(task_family, param_name)\n\n        def list_to_tuple(x):\n            \"\"\" Make tuples out of lists and sets to allow hashing \"\"\"\n            if isinstance(x, list) or isinstance(x, set):\n                return tuple(x)\n            else:\n                return x\n        # Sort it by the correct order and make a list\n        return [(param_name, list_to_tuple(result[param_name])) for param_name, param_obj in params]", "is_method": true, "class_name": "Task", "function_description": "Parses and validates arguments (args, kwargs) against a task's defined parameters. It assigns and normalizes values, including defaults, preparing a complete and correct parameter set for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "param_args", "line_number": 451, "body": "def param_args(self):\n        warnings.warn(\"Use of param_args has been deprecated.\", DeprecationWarning)\n        return tuple(self.param_kwargs[k] for k, v in self.get_params())", "is_method": true, "class_name": "Task", "function_description": "This deprecated method of the Task class retrieves task-specific arguments as a tuple, but warns against its continued use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "initialized", "line_number": 455, "body": "def initialized(self):\n        \"\"\"\n        Returns ``True`` if the Task is initialized and ``False`` otherwise.\n        \"\"\"\n        return hasattr(self, 'task_id')", "is_method": true, "class_name": "Task", "function_description": "Provides a status check to determine if a Task instance has been initialized. It indicates whether the task is ready for operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_warn_on_wrong_param_types", "line_number": 461, "body": "def _warn_on_wrong_param_types(self):\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            params[param_name]._warn_on_wrong_param_type(param_name, param_value)", "is_method": true, "class_name": "Task", "function_description": "Issues warnings for parameters of the Task instance that have incorrect types. This internal check helps ensure the Task operates with valid inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "from_str_params", "line_number": 467, "body": "def from_str_params(cls, params_str):\n        \"\"\"\n        Creates an instance from a str->str hash.\n\n        :param params_str: dict of param name -> value as string.\n        \"\"\"\n        kwargs = {}\n        for param_name, param in cls.get_params():\n            if param_name in params_str:\n                param_str = params_str[param_name]\n                if isinstance(param_str, list):\n                    kwargs[param_name] = param._parse_list(param_str)\n                else:\n                    kwargs[param_name] = param.parse(param_str)\n\n        return cls(**kwargs)", "is_method": true, "class_name": "Task", "function_description": "This class method creates a Task instance by parsing string-based parameter values into their correct types. It provides a convenient way to instantiate Task objects from string-only configurations or external inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "to_str_params", "line_number": 484, "body": "def to_str_params(self, only_significant=False, only_public=False):\n        \"\"\"\n        Convert all parameters to a str->str hash.\n        \"\"\"\n        params_str = {}\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            if (((not only_significant) or params[param_name].significant)\n                    and ((not only_public) or params[param_name].visibility == ParameterVisibility.PUBLIC)\n                    and params[param_name].visibility != ParameterVisibility.PRIVATE):\n                params_str[param_name] = params[param_name].serialize(param_value)\n\n        return params_str", "is_method": true, "class_name": "Task", "function_description": "This method serializes a `Task`'s parameters into a dictionary of string key-value pairs. It provides filtering options to include only significant or public parameters, useful for logging or external representation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_get_param_visibilities", "line_number": 498, "body": "def _get_param_visibilities(self):\n        param_visibilities = {}\n        params = dict(self.get_params())\n        for param_name, param_value in self.param_kwargs.items():\n            if params[param_name].visibility != ParameterVisibility.PRIVATE:\n                param_visibilities[param_name] = params[param_name].visibility.serialize()\n\n        return param_visibilities", "is_method": true, "class_name": "Task", "function_description": "Retrieves a dictionary mapping non-private parameter names to their visibility states for the task. It provides an overview of which parameters are exposed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "clone", "line_number": 507, "body": "def clone(self, cls=None, **kwargs):\n        \"\"\"\n        Creates a new instance from an existing instance where some of the args have changed.\n\n        There's at least two scenarios where this is useful (see test/clone_test.py):\n\n        * remove a lot of boiler plate when you have recursive dependencies and lots of args\n        * there's task inheritance and some logic is on the base class\n\n        :param cls:\n        :param kwargs:\n        :return:\n        \"\"\"\n        if cls is None:\n            cls = self.__class__\n\n        new_k = {}\n        for param_name, param_class in cls.get_params():\n            if param_name in kwargs:\n                new_k[param_name] = kwargs[param_name]\n            elif hasattr(self, param_name):\n                new_k[param_name] = getattr(self, param_name)\n\n        return cls(**new_k)", "is_method": true, "class_name": "Task", "function_description": "The `clone` method creates a new instance of the Task, or its subclass, by copying most attributes from the current instance while allowing specified parameters to be overridden. This simplifies creating modified task variations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__hash__", "line_number": 532, "body": "def __hash__(self):\n        return self.__hash", "is_method": true, "class_name": "Task", "function_description": "Provides a hash value for the Task object, allowing it to be used as a key in dictionaries or an element in sets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__repr__", "line_number": 535, "body": "def __repr__(self):\n        \"\"\"\n        Build a task representation like `MyTask(param1=1.5, param2='5')`\n        \"\"\"\n        params = self.get_params()\n        param_values = self.get_param_values(params, [], self.param_kwargs)\n\n        # Build up task id\n        repr_parts = []\n        param_objs = dict(params)\n        for param_name, param_value in param_values:\n            if param_objs[param_name].significant:\n                repr_parts.append('%s=%s' % (param_name, param_objs[param_name].serialize(param_value)))\n\n        task_str = '{}({})'.format(self.get_task_family(), ', '.join(repr_parts))\n\n        return task_str", "is_method": true, "class_name": "Task", "function_description": "Provides a structured, human-readable string representation of a Task instance. It includes the task family and its significant serialized parameters, aiding debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "__eq__", "line_number": 553, "body": "def __eq__(self, other):\n        return self.__class__ == other.__class__ and self.task_id == other.task_id", "is_method": true, "class_name": "Task", "function_description": "Defines equality for Task objects, considering them equal if they are of the same class and share the same task identifier. This allows comparing tasks for sameness."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "complete", "line_number": 556, "body": "def complete(self):\n        \"\"\"\n        If the task has any outputs, return ``True`` if all outputs exist.\n        Otherwise, return ``False``.\n\n        However, you may freely override this method with custom logic.\n        \"\"\"\n        outputs = flatten(self.output())\n        if len(outputs) == 0:\n            warnings.warn(\n                \"Task %r without outputs has no custom complete() method\" % self,\n                stacklevel=2\n            )\n            return False\n\n        return all(map(lambda output: output.exists(), outputs))", "is_method": true, "class_name": "Task", "function_description": "Determines if a task is complete by checking if all its defined outputs exist. This method provides a flexible, overridable mechanism for task status management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "output", "line_number": 583, "body": "def output(self):\n        \"\"\"\n        The output that this Task produces.\n\n        The output of the Task determines if the Task needs to be run--the task\n        is considered finished iff the outputs all exist. Subclasses should\n        override this method to return a single :py:class:`Target` or a list of\n        :py:class:`Target` instances.\n\n        Implementation note\n          If running multiple workers, the output must be a resource that is accessible\n          by all workers, such as a DFS or database. Otherwise, workers might compute\n          the same output since they don't see the work done by other workers.\n\n        See :ref:`Task.output`\n        \"\"\"\n        return []", "is_method": true, "class_name": "Task", "function_description": "This method specifies the outputs a task produces. These outputs determine if the task is considered finished and needs to be executed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "requires", "line_number": 601, "body": "def requires(self):\n        \"\"\"\n        The Tasks that this Task depends on.\n\n        A Task will only run if all of the Tasks that it requires are completed.\n        If your Task does not require any other Tasks, then you don't need to\n        override this method. Otherwise, a subclass can override this method\n        to return a single Task, a list of Task instances, or a dict whose\n        values are Task instances.\n\n        See :ref:`Task.requires`\n        \"\"\"\n        return []", "is_method": true, "class_name": "Task", "function_description": "This method specifies the prerequisites that must be completed before the current Task can execute. It defines the dependencies essential for task scheduling and workflow orchestration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "_requires", "line_number": 615, "body": "def _requires(self):\n        \"\"\"\n        Override in \"template\" tasks which themselves are supposed to be\n        subclassed and thus have their requires() overridden (name preserved to\n        provide consistent end-user experience), yet need to introduce\n        (non-input) dependencies.\n\n        Must return an iterable which among others contains the _requires() of\n        the superclass.\n        \"\"\"\n        return flatten(self.requires())", "is_method": true, "class_name": "Task", "function_description": "This internal method allows base `Task` classes to declare additional non-input dependencies. It ensures consistent, aggregated dependency resolution across the inheritance hierarchy for template tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "process_resources", "line_number": 627, "body": "def process_resources(self):\n        \"\"\"\n        Override in \"template\" tasks which provide common resource functionality\n        but allow subclasses to specify additional resources while preserving\n        the name for consistent end-user experience.\n        \"\"\"\n        return self.resources", "is_method": true, "class_name": "Task", "function_description": "Retrieves the task's associated resources. It functions as a template method, intended for subclasses to override and customize resource handling while ensuring a consistent interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "input", "line_number": 635, "body": "def input(self):\n        \"\"\"\n        Returns the outputs of the Tasks returned by :py:meth:`requires`\n\n        See :ref:`Task.input`\n\n        :return: a list of :py:class:`Target` objects which are specified as\n                 outputs of all required Tasks.\n        \"\"\"\n        return getpaths(self.requires())", "is_method": true, "class_name": "Task", "function_description": "This method retrieves the output targets generated by all prerequisite tasks that the current task depends on. It provides access to the results from upstream dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "deps", "line_number": 646, "body": "def deps(self):\n        \"\"\"\n        Internal method used by the scheduler.\n\n        Returns the flattened list of requires.\n        \"\"\"\n        # used by scheduler\n        return flatten(self._requires())", "is_method": true, "class_name": "Task", "function_description": "Provides the flattened list of dependencies required by a task. This internal method is used by a scheduler to determine task execution order and manage dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "on_failure", "line_number": 663, "body": "def on_failure(self, exception):\n        \"\"\"\n        Override for custom error handling.\n\n        This method gets called if an exception is raised in :py:meth:`run`.\n        The returned value of this method is json encoded and sent to the scheduler\n        as the `expl` argument. Its string representation will be used as the\n        body of the error email sent out if any.\n\n        Default behavior is to return a string representation of the stack trace.\n        \"\"\"\n\n        traceback_string = traceback.format_exc()\n        return \"Runtime error:\\n%s\" % traceback_string", "is_method": true, "class_name": "Task", "function_description": "This method handles exceptions during task execution. It formats error details for the scheduler and email notifications, providing custom failure reporting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "no_unpicklable_properties", "line_number": 690, "body": "def no_unpicklable_properties(self):\n        \"\"\"\n        Remove unpicklable properties before dump task and resume them after.\n\n        This method could be called in subtask's dump method, to ensure unpicklable\n        properties won't break dump.\n\n        This method is a context-manager which can be called as below:\n\n        .. code-block: python\n\n            class DummyTask(luigi):\n\n                def _dump(self):\n                    with self.no_unpicklable_properties():\n                        pickle.dumps(self)\n\n        \"\"\"\n        unpicklable_properties = tuple(luigi.worker.TaskProcess.forward_reporter_attributes.values())\n        reserved_properties = {}\n        for property_name in unpicklable_properties:\n            if hasattr(self, property_name):\n                reserved_properties[property_name] = getattr(self, property_name)\n                setattr(self, property_name, 'placeholder_during_pickling')\n\n        yield\n\n        for property_name, value in reserved_properties.items():\n            setattr(self, property_name, value)", "is_method": true, "class_name": "Task", "function_description": "This context manager ensures a Task instance can be successfully pickled by temporarily removing and then restoring its unpicklable properties. It facilitates reliable serialization of tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "bulk_complete", "line_number": 730, "body": "def bulk_complete(cls, parameter_tuples):\n        generated_tuples = []\n        for parameter_tuple in parameter_tuples:\n            if isinstance(parameter_tuple, (list, tuple)):\n                if cls(*parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n            elif isinstance(parameter_tuple, dict):\n                if cls(**parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n            else:\n                if cls(parameter_tuple).complete():\n                    generated_tuples.append(parameter_tuple)\n        return generated_tuples", "is_method": true, "class_name": "MixinNaiveBulkComplete", "function_description": "As a class method, it processes a batch of parameter sets, instantiating objects and calling their `complete()` method. It collects and returns only the parameter sets that successfully completed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "complete", "line_number": 821, "body": "def complete(self):\n        return all(r.complete() for r in flatten(self.requires()))", "is_method": true, "class_name": "WrapperTask", "function_description": "Determines if this `WrapperTask` and all its required sub-tasks have successfully completed. It provides an aggregate completion status for complex workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "wrapped", "line_number": 260, "body": "def wrapped(callback):\n            cls._event_callbacks.setdefault(cls, {}).setdefault(event, set()).add(callback)\n            return callback", "is_method": true, "class_name": "Task", "function_description": "Registers a callback function to be executed when a specific event occurs on a particular class. It associates the callback with the class and event for later dispatch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task.py", "function": "list_to_tuple", "line_number": 422, "body": "def list_to_tuple(x):\n            \"\"\" Make tuples out of lists and sets to allow hashing \"\"\"\n            if isinstance(x, list) or isinstance(x, set):\n                return tuple(x)\n            else:\n                return x", "is_method": true, "class_name": "Task", "function_description": "Converts lists and sets into tuples. This service makes mutable collections hashable, enabling their use as dictionary keys or set members."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "from_utc", "line_number": 227, "body": "def from_utc(utcTime, fmt=None):\n    \"\"\"convert UTC time string to time.struct_time: change datetime.datetime to time, return time.struct_time type\"\"\"\n    if fmt is None:\n        try_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\"]\n    else:\n        try_formats = [fmt]\n\n    for fmt in try_formats:\n        try:\n            time_struct = datetime.datetime.strptime(utcTime, fmt)\n        except ValueError:\n            pass\n        else:\n            date = int(time.mktime(time_struct.timetuple()))\n            return date\n    else:\n        raise ValueError(\"No UTC format matches {}\".format(utcTime))", "is_method": false, "function_description": "Converts a UTC time string, supporting common formats, into an integer Unix timestamp. This provides a standardized numerical representation of time for storage or calculations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "app", "line_number": 300, "body": "def app(scheduler):\n    settings = {\"static_path\": os.path.join(os.path.dirname(__file__), \"static\"),\n                \"unescape\": tornado.escape.xhtml_unescape,\n                \"compress_response\": True,\n                }\n    handlers = [\n        (r'/api/(.*)', RPCHandler, {\"scheduler\": scheduler}),\n        (r'/', RootPathHandler, {'scheduler': scheduler}),\n        (r'/tasklist', AllRunHandler, {'scheduler': scheduler}),\n        (r'/tasklist/(.*?)', SelectedRunHandler, {'scheduler': scheduler}),\n        (r'/history', RecentRunHandler, {'scheduler': scheduler}),\n        (r'/history/by_name/(.*?)', ByNameHandler, {'scheduler': scheduler}),\n        (r'/history/by_id/(.*?)', ByIdHandler, {'scheduler': scheduler}),\n        (r'/history/by_params/(.*?)', ByParamsHandler, {'scheduler': scheduler}),\n        (r'/metrics', MetricsHandler, {'scheduler': scheduler})\n    ]\n    api_app = tornado.web.Application(handlers, **settings)\n    return api_app", "is_method": false, "function_description": "Configures and returns a Tornado web application that provides a comprehensive web interface and API for managing, monitoring, and interacting with a scheduler's tasks and history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_init_api", "line_number": 320, "body": "def _init_api(scheduler, api_port=None, address=None, unix_socket=None):\n    api_app = app(scheduler)\n    if unix_socket is not None:\n        api_sockets = [tornado.netutil.bind_unix_socket(unix_socket)]\n    else:\n        api_sockets = tornado.netutil.bind_sockets(api_port, address=address)\n    server = tornado.httpserver.HTTPServer(api_app)\n    server.add_sockets(api_sockets)\n\n    # Return the bound socket names.  Useful for connecting client in test scenarios.\n    return [s.getsockname() for s in api_sockets]", "is_method": false, "function_description": "Initializes and binds an HTTP API server for a scheduler, enabling external communication. It configures the server to listen on specified network addresses or Unix sockets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "run", "line_number": 333, "body": "def run(api_port=8082, address=None, unix_socket=None, scheduler=None):\n    \"\"\"\n    Runs one instance of the API server.\n    \"\"\"\n    if scheduler is None:\n        scheduler = Scheduler()\n\n    # load scheduler state\n    scheduler.load()\n\n    _init_api(\n        scheduler=scheduler,\n        api_port=api_port,\n        address=address,\n        unix_socket=unix_socket,\n    )\n\n    # prune work DAG every 60 seconds\n    pruner = tornado.ioloop.PeriodicCallback(scheduler.prune, 60000)\n    pruner.start()\n\n    def shutdown_handler(signum, frame):\n        exit_handler()\n        sys.exit(0)\n\n    @atexit.register\n    def exit_handler():\n        logger.info(\"Scheduler instance shutting down\")\n        scheduler.dump()\n        stop()\n\n    signal.signal(signal.SIGINT, shutdown_handler)\n    signal.signal(signal.SIGTERM, shutdown_handler)\n    if os.name == 'nt':\n        signal.signal(signal.SIGBREAK, shutdown_handler)\n    else:\n        signal.signal(signal.SIGQUIT, shutdown_handler)\n\n    logger.info(\"Scheduler starting up\")\n\n    tornado.ioloop.IOLoop.instance().start()", "is_method": false, "function_description": "This function runs an API server instance, initializing a scheduler and setting up its persistent state management. It handles server startup, periodic tasks, and graceful shutdown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "stop", "line_number": 376, "body": "def stop():\n    tornado.ioloop.IOLoop.instance().stop()", "is_method": false, "function_description": "Stops the main Tornado I/O event loop. This action effectively terminates the application's asynchronous operations, facilitating a graceful shutdown."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 102, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "RPCHandler", "function_description": "Initializes the RPCHandler by associating it with a scheduler. This enables the handler to manage or execute scheduled tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "options", "line_number": 105, "body": "def options(self, *args):\n        if self._cors_config.enabled:\n            self._handle_cors_preflight()\n\n        self.set_status(204)\n        self.finish()", "is_method": true, "class_name": "RPCHandler", "function_description": "Handles HTTP OPTIONS requests, primarily serving CORS preflight checks. This enables secure cross-origin communication for API clients."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 112, "body": "def get(self, method):\n        if method not in RPC_METHODS:\n            self.send_error(404)\n            return\n        payload = self.get_argument('data', default=\"{}\")\n        arguments = json.loads(payload)\n\n        if hasattr(self._scheduler, method):\n            result = getattr(self._scheduler, method)(**arguments)\n\n            if self._cors_config.enabled:\n                self._handle_cors()\n\n            self.write({\"response\": result})  # wrap all json response in a dictionary\n        else:\n            self.send_error(404)", "is_method": true, "class_name": "RPCHandler", "function_description": "Handles incoming RPC GET requests by dispatching the requested method to an internal scheduler object. It executes the method with provided arguments and returns the operation's result to the client."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_handle_cors_preflight", "line_number": 131, "body": "def _handle_cors_preflight(self):\n        origin = self.request.headers.get('Origin')\n        if not origin:\n            return\n\n        if origin == 'null':\n            if self._cors_config.allow_null_origin:\n                self.set_header('Access-Control-Allow-Origin', 'null')\n                self._set_other_cors_headers()\n        else:\n            if self._cors_config.allow_any_origin:\n                self.set_header('Access-Control-Allow-Origin', '*')\n                self._set_other_cors_headers()\n            elif origin in self._cors_config.allowed_origins:\n                self.set_header('Access-Control-Allow-Origin', origin)\n                self._set_other_cors_headers()", "is_method": true, "class_name": "RPCHandler", "function_description": "Configures CORS preflight responses for the RPC handler. It sets `Access-Control-Allow-Origin` and other headers based on the request's origin and server's CORS settings, enabling controlled cross-origin access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_handle_cors", "line_number": 148, "body": "def _handle_cors(self):\n        origin = self.request.headers.get('Origin')\n        if not origin:\n            return\n\n        if origin == 'null':\n            if self._cors_config.allow_null_origin:\n                self.set_header('Access-Control-Allow-Origin', 'null')\n        else:\n            if self._cors_config.allow_any_origin:\n                self.set_header('Access-Control-Allow-Origin', '*')\n            elif origin in self._cors_config.allowed_origins:\n                self.set_header('Access-Control-Allow-Origin', origin)\n                self.set_header('Vary', 'Origin')", "is_method": true, "class_name": "RPCHandler", "function_description": "This method processes the request's `Origin` header to apply Cross-Origin Resource Sharing (CORS) policies. It sets the appropriate `Access-Control-Allow-Origin` header, controlling cross-origin access to the RPC service."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "_set_other_cors_headers", "line_number": 163, "body": "def _set_other_cors_headers(self):\n        self.set_header('Access-Control-Max-Age', str(self._cors_config.max_age))\n        self.set_header('Access-Control-Allow-Methods', self._cors_config.allowed_methods)\n        self.set_header('Access-Control-Allow-Headers', self._cors_config.allowed_headers)\n        if self._cors_config.allow_credentials:\n            self.set_header('Access-Control-Allow-Credentials', 'true')\n        if self._cors_config.exposed_headers:\n            self.set_header('Access-Control-Expose-Headers', self._cors_config.exposed_headers)", "is_method": true, "class_name": "RPCHandler", "function_description": "Sets various HTTP CORS (Cross-Origin Resource Sharing) response headers based on configured policies. This enables secure cross-origin requests and controls browser access to resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 174, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "BaseTaskHistoryHandler", "function_description": "Sets up the BaseTaskHistoryHandler by associating it with a scheduler, enabling it to interact with the scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get_template_path", "line_number": 177, "body": "def get_template_path(self):\n        return pkg_resources.resource_filename(__name__, 'templates')", "is_method": true, "class_name": "BaseTaskHistoryHandler", "function_description": "Provides the file path to the package's bundled templates directory. This enables other components to locate and utilize static template files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 182, "body": "def get(self):\n        all_tasks = self._scheduler.task_history.find_all_runs()\n        tasknames = [task.name for task in all_tasks]\n        # show all tasks with their name list to be selected\n        # why all tasks? the duration of the event history of a selected task\n        # can be more than 24 hours.\n        self.render(\"menu.html\", tasknames=tasknames)", "is_method": true, "class_name": "AllRunHandler", "function_description": "Retrieves all historical task names from the scheduler's history. It then renders a menu page to display these tasks to the user."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 192, "body": "def get(self, name):\n        statusResults = {}\n        taskResults = []\n        # get all tasks that has been updated\n        all_tasks = self._scheduler.task_history.find_all_runs()\n        # get events history for all tasks\n        all_tasks_event_history = self._scheduler.task_history.find_all_events()\n\n        # build the dictionary tasks with index: id, value: task_name\n        tasks = {task.id: str(task.name) for task in all_tasks}\n\n        for task in all_tasks_event_history:\n            # if the name of user-selected task is in tasks, get its task_id\n            if tasks.get(task.task_id) == str(name):\n                status = str(task.event_name)\n                if status not in statusResults:\n                    statusResults[status] = []\n                # append the id, task_id, ts, y with 0, next_process with null\n                # for the status(running/failed/done) of the selected task\n                statusResults[status].append(({\n                                                  'id': str(task.id), 'task_id': str(task.task_id),\n                                                  'x': from_utc(str(task.ts)), 'y': 0, 'next_process': ''}))\n                # append the id, task_name, task_id, status, datetime, timestamp\n                # for the selected task\n                taskResults.append({\n                    'id': str(task.id), 'taskName': str(name), 'task_id': str(task.task_id),\n                    'status': str(task.event_name), 'datetime': str(task.ts),\n                    'timestamp': from_utc(str(task.ts))})\n        statusResults = json.dumps(statusResults)\n        taskResults = json.dumps(taskResults)\n        statusResults = tornado.escape.xhtml_unescape(str(statusResults))\n        taskResults = tornado.escape.xhtml_unescape(str(taskResults))\n        self.render('history.html', name=name, statusResults=statusResults, taskResults=taskResults)", "is_method": true, "class_name": "SelectedRunHandler", "function_description": "Retrieves and formats the complete execution history and status events for a specific named task. It prepares this historical data for rendering on a web history page."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 247, "body": "def get(self):\n        tasks = self._scheduler.task_history.find_latest_runs()\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "RecentRunHandler", "function_description": "Provides a web-based view of recent task execution history. It fetches the latest runs from the scheduler and renders them on the 'recent.html' template."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 253, "body": "def get(self, name):\n        tasks = self._scheduler.task_history.find_all_by_name(name)\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "ByNameHandler", "function_description": "This method retrieves all tasks matching a specified name from the scheduler's history. It then displays these tasks on a dedicated web page for user review."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 259, "body": "def get(self, id):\n        task = self._scheduler.task_history.find_task_by_id(id)\n        self.render(\"show.html\", task=task)", "is_method": true, "class_name": "ByIdHandler", "function_description": "Retrieves a specific task by its ID from the scheduler's history. It then renders an HTML page to display the task's details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 265, "body": "def get(self, name):\n        payload = self.get_argument('data', default=\"{}\")\n        arguments = json.loads(payload)\n        tasks = self._scheduler.task_history.find_all_by_parameters(name, session=None, **arguments)\n        self.render(\"recent.html\", tasks=tasks)", "is_method": true, "class_name": "ByParamsHandler", "function_description": "As part of ByParamsHandler, this method fetches and displays historical tasks, filtered by parameters provided in the request's data payload. It renders the results in a web page."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 273, "body": "def get(self):\n        # we omit the leading slash in case the visualizer is behind a different\n        # path (as in a reverse proxy setup)\n        #\n        # For example, if luigi is behind my.app.com/my/luigi/, we want / to\n        # redirect relative (so it goes to my.app.com/my/luigi/static/visualizer/index.html)\n        # instead of absolute (which would be my.app.com/static/visualizer/index.html)\n        self.redirect(\"static/visualiser/index.html\")", "is_method": true, "class_name": "RootPathHandler", "function_description": "Redirects requests to the root path to the application's visualizer index page. It uses a relative path to ensure proper functionality in reverse proxy environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "head", "line_number": 282, "body": "def head(self):\n        \"\"\"HEAD endpoint for health checking the scheduler\"\"\"\n        self.set_status(204)\n        self.finish()", "is_method": true, "class_name": "RootPathHandler", "function_description": "Provides a HEAD endpoint for health checking the scheduler's availability, responding with a 204 status to confirm operational status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "initialize", "line_number": 289, "body": "def initialize(self, scheduler):\n        self._scheduler = scheduler", "is_method": true, "class_name": "MetricsHandler", "function_description": "Associates the MetricsHandler instance with a scheduler object, enabling the handler to utilize its functionalities."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "get", "line_number": 292, "body": "def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n        if metrics:\n            metrics_collector.configure_http_handler(self)\n            self.write(metrics)", "is_method": true, "class_name": "MetricsHandler", "function_description": "Retrieves and serves the latest operational metrics from the system. This method provides an endpoint for external monitoring and data collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "shutdown_handler", "line_number": 354, "body": "def shutdown_handler(signum, frame):\n        exit_handler()\n        sys.exit(0)", "is_method": false, "function_description": "Handles system shutdown signals by executing a custom exit handler and then gracefully terminating the program."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/server.py", "function": "exit_handler", "line_number": 359, "body": "def exit_handler():\n        logger.info(\"Scheduler instance shutting down\")\n        scheduler.dump()\n        stop()", "is_method": false, "function_description": "Handles the graceful shutdown of a scheduler instance. It logs the event, persists the scheduler's state, and stops its operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_empty_retry_policy", "line_number": 90, "body": "def _get_empty_retry_policy():\n    return RetryPolicy(*[None] * len(_retry_policy_fields))", "is_method": false, "function_description": "Provides an instance of `RetryPolicy` with all its configuration fields explicitly set to `None`. This serves as a default or unconfigured retry policy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "rpc_method", "line_number": 94, "body": "def rpc_method(**request_args):\n    def _rpc_method(fn):\n        # If request args are passed, return this function again for use as\n        # the decorator function with the request args attached.\n        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = inspect.getfullargspec(fn)\n        assert not varargs\n        first_arg, *all_args = args\n        assert first_arg == 'self'\n        defaults = dict(zip(reversed(all_args), reversed(defaults or ())))\n        required_args = frozenset(arg for arg in all_args if arg not in defaults)\n        fn_name = fn.__name__\n\n        @functools.wraps(fn)\n        def rpc_func(self, *args, **kwargs):\n            actual_args = defaults.copy()\n            actual_args.update(dict(zip(all_args, args)))\n            actual_args.update(kwargs)\n            if not all(arg in actual_args for arg in required_args):\n                raise TypeError('{} takes {} arguments ({} given)'.format(\n                    fn_name, len(all_args), len(actual_args)))\n            return self._request('/api/{}'.format(fn_name), actual_args, **request_args)\n\n        RPC_METHODS[fn_name] = rpc_func\n        return fn\n\n    return _rpc_method", "is_method": false, "function_description": "Provides a decorator factory to transform instance methods into Remote Procedure Call (RPC) endpoints. It handles argument mapping for the RPC request and registers the method for remote invocation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_default", "line_number": 159, "body": "def _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default", "is_method": false, "function_description": "Returns `x` if it is not `None`, otherwise provides a specified `default` value. This utility ensures a fallback value is always available for potentially `None` inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_rpc_method", "line_number": 95, "body": "def _rpc_method(fn):\n        # If request args are passed, return this function again for use as\n        # the decorator function with the request args attached.\n        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = inspect.getfullargspec(fn)\n        assert not varargs\n        first_arg, *all_args = args\n        assert first_arg == 'self'\n        defaults = dict(zip(reversed(all_args), reversed(defaults or ())))\n        required_args = frozenset(arg for arg in all_args if arg not in defaults)\n        fn_name = fn.__name__\n\n        @functools.wraps(fn)\n        def rpc_func(self, *args, **kwargs):\n            actual_args = defaults.copy()\n            actual_args.update(dict(zip(all_args, args)))\n            actual_args.update(kwargs)\n            if not all(arg in actual_args for arg in required_args):\n                raise TypeError('{} takes {} arguments ({} given)'.format(\n                    fn_name, len(all_args), len(actual_args)))\n            return self._request('/api/{}'.format(fn_name), actual_args, **request_args)\n\n        RPC_METHODS[fn_name] = rpc_func\n        return fn", "is_method": false, "function_description": "This decorator transforms a Python method into an RPC client function. It automatically handles argument parsing, then makes an internal API request to a corresponding endpoint, facilitating remote procedure calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_get_retry_policy", "line_number": 155, "body": "def _get_retry_policy(self):\n        return RetryPolicy(self.retry_count, self.disable_hard_timeout, self.disable_window)", "is_method": true, "class_name": "scheduler", "function_description": "Creates and returns a `RetryPolicy` object, encapsulating the retry configuration for the scheduler's operations. This provides a clear definition of how tasks should be retried."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__len__", "line_number": 180, "body": "def __len__(self):\n        return len(self.map)", "is_method": true, "class_name": "OrderedSet", "function_description": "Returns the total count of unique elements within the `OrderedSet` instance. This method enables the standard `len()` built-in function to determine the set's size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__contains__", "line_number": 183, "body": "def __contains__(self, key):\n        return key in self.map", "is_method": true, "class_name": "OrderedSet", "function_description": "Determines if an element is a member of the `OrderedSet`. This method implements the `in` operator for membership testing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add", "line_number": 186, "body": "def add(self, key):\n        if key not in self.map:\n            end = self.end\n            curr = end[1]\n            curr[2] = end[1] = self.map[key] = [key, curr, end]", "is_method": true, "class_name": "OrderedSet", "function_description": "Adds a unique element to the OrderedSet, maintaining insertion order by appending it to the end of the collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "discard", "line_number": 192, "body": "def discard(self, key):\n        if key in self.map:\n            key, prev, next = self.map.pop(key)\n            prev[2] = next\n            next[1] = prev", "is_method": true, "class_name": "OrderedSet", "function_description": "Removes a specified element from the ordered set if it exists, without raising an error."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__iter__", "line_number": 198, "body": "def __iter__(self):\n        end = self.end\n        curr = end[2]\n        while curr is not end:\n            yield curr[0]\n            curr = curr[2]", "is_method": true, "class_name": "OrderedSet", "function_description": "Allows iterating through the set's elements in the order they were originally added, enabling standard loop constructs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__reversed__", "line_number": 205, "body": "def __reversed__(self):\n        end = self.end\n        curr = end[1]\n        while curr is not end:\n            yield curr[0]\n            curr = curr[1]", "is_method": true, "class_name": "OrderedSet", "function_description": "Provides an iterator that yields elements of the OrderedSet in reverse order of their insertion. This allows direct use with the built-in `reversed()` function."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "peek", "line_number": 212, "body": "def peek(self, last=True):\n        if not self:\n            raise KeyError('set is empty')\n        key = self.end[1][0] if last else self.end[2][0]\n        return key", "is_method": true, "class_name": "OrderedSet", "function_description": "Allows viewing the first or last element of the ordered set without removal. Raises an error if the set is empty."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pop", "line_number": 218, "body": "def pop(self, last=True):\n        key = self.peek(last)\n        self.discard(key)\n        return key", "is_method": true, "class_name": "OrderedSet", "function_description": "Removes and returns an element from the OrderedSet. This method allows retrieval and removal of either the last or first element based on preference."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__repr__", "line_number": 223, "body": "def __repr__(self):\n        if not self:\n            return '%s()' % (self.__class__.__name__,)\n        return '%s(%r)' % (self.__class__.__name__, list(self))", "is_method": true, "class_name": "OrderedSet", "function_description": "Provides a clear, unambiguous string representation of the `OrderedSet` instance, suitable for debugging and reconstruction. It displays the class name and the ordered list of its elements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__eq__", "line_number": 228, "body": "def __eq__(self, other):\n        if isinstance(other, OrderedSet):\n            return len(self) == len(other) and list(self) == list(other)\n        return set(self) == set(other)", "is_method": true, "class_name": "OrderedSet", "function_description": "Compares this OrderedSet to another collection for equality. It verifies elements and their order if the other is an OrderedSet; otherwise, it performs a set-based comparison."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__repr__", "line_number": 275, "body": "def __repr__(self):\n        return \"Task(%r)\" % vars(self)", "is_method": true, "class_name": "Task", "function_description": "Provides an unambiguous string representation of a Task object. Essential for debugging and developer inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_params", "line_number": 278, "body": "def set_params(self, params):\n        self.params = _get_default(params, {})\n        self.public_params = {key: value for key, value in self.params.items() if\n                              self.param_visibilities.get(key, ParameterVisibility.PUBLIC) == ParameterVisibility.PUBLIC}\n        self.hidden_params = {key: value for key, value in self.params.items() if\n                              self.param_visibilities.get(key, ParameterVisibility.PUBLIC) == ParameterVisibility.HIDDEN}", "is_method": true, "class_name": "Task", "function_description": "Configures the task by setting its parameters and categorizing them as public or hidden based on predefined visibility rules."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_batchable", "line_number": 287, "body": "def is_batchable(self):\n        try:\n            return self.batchable\n        except AttributeError:\n            return False", "is_method": true, "class_name": "Task", "function_description": "Checks if a task instance can be processed in a batch. It returns the task's batchable status, defaulting to false if undefined."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_failure", "line_number": 293, "body": "def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        failure_time = time.time()\n\n        if not self.first_failure_time:\n            self.first_failure_time = failure_time\n\n        self.failures.append(failure_time)", "is_method": true, "class_name": "Task", "function_description": "Records a failure event for the task at the current time. It tracks both individual failures and the timestamp of the very first failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "num_failures", "line_number": 304, "body": "def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.retry_policy.disable_window\n\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)", "is_method": true, "class_name": "Task", "function_description": "For a `Task`, this method provides the current number of failures within its defined time window, pruning old records. It's used to assess recent task reliability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_excessive_failures", "line_number": 315, "body": "def has_excessive_failures(self):\n        if self.first_failure_time is not None:\n            if time.time() >= self.first_failure_time + self.retry_policy.disable_hard_timeout:\n                return True\n\n        logger.debug('%s task num failures is %s and limit is %s', self.id, self.num_failures(), self.retry_policy.retry_count)\n        if self.num_failures() >= self.retry_policy.retry_count:\n            logger.debug('%s task num failures limit(%s) is exceeded', self.id, self.retry_policy.retry_count)\n            return True\n\n        return False", "is_method": true, "class_name": "Task", "function_description": "This method determines if a task has experienced an excessive number of failures, either by exceeding a retry count or a time-based failure timeout. It signals when a task should no longer be retried."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "clear_failures", "line_number": 327, "body": "def clear_failures(self):\n        \"\"\"\n        Clear the failures history\n        \"\"\"\n        self.failures.clear()\n        self.first_failure_time = None", "is_method": true, "class_name": "Task", "function_description": "Enables a task to clear its entire failure history, resetting both recorded failures and the initial failure timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pretty_id", "line_number": 335, "body": "def pretty_id(self):\n        param_str = ', '.join(u'{}={}'.format(key, value) for key, value in sorted(self.public_params.items()))\n        return u'{}({})'.format(self.family, param_str)", "is_method": true, "class_name": "Task", "function_description": "Generates a unique, human-readable string ID for the task instance. It combines the task family with its sorted public parameters for clear identification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_info", "line_number": 356, "body": "def add_info(self, info):\n        self.info.update(info)", "is_method": true, "class_name": "Worker", "function_description": "Updates the worker's internal information by incorporating new key-value pairs from the provided data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update", "line_number": 359, "body": "def update(self, worker_reference, get_work=False):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n        if get_work:\n            self.last_get_work = time.time()", "is_method": true, "class_name": "Worker", "function_description": "Updates a worker's last active timestamp and optionally its reference or last work request time. This method tracks the worker's current liveness and activity within a system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "prune", "line_number": 366, "body": "def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True", "is_method": true, "class_name": "Worker", "function_description": "Determines if a worker instance has been inactive for a defined period. This signals that the worker is likely disconnected and should be removed from the system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_tasks", "line_number": 371, "body": "def get_tasks(self, state, *statuses):\n        num_self_tasks = len(self.tasks)\n        num_state_tasks = sum(len(state._status_tasks[status]) for status in statuses)\n        if num_self_tasks < num_state_tasks:\n            return filter(lambda task: task.status in statuses, self.tasks)\n        else:\n            return filter(lambda task: self.id in task.workers, state.get_active_tasks_by_status(*statuses))", "is_method": true, "class_name": "Worker", "function_description": "Retrieves tasks for the worker that match specified statuses. It intelligently selects between the worker's local task list or a global state for efficiency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_trivial_worker", "line_number": 379, "body": "def is_trivial_worker(self, state):\n        \"\"\"\n        If it's not an assistant having only tasks that are without\n        requirements.\n\n        We have to pass the state parameter for optimization reasons.\n        \"\"\"\n        if self.assistant:\n            return False\n        return all(not task.resources for task in self.get_tasks(state, PENDING))", "is_method": true, "class_name": "Worker", "function_description": "This method determines if a worker is \"trivial,\" meaning it's not an assistant and all its pending tasks lack resource requirements. It helps identify workers suitable for basic, resource-independent assignments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "assistant", "line_number": 391, "body": "def assistant(self):\n        return self.info.get('assistant', False)", "is_method": true, "class_name": "Worker", "function_description": "Determines if the Worker instance is configured as an assistant. It returns a boolean status based on the worker's internal information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "enabled", "line_number": 395, "body": "def enabled(self):\n        return not self.disabled", "is_method": true, "class_name": "Worker", "function_description": "Determines if the worker is currently active and operational. It provides a simple boolean status check for the worker's availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "state", "line_number": 399, "body": "def state(self):\n        if self.enabled:\n            return WORKER_STATE_ACTIVE\n        else:\n            return WORKER_STATE_DISABLED", "is_method": true, "class_name": "Worker", "function_description": "Reports the current operational state of the worker, indicating if it's active or disabled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_rpc_message", "line_number": 405, "body": "def add_rpc_message(self, name, **kwargs):\n        # the message has the format {'name': <function_name>, 'kwargs': <function_kwargs>}\n        self.rpc_messages.append({'name': name, 'kwargs': kwargs})", "is_method": true, "class_name": "Worker", "function_description": "Adds a Remote Procedure Call (RPC) message to the worker's internal queue. It packages a function name and its arguments for future remote execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fetch_rpc_messages", "line_number": 409, "body": "def fetch_rpc_messages(self):\n        messages = self.rpc_messages[:]\n        del self.rpc_messages[:]\n        return messages", "is_method": true, "class_name": "Worker", "function_description": "Provides all queued RPC messages from the worker. It clears the internal message list after retrieval to ensure messages are processed only once."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "__str__", "line_number": 414, "body": "def __str__(self):\n        return self.id", "is_method": true, "class_name": "Worker", "function_description": "Provides a string representation of the Worker object, returning its unique ID. This is useful for logging, debugging, or displaying worker information."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_state", "line_number": 435, "body": "def get_state(self):\n        return self._tasks, self._active_workers, self._task_batchers", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides a comprehensive snapshot of the system's current task queue, active worker count, and task batcher states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_state", "line_number": 438, "body": "def set_state(self, state):\n        self._tasks, self._active_workers = state[:2]\n        if len(state) >= 3:\n            self._task_batchers = state[2]", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Sets the internal state of the task manager. It restores the active tasks, worker information, and optionally task batchers from a provided state object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dump", "line_number": 443, "body": "def dump(self):\n        try:\n            with open(self._state_path, 'wb') as fobj:\n                pickle.dump(self.get_state(), fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Persists the current state of the task to a file on disk. This enables checkpointing and later resumption of operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "load", "line_number": 453, "body": "def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from empty state.\")\n                return\n\n            self.set_state(state)\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.values():\n                self._status_tasks[task.status][task.id] = task\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with empty state\", self._state_path)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Restores the `SimpleTaskState` instance's previous operational state by loading it from a persistent file, managing load errors gracefully."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_tasks", "line_number": 470, "body": "def get_active_tasks(self):\n        return self._tasks.values()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Retrieves all tasks currently managed by the `SimpleTaskState` instance. It provides access to the collection of active tasks for external processing or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_tasks_by_status", "line_number": 473, "body": "def get_active_tasks_by_status(self, *statuses):\n        return itertools.chain.from_iterable(self._status_tasks[status].values() for status in statuses)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides an iterable of all tasks matching the specified statuses. This utility efficiently combines tasks from multiple status categories for retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_task_count_for_status", "line_number": 476, "body": "def get_active_task_count_for_status(self, status):\n        if status:\n            return len(self._status_tasks[status])\n        else:\n            return len(self._tasks)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Returns the count of tasks associated with a specific status, or the total number of managed tasks if no status is provided."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_batch_running_tasks", "line_number": 482, "body": "def get_batch_running_tasks(self, batch_id):\n        assert batch_id is not None\n        return [\n            task for task in self.get_active_tasks_by_status(BATCH_RUNNING)\n            if task.batch_id == batch_id\n        ]", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Retrieves active tasks from the system state that are currently running and belong to a specified batch ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_batcher", "line_number": 489, "body": "def set_batcher(self, worker_id, family, batcher_args, max_batch_size):\n        self._task_batchers.setdefault(worker_id, {})\n        self._task_batchers[worker_id][family] = (batcher_args, max_batch_size)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Registers or updates batching configuration for a specific worker and task family. It stores batcher arguments and maximum batch size for subsequent task processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_batcher", "line_number": 493, "body": "def get_batcher(self, worker_id, family):\n        return self._task_batchers.get(worker_id, {}).get(family, (None, 1))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Retrieves the task batcher configuration for a specified worker and task family. It provides a default configuration if none is found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "num_pending_tasks", "line_number": 496, "body": "def num_pending_tasks(self):\n        \"\"\"\n        Return how many tasks are PENDING + RUNNING. O(1).\n        \"\"\"\n        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Counts all tasks currently in a pending or running state within the task management system. This provides a quick overview of active and queued workload."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task", "line_number": 502, "body": "def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides access to a task by its ID. It can optionally create and register a new task with its status if it doesn't already exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_task", "line_number": 510, "body": "def has_task(self, task_id):\n        return task_id in self._tasks", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Determines if a given task ID is currently tracked or managed by the task state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_batch_running", "line_number": 520, "body": "def set_batch_running(self, task, batch_id, worker_id):\n        self.set_status(task, BATCH_RUNNING)\n        task.batch_id = batch_id\n        task.worker_running = worker_id\n        task.resources_running = task.resources\n        task.time_running = time.time()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Sets a task's status to `BATCH_RUNNING`, associating it with a specific batch ID and worker. It formally registers the start of a task's batch execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_status", "line_number": 527, "body": "def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        if new_status == DISABLED and task.status in (RUNNING, BATCH_RUNNING):\n            return\n\n        remove_on_failure = task.batch_id is not None and not task.batchable\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                return\n\n        if task.status == RUNNING and task.batch_id is not None and new_status != RUNNING:\n            for batch_task in self.get_batch_running_tasks(task.batch_id):\n                self.set_status(batch_task, new_status, config)\n                batch_task.batch_id = None\n            task.batch_id = None\n\n        if new_status == FAILED and task.status != DISABLED:\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                if not config.batch_emails:\n                    notifications.send_error_email(\n                        'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                        '{task} failed {failures} times in the last {window} seconds, so it is being '\n                        'disabled for {persist} seconds'.format(\n                            failures=task.retry_policy.retry_count,\n                            task=task.id,\n                            window=task.retry_policy.disable_window,\n                            persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        if new_status != task.status:\n            self._status_tasks[task.status].pop(task.id)\n            self._status_tasks[new_status][task.id] = task\n            task.status = new_status\n            task.updated = time.time()\n            self.update_metrics(task, config)\n\n        if new_status == FAILED:\n            task.retry = time.time() + config.retry_delay\n            if remove_on_failure:\n                task.remove = time.time()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Manages and updates a task's status within a workflow system. It handles complex transitions, including batch task propagation and automatic disabling due to excessive failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fail_dead_worker_task", "line_number": 580, "body": "def fail_dead_worker_task(self, task, config, assistants):\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status in (BATCH_RUNNING, RUNNING) and task.worker_running and task.worker_running not in task.stakeholders | assistants:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Identifies and marks tasks as FAILED when their assigned worker disconnects. This ensures resilience by scheduling affected tasks for a delayed retry."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_status", "line_number": 590, "body": "def update_status(self, task, config):\n        # Mark tasks with no remaining active stakeholders for deletion\n        if (not task.stakeholders) and (task.remove is None) and (task.status != RUNNING):\n            # We don't check for the RUNNING case, because that is already handled\n            # by the fail_dead_worker_task function.\n            logger.debug(\"Task %r has no stakeholders anymore -> might remove \"\n                         \"task in %s seconds\", task.id, config.remove_delay)\n            task.remove = time.time() + config.remove_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time is not None:\n            if time.time() - task.scheduler_disable_time > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "This method updates a task's state within a scheduling system. It manages task cleanup, re-enables temporarily disabled tasks, and triggers retries for failed tasks based on configured delays."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "may_prune", "line_number": 608, "body": "def may_prune(self, task):\n        return task.remove and time.time() >= task.remove", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Determines if a task is eligible for pruning by checking if it's marked for removal and if its scheduled removal time has been reached."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inactivate_tasks", "line_number": 611, "body": "def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Inactivates specified tasks by removing them from the class's active and status-based tracking. This effectively ceases their active management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_active_workers", "line_number": 619, "body": "def get_active_workers(self, last_active_lt=None, last_get_work_gt=None):\n        for worker in self._active_workers.values():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            last_get_work = worker.last_get_work\n            if last_get_work_gt is not None and (\n                            last_get_work is None or last_get_work <= last_get_work_gt):\n                continue\n            yield worker", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Retrieves active workers from the task state, filtering them by their last activity and last work retrieval timestamps."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_assistants", "line_number": 629, "body": "def get_assistants(self, last_active_lt=None):\n        return filter(lambda w: w.assistant, self.get_active_workers(last_active_lt))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Retrieves active workers from the task state that are designated as assistants, optionally filtered by their last activity time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_worker_ids", "line_number": 632, "body": "def get_worker_ids(self):\n        return self._active_workers.keys()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Provides the identifiers for all currently active workers managed by the task state. This is useful for monitoring or managing worker activity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_worker", "line_number": 635, "body": "def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))", "is_method": true, "class_name": "SimpleTaskState", "function_description": "This method retrieves an existing worker object or creates a new one if it doesn't exist, associating it with the given ID. It ensures a unique Worker instance for each worker_id within the task state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inactivate_workers", "line_number": 638, "body": "def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n        self._remove_workers_from_tasks(delete_workers)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Deactivates specified workers by removing them from the active worker pool and unassigning them from any current tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_remove_workers_from_tasks", "line_number": 644, "body": "def _remove_workers_from_tasks(self, workers, remove_stakeholders=True):\n        for task in self.get_active_tasks():\n            if remove_stakeholders:\n                task.stakeholders.difference_update(workers)\n            task.workers -= workers", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Removes a specified set of workers from all active tasks managed by this state, and optionally from their stakeholders."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "disable_workers", "line_number": 650, "body": "def disable_workers(self, worker_ids):\n        self._remove_workers_from_tasks(worker_ids, remove_stakeholders=False)\n        for worker_id in worker_ids:\n            worker = self.get_worker(worker_id)\n            worker.disabled = True\n            worker.tasks.clear()", "is_method": true, "class_name": "SimpleTaskState", "function_description": "This method disables specified workers, removing them from any current tasks and preventing them from being assigned new work. It effectively takes workers out of active duty within the task management system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_metrics", "line_number": 657, "body": "def update_metrics(self, task, config):\n        if task.status == DISABLED:\n            self._metrics_collector.handle_task_disabled(task, config)\n        elif task.status == DONE:\n            self._metrics_collector.handle_task_done(task)\n        elif task.status == FAILED:\n            self._metrics_collector.handle_task_failed(task)", "is_method": true, "class_name": "SimpleTaskState", "function_description": "Updates system metrics based on a task's current status (disabled, done, or failed), ensuring appropriate metric collection for different task outcomes.\nUpdates system metrics based on a task's current status (disabled, done, or failed), ensuring appropriate metric collection for different task outcomes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "load", "line_number": 700, "body": "def load(self):\n        self._state.load()", "is_method": true, "class_name": "Scheduler", "function_description": "Loads the scheduler's internal state from a persistent source, enabling it to resume operations from a previously saved point."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dump", "line_number": 703, "body": "def dump(self):\n        self._state.dump()\n        if self._config.batch_emails:\n            self._email_batcher.send_email()", "is_method": true, "class_name": "Scheduler", "function_description": "The Scheduler's `dump` method saves its current operational state. It also dispatches any batched emails if configured to do so."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "prune", "line_number": 709, "body": "def prune(self):\n        logger.debug(\"Starting pruning of task graph\")\n        self._prune_workers()\n        self._prune_tasks()\n        self._prune_emails()\n        logger.debug(\"Done pruning task graph\")", "is_method": true, "class_name": "Scheduler", "function_description": "This method cleans up and optimizes the scheduler's internal state. It prunes inactive workers, completed tasks, and old email records to maintain an efficient task graph."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_workers", "line_number": 716, "body": "def _prune_workers(self):\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.debug(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)", "is_method": true, "class_name": "Scheduler", "function_description": "This Scheduler method identifies and removes workers that have timed out and failed to maintain contact. It ensures the active worker list remains current and responsive."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_tasks", "line_number": 725, "body": "def _prune_tasks(self):\n        assistant_ids = {w.id for w in self._state.get_assistants()}\n        remove_tasks = []\n\n        for task in self._state.get_active_tasks():\n            self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            self._state.update_status(task, self._config)\n            if self._state.may_prune(task):\n                logger.info(\"Removing task %r\", task.id)\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)", "is_method": true, "class_name": "Scheduler", "function_description": "Maintains the Scheduler's task queue by failing tasks assigned to dead workers and removing completed or inactive tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_prune_emails", "line_number": 738, "body": "def _prune_emails(self):\n        if self._config.batch_emails:\n            self._email_batcher.update()", "is_method": true, "class_name": "Scheduler", "function_description": "Provides an internal service to conditionally manage and optimize batched emails. It triggers an update on the email batcher when email batching is active."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_worker", "line_number": 742, "body": "def _update_worker(self, worker_id, worker_reference=None, get_work=False):\n        # Keep track of whenever the worker was last active.\n        # For convenience also return the worker object.\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference, get_work=get_work)\n        return worker", "is_method": true, "class_name": "Scheduler", "function_description": "Updates a worker's status and last activity within the scheduler's internal state. This enables the scheduler to track worker availability and manage ongoing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_priority", "line_number": 749, "body": "def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)", "is_method": true, "class_name": "Scheduler", "function_description": "This private method updates a task's priority and recursively raises priorities of its dependencies, ensuring that a task's priority can only increase and propagate through its dependency chain."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_task_batcher", "line_number": 763, "body": "def add_task_batcher(self, worker, task_family, batched_args, max_batch_size=float('inf')):\n        self._state.set_batcher(worker, task_family, batched_args, max_batch_size)", "is_method": true, "class_name": "Scheduler", "function_description": "This method configures batch processing for a specific task family on a worker, enabling the scheduler to group tasks with common arguments for efficient execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "forgive_failures", "line_number": 767, "body": "def forgive_failures(self, task_id=None):\n        status = PENDING\n        task = self._state.get_task(task_id)\n        if task is None:\n            return {\"task_id\": task_id, \"status\": None}\n\n        # we forgive only failures\n        if task.status == FAILED:\n            # forgive but do not forget\n            self._update_task_history(task, status)\n            self._state.set_status(task, status, self._config)\n        return {\"task_id\": task_id, \"status\": task.status}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a mechanism within the Scheduler to reset a failed task's status to pending. This allows the task to be re-scheduled for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "mark_as_done", "line_number": 781, "body": "def mark_as_done(self, task_id=None):\n        status = DONE\n        task = self._state.get_task(task_id)\n        if task is None:\n            return {\"task_id\": task_id, \"status\": None}\n\n        # we can force mark DONE for running or failed tasks\n        if task.status in {RUNNING, FAILED, DISABLED}:\n            self._update_task_history(task, status)\n            self._state.set_status(task, status, self._config)\n        return {\"task_id\": task_id, \"status\": task.status}", "is_method": true, "class_name": "Scheduler", "function_description": "This method marks a specified task as \"done\" within the scheduler. It allows updating the status of tasks that are currently running, failed, or disabled to a completed state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_task", "line_number": 794, "body": "def add_task(self, task_id=None, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', module=None, params=None, param_visibilities=None, accepts_messages=False,\n                 assistant=False, tracking_url=None, worker=None, batchable=None,\n                 batch_id=None, retry_policy_dict=None, owners=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        assert worker is not None\n        worker_id = worker\n        worker = self._update_worker(worker_id)\n\n        resources = {} if resources is None else resources.copy()\n\n        if retry_policy_dict is None:\n            retry_policy_dict = {}\n\n        retry_policy = self._generate_retry_policy(retry_policy_dict)\n\n        if worker.enabled:\n            _default_task = self._make_task(\n                task_id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, module=module, params=params, param_visibilities=param_visibilities,\n            )\n        else:\n            _default_task = None\n\n        task = self._state.get_task(task_id, setdefault=_default_task)\n\n        if task is None or (task.status != RUNNING and not worker.enabled):\n            return\n\n        # Ignore claims that the task is PENDING if it very recently was marked as DONE.\n        if status == PENDING and task.status == DONE and (time.time() - task.updated) < self._config.stable_done_cooldown_secs:\n            return\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not getattr(task, 'module', None):\n            task.module = module\n        if not getattr(task, 'param_visibilities', None):\n            task.param_visibilities = _get_default(param_visibilities, {})\n        if not task.params:\n            task.set_params(params)\n\n        if batch_id is not None:\n            task.batch_id = batch_id\n        if status == RUNNING and not task.worker_running:\n            task.worker_running = worker_id\n            if batch_id:\n                # copy resources_running of the first batch task\n                batch_tasks = self._state.get_batch_running_tasks(batch_id)\n                task.resources_running = batch_tasks[0].resources_running.copy()\n            task.time_running = time.time()\n\n        if accepts_messages is not None:\n            task.accepts_messages = accepts_messages\n\n        if tracking_url is not None or task.status != RUNNING:\n            task.tracking_url = tracking_url\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.tracking_url = tracking_url\n\n        if batchable is not None:\n            task.batchable = batchable\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if expl is not None:\n            task.expl = expl\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.expl = expl\n\n        task_is_not_running = task.status not in (RUNNING, BATCH_RUNNING)\n        task_started_a_run = status in (DONE, FAILED, RUNNING)\n        running_on_this_worker = task.worker_running == worker_id\n        if task_is_not_running or (task_started_a_run and running_on_this_worker) or new_deps:\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it\n            if status != task.status or status == PENDING:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n\n        if status == FAILED and self._config.batch_emails:\n            batched_params, _ = self._state.get_batcher(worker_id, family)\n            if batched_params:\n                unbatched_params = {\n                    param: value\n                    for param, value in task.params.items()\n                    if param not in batched_params\n                }\n            else:\n                unbatched_params = task.params\n            try:\n                expl_raw = json.loads(expl)\n            except ValueError:\n                expl_raw = expl\n\n            self._email_batcher.add_failure(\n                task.pretty_id, task.family, unbatched_params, expl_raw, owners)\n            if task.status == DISABLED:\n                self._email_batcher.add_disable(\n                    task.pretty_id, task.family, unbatched_params, owners)\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        if resources is not None:\n            task.resources = resources\n\n        if worker.enabled and not assistant:\n            task.stakeholders.add(worker_id)\n\n            # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n            # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n            for dep in task.deps or []:\n                t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n                t.stakeholders.add(worker_id)\n\n        self._update_priority(task, priority, worker_id)\n\n        # Because some tasks (non-dynamic dependencies) are `_make_task`ed\n        # before we know their retry_policy, we always set it here\n        task.retry_policy = retry_policy\n\n        if runnable and status != FAILED and worker.enabled:\n            task.workers.add(worker_id)\n            self._state.get_worker(worker_id).tasks.add(task)\n            task.runnable = runnable", "is_method": true, "class_name": "Scheduler", "function_description": "Adds a new task or updates an existing one within the scheduler, managing its status, dependencies, and various properties. It integrates tasks into the scheduling system and ensures their state is current."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "announce_scheduling_failure", "line_number": 938, "body": "def announce_scheduling_failure(self, task_name, family, params, expl, owners, **kwargs):\n        if not self._config.batch_emails:\n            return\n        worker_id = kwargs['worker']\n        batched_params, _ = self._state.get_batcher(worker_id, family)\n        if batched_params:\n            unbatched_params = {\n                param: value\n                for param, value in params.items()\n                if param not in batched_params\n            }\n        else:\n            unbatched_params = params\n        self._email_batcher.add_scheduling_fail(task_name, family, unbatched_params, expl, owners)", "is_method": true, "class_name": "Scheduler", "function_description": "Within the Scheduler, this method records details of a task scheduling failure. It prepares these failure details for batched email notification to relevant owners, considering configuration and task parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_worker", "line_number": 954, "body": "def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)", "is_method": true, "class_name": "Scheduler", "function_description": "Adds or updates supplementary information for a specific worker managed by the scheduler. This service helps the scheduler maintain up-to-date worker state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "disable_worker", "line_number": 958, "body": "def disable_worker(self, worker):\n        self._state.disable_workers({worker})", "is_method": true, "class_name": "Scheduler", "function_description": "Disables a specific worker within the scheduler's management. This prevents the worker from being assigned any new tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_worker_processes", "line_number": 962, "body": "def set_worker_processes(self, worker, n):\n        self._state.get_worker(worker).add_rpc_message('set_worker_processes', n=n)", "is_method": true, "class_name": "Scheduler", "function_description": "Sets the number of processes for a specified worker. This allows the scheduler to remotely configure and manage the computational resources of its workers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "send_scheduler_message", "line_number": 966, "body": "def send_scheduler_message(self, worker, task, content):\n        if not self._config.send_messages:\n            return {\"message_id\": None}\n\n        message_id = str(uuid.uuid4())\n        self._state.get_worker(worker).add_rpc_message('dispatch_scheduler_message', task_id=task,\n                                                       message_id=message_id, content=content)\n\n        return {\"message_id\": message_id}", "is_method": true, "class_name": "Scheduler", "function_description": "Dispatches a message from the scheduler to a specified worker for a specific task. This enables crucial communication within the scheduling system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "add_scheduler_message_response", "line_number": 977, "body": "def add_scheduler_message_response(self, task_id, message_id, response):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.scheduler_message_responses[message_id] = response", "is_method": true, "class_name": "Scheduler", "function_description": "Records a response to a scheduler message for a specific task. This method helps the Scheduler track communication and progress related to its tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_scheduler_message_response", "line_number": 983, "body": "def get_scheduler_message_response(self, task_id, message_id):\n        response = None\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            response = task.scheduler_message_responses.pop(message_id, None)\n        return {\"response\": response}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a service to retrieve and remove a specific message response associated with a given task from the scheduler's internal state. This enables one-time retrieval of communication outcomes for a task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "has_task_history", "line_number": 991, "body": "def has_task_history(self):\n        return self._config.record_task_history", "is_method": true, "class_name": "Scheduler", "function_description": "Checks if the scheduler is configured to record task execution history. This determines whether historical task data will be stored or retrieved."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_pause_enabled", "line_number": 995, "body": "def is_pause_enabled(self):\n        return {'enabled': self._config.pause_enabled}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the current status of the scheduler's pause functionality, indicating if it's enabled based on its configuration. This method allows other components to check if pausing is allowed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "is_paused", "line_number": 999, "body": "def is_paused(self):\n        return {'paused': self._paused}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the current paused status of the scheduler instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "pause", "line_number": 1003, "body": "def pause(self):\n        if self._config.pause_enabled:\n            self._paused = True", "is_method": true, "class_name": "Scheduler", "function_description": "Allows pausing the scheduler's operations, provided the pause functionality is enabled via its configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "unpause", "line_number": 1008, "body": "def unpause(self):\n        if self._config.pause_enabled:\n            self._paused = False", "is_method": true, "class_name": "Scheduler", "function_description": "Allows the Scheduler to resume its operations. It unpauses the scheduler if the pausing feature is enabled in its configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_resources", "line_number": 1013, "body": "def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a way to dynamically set or update the resources managed by the scheduler. This enables the scheduler to adapt its operations based on available capacity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_resource", "line_number": 1019, "body": "def update_resource(self, resource, amount):\n        if not isinstance(amount, int) or amount < 0:\n            return False\n        self._resources[resource] = amount\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Updates the available quantity for a specified resource managed by the scheduler. It ensures the new amount is a valid non-negative integer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_generate_retry_policy", "line_number": 1025, "body": "def _generate_retry_policy(self, task_retry_policy_dict):\n        retry_policy_dict = self._config._get_retry_policy()._asdict()\n        retry_policy_dict.update({k: v for k, v in task_retry_policy_dict.items() if v is not None})\n        return RetryPolicy(**retry_policy_dict)", "is_method": true, "class_name": "Scheduler", "function_description": "Generates a complete retry policy by combining the scheduler's default settings with task-specific overrides. This enables the scheduler to manage how individual tasks are retried."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_has_resources", "line_number": 1030, "body": "def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Checks if the Scheduler has sufficient resources to accommodate new requirements. It ensures that the sum of needed and currently used resources does not exceed total availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_used_resources", "line_number": 1040, "body": "def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                resources_running = getattr(task, \"resources_running\", task.resources)\n                if resources_running:\n                    for resource, amount in resources_running.items():\n                        used_resources[resource] += amount\n        return used_resources", "is_method": true, "class_name": "Scheduler", "function_description": "This Scheduler method computes and returns the aggregate resources currently being utilized by all active, running tasks. It provides an essential overview of resource consumption."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_rank", "line_number": 1050, "body": "def _rank(self, task):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n\n        return task.priority, -task.time", "is_method": true, "class_name": "Scheduler", "function_description": "This method calculates a task's ranking criteria based on its priority and inverse time, used by the scheduler to determine execution order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_schedulable", "line_number": 1059, "body": "def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Determines if a given task is ready to be scheduled. A task is schedulable if it is pending and all its dependencies have been completed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_reset_orphaned_batch_running_tasks", "line_number": 1068, "body": "def _reset_orphaned_batch_running_tasks(self, worker_id):\n        running_batch_ids = {\n            task.batch_id\n            for task in self._state.get_active_tasks_by_status(RUNNING)\n            if task.worker_running == worker_id\n        }\n        orphaned_tasks = [\n            task for task in self._state.get_active_tasks_by_status(BATCH_RUNNING)\n            if task.worker_running == worker_id and task.batch_id not in running_batch_ids\n        ]\n        for task in orphaned_tasks:\n            self._state.set_status(task, PENDING)", "is_method": true, "class_name": "Scheduler", "function_description": "Resets orphaned batch tasks on a specific worker. These are tasks marked as batch running but lack active individual running tasks, reverting them to pending status for rescheduling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "count_pending", "line_number": 1082, "body": "def count_pending(self, worker):\n        worker_id, worker = worker, self._state.get_worker(worker)\n\n        num_pending, num_unique_pending, num_pending_last_scheduled = 0, 0, 0\n        running_tasks = []\n\n        upstream_status_table = {}\n        for task in worker.get_tasks(self._state, RUNNING):\n            if self._upstream_status(task.id, upstream_status_table) == UPSTREAM_DISABLED:\n                continue\n            # Return a list of currently running tasks to the client,\n            # makes it easier to troubleshoot\n            other_worker = self._state.get_worker(task.worker_running)\n            if other_worker is not None:\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n        for task in worker.get_tasks(self._state, PENDING, FAILED):\n            if self._upstream_status(task.id, upstream_status_table) == UPSTREAM_DISABLED:\n                continue\n            num_pending += 1\n            num_unique_pending += int(len(task.workers) == 1)\n            num_pending_last_scheduled += int(task.workers.peek(last=True) == worker_id)\n\n        return {\n            'n_pending_tasks': num_pending,\n            'n_unique_pending': num_unique_pending,\n            'n_pending_last_scheduled': num_pending_last_scheduled,\n            'worker_state': worker.state,\n            'running_tasks': running_tasks,\n        }", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a comprehensive status report for a given worker, detailing pending and failed tasks, unique assignments, and active running tasks to support scheduling oversight."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_work", "line_number": 1116, "body": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        if self._config.prune_on_get_work:\n            self.prune()\n\n        assert worker is not None\n        worker_id = worker\n        worker = self._update_worker(\n            worker_id,\n            worker_reference={'host': host},\n            get_work=True)\n        if not worker.enabled:\n            reply = {'n_pending_tasks': 0,\n                     'running_tasks': [],\n                     'task_id': None,\n                     'n_unique_pending': 0,\n                     'worker_state': worker.state,\n                     }\n            return reply\n\n        if assistant:\n            self.add_worker(worker_id, [('assistant', assistant)])\n\n        batched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\n        best_task = None\n        if current_tasks is not None:\n            ct_set = set(current_tasks)\n            for task in sorted(self._state.get_active_tasks_by_status(RUNNING), key=self._rank):\n                if task.worker_running == worker_id and task.id not in ct_set:\n                    best_task = task\n\n        if current_tasks is not None:\n            # batch running tasks that weren't claimed since the last get_work go back in the pool\n            self._reset_orphaned_batch_running_tasks(worker_id)\n\n        greedy_resources = collections.defaultdict(int)\n\n        worker = self._state.get_worker(worker_id)\n        if self._paused:\n            relevant_tasks = []\n        elif worker.is_trivial_worker(self._state):\n            relevant_tasks = worker.get_tasks(self._state, PENDING, RUNNING)\n            used_resources = collections.defaultdict(int)\n            greedy_workers = dict()  # If there's no resources, then they can grab any task\n        else:\n            relevant_tasks = self._state.get_active_tasks_by_status(PENDING, RUNNING)\n            used_resources = self._used_resources()\n            activity_limit = time.time() - self._config.worker_disconnect_delay\n            active_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)\n            greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                                  for worker in active_workers)\n        tasks = list(relevant_tasks)\n        tasks.sort(key=self._rank, reverse=True)\n\n        for task in tasks:\n            if (best_task and batched_params and task.family == best_task.family and\n                    len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    task.resources == best_task.resources and self._schedulable(task)):\n                for name, params in batched_params.items():\n                    params.append(task.params.get(name))\n                batched_tasks.append(task)\n            if best_task:\n                continue\n\n            if task.status == RUNNING and (task.worker_running in greedy_workers):\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (getattr(task, 'resources_running', task.resources) or {}).items():\n                    greedy_resources[resource] += amount\n\n            if self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                in_workers = (assistant and task.runnable) or worker_id in task.workers\n                if in_workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    batch_param_names, max_batch_size = self._state.get_batcher(\n                        worker_id, task.family)\n                    if batch_param_names and task.is_batchable():\n                        try:\n                            batched_params = {\n                                name: [task.params[name]] for name in batch_param_names\n                            }\n                            unbatched_params = {\n                                name: value for name, value in task.params.items()\n                                if name not in batched_params\n                            }\n                            batched_tasks.append(task)\n                        except KeyError:\n                            batched_params, unbatched_params = None, None\n                else:\n                    workers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers\n                    for task_worker in workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        reply = self.count_pending(worker_id)\n\n        if len(batched_tasks) > 1:\n            batch_string = '|'.join(task.id for task in batched_tasks)\n            batch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()\n            for task in batched_tasks:\n                self._state.set_batch_running(task, batch_id, worker_id)\n\n            combined_params = best_task.params.copy()\n            combined_params.update(batched_params)\n\n            reply['task_id'] = None\n            reply['task_family'] = best_task.family\n            reply['task_module'] = getattr(best_task, 'module', None)\n            reply['task_params'] = combined_params\n            reply['batch_id'] = batch_id\n            reply['batch_task_ids'] = [task.id for task in batched_tasks]\n\n        elif best_task:\n            self.update_metrics_task_started(best_task)\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker_id\n            best_task.resources_running = best_task.resources.copy()\n            best_task.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n            reply['task_id'] = best_task.id\n            reply['task_family'] = best_task.family\n            reply['task_module'] = getattr(best_task, 'module', None)\n            reply['task_params'] = best_task.params\n\n        else:\n            reply['task_id'] = None\n\n        return reply", "is_method": true, "class_name": "Scheduler", "function_description": "This method selects and assigns the highest priority task or a batch of tasks to a requesting worker. It considers resource availability and worker capabilities, updating task statuses accordingly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "ping", "line_number": 1266, "body": "def ping(self, **kwargs):\n        worker_id = kwargs['worker']\n        worker = self._update_worker(worker_id)\n        return {\"rpc_messages\": worker.fetch_rpc_messages()}", "is_method": true, "class_name": "Scheduler", "function_description": "Enables a worker to register its presence with the scheduler, updating its status. It also retrieves any pending communication messages for that worker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_upstream_status", "line_number": 1271, "body": "def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                dep = self._state.get_task(dep_id)\n                if dep:\n                    if dep.status == DONE:\n                        continue\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack += [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        status = max((upstream_status_table.get(a_task_id, '')\n                                      for a_task_id in dep.deps),\n                                     key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]", "is_method": true, "class_name": "Scheduler", "function_description": "Within the Scheduler, this method calculates a task's overall upstream status by traversing its dependency graph. It aggregates individual dependency statuses and memoizes results, ensuring efficient checks for task readiness."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_serialize_task", "line_number": 1299, "body": "def _serialize_task(self, task_id, include_deps=True, deps=None):\n        task = self._state.get_task(task_id)\n\n        ret = {\n            'display_name': task.pretty_id,\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'last_updated': getattr(task, \"updated\", task.time),\n            'params': task.public_params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n            'resources_running': getattr(task, \"resources_running\", None),\n            'tracking_url': getattr(task, \"tracking_url\", None),\n            'status_message': getattr(task, \"status_message\", None),\n            'progress_percentage': getattr(task, \"progress_percentage\", None),\n        }\n        if task.status == DISABLED:\n            ret['re_enable_able'] = task.scheduler_disable_time is not None\n        if include_deps:\n            ret['deps'] = list(task.deps if deps is None else deps)\n        if self._config.send_messages and task.status == RUNNING:\n            ret['accepts_messages'] = task.accepts_messages\n        return ret", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a standardized dictionary representation of a scheduled task. This includes its status, progress, dependencies, and other key attributes, suitable for display or API responses."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "graph", "line_number": 1328, "body": "def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        seen = set()\n        for task in self._state.get_active_tasks():\n            serialized.update(self._traverse_graph(task.id, seen))\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "This method generates a serialized representation of the scheduler's active task graph. It provides a comprehensive view of current task dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_filter_done", "line_number": 1336, "body": "def _filter_done(self, task_ids):\n        for task_id in task_ids:\n            task = self._state.get_task(task_id)\n            if task is None or task.status != DONE:\n                yield task_id", "is_method": true, "class_name": "Scheduler", "function_description": "Filters a list of task IDs, yielding only those tasks that are not yet complete or are missing from the scheduler's state. This identifies pending or invalid tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_traverse_graph", "line_number": 1342, "body": "def _traverse_graph(self, root_task_id, seen=None, dep_func=None, include_done=True):\n        \"\"\" Returns the dependency graph rooted at task_id\n\n        This does a breadth-first traversal to find the nodes closest to the\n        root before hitting the scheduler.max_graph_nodes limit.\n\n        :param root_task_id: the id of the graph's root\n        :return: A map of task id to serialized node\n        \"\"\"\n\n        if seen is None:\n            seen = set()\n        elif root_task_id in seen:\n            return {}\n\n        if dep_func is None:\n            def dep_func(t):\n                return t.deps\n\n        seen.add(root_task_id)\n        serialized = {}\n        queue = collections.deque([root_task_id])\n        while queue:\n            task_id = queue.popleft()\n\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.debug('Missing task for id [%s]', task_id)\n\n                # NOTE : If a dependency is missing from self._state there is no way to deduce the\n                #        task family and parameters.\n                family_match = TASK_FAMILY_RE.match(task_id)\n                family = family_match.group(1) if family_match else UNKNOWN\n                params = {'task_id': task_id}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'display_name': task_id,\n                    'priority': 0,\n                }\n            else:\n                deps = dep_func(task)\n                if not include_done:\n                    deps = list(self._filter_done(deps))\n                serialized[task_id] = self._serialize_task(task_id, deps=deps)\n                for dep in sorted(deps):\n                    if dep not in seen:\n                        seen.add(dep)\n                        queue.append(dep)\n\n            if task_id != root_task_id:\n                del serialized[task_id]['display_name']\n            if len(serialized) >= self._config.max_graph_nodes:\n                break\n\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "Generates a serialized dependency graph rooted at a given task ID. It maps task IDs to their details and relationships, useful for visualizing or analyzing scheduler workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dep_graph", "line_number": 1404, "body": "def dep_graph(self, task_id, include_done=True, **kwargs):\n        self.prune()\n        if not self._state.has_task(task_id):\n            return {}\n        return self._traverse_graph(task_id, include_done=include_done)", "is_method": true, "class_name": "Scheduler", "function_description": "This method generates the dependency graph for a given task within the scheduler. It reveals all related tasks, including completed ones, to understand their relationships."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "inverse_dep_graph", "line_number": 1411, "body": "def inverse_dep_graph(self, task_id, include_done=True, **kwargs):\n        self.prune()\n        if not self._state.has_task(task_id):\n            return {}\n        inverse_graph = collections.defaultdict(set)\n        for task in self._state.get_active_tasks():\n            for dep in task.deps:\n                inverse_graph[dep].add(task.id)\n        return self._traverse_graph(\n            task_id, dep_func=lambda t: inverse_graph[t.id], include_done=include_done)", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the inverse dependency graph for a task, identifying all tasks that directly or indirectly rely on its completion. This helps determine downstream impacts within the scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_list", "line_number": 1423, "body": "def task_list(self, status='', upstream_status='', limit=True, search=None, max_shown_tasks=None,\n                  **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        if not search:\n            count_limit = max_shown_tasks or self._config.max_shown_tasks\n            pre_count = self._state.get_active_task_count_for_status(status)\n            if limit and pre_count > count_limit:\n                return {'num_tasks': -1 if upstream_status else pre_count}\n        self.prune()\n\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        if search is None:\n            def filter_func(_):\n                return True\n        else:\n            terms = search.split()\n\n            def filter_func(t):\n                return all(term in t.pretty_id for term in terms)\n\n        tasks = self._state.get_active_tasks_by_status(status) if status else self._state.get_active_tasks()\n        for task in filter(filter_func, tasks):\n            if task.status != PENDING or not upstream_status or upstream_status == self._upstream_status(task.id, upstream_status_table):\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.id] = serialized\n        if limit and len(result) > (max_shown_tasks or self._config.max_shown_tasks):\n            return {'num_tasks': len(result)}\n        return result", "is_method": true, "class_name": "Scheduler", "function_description": "Queries and retrieves active tasks from the scheduler based on status, upstream status, or a search term. It can limit the number of returned tasks for display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_first_task_display_name", "line_number": 1455, "body": "def _first_task_display_name(self, worker):\n        task_id = worker.info.get('first_task', '')\n        if self._state.has_task(task_id):\n            return self._state.get_task(task_id).pretty_id\n        else:\n            return task_id", "is_method": true, "class_name": "Scheduler", "function_description": "This method retrieves the display name for the primary task associated with a specific worker. It provides a formatted identifier if the task is known to the scheduler, otherwise returning its raw ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "worker_list", "line_number": 1463, "body": "def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=worker.started,\n                state=worker.state,\n                first_task_display_name=self._first_task_display_name(worker),\n                num_unread_rpc_messages=len(worker.rpc_messages),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                if task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, include_deps=False)\n\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_active_tasks_by_status(PENDING):\n                for worker in task.workers:\n                    num_pending[worker] += 1\n                if len(task.workers) == 1:\n                    num_uniques[list(task.workers)[0]] += 1\n\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a detailed list of active workers managed by the scheduler. It includes each worker's status and can optionally detail tasks currently running or pending on them."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "resource_list", "line_number": 1499, "body": "def resource_list(self):\n        \"\"\"\n        Resources usage info and their consumers (tasks).\n        \"\"\"\n        self.prune()\n        resources = [\n            dict(\n                name=resource,\n                num_total=r_dict['total'],\n                num_used=r_dict['used']\n            ) for resource, r_dict in self.resources().items()]\n        if self._resources is not None:\n            consumers = collections.defaultdict(dict)\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        consumers[resource][task.id] = self._serialize_task(task.id, include_deps=False)\n            for resource in resources:\n                tasks = consumers[resource['name']]\n                resource['num_consumer'] = len(tasks)\n                resource['running'] = tasks\n        return resources", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a detailed list of all tracked resources within the scheduler. It includes total, used amounts, and identifies all running tasks currently consuming each resource."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "resources", "line_number": 1522, "body": "def resources(self):\n        ''' get total resources and available ones '''\n        used_resources = self._used_resources()\n        ret = collections.defaultdict(dict)\n        for resource, total in self._resources.items():\n            ret[resource]['total'] = total\n            if resource in used_resources:\n                ret[resource]['used'] = used_resources[resource]\n            else:\n                ret[resource]['used'] = 0\n        return ret", "is_method": true, "class_name": "Scheduler", "function_description": "Retrieves the scheduler's current resource status, detailing both total capacity and currently utilized amounts for all resource types. This helps monitor resource availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_search", "line_number": 1535, "body": "def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.status][task.id] = serialized\n        return result", "is_method": true, "class_name": "Scheduler", "function_description": "This method allows querying for active tasks by matching a substring within their IDs. It returns the found tasks, organized by their current status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "re_enable_task", "line_number": 1551, "body": "def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized", "is_method": true, "class_name": "Scheduler", "function_description": "This method re-enables a specific task that was previously disabled by the scheduler. It returns the serialized task if successfully reactivated."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "fetch_error", "line_number": 1560, "body": "def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"error\": task.expl, 'displayName': task.pretty_id}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}", "is_method": true, "class_name": "Scheduler", "function_description": "Retrieves error details for a specified task managed by the scheduler. It provides the task ID, its explanation, and a display name, indicating if the task is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_task_status_message", "line_number": 1568, "body": "def set_task_status_message(self, task_id, status_message):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.status_message = status_message\n            if task.status == RUNNING and task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.status_message = status_message", "is_method": true, "class_name": "Scheduler", "function_description": "This method updates the operational status message for a specific scheduled task. For tasks running as part of a batch, it also propagates the message to other running tasks within that batch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task_status_message", "line_number": 1577, "body": "def get_task_status_message(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"statusMessage\": task.status_message}\n        else:\n            return {\"taskId\": task_id, \"statusMessage\": \"\"}", "is_method": true, "class_name": "Scheduler", "function_description": "Provides the current status message for a specific task managed by the scheduler. It returns the task's status if found, or an empty message otherwise."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "set_task_progress_percentage", "line_number": 1585, "body": "def set_task_progress_percentage(self, task_id, progress_percentage):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            task.progress_percentage = progress_percentage\n            if task.status == RUNNING and task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.progress_percentage = progress_percentage", "is_method": true, "class_name": "Scheduler", "function_description": "This method updates the progress percentage for a specific task. If the task is running and part of a batch, it also synchronizes the progress for all other running tasks in that batch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_task_progress_percentage", "line_number": 1594, "body": "def get_task_progress_percentage(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"progressPercentage\": task.progress_percentage}\n        else:\n            return {\"taskId\": task_id, \"progressPercentage\": None}", "is_method": true, "class_name": "Scheduler", "function_description": "This method provides the current progress status for a specified task. It returns the task's completion percentage or indicates if the task is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "decrease_running_task_resources", "line_number": 1602, "body": "def decrease_running_task_resources(self, task_id, decrease_resources):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            if task.status != RUNNING:\n                return\n\n            def decrease(resources, decrease_resources):\n                for resource, decrease_amount in decrease_resources.items():\n                    if decrease_amount > 0 and resource in resources:\n                        resources[resource] = max(0, resources[resource] - decrease_amount)\n\n            decrease(task.resources_running, decrease_resources)\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    decrease(batch_task.resources_running, decrease_resources)", "is_method": true, "class_name": "Scheduler", "function_description": "Adjusts the allocated resources for a specified running task. If the task is part of a batch, it applies the same resource reduction to all other running tasks within that batch."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "get_running_task_resources", "line_number": 1619, "body": "def get_running_task_resources(self, task_id):\n        if self._state.has_task(task_id):\n            task = self._state.get_task(task_id)\n            return {\"taskId\": task_id, \"resources\": getattr(task, \"resources_running\", None)}\n        else:\n            return {\"taskId\": task_id, \"resources\": None}", "is_method": true, "class_name": "Scheduler", "function_description": "This method of the Scheduler class provides the current resources allocated to a specified running task. It indicates if the task is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "_update_task_history", "line_number": 1626, "body": "def _update_task_history(self, task, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task)\n            elif status == RUNNING:\n                self._task_history.task_started(task, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)", "is_method": true, "class_name": "Scheduler", "function_description": "This internal method updates the scheduler's task history based on the task's current status. It records lifecycle events like scheduling, starting, and finishing for a given task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "task_history", "line_number": 1639, "body": "def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history", "is_method": true, "class_name": "Scheduler", "function_description": "Retrieves the complete history of tasks managed by the scheduler. This method is used to expose task execution logs for external monitoring or API access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "update_metrics_task_started", "line_number": 1644, "body": "def update_metrics_task_started(self, task):\n        self._state._metrics_collector.handle_task_started(task)", "is_method": true, "class_name": "Scheduler", "function_description": "Updates the scheduler's internal metrics collector to record that a specific task has started, aiding in performance monitoring and tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "rpc_func", "line_number": 107, "body": "def rpc_func(self, *args, **kwargs):\n            actual_args = defaults.copy()\n            actual_args.update(dict(zip(all_args, args)))\n            actual_args.update(kwargs)\n            if not all(arg in actual_args for arg in required_args):\n                raise TypeError('{} takes {} arguments ({} given)'.format(\n                    fn_name, len(all_args), len(actual_args)))\n            return self._request('/api/{}'.format(fn_name), actual_args, **request_args)", "is_method": false, "function_description": "This function serves as a generic RPC client method that prepares, validates, and dispatches remote procedure calls to a specified API endpoint. It packages arguments and handles request dispatching, abstracting the details of remote communication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "dep_func", "line_number": 1358, "body": "def dep_func(t):\n                return t.deps", "is_method": true, "class_name": "Scheduler", "function_description": "Retrieves the direct dependencies for a given task or entity. This helper function assists the Scheduler in identifying prerequisites for execution order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "filter_func", "line_number": 1438, "body": "def filter_func(_):\n                return True", "is_method": true, "class_name": "Scheduler", "function_description": "Provides a default filter that always returns True, effectively allowing all items or tasks to be processed by the Scheduler without specific criteria."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "filter_func", "line_number": 1443, "body": "def filter_func(t):\n                return all(term in t.pretty_id for term in terms)", "is_method": true, "class_name": "Scheduler", "function_description": "This internal helper function checks if an item's ID contains all specified search terms. It serves as a predicate for filtering items within the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/scheduler.py", "function": "decrease", "line_number": 1608, "body": "def decrease(resources, decrease_resources):\n                for resource, decrease_amount in decrease_resources.items():\n                    if decrease_amount > 0 and resource in resources:\n                        resources[resource] = max(0, resources[resource] - decrease_amount)", "is_method": true, "class_name": "Scheduler", "function_description": "The Scheduler's method to reduce the available quantity of specified resources. It ensures resource levels do not fall below zero."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_section", "line_number": 35, "body": "def _section(cls, opts):\n        \"\"\"Get logging settings from config file section \"logging\".\"\"\"\n        if isinstance(cls.config, LuigiConfigParser):\n            return False\n        try:\n            logging_config = cls.config['logging']\n        except (TypeError, KeyError, NoSectionError):\n            return False\n        logging.config.dictConfig(logging_config)\n        return True", "is_method": true, "class_name": "BaseLogging", "function_description": "Configures the Python logging system. It retrieves logging settings from the 'logging' section of the class's configuration object and applies them."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "setup", "line_number": 47, "body": "def setup(cls,\n              opts=type('opts', (), {\n                  'background': None,\n                  'logdir': None,\n                  'logging_conf_file': None,\n                  'log_level': 'DEBUG'\n              })):\n        \"\"\"Setup logging via CLI params and config.\"\"\"\n        logger = logging.getLogger('luigi')\n\n        if cls._configured:\n            logger.info('logging already configured')\n            return False\n        cls._configured = True\n\n        if cls.config.getboolean('core', 'no_configure_logging', False):\n            logger.info('logging disabled in settings')\n            return False\n\n        configured = cls._cli(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via special settings')\n            return True\n\n        configured = cls._conf(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via *.conf file')\n            return True\n\n        configured = cls._section(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured via config section')\n            return True\n\n        configured = cls._default(opts)\n        if configured:\n            logger = logging.getLogger('luigi')\n            logger.info('logging configured by default settings')\n        return configured", "is_method": true, "class_name": "BaseLogging", "function_description": "Configures the application's logging system. It initializes the logger using a prioritized search through command-line parameters, configuration files, or default settings to ensure consistent logging setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_cli", "line_number": 98, "body": "def _cli(cls, opts):\n        \"\"\"Setup logging via CLI options\n\n        If `--background` -- set INFO level for root logger.\n        If `--logdir` -- set logging with next params:\n            default Luigi's formatter,\n            INFO level,\n            output in logdir in `luigi-server.log` file\n        \"\"\"\n        if opts.background:\n            logging.getLogger().setLevel(logging.INFO)\n            return True\n\n        if opts.logdir:\n            logging.basicConfig(\n                level=logging.INFO,\n                format=cls._log_format,\n                filename=os.path.join(opts.logdir, \"luigi-server.log\"))\n            return True\n\n        return False", "is_method": true, "class_name": "DaemonLogging", "function_description": "Configures logging for a daemon process based on command-line options. It sets the log level for background execution or directs logs to a specified file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_conf", "line_number": 121, "body": "def _conf(cls, opts):\n        \"\"\"Setup logging via ini-file from logging_conf_file option.\"\"\"\n        logging_conf = cls.config.get('core', 'logging_conf_file', None)\n        if logging_conf is None:\n            return False\n\n        if not os.path.exists(logging_conf):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(logging_conf)\n        return True", "is_method": true, "class_name": "DaemonLogging", "function_description": "Configures the daemon's logging system by loading settings from a specified INI file, providing external control over log behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_default", "line_number": 136, "body": "def _default(cls, opts):\n        \"\"\"Setup default logger\"\"\"\n        logging.basicConfig(level=logging.INFO, format=cls._log_format)\n        return True", "is_method": true, "class_name": "DaemonLogging", "function_description": "Sets up the default logger for the daemon, configuring its level to INFO and applying a consistent format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_conf", "line_number": 152, "body": "def _conf(cls, opts):\n        \"\"\"Setup logging via ini-file from logging_conf_file option.\"\"\"\n        if not opts.logging_conf_file:\n            return False\n\n        if not os.path.exists(opts.logging_conf_file):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(opts.logging_conf_file, disable_existing_loggers=False)\n        return True", "is_method": true, "class_name": "InterfaceLogging", "function_description": "Configures the application's logging system. It initializes or updates logging settings by reading from a specified INI configuration file, ensuring proper log management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/setup_logging.py", "function": "_default", "line_number": 166, "body": "def _default(cls, opts):\n        \"\"\"Setup default logger\"\"\"\n        level = getattr(logging, opts.log_level, logging.DEBUG)\n\n        logger = logging.getLogger('luigi-interface')\n        logger.setLevel(level)\n\n        stream_handler = logging.StreamHandler()\n        stream_handler.setLevel(level)\n\n        formatter = logging.Formatter('%(levelname)s: %(message)s')\n        stream_handler.setFormatter(formatter)\n\n        logger.addHandler(stream_handler)\n        return True", "is_method": true, "class_name": "InterfaceLogging", "function_description": "Configures and sets up a default console logger for the 'luigi-interface'. It provides a standard logging mechanism ready for use by other parts of the application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "common_params", "line_number": 230, "body": "def common_params(task_instance, task_cls):\n    \"\"\"\n    Grab all the values in task_instance that are found in task_cls.\n    \"\"\"\n    if not isinstance(task_cls, task.Register):\n        raise TypeError(\"task_cls must be an uninstantiated Task\")\n\n    task_instance_param_names = dict(task_instance.get_params()).keys()\n    task_cls_params_dict = dict(task_cls.get_params())\n    task_cls_param_names = task_cls_params_dict.keys()\n    common_param_names = set(task_instance_param_names).intersection(set(task_cls_param_names))\n    common_param_vals = [(key, task_cls_params_dict[key]) for key in common_param_names]\n    common_kwargs = dict((key, task_instance.param_kwargs[key]) for key in common_param_names)\n    vals = dict(task_instance.get_param_values(common_param_vals, [], common_kwargs))\n    return vals", "is_method": false, "function_description": "Extracts parameter values common to both a task instance and its class definition. It provides a dictionary of shared configuration, useful for aligning or validating task parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "delegates", "line_number": 376, "body": "def delegates(task_that_delegates):\n    \"\"\" Lets a task call methods on subtask(s).\n\n    The way this works is that the subtask is run as a part of the task, but\n    the task itself doesn't have to care about the requirements of the subtasks.\n    The subtask doesn't exist from the scheduler's point of view, and\n    its dependencies are instead required by the main task.\n\n    Example:\n\n    .. code-block:: python\n\n        class PowersOfN(luigi.Task):\n            n = luigi.IntParameter()\n            def f(self, x): return x ** self.n\n\n        @delegates\n        class T(luigi.Task):\n            def subtasks(self): return PowersOfN(5)\n            def run(self): print self.subtasks().f(42)\n    \"\"\"\n    if not hasattr(task_that_delegates, 'subtasks'):\n        # This method can (optionally) define a couple of delegate tasks that\n        # will be accessible as interfaces, meaning that the task can access\n        # those tasks and run methods defined on them, etc\n        raise AttributeError('%s needs to implement the method \"subtasks\"' % task_that_delegates)\n\n    @task._task_wraps(task_that_delegates)\n    class Wrapped(task_that_delegates):\n\n        def deps(self):\n            # Overrides method in base class\n            return task.flatten(self.requires()) + task.flatten([t.deps() for t in task.flatten(self.subtasks())])\n\n        def run(self):\n            for t in task.flatten(self.subtasks()):\n                t.run()\n            task_that_delegates.run(self)\n\n    return Wrapped", "is_method": false, "function_description": "Provides a decorator to empower a task to delegate execution and dependency management to internal 'subtasks'. It consolidates subtask requirements and runs them as part of the main task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "previous", "line_number": 418, "body": "def previous(task):\n    \"\"\"\n    Return a previous Task of the same family.\n\n    By default checks if this task family only has one non-global parameter and if\n    it is a DateParameter, DateHourParameter or DateIntervalParameter in which case\n    it returns with the time decremented by 1 (hour, day or interval)\n    \"\"\"\n    params = task.get_params()\n    previous_params = {}\n    previous_date_params = {}\n\n    for param_name, param_obj in params:\n        param_value = getattr(task, param_name)\n\n        if isinstance(param_obj, parameter.DateParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(days=1)\n        elif isinstance(param_obj, parameter.DateSecondParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(seconds=1)\n        elif isinstance(param_obj, parameter.DateMinuteParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(minutes=1)\n        elif isinstance(param_obj, parameter.DateHourParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(hours=1)\n        elif isinstance(param_obj, parameter.DateIntervalParameter):\n            previous_date_params[param_name] = param_value.prev()\n        else:\n            previous_params[param_name] = param_value\n\n    previous_params.update(previous_date_params)\n\n    if len(previous_date_params) == 0:\n        raise NotImplementedError(\"No task parameter - can't determine previous task\")\n    elif len(previous_date_params) > 1:\n        raise NotImplementedError(\"Too many date-related task parameters - can't determine previous task\")\n    else:\n        return task.clone(**previous_params)", "is_method": false, "function_description": "Generates a new task instance by decrementing the single date/time parameter of the original task. This enables sequential processing of tasks for previous time periods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "get_previous_completed", "line_number": 456, "body": "def get_previous_completed(task, max_steps=10):\n    prev = task\n    for _ in range(max_steps):\n        prev = previous(prev)\n        logger.debug(\"Checking if %s is complete\", prev)\n        if prev.complete():\n            return prev\n    return None", "is_method": false, "function_description": "This function traverses backward from a given task to find the most recent previous task that has been marked as complete. It helps identify completed predecessors in a sequence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 289, "body": "def __call__(self, task_that_inherits):\n        # Get all parameter objects from each of the underlying tasks\n        for task_to_inherit in self.tasks_to_inherit:\n            for param_name, param_obj in task_to_inherit.get_params():\n                # Check if the parameter exists in the inheriting task\n                if not hasattr(task_that_inherits, param_name):\n                    # If not, add it to the inheriting task\n                    setattr(task_that_inherits, param_name, param_obj)\n\n        # Modify task_that_inherits by adding methods\n        def clone_parent(_self, **kwargs):\n            return _self.clone(cls=self.tasks_to_inherit[0], **kwargs)\n        task_that_inherits.clone_parent = clone_parent\n\n        def clone_parents(_self, **kwargs):\n            return [\n                _self.clone(cls=task_to_inherit, **kwargs)\n                for task_to_inherit in self.tasks_to_inherit\n            ]\n        task_that_inherits.clone_parents = clone_parents\n\n        return task_that_inherits", "is_method": true, "class_name": "inherits", "function_description": "This method configures a target task by injecting parameters and adding cloning capabilities from its designated parent tasks. It enables a task to inherit attributes and methods dynamically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 328, "body": "def __call__(self, task_that_requires):\n        task_that_requires = inherits(*self.tasks_to_require)(task_that_requires)\n\n        # Modify task_that_requires by adding requires method.\n        # If only one task is required, this single task is returned.\n        # Otherwise, list of tasks is returned\n        def requires(_self):\n            return _self.clone_parent() if len(self.tasks_to_require) == 1 else _self.clone_parents()\n        task_that_requires.requires = requires\n\n        return task_that_requires", "is_method": true, "class_name": "requires", "function_description": "Callable that transforms a task by making it inherit specified prerequisite tasks. It also injects a `requires` method into the task, allowing it to declare its dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "__call__", "line_number": 359, "body": "def __call__(self, task_that_copies):\n        task_that_copies = self.requires_decorator(task_that_copies)\n\n        # Modify task_that_copies by subclassing it and adding methods\n        @task._task_wraps(task_that_copies)\n        class Wrapped(task_that_copies):\n\n            def run(_self):\n                i, o = _self.input(), _self.output()\n                f = o.open('w')  # TODO: assert that i, o are Target objects and not complex datastructures\n                for line in i.open('r'):\n                    f.write(line)\n                f.close()\n\n        return Wrapped", "is_method": true, "class_name": "copies", "function_description": "This function generates a specialized task class that, upon execution, copies data from its input source to its output destination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "clone_parent", "line_number": 299, "body": "def clone_parent(_self, **kwargs):\n            return _self.clone(cls=self.tasks_to_inherit[0], **kwargs)", "is_method": true, "class_name": "inherits", "function_description": "Clones the current object into its first designated parent task class. This enables creating a new instance that inherits characteristics of a specified base task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "clone_parents", "line_number": 303, "body": "def clone_parents(_self, **kwargs):\n            return [\n                _self.clone(cls=task_to_inherit, **kwargs)\n                for task_to_inherit in self.tasks_to_inherit\n            ]", "is_method": true, "class_name": "inherits", "function_description": "Clones each task from `self.tasks_to_inherit`, creating new instances with specified keyword arguments. It provides a mechanism to instantiate inherited tasks dynamically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "requires", "line_number": 334, "body": "def requires(_self):\n            return _self.clone_parent() if len(self.tasks_to_require) == 1 else _self.clone_parents()", "is_method": true, "class_name": "requires", "function_description": "This method defines and returns the parent dependencies required by the current object, providing either a single parent instance or multiple parent instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "deps", "line_number": 406, "body": "def deps(self):\n            # Overrides method in base class\n            return task.flatten(self.requires()) + task.flatten([t.deps() for t in task.flatten(self.subtasks())])", "is_method": true, "class_name": "Wrapped", "function_description": "Computes the full set of direct and recursive dependencies for the current task and all its sub-tasks. This provides a comprehensive list of prerequisites."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "run", "line_number": 410, "body": "def run(self):\n            for t in task.flatten(self.subtasks()):\n                t.run()\n            task_that_delegates.run(self)", "is_method": true, "class_name": "Wrapped", "function_description": "This method orchestrates the sequential execution of all defined subtasks before executing its own delegated task functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/util.py", "function": "run", "line_number": 366, "body": "def run(_self):\n                i, o = _self.input(), _self.output()\n                f = o.open('w')  # TODO: assert that i, o are Target objects and not complex datastructures\n                for line in i.open('r'):\n                    f.write(line)\n                f.close()", "is_method": true, "class_name": "Wrapped", "function_description": "This method copies all content, line by line, from the object's designated input to its output. It provides a simple data transfer or replication service between file-like targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "exists", "line_number": 98, "body": "def exists(self, path):\n        \"\"\"\n        Return ``True`` if file or directory at ``path`` exist, ``False`` otherwise\n\n        :param str path: a path within the FileSystem to check for existence.\n        \"\"\"\n        pass", "is_method": true, "class_name": "FileSystem", "function_description": "Checks if a file or directory exists at the specified path within the file system. Returns True if present, False otherwise."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "rename_dont_move", "line_number": 160, "body": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Potentially rename ``path`` to ``dest``, but don't move it into the\n        ``dest`` folder (if it is a folder).  This relates to :ref:`AtomicWrites`.\n\n        This method has a reasonable but not bullet proof default\n        implementation.  It will just do ``move()`` if the file doesn't\n        ``exists()`` already.\n        \"\"\"\n        warnings.warn(\"File system {} client doesn't support atomic mv.\".format(self.__class__.__name__))\n        if self.exists(dest):\n            raise FileAlreadyExists()\n        self.move(path, dest)", "is_method": true, "class_name": "FileSystem", "function_description": "Renames a file or directory to a new path, preventing it from being moved *into* an existing destination folder. It ensures the new path does not already exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "rename", "line_number": 174, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Alias for ``move()``\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "FileSystem", "function_description": "Renames a file or directory by serving as an alias for the `move()` operation within the file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "exists", "line_number": 241, "body": "def exists(self):\n        \"\"\"\n        Returns ``True`` if the path for this FileSystemTarget exists; ``False`` otherwise.\n\n        This method is implemented by using :py:attr:`fs`.\n        \"\"\"\n        path = self.path\n        if '*' in path or '?' in path or '[' in path or '{' in path:\n            logger.warning(\"Using wildcards in path %s might lead to processing of an incomplete dataset; \"\n                           \"override exists() to suppress the warning.\", path)\n        return self.fs.exists(path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Checks if the `FileSystemTarget`'s associated path exists on the file system. This capability allows other functions to verify resource availability before interaction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "remove", "line_number": 253, "body": "def remove(self):\n        \"\"\"\n        Remove the resource at the path specified by this FileSystemTarget.\n\n        This method is implemented by using :py:attr:`fs`.\n        \"\"\"\n        self.fs.remove(self.path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Removes the file system resource (file or directory) represented by this target object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "temporary_path", "line_number": 262, "body": "def temporary_path(self):\n        \"\"\"\n        A context manager that enables a reasonably short, general and\n        magic-less way to solve the :ref:`AtomicWrites`.\n\n         * On *entering*, it will create the parent directories so the\n           temporary_path is writeable right away.\n           This step uses :py:meth:`FileSystem.mkdir`.\n         * On *exiting*, it will move the temporary file if there was no exception thrown.\n           This step uses :py:meth:`FileSystem.rename_dont_move`\n\n        The file system operations will be carried out by calling them on :py:attr:`fs`.\n\n        The typical use case looks like this:\n\n        .. code:: python\n\n            class MyTask(luigi.Task):\n                def output(self):\n                    return MyFileSystemTarget(...)\n\n                def run(self):\n                    with self.output().temporary_path() as self.temp_output_path:\n                        run_some_external_command(output_path=self.temp_output_path)\n        \"\"\"\n        num = random.randrange(0, 1e10)\n        slashless_path = self.path.rstrip('/').rstrip(\"\\\\\")\n        _temp_path = '{}-luigi-tmp-{:010}{}'.format(\n            slashless_path,\n            num,\n            self._trailing_slash())\n        # TODO: os.path doesn't make sense here as it's os-dependent\n        tmp_dir = os.path.dirname(slashless_path)\n        if tmp_dir:\n            self.fs.mkdir(tmp_dir, parents=True, raise_if_exists=False)\n\n        yield _temp_path\n        # We won't reach here if there was an user exception.\n        self.fs.rename_dont_move(_temp_path, self.path)", "is_method": true, "class_name": "FileSystemTarget", "function_description": "This context manager facilitates atomic file writes by providing a temporary path. It creates necessary directories and safely renames the completed temporary file to its final destination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "_touchz", "line_number": 302, "body": "def _touchz(self):\n        with self.open('w'):\n            pass", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Creates an empty file for the target if it doesn't exist, or truncates an existing one. This signals the target's presence or completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "_trailing_slash", "line_number": 306, "body": "def _trailing_slash(self):\n        # I suppose one day schema-like paths, like\n        # file:///path/blah.txt?params=etc can be parsed too\n        return self.path[-1] if self.path[-1] in r'\\/' else ''", "is_method": true, "class_name": "FileSystemTarget", "function_description": "Checks if the file system path ends with a directory separator. It returns the trailing slash or an empty string, aiding in path normalization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "close", "line_number": 326, "body": "def close(self):\n        super(AtomicLocalFile, self).close()\n        self.move_to_final_destination()", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Closes the file handle and finalizes the atomic file operation by moving the temporary file to its permanent destination, ensuring data integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "generate_tmp_path", "line_number": 330, "body": "def generate_tmp_path(self, path):\n        return os.path.join(tempfile.gettempdir(), 'luigi-s3-tmp-%09d' % random.randrange(0, 1e10))", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Generates a unique temporary file path within the system's temporary directory. This is useful for creating interim files for atomic operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "__del__", "line_number": 336, "body": "def __del__(self):\n        if os.path.exists(self.tmp_path):\n            os.remove(self.tmp_path)", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Ensures the temporary file associated with this `AtomicLocalFile` instance is automatically deleted when the object is destroyed. This prevents orphaned temporary files from accumulating."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "tmp_path", "line_number": 341, "body": "def tmp_path(self):\n        return self.__tmp_path", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "Provides the path to the temporary file associated with the atomic file operation managed by this instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/target.py", "function": "__exit__", "line_number": 344, "body": "def __exit__(self, exc_type, exc, traceback):\n        \" Close/commit the file if there are no exception \"\n        if exc_type:\n            return\n        return super(AtomicLocalFile, self).__exit__(exc_type, exc, traceback)", "is_method": true, "class_name": "AtomicLocalFile", "function_description": "This method finalizes the atomic file operation. It commits the changes only if no exceptions occurred within the `with` block, ensuring data integrity and atomicity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_fail_queue", "line_number": 45, "body": "def _fail_queue(num_messages):\n    return lambda: collections.defaultdict(lambda: ExplQueue(num_messages))", "is_method": false, "function_description": "Provides a factory for creating a dictionary of `ExplQueue` instances. This dictionary dynamically generates new `ExplQueue` objects (configured with `num_messages`) for distinct categories, typically for managing failed items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_plural_format", "line_number": 49, "body": "def _plural_format(template, number, plural='s'):\n    if number == 0:\n        return ''\n    return template.format(number, '' if number == 1 else plural)", "is_method": false, "function_description": "Formats a template string to correctly display singular or plural nouns based on a given number. It ensures grammatical accuracy for counts, returning an empty string if the number is zero."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "enqueue", "line_number": 38, "body": "def enqueue(self, item):\n        self.pop(item, None)\n        self[item] = datetime.now()\n        if len(self) > self.num_items:\n            self.popitem(last=False)", "is_method": true, "class_name": "ExplQueue", "function_description": "This function adds an item to a fixed-size queue, updating its recency if already present. It automatically removes the least recently used item when the queue exceeds its maximum capacity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_update_next_send", "line_number": 70, "body": "def _update_next_send(self):\n        self._next_send = time.time() + 60 * self._config.email_interval", "is_method": true, "class_name": "BatchNotifier", "function_description": "Establishes the precise time for the BatchNotifier to dispatch its next batch of notifications, based on a configurable interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_key", "line_number": 73, "body": "def _key(self, task_name, family, unbatched_args):\n        if self._config.batch_mode == 'all':\n            return task_name\n        elif self._config.batch_mode == 'family':\n            return family\n        elif self._config.batch_mode == 'unbatched_params':\n            param_str = ', '.join('{}={}'.format(k, v) for k, v in unbatched_args.items())\n            return '{}({})'.format(family, param_str)\n        else:\n            raise ValueError('Unknown batch mode for batch notifier: {}'.format(\n                self._config.batch_mode))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Generates a unique grouping key for tasks or notifications based on the configured batching mode (task name, family, or parameters), enabling the `BatchNotifier` to manage and group items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_expl", "line_number": 85, "body": "def _format_expl(self, expl):\n        lines = expl.rstrip().split('\\n')[-self._config.error_lines:]\n        if self._email_format == 'html':\n            return '<pre>{}</pre>'.format('\\n'.join(lines))\n        else:\n            return '\\n{}'.format('\\n'.join(map('      {}'.format, lines)))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Formats a multi-line explanation, truncating it to a configured number of end lines. It applies either HTML pre-tag formatting or plain text indentation for consistent display in notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expl_body", "line_number": 92, "body": "def _expl_body(self, expls):\n        lines = [self._format_expl(expl) for expl in expls]\n        if lines and self._email_format != 'html':\n            lines.append('')\n        return '\\n'.join(lines)", "is_method": true, "class_name": "BatchNotifier", "function_description": "This method of `BatchNotifier` formats a list of explanations into a plain-text body string. It ensures proper line breaks and includes an extra line for non-HTML notification formats."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_task", "line_number": 98, "body": "def _format_task(self, task_tuple):\n        task, failure_count, disable_count, scheduling_count = task_tuple\n        counts = [\n            _plural_format('{} failure{}', failure_count),\n            _plural_format('{} disable{}', disable_count),\n            _plural_format('{} scheduling failure{}', scheduling_count),\n        ]\n        count_str = ', '.join(filter(None, counts))\n        return '{} ({})'.format(task, count_str)", "is_method": true, "class_name": "BatchNotifier", "function_description": "This private helper method of `BatchNotifier` formats a task's details and associated error counts into a concise, human-readable string. It generates a summary suitable for system notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_format_tasks", "line_number": 108, "body": "def _format_tasks(self, tasks):\n        lines = map(self._format_task, sorted(tasks, key=self._expl_key))\n        if self._email_format == 'html':\n            return '<li>{}'.format('\\n<br>'.join(lines))\n        else:\n            return '- {}'.format('\\n  '.join(lines))", "is_method": true, "class_name": "BatchNotifier", "function_description": "This method formats a collection of tasks into a single string. It can generate either HTML list items or plain text bullet points for inclusion in a batch notification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_owners", "line_number": 115, "body": "def _owners(self, owners):\n        return self._default_owner | set(owners)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Computes the complete set of recipients for a notification by combining default owners with additional specified owners."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_failure", "line_number": 118, "body": "def add_failure(self, task_name, family, unbatched_args, expl, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._fail_counts[owner][key] += 1\n            self._fail_expls[owner][key].enqueue(expl)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Records a specific task failure, incrementing its failure count and queuing the explanation. This method helps track and attribute batch processing issues to relevant owners."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_disable", "line_number": 124, "body": "def add_disable(self, task_name, family, unbatched_args, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._disabled_counts[owner][key] += 1\n            self._fail_counts[owner].setdefault(key, 0)", "is_method": true, "class_name": "BatchNotifier", "function_description": "For the BatchNotifier class, this method registers a specific task as disabled. It increments a disabled counter for each task owner and initializes a failure count entry."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "add_scheduling_fail", "line_number": 130, "body": "def add_scheduling_fail(self, task_name, family, unbatched_args, expl, owners):\n        key = self._key(task_name, family, unbatched_args)\n        for owner in self._owners(owners):\n            self._scheduling_fail_counts[owner][key] += 1\n            self._fail_expls[owner][key].enqueue(expl)\n            self._fail_counts[owner].setdefault(key, 0)", "is_method": true, "class_name": "BatchNotifier", "function_description": "Logs details of a failed batch task scheduling attempt. It tracks failure counts and explanations per owner for subsequent notification."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_task_expl_groups", "line_number": 137, "body": "def _task_expl_groups(self, expls):\n        if not self._config.group_by_error_messages:\n            return [((task,), msg) for task, msg in expls.items()]\n\n        groups = collections.defaultdict(list)\n        for task, msg in expls.items():\n            groups[msg].append(task)\n        return [(tasks, msg) for msg, tasks in groups.items()]", "is_method": true, "class_name": "BatchNotifier", "function_description": "This helper method prepares a list of tasks and their associated messages. It groups multiple tasks with identical messages or keeps them separate based on configuration, facilitating structured notifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expls_key", "line_number": 146, "body": "def _expls_key(self, expls_tuple):\n        expls = expls_tuple[0]\n        num_failures = sum(failures + scheduling_fails for (_1, failures, _2, scheduling_fails) in expls)\n        num_disables = sum(disables for (_1, _2, disables, _3) in expls)\n        min_name = min(expls)[0]\n        return -num_failures, -num_disables, min_name", "is_method": true, "class_name": "BatchNotifier", "function_description": "This method generates a sort key for a collection of batch processing records. It prioritizes items with more failures and disables, then by the lexicographically smallest name for sorting purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_expl_key", "line_number": 153, "body": "def _expl_key(self, expl):\n        return self._expls_key(((expl,), None))", "is_method": true, "class_name": "BatchNotifier", "function_description": "It generates a unique key for a single input item (`expl`) by formatting it for an internal key generation utility. This method standardizes key creation for individual items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_email_body", "line_number": 156, "body": "def _email_body(self, fail_counts, disable_counts, scheduling_counts, fail_expls):\n        expls = {\n            (name, fail_count, disable_counts[name], scheduling_counts[name]): self._expl_body(fail_expls[name])\n            for name, fail_count in fail_counts.items()\n        }\n        expl_groups = sorted(self._task_expl_groups(expls), key=self._expls_key)\n        body_lines = []\n        for tasks, msg in expl_groups:\n            body_lines.append(self._format_tasks(tasks))\n            body_lines.append(msg)\n        body = '\\n'.join(filter(None, body_lines)).rstrip()\n        if self._email_format == 'html':\n            return '<ul>\\n{}\\n</ul>'.format(body)\n        else:\n            return body", "is_method": true, "class_name": "BatchNotifier", "function_description": "Generates the structured body content for a batch notification email. It compiles task statuses, counts, and explanations into a formatted message, supporting both plain text and HTML."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "_send_email", "line_number": 172, "body": "def _send_email(self, fail_counts, disable_counts, scheduling_counts, fail_expls, owner):\n        num_failures = sum(fail_counts.values())\n        num_disables = sum(disable_counts.values())\n        num_scheduling_failures = sum(scheduling_counts.values())\n        subject_parts = [\n            _plural_format('{} failure{}', num_failures),\n            _plural_format('{} disable{}', num_disables),\n            _plural_format('{} scheduling failure{}', num_scheduling_failures),\n        ]\n        subject_base = ', '.join(filter(None, subject_parts))\n        if subject_base:\n            prefix = '' if owner in self._default_owner else 'Your tasks have '\n            subject = 'Luigi: {}{} in the last {} minutes'.format(\n                prefix, subject_base, self._config.email_interval)\n            email_body = self._email_body(fail_counts, disable_counts, scheduling_counts, fail_expls)\n            send_email(subject, email_body, email().sender, (owner,))", "is_method": true, "class_name": "BatchNotifier", "function_description": "Sends an email notification to a task owner summarizing batch job failures, disables, and scheduling issues. It dynamically formats the subject and body."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "send_email", "line_number": 189, "body": "def send_email(self):\n        try:\n            for owner, failures in self._fail_counts.items():\n                self._send_email(\n                    fail_counts=failures,\n                    disable_counts=self._disabled_counts[owner],\n                    scheduling_counts=self._scheduling_fail_counts[owner],\n                    fail_expls=self._fail_expls[owner],\n                    owner=owner,\n                )\n        finally:\n            self._update_next_send()\n            self._fail_counts.clear()\n            self._disabled_counts.clear()\n            self._scheduling_fail_counts.clear()\n            self._fail_expls.clear()", "is_method": true, "class_name": "BatchNotifier", "function_description": "Sends summarized email notifications to owners detailing various system failures and operational statuses, then prepares the internal state for the next reporting cycle.\nSends summarized email notifications to owners detailing various system failures and operational statuses, then prepares the internal state for the next reporting cycle."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/batch_notifier.py", "function": "update", "line_number": 206, "body": "def update(self):\n        if time.time() >= self._next_send:\n            self.send_email()", "is_method": true, "class_name": "BatchNotifier", "function_description": "This method of BatchNotifier checks if the next scheduled time for a notification has been reached. If so, it triggers the sending of a batch email."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "_schedule_and_run", "line_number": 129, "body": "def _schedule_and_run(tasks, worker_scheduler_factory=None, override_defaults=None):\n    \"\"\"\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param override_defaults:\n    :return: True if all tasks and their dependencies were successfully run (or already completed);\n             False if any error occurred. It will return a detailed response of type LuigiRunResult\n             instead of a boolean if detailed_summary=True.\n    \"\"\"\n\n    if worker_scheduler_factory is None:\n        worker_scheduler_factory = _WorkerSchedulerFactory()\n    if override_defaults is None:\n        override_defaults = {}\n    env_params = core(**override_defaults)\n\n    InterfaceLogging.setup(env_params)\n\n    kill_signal = signal.SIGUSR1 if env_params.take_lock else None\n    if (not env_params.no_lock and\n            not(lock.acquire_for(env_params.lock_pid_dir, env_params.lock_size, kill_signal))):\n        raise PidLockAlreadyTakenExit()\n\n    if env_params.local_scheduler:\n        sch = worker_scheduler_factory.create_local_scheduler()\n    else:\n        if env_params.scheduler_url != '':\n            url = env_params.scheduler_url\n        else:\n            url = 'http://{host}:{port:d}/'.format(\n                host=env_params.scheduler_host,\n                port=env_params.scheduler_port,\n            )\n        sch = worker_scheduler_factory.create_remote_scheduler(url=url)\n\n    worker = worker_scheduler_factory.create_worker(\n        scheduler=sch, worker_processes=env_params.workers, assistant=env_params.assistant)\n\n    success = True\n    logger = logging.getLogger('luigi-interface')\n    with worker:\n        for t in tasks:\n            success &= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)\n        logger.info('Done scheduling tasks')\n        success &= worker.run()\n    luigi_run_result = LuigiRunResult(worker, success)\n    logger.info(luigi_run_result.summary_text)\n    return luigi_run_result", "is_method": false, "function_description": "Orchestrates and executes a set of Luigi tasks by setting up the necessary scheduler and worker environment. It manages the entire workflow run, returning its detailed outcome."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "run", "line_number": 186, "body": "def run(*args, **kwargs):\n    \"\"\"\n    Please dont use. Instead use `luigi` binary.\n\n    Run from cmdline using argparse.\n\n    :param use_dynamic_argparse: Deprecated and ignored\n    \"\"\"\n    luigi_run_result = _run(*args, **kwargs)\n    return luigi_run_result if kwargs.get('detailed_summary') else luigi_run_result.scheduling_succeeded", "is_method": false, "function_description": "This function serves as a deprecated command-line entry point for running Luigi tasks. Users are advised to use the `luigi` binary instead for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "_run", "line_number": 198, "body": "def _run(cmdline_args=None, main_task_cls=None,\n         worker_scheduler_factory=None, use_dynamic_argparse=None, local_scheduler=False, detailed_summary=False):\n    if use_dynamic_argparse is not None:\n        warnings.warn(\"use_dynamic_argparse is deprecated, don't set it.\",\n                      DeprecationWarning, stacklevel=2)\n    if cmdline_args is None:\n        cmdline_args = sys.argv[1:]\n\n    if main_task_cls:\n        cmdline_args.insert(0, main_task_cls.task_family)\n    if local_scheduler:\n        cmdline_args.append('--local-scheduler')\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)", "is_method": false, "function_description": "Parses command-line arguments to prepare and execute a main task. It orchestrates the task's scheduling and running, serving as an entry point."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "build", "line_number": 214, "body": "def build(tasks, worker_scheduler_factory=None, detailed_summary=False, **env_params):\n    \"\"\"\n    Run internally, bypassing the cmdline parsing.\n\n    Useful if you have some luigi code that you want to run internally.\n    Example:\n\n    .. code-block:: python\n\n        luigi.build([MyTask1(), MyTask2()], local_scheduler=True)\n\n    One notable difference is that `build` defaults to not using\n    the identical process lock. Otherwise, `build` would only be\n    callable once from each process.\n\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param env_params:\n    :return: True if there were no scheduling errors, even if tasks may fail.\n    \"\"\"\n    if \"no_lock\" not in env_params:\n        env_params[\"no_lock\"] = True\n\n    luigi_run_result = _schedule_and_run(tasks, worker_scheduler_factory, override_defaults=env_params)\n    return luigi_run_result if detailed_summary else luigi_run_result.scheduling_succeeded", "is_method": false, "function_description": "Programmatically runs a list of Luigi tasks, bypassing command-line parsing for internal execution. It allows embedding task workflows within Python code, returning a summary of the scheduling outcome."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_local_scheduler", "line_number": 118, "body": "def create_local_scheduler(self):\n        return scheduler.Scheduler(prune_on_get_work=True, record_task_history=False)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Provides a pre-configured local task scheduler instance. This scheduler is optimized for efficient work retrieval and does not record task history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_remote_scheduler", "line_number": 121, "body": "def create_remote_scheduler(self, url):\n        return rpc.RemoteScheduler(url)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Chain of Thought:\nThe function `create_remote_scheduler` is a method within the `_WorkerSchedulerFactory` class. Its sole action is to instantiate and return an `rpc.RemoteScheduler` object, using a provided URL. This indicates it serves as a factory for creating objects that can interact with a scheduler located remotely. It provides the capability to establish a connection or interface to a distributed scheduling component.The function creates an instance of a remote scheduler from a given URL. It enables the system to connect and interact with a distributed task scheduler.\nThe function creates an instance of a remote scheduler from a given URL. It enables the system to connect and interact with a distributed task scheduler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/interface.py", "function": "create_worker", "line_number": 124, "body": "def create_worker(self, scheduler, worker_processes, assistant=False):\n        return worker.Worker(\n            scheduler=scheduler, worker_processes=worker_processes, assistant=assistant)", "is_method": true, "class_name": "_WorkerSchedulerFactory", "function_description": "Creates and configures a new worker instance for a given scheduler, optionally as an assistant. This factory method provides a standardized way to build worker objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "load_task", "line_number": 245, "body": "def load_task(module, task_name, params_str):\n    \"\"\"\n    Imports task dynamically given a module and a task name.\n    \"\"\"\n    if module is not None:\n        __import__(module)\n    task_cls = Register.get_task_cls(task_name)\n    return task_cls.from_str_params(params_str)", "is_method": false, "function_description": "Provides a service to dynamically load and instantiate a registered task class. It enables runtime selection and configuration of tasks from their string parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "__new__", "line_number": 57, "body": "def __new__(metacls, classname, bases, classdict):\n        \"\"\"\n        Custom class creation for namespacing.\n\n        Also register all subclasses.\n\n        When the set or inherited namespace evaluates to ``None``, set the task namespace to\n        whatever the currently declared namespace is.\n        \"\"\"\n        cls = super(Register, metacls).__new__(metacls, classname, bases, classdict)\n        cls._namespace_at_class_time = metacls._get_namespace(cls.__module__)\n        metacls._reg.append(cls)\n        return cls", "is_method": true, "class_name": "Register", "function_description": "This `__new__` method of the `Register` metaclass creates and automatically registers new classes. It provides a central mechanism to track and namespace all classes using this metaclass."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "__call__", "line_number": 71, "body": "def __call__(cls, *args, **kwargs):\n        \"\"\"\n        Custom class instantiation utilizing instance cache.\n\n        If a Task has already been instantiated with the same parameters,\n        the previous instance is returned to reduce number of object instances.\n        \"\"\"\n        def instantiate():\n            return super(Register, cls).__call__(*args, **kwargs)\n\n        h = cls.__instance_cache\n\n        if h is None:  # disabled\n            return instantiate()\n\n        params = cls.get_params()\n        param_values = cls.get_param_values(params, args, kwargs)\n\n        k = (cls, tuple(param_values))\n\n        try:\n            hash(k)\n        except TypeError:\n            logger.debug(\"Not all parameter values are hashable so instance isn't coming from the cache\")\n            return instantiate()  # unhashable types in parameters\n\n        if k not in h:\n            h[k] = instantiate()\n\n        return h[k]", "is_method": true, "class_name": "Register", "function_description": "This method provides an instance caching mechanism for class instantiation. It reuses existing objects for identical creation parameters, optimizing resource usage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "clear_instance_cache", "line_number": 103, "body": "def clear_instance_cache(cls):\n        \"\"\"\n        Clear/Reset the instance cache.\n        \"\"\"\n        cls.__instance_cache = {}", "is_method": true, "class_name": "Register", "function_description": "Resets the class's internal instance cache to an empty state. This ensures all previously cached instances are removed, allowing for fresh instance management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "disable_instance_cache", "line_number": 110, "body": "def disable_instance_cache(cls):\n        \"\"\"\n        Disables the instance cache.\n        \"\"\"\n        cls.__instance_cache = None", "is_method": true, "class_name": "Register", "function_description": "Disables and clears the instance cache for the class. This ensures all subsequent requests for instances will create new objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_get_reg", "line_number": 127, "body": "def _get_reg(cls):\n        \"\"\"Return all of the registered classes.\n\n        :return:  an ``dict`` of task_family -> class\n        \"\"\"\n        # We have to do this on-demand in case task names have changed later\n        reg = dict()\n        for task_cls in cls._reg:\n            if not task_cls._visible_in_registry:\n                continue\n\n            name = task_cls.get_task_family()\n            if name in reg and \\\n                    (reg[name] == Register.AMBIGUOUS_CLASS or  # Check so issubclass doesn't crash\n                     not issubclass(task_cls, reg[name])):\n                # Registering two different classes - this means we can't instantiate them by name\n                # The only exception is if one class is a subclass of the other. In that case, we\n                # instantiate the most-derived class (this fixes some issues with decorator wrappers).\n                reg[name] = Register.AMBIGUOUS_CLASS\n            else:\n                reg[name] = task_cls\n\n        return reg", "is_method": true, "class_name": "Register", "function_description": "This internal method compiles a dictionary mapping \"task family\" names to their registered class objects. It resolves naming conflicts by prioritizing subclasses or marking ambiguous entries, enabling class lookup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_set_reg", "line_number": 152, "body": "def _set_reg(cls, reg):\n        \"\"\"The writing complement of _get_reg\n        \"\"\"\n        cls._reg = [task_cls for task_cls in reg.values() if task_cls is not cls.AMBIGUOUS_CLASS]", "is_method": true, "class_name": "Register", "function_description": "Updates the class's internal registry by filtering and storing valid task classes. It excludes ambiguous class entries from the provided collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "task_names", "line_number": 158, "body": "def task_names(cls):\n        \"\"\"\n        List of task names as strings\n        \"\"\"\n        return sorted(cls._get_reg().keys())", "is_method": true, "class_name": "Register", "function_description": "Provides a sorted list of all registered task names. This capability allows other components to enumerate available tasks within the system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "tasks_str", "line_number": 165, "body": "def tasks_str(cls):\n        \"\"\"\n        Human-readable register contents dump.\n        \"\"\"\n        return ','.join(cls.task_names())", "is_method": true, "class_name": "Register", "function_description": "Generates a human-readable, comma-separated string of all registered task names. This function provides a quick way to dump or inspect the current contents of the task register."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "get_task_cls", "line_number": 172, "body": "def get_task_cls(cls, name):\n        \"\"\"\n        Returns an unambiguous class or raises an exception.\n        \"\"\"\n        task_cls = cls._get_reg().get(name)\n        if not task_cls:\n            raise TaskClassNotFoundException(cls._missing_task_msg(name))\n\n        if task_cls == cls.AMBIGUOUS_CLASS:\n            raise TaskClassAmbigiousException('Task %r is ambiguous' % name)\n        return task_cls", "is_method": true, "class_name": "Register", "function_description": "Retrieves a specific task class by name from the registry. It validates that the class exists and is unambiguous, raising appropriate exceptions if not."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "get_all_params", "line_number": 185, "body": "def get_all_params(cls):\n        \"\"\"\n        Compiles and returns all parameters for all :py:class:`Task`.\n\n        :return: a generator of tuples (TODO: we should make this more elegant)\n        \"\"\"\n        for task_name, task_cls in cls._get_reg().items():\n            if task_cls == cls.AMBIGUOUS_CLASS:\n                continue\n            for param_name, param_obj in task_cls.get_params():\n                yield task_name, (not task_cls.use_cmdline_section), param_name, param_obj", "is_method": true, "class_name": "Register", "function_description": "This method compiles and returns all parameters from every registered `Task` class. It provides a comprehensive overview of all configurable parameters across the system's tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_editdistance", "line_number": 198, "body": "def _editdistance(a, b):\n        \"\"\" Simple unweighted Levenshtein distance \"\"\"\n        r0 = range(0, len(b) + 1)\n        r1 = [0] * (len(b) + 1)\n\n        for i in range(0, len(a)):\n            r1[0] = i + 1\n\n            for j in range(0, len(b)):\n                c = 0 if a[i] is b[j] else 1\n                r1[j + 1] = min(r1[j] + 1, r0[j + 1] + 1, r0[j] + c)\n\n            r0 = r1[:]\n\n        return r1[len(b)]", "is_method": true, "class_name": "Register", "function_description": "Computes the simple unweighted Levenshtein distance between two input strings. It measures string dissimilarity by counting the minimum edits required to transform one into the other."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_missing_task_msg", "line_number": 215, "body": "def _missing_task_msg(cls, task_name):\n        weighted_tasks = [(Register._editdistance(task_name, task_name_2), task_name_2) for task_name_2 in cls.task_names()]\n        ordered_tasks = sorted(weighted_tasks, key=lambda pair: pair[0])\n        candidates = [task for (dist, task) in ordered_tasks if dist <= 5 and dist < len(task)]\n        if candidates:\n            return \"No task %s. Did you mean:\\n%s\" % (task_name, '\\n'.join(candidates))\n        else:\n            return \"No task %s. Candidates are: %s\" % (task_name, cls.tasks_str())", "is_method": true, "class_name": "Register", "function_description": "Generates a helpful error message for an unregistered task name. It suggests close matches using edit distance or lists all available tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_get_namespace", "line_number": 225, "body": "def _get_namespace(mcs, module_name):\n        for parent in mcs._module_parents(module_name):\n            entry = mcs._default_namespace_dict.get(parent)\n            if entry:\n                return entry\n        return ''", "is_method": true, "class_name": "Register", "function_description": "This method provides the namespace for a given module. It retrieves the first applicable namespace by traversing the module's parent hierarchy against default settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_register.py", "function": "_module_parents", "line_number": 233, "body": "def _module_parents(module_name):\n        '''\n        >>> list(Register._module_parents('a.b'))\n        ['a.b', 'a', '']\n        '''\n        spl = module_name.split('.')\n        for i in range(len(spl), 0, -1):\n            yield '.'.join(spl[0:i])\n        if module_name:\n            yield ''", "is_method": true, "class_name": "Register", "function_description": "Generates all hierarchical parent module paths for a given module name, including the module itself and the top-level package. This is useful for systems requiring module-aware lookups or registration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "dates", "line_number": 60, "body": "def dates(self):\n        ''' Returns a list of dates in this date interval.'''\n        dates = []\n        d = self.date_a\n        while d < self.date_b:\n            dates.append(d)\n            d += datetime.timedelta(1)\n\n        return dates", "is_method": true, "class_name": "DateInterval", "function_description": "This method of the DateInterval class provides a list of all individual dates within the specified interval, from the start date up to but not including the end date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "hours", "line_number": 70, "body": "def hours(self):\n        ''' Same as dates() but returns 24 times more info: one for each hour.'''\n        for date in self.dates():\n            for hour in range(24):\n                yield datetime.datetime.combine(date, datetime.time(hour))", "is_method": true, "class_name": "DateInterval", "function_description": "Generates a sequence of datetime objects, representing each hour across the interval's dates. It provides hourly granularity for time-based operations or analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__str__", "line_number": 76, "body": "def __str__(self):\n        return self.to_string()", "is_method": true, "class_name": "DateInterval", "function_description": "Provides a human-readable string representation for a DateInterval object. It delegates the actual formatting to the `to_string` method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__repr__", "line_number": 79, "body": "def __repr__(self):\n        return self.to_string()", "is_method": true, "class_name": "DateInterval", "function_description": "Provides a developer-friendly, unambiguous string representation of the DateInterval object, useful for debugging and introspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "prev", "line_number": 82, "body": "def prev(self):\n        ''' Returns the preceding corresponding date interval (eg. May -> April).'''\n        return self.from_date(self.date_a - datetime.timedelta(1))", "is_method": true, "class_name": "DateInterval", "function_description": "Calculates and returns the date interval immediately preceding the current instance. It enables easy navigation backwards through sequential time periods."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "next", "line_number": 86, "body": "def next(self):\n        ''' Returns the subsequent corresponding date interval (eg. 2014 -> 2015).'''\n        return self.from_date(self.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "This method returns the date interval immediately following the current one. It provides the next chronological date interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__contains__", "line_number": 107, "body": "def __contains__(self, date):\n        return date in self.dates()", "is_method": true, "class_name": "DateInterval", "function_description": "Checks if a given date is part of the date interval. This enables using the `in` operator for convenient date range validation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__iter__", "line_number": 110, "body": "def __iter__(self):\n        for d in self.dates():\n            yield d", "is_method": true, "class_name": "DateInterval", "function_description": "This method makes a `DateInterval` object directly iterable, allowing callers to easily loop through all the dates within the interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__hash__", "line_number": 114, "body": "def __hash__(self):\n        return hash(repr(self))", "is_method": true, "class_name": "DateInterval", "function_description": "Provides a hash value for `DateInterval` instances based on their string representation. This enables `DateInterval` objects to be used as keys in dictionaries or elements in sets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__cmp__", "line_number": 117, "body": "def __cmp__(self, other):\n        if not isinstance(self, type(other)):\n            # doing this because it's not well defined if eg. 2012-01-01-2013-01-01 == 2012\n            raise TypeError('Date interval type mismatch')\n\n        return (self > other) - (self < other)", "is_method": true, "class_name": "DateInterval", "function_description": "Defines the comparison behavior between two DateInterval objects. It enforces type compatibility and returns an integer indicating their relative order."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__lt__", "line_number": 124, "body": "def __lt__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) < (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Enables comparing two `DateInterval` objects using the less-than operator (`<`). It defines an ordering based on their start and then end dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__le__", "line_number": 129, "body": "def __le__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) <= (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Enables the 'less than or equal to' comparison (`<=`) between two `DateInterval` objects. It determines if one interval is ordered before or identically to another based on their start and end dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__gt__", "line_number": 134, "body": "def __gt__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) > (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Enables comparison of `DateInterval` objects using the greater than (>) operator. It determines if one interval is lexicographically greater than another based on its start and end dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__ge__", "line_number": 139, "body": "def __ge__(self, other):\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        return (self.date_a, self.date_b) >= (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "Enables comparison of `DateInterval` objects using the greater-than-or-equal-to operator (`>=`). It determines if one interval's start and end dates are lexicographically greater than or equal to another's."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__eq__", "line_number": 144, "body": "def __eq__(self, other):\n        if not isinstance(other, DateInterval):\n            return False\n        if not isinstance(self, type(other)):\n            raise TypeError('Date interval type mismatch')\n        else:\n            return (self.date_a, self.date_b) == (other.date_a, other.date_b)", "is_method": true, "class_name": "DateInterval", "function_description": "This method defines how two `DateInterval` objects are compared for equality. It returns true if their start and end dates are identical, enforcing strict type matching."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "__ne__", "line_number": 152, "body": "def __ne__(self, other):\n        return not self.__eq__(other)", "is_method": true, "class_name": "DateInterval", "function_description": "Enables comparing two DateInterval objects for inequality, returning true if they are not equal based on the defined equality comparison."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 165, "body": "def to_string(self):\n        return self.date_a.strftime('%Y-%m-%d')", "is_method": true, "class_name": "Date", "function_description": "Converts the Date object's internal date into a 'YYYY-MM-DD' formatted string. This provides a consistent string representation for display or serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 169, "body": "def from_date(cls, d):\n        return Date(d.year, d.month, d.day)", "is_method": true, "class_name": "Date", "function_description": "This class method creates a new `Date` object from an existing date-like object. It provides a convenient way to convert other date representations into an instance of the `Date` class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 173, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d$', s):\n            return Date(*map(int, s.split('-')))", "is_method": true, "class_name": "Date", "function_description": "The `Date` class method parses a 'YYYY-MM-DD' formatted string and returns a corresponding `Date` object. It provides a standard way to convert date strings into structured date instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 196, "body": "def to_string(self):\n        return '%d-W%02d' % self.date_a.isocalendar()[:2]", "is_method": true, "class_name": "Week", "function_description": "Provides a standard ISO 8601 string representation (YYYY-WNN) for the Week object. This is useful for unambiguous display or serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 200, "body": "def from_date(cls, d):\n        return Week(*d.isocalendar()[:2])", "is_method": true, "class_name": "Week", "function_description": "Constructs a `Week` object representing the calendar week to which a given date belongs. This class method provides a convenient way to initialize `Week` instances from standard date objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 204, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-W\\d\\d$', s):\n            y, w = map(int, s.split('-W'))\n            return Week(y, w)", "is_method": true, "class_name": "Week", "function_description": "This method converts a 'YYYY-WNN' formatted string into a Week object, enabling easy instantiation from standard week representations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 217, "body": "def to_string(self):\n        return self.date_a.strftime('%Y-%m')", "is_method": true, "class_name": "Month", "function_description": "Provides a string representation of the Month object, formatted as 'YYYY-MM', for display or data serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 221, "body": "def from_date(cls, d):\n        return Month(d.year, d.month)", "is_method": true, "class_name": "Month", "function_description": "Creates a `Month` object from a standard `date` object by extracting its year and month components. It provides a convenient way to instantiate a `Month` from an existing date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 225, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d$', s):\n            y, m = map(int, s.split('-'))\n            return Month(y, m)", "is_method": true, "class_name": "Month", "function_description": "This class method parses a string representing a month in 'YYYY-MM' format and constructs a corresponding Month object from it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 238, "body": "def to_string(self):\n        return self.date_a.strftime('%Y')", "is_method": true, "class_name": "Year", "function_description": "Converts the `Year` object's internal date into a four-digit string representation of its year. This is useful for displaying or textual processing of the year."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "from_date", "line_number": 242, "body": "def from_date(cls, d):\n        return Year(d.year)", "is_method": true, "class_name": "Year", "function_description": "Creates a Year object from a standard date object. It provides a convenient way to initialize a Year instance using the year component of the date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 246, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d$', s):\n            return Year(int(s))", "is_method": true, "class_name": "Year", "function_description": "This class method parses a four-digit string into a Year object. It ensures the string matches the expected year format for safe conversion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "to_string", "line_number": 259, "body": "def to_string(self):\n        return '-'.join([d.strftime('%Y-%m-%d') for d in (self.date_a, self.date_b)])", "is_method": true, "class_name": "Custom", "function_description": "Generates a hyphen-separated string combining the object's two internal dates, formatted as YYYY-MM-DD. This provides a concise string representation of the date range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/date_interval.py", "function": "parse", "line_number": 263, "body": "def parse(cls, s):\n        if re.match(r'\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\\-\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d$', s):\n            x = list(map(int, s.split('-')))\n            date_a = datetime.date(*x[:3])\n            date_b = datetime.date(*x[3:])\n            return Custom(date_a, date_b)", "is_method": true, "class_name": "Custom", "function_description": "This method parses a specific date string format (YYYY-MM-DD-YYYY-MM-DD) to construct and return a new instance of the Custom class, representing a pair of dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_history.py", "function": "task_family", "line_number": 44, "body": "def task_family(self):\n        return self._task.family", "is_method": true, "class_name": "StoredTask", "function_description": "Provides access to the family identifier of the task encapsulated by this `StoredTask` object, useful for grouping or categorizing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/task_history.py", "function": "parameters", "line_number": 48, "body": "def parameters(self):\n        return self._task.params", "is_method": true, "class_name": "StoredTask", "function_description": "Returns the operational parameters of the stored task. This provides access to the task's configuration details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "check_pid", "line_number": 30, "body": "def check_pid(pidfile):\n    if pidfile and os.path.exists(pidfile):\n        try:\n            pid = int(open(pidfile).read().strip())\n            os.kill(pid, 0)\n            return pid\n        except BaseException:\n            return 0\n    return 0", "is_method": false, "function_description": "This function checks if a process, whose ID is stored in the given `pidfile`, is currently running. It returns the PID if the process is active, otherwise 0."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "write_pid", "line_number": 41, "body": "def write_pid(pidfile):\n    server_logger.info(\"Writing pid file\")\n    piddir = os.path.dirname(pidfile)\n    if piddir != '':\n        try:\n            os.makedirs(piddir)\n        except OSError:\n            pass\n\n    with open(pidfile, 'w') as fobj:\n        fobj.write(str(os.getpid()))", "is_method": false, "function_description": "Records the current process ID into a specified PID file. This helps other processes monitor or manage the running application, especially for daemon services."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "get_log_format", "line_number": 54, "body": "def get_log_format():\n    return \"%(asctime)s %(name)s[%(process)s] %(levelname)s: %(message)s\"", "is_method": false, "function_description": "Provides a standard string format for log messages. This function ensures consistent logging configuration across an application."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "get_spool_handler", "line_number": 58, "body": "def get_spool_handler(filename):\n    handler = logging.handlers.TimedRotatingFileHandler(\n        filename=filename,\n        when='d',\n        encoding='utf8',\n        backupCount=7  # keep one week of historical logs\n    )\n    formatter = logging.Formatter(get_log_format())\n    handler.setFormatter(formatter)\n    return handler", "is_method": false, "function_description": "Provides a pre-configured logging handler for daily rotating log files, retaining one week of historical logs. It ensures automated log management and archival for applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "_server_already_running", "line_number": 70, "body": "def _server_already_running(pidfile):\n    existing_pid = check_pid(pidfile)\n    if pidfile and existing_pid:\n        return True\n    return False", "is_method": false, "function_description": "Checks if a server process is already running by verifying if a valid process ID exists in the specified PID file. This prevents multiple instances from starting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/process.py", "function": "daemonize", "line_number": 77, "body": "def daemonize(cmd, pidfile=None, logdir=None, api_port=8082, address=None, unix_socket=None):\n    import daemon\n\n    logdir = logdir or \"/var/log/luigi\"\n    if not os.path.exists(logdir):\n        os.makedirs(logdir)\n\n    log_path = os.path.join(logdir, \"luigi-server.log\")\n\n    # redirect stdout/stderr\n    today = datetime.date.today()\n    stdout_path = os.path.join(\n        logdir,\n        \"luigi-server-{0:%Y-%m-%d}.out\".format(today)\n    )\n    stderr_path = os.path.join(\n        logdir,\n        \"luigi-server-{0:%Y-%m-%d}.err\".format(today)\n    )\n    stdout_proxy = open(stdout_path, 'a+')\n    stderr_proxy = open(stderr_path, 'a+')\n\n    try:\n        ctx = daemon.DaemonContext(\n            stdout=stdout_proxy,\n            stderr=stderr_proxy,\n            working_directory='.',\n            initgroups=False,\n        )\n    except TypeError:\n        # Older versions of python-daemon cannot deal with initgroups arg.\n        ctx = daemon.DaemonContext(\n            stdout=stdout_proxy,\n            stderr=stderr_proxy,\n            working_directory='.',\n        )\n\n    with ctx:\n        loghandler = get_spool_handler(log_path)\n        rootlogger.addHandler(loghandler)\n\n        if pidfile:\n            server_logger.info(\"Checking pid file\")\n            existing_pid = check_pid(pidfile)\n            if pidfile and existing_pid:\n                server_logger.info(\"Server already running (pid=%s)\", existing_pid)\n                return\n            write_pid(pidfile)\n\n        cmd(api_port=api_port, address=address, unix_socket=unix_socket)", "is_method": false, "function_description": "Daemonizes a given callable, running it as a background service. It manages PID files, redirects output to logs, and ensures only one instance runs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_urljoin", "line_number": 53, "body": "def _urljoin(base, url):\n    \"\"\"\n    Join relative URLs to base URLs like urllib.parse.urljoin but support\n    arbitrary URIs (esp. 'http+unix://').\n    \"\"\"\n    parsed = urlparse(base)\n    scheme = parsed.scheme\n    return urlparse(\n        urljoin(parsed._replace(scheme='http').geturl(), url)\n    )._replace(scheme=scheme).geturl()", "is_method": false, "function_description": "This function joins relative URLs to a base URL, extending standard URL joining to support arbitrary URI schemes like 'http+unix://'. It provides robust URI resolution for non-standard protocols."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_create_request", "line_number": 75, "body": "def _create_request(self, full_url, body=None):\n        # when full_url contains basic auth info, extract it and set the Authorization header\n        url = urlparse(full_url)\n        if url.username:\n            # base64 encoding of username:password\n            auth = base64.b64encode('{}:{}'.format(url.username, url.password or '').encode('utf-8'))\n            auth = auth.decode('utf-8')\n            # update full_url and create a request object with the auth header set\n            full_url = url._replace(netloc=url.netloc.split('@', 1)[-1]).geturl()\n            req = Request(full_url)\n            req.add_header('Authorization', 'Basic {}'.format(auth))\n        else:\n            req = Request(full_url)\n\n        # add the request body\n        if body:\n            req.data = urlencode(body).encode('utf-8')\n\n        return req", "is_method": true, "class_name": "URLLibFetcher", "function_description": "This method creates a `urllib.request.Request` object. It extracts basic authentication from the URL and encodes the request body, preparing it for network transmission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "fetch", "line_number": 95, "body": "def fetch(self, full_url, body, timeout):\n        req = self._create_request(full_url, body=body)\n        return urlopen(req, timeout=timeout).read().decode('utf-8')", "is_method": true, "class_name": "URLLibFetcher", "function_description": "This method fetches and decodes the content from a specified URL. It provides a core service for retrieving web data using an HTTP request."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "check_pid", "line_number": 107, "body": "def check_pid(self):\n        # if the process id change changed from when the session was created\n        # a new session needs to be setup since requests isn't multiprocessing safe.\n        if os.getpid() != self.process_id:\n            self.session = requests.Session()\n            self.process_id = os.getpid()", "is_method": true, "class_name": "RequestsFetcher", "function_description": "The method reinitializes the internal `requests.Session` if the current process ID differs from the one it was created with. This ensures multiprocessing safety for HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "fetch", "line_number": 114, "body": "def fetch(self, full_url, body, timeout):\n        self.check_pid()\n        resp = self.session.post(full_url, data=body, timeout=timeout)\n        resp.raise_for_status()\n        return resp.text", "is_method": true, "class_name": "RequestsFetcher", "function_description": "Sends an HTTP POST request to a specified URL with given data. It returns the response body as text, raising an error for non-successful HTTP status codes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_get_retryer", "line_number": 147, "body": "def _get_retryer(self):\n        def retry_logging(retry_state):\n            if self._rpc_log_retries:\n                logger.warning(\"Failed connecting to remote scheduler %r\", self._url, exc_info=True)\n                logger.info(\"Retrying attempt %r of %r (max)\" % (retry_state.attempt_number + 1, self._rpc_retry_attempts))\n                logger.info(\"Wait for %d seconds\" % self._rpc_retry_wait)\n\n        return Retrying(wait=wait_fixed(self._rpc_retry_wait),\n                        stop=stop_after_attempt(self._rpc_retry_attempts),\n                        reraise=True,\n                        after=retry_logging)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Configures and returns a `Retrying` object. This internal utility ensures robust connection attempts to the remote scheduler, logging failures for each retry.\nConfigures and returns a `Retrying` object. This internal utility ensures robust connection attempts to the remote scheduler, logging failures for each retry."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_fetch", "line_number": 159, "body": "def _fetch(self, url_suffix, body):\n        full_url = _urljoin(self._url, url_suffix)\n        scheduler_retry = self._get_retryer()\n\n        try:\n            response = scheduler_retry(self._fetcher.fetch, full_url, body, self._connect_timeout)\n        except self._fetcher.raises as e:\n            raise RPCError(\n                \"Errors (%d attempts) when connecting to remote scheduler %r\" %\n                (self._rpc_retry_attempts, self._url),\n                e\n            )\n        return response", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Facilitates robust communication with a remote scheduler by sending HTTP requests. It includes retry mechanisms and comprehensive error handling for reliable operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "_request", "line_number": 173, "body": "def _request(self, url, data, attempts=3, allow_null=True):\n        body = {'data': json.dumps(data)}\n\n        for _ in range(attempts):\n            page = self._fetch(url, body)\n            response = json.loads(page)[\"response\"]\n            if allow_null or response is not None:\n                return response\n        raise RPCError(\"Received null response from remote scheduler %r\" % self._url)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Sends requests to the remote scheduler with built-in retries. It parses the JSON response, ensuring a valid result or raising an error after multiple attempts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/rpc.py", "function": "retry_logging", "line_number": 148, "body": "def retry_logging(retry_state):\n            if self._rpc_log_retries:\n                logger.warning(\"Failed connecting to remote scheduler %r\", self._url, exc_info=True)\n                logger.info(\"Retrying attempt %r of %r (max)\" % (retry_state.attempt_number + 1, self._rpc_retry_attempts))\n                logger.info(\"Wait for %d seconds\" % self._rpc_retry_wait)", "is_method": true, "class_name": "RemoteScheduler", "function_description": "Logs connection failures and retry attempts when connecting to a remote scheduler. It provides visibility into the retry mechanism for monitoring and debugging remote communication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "has_value", "line_number": 55, "body": "def has_value(cls, value):\n        return any(value == item.value for item in cls)", "is_method": true, "class_name": "ParameterVisibility", "function_description": "Determines if a given value exists among the `ParameterVisibility` class's defined options. It provides a convenient way to validate parameter visibility states."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 58, "body": "def serialize(self):\n        return self.value", "is_method": true, "class_name": "ParameterVisibility", "function_description": "Provides the underlying value of the parameter visibility object for serialization or external representation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_get_value_from_config", "line_number": 185, "body": "def _get_value_from_config(self, section, name):\n        \"\"\"Loads the default from the config. Returns _no_value if it doesn't exist\"\"\"\n\n        conf = configuration.get_config()\n\n        try:\n            value = conf.get(section, name)\n        except (NoSectionError, NoOptionError, KeyError):\n            return _no_value\n\n        return self.parse(value)", "is_method": true, "class_name": "Parameter", "function_description": "Chain of Thought:The function `_get_value_from_config` is a private method within the `Parameter` class. Its main role is to fetch a specific value from a global configuration object, identified by a `section` and `name`. It handles cases where the configuration key doesn't exist by returning `_no_value`. Crucially, it then calls `self.parse(value)`, indicating that it also processes or converts the retrieved string value into a more usable format for the `Parameter` object.\n\nTherefore, this method provides the service of safely retrieving and preparing a parameter's value from a configuration source.\n\nDescription: Retrieves and parses a specific parameter value from the application's configuration. It returns a special indicator if the value is not found.\nRetrieves and parses a specific parameter's value from the application's configuration. It returns a special indicator if the value is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_get_value", "line_number": 197, "body": "def _get_value(self, task_name, param_name):\n        for value, warn in self._value_iterator(task_name, param_name):\n            if value != _no_value:\n                if warn:\n                    warnings.warn(warn, DeprecationWarning)\n                return value\n        return _no_value", "is_method": true, "class_name": "Parameter", "function_description": "Retrieves the effective value for a specific parameter by iterating through potential sources. It returns the first valid value found, issuing deprecation warnings when applicable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_value_iterator", "line_number": 205, "body": "def _value_iterator(self, task_name, param_name):\n        \"\"\"\n        Yield the parameter values, with optional deprecation warning as second tuple value.\n\n        The parameter value will be whatever non-_no_value that is yielded first.\n        \"\"\"\n        cp_parser = CmdlineParser.get_instance()\n        if cp_parser:\n            dest = self._parser_global_dest(param_name, task_name)\n            found = getattr(cp_parser.known_args, dest, None)\n            yield (self._parse_or_no_value(found), None)\n        yield (self._get_value_from_config(task_name, param_name), None)\n        if self._config_path:\n            yield (self._get_value_from_config(self._config_path['section'], self._config_path['name']),\n                   'The use of the configuration [{}] {} is deprecated. Please use [{}] {}'.format(\n                       self._config_path['section'], self._config_path['name'], task_name, param_name))\n        yield (self._default, None)", "is_method": true, "class_name": "Parameter", "function_description": "Iterates through potential parameter values from command-line, configuration, and default sources. It provides values in order of precedence, including deprecation warnings for specific configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "has_task_value", "line_number": 223, "body": "def has_task_value(self, task_name, param_name):\n        return self._get_value(task_name, param_name) != _no_value", "is_method": true, "class_name": "Parameter", "function_description": "Determines if a specific parameter for a given task has an assigned value, indicating it's not undefined or uninitialized."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "task_value", "line_number": 226, "body": "def task_value(self, task_name, param_name):\n        value = self._get_value(task_name, param_name)\n        if value == _no_value:\n            raise MissingParameterException(\"No default specified\")\n        else:\n            return self.normalize(value)", "is_method": true, "class_name": "Parameter", "function_description": "Retrieves a specific parameter's value for a given task. It ensures the parameter exists, raising an exception if missing, and then normalizes the value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_is_batchable", "line_number": 233, "body": "def _is_batchable(self):\n        return self._batch_method is not None", "is_method": true, "class_name": "Parameter", "function_description": "This method determines if the parameter supports batch processing. It indicates whether batch operations can be applied to the parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 236, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        The default implementation is the identity function, but subclasses should override\n        this method for specialized parsing.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        return x", "is_method": true, "class_name": "Parameter", "function_description": "This method defines a customizable interface for parsing individual input values. Subclasses should override it to implement specialized parsing logic for different parameter types."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parse_list", "line_number": 248, "body": "def _parse_list(self, xs):\n        \"\"\"\n        Parse a list of values from the scheduler.\n\n        Only possible if this is_batchable() is True. This will combine the list into a single\n        parameter value using batch method. This should never need to be overridden.\n\n        :param xs: list of values to parse and combine\n        :return: the combined parsed values\n        \"\"\"\n        if not self._is_batchable():\n            raise NotImplementedError('No batch method found')\n        elif not xs:\n            raise ValueError('Empty parameter list passed to parse_list')\n        else:\n            return self._batch_method(map(self.parse, xs))", "is_method": true, "class_name": "Parameter", "function_description": "This method combines a list of individual parameter values into a single, unified parameter value, provided the parameter type supports batching. It facilitates processing multiple discrete inputs as one consolidated parameter."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 265, "body": "def serialize(self, x):\n        \"\"\"\n        Opposite of :py:meth:`parse`.\n\n        Converts the value ``x`` to a string.\n\n        :param x: the value to serialize.\n        \"\"\"\n        return str(x)", "is_method": true, "class_name": "Parameter", "function_description": "For a `Parameter` object, this method converts a given value into its string representation. It enables serializing parameter values for storage or external use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 275, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != Parameter:\n            return\n        if not isinstance(param_value, str):\n            warnings.warn('Parameter \"{}\" with value \"{}\" is not of type string.'.format(param_name, param_value))", "is_method": true, "class_name": "Parameter", "function_description": "This method issues a warning if a parameter's value is not a string. It specifically applies to instances of the `Parameter` class to help enforce type consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 281, "body": "def normalize(self, x):\n        \"\"\"\n        Given a parsed parameter value, normalizes it.\n\n        The value can either be the result of parse(), the default value or\n        arguments passed into the task's constructor by instantiation.\n\n        This is very implementation defined, but can be used to validate/clamp\n        valid values. For example, if you wanted to only accept even integers,\n        and \"correct\" odd values to the nearest integer, you can implement\n        normalize as ``x // 2 * 2``.\n        \"\"\"\n        return x", "is_method": true, "class_name": "Parameter", "function_description": "Normalizes a parameter value, ensuring it conforms to specific rules or constraints. It can validate, clamp, or transform the input for consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 295, "body": "def next_in_enumeration(self, _value):\n        \"\"\"\n        If your Parameter type has an enumerable ordering of values. You can\n        choose to override this method. This method is used by the\n        :py:mod:`luigi.execution_summary` module for pretty printing\n        purposes. Enabling it to pretty print tasks like ``MyTask(num=1),\n        MyTask(num=2), MyTask(num=3)`` to ``MyTask(num=1..3)``.\n\n        :param value: The value\n        :return: The next value, like \"value + 1\". Or ``None`` if there's no enumerable ordering.\n        \"\"\"\n        return None", "is_method": true, "class_name": "Parameter", "function_description": "This method, intended for override, defines the next value in an enumerable sequence for a parameter. It enables Luigi's execution summary to group and pretty print sequential tasks concisely."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parse_or_no_value", "line_number": 308, "body": "def _parse_or_no_value(self, x):\n        if not x:\n            return _no_value\n        else:\n            return self.parse(x)", "is_method": true, "class_name": "Parameter", "function_description": "Checks if an input signifies the absence of a value. If so, it returns a 'no value' sentinel; otherwise, it parses the input using the parameter's logic."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_global_dest", "line_number": 315, "body": "def _parser_global_dest(param_name, task_name):\n        return task_name + '_' + param_name", "is_method": true, "class_name": "Parameter", "function_description": "This helper function generates a unique global destination name for a parameter by combining its name with a given task name."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_kwargs", "line_number": 319, "body": "def _parser_kwargs(cls, param_name, task_name=None):\n        return {\n            \"action\": \"store\",\n            \"dest\": cls._parser_global_dest(param_name, task_name) if task_name else param_name,\n        }", "is_method": true, "class_name": "Parameter", "function_description": "Provides a dictionary of keyword arguments for an argument parser. It specifies the storage action as 'store' and computes the destination name for a parameter, optionally making it task-specific."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 329, "body": "def serialize(self, x):\n        if x is None:\n            return ''\n        else:\n            return str(x)", "is_method": true, "class_name": "OptionalParameter", "function_description": "Converts an input value to its string representation. It specifically returns an empty string if the value is `None`, otherwise its standard string form."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 335, "body": "def parse(self, x):\n        return x or None", "is_method": true, "class_name": "OptionalParameter", "function_description": "Standardizes optional parameter values by converting any falsy input into `None`. This ensures a consistent representation for unprovided or empty values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 338, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != OptionalParameter:\n            return\n        if not isinstance(param_value, str) and param_value is not None:\n            warnings.warn('OptionalParameter \"{}\" with value \"{}\" is not of type string or None.'.format(\n                param_name, param_value))", "is_method": true, "class_name": "OptionalParameter", "function_description": "Within the `OptionalParameter` class, this internal method warns if a parameter's value is neither a string nor `None`, enforcing expected type adherence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 367, "body": "def parse(self, s):\n        \"\"\"\n        Parses a date string formatted like ``YYYY-MM-DD``.\n        \"\"\"\n        return datetime.datetime.strptime(s, self.date_format).date()", "is_method": true, "class_name": "_DateParameterBase", "function_description": "The `_DateParameterBase`'s `parse` method converts a `YYYY-MM-DD` formatted string into a Python `date` object. This enables reliable date handling from string inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 373, "body": "def serialize(self, dt):\n        \"\"\"\n        Converts the date to a string using the :py:attr:`~_DateParameterBase.date_format`.\n        \"\"\"\n        if dt is None:\n            return str(dt)\n        return dt.strftime(self.date_format)", "is_method": true, "class_name": "_DateParameterBase", "function_description": "Converts a datetime object into its string representation using the instance's configured date format. This standardizes date serialization for various applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 415, "body": "def next_in_enumeration(self, value):\n        return value + datetime.timedelta(days=self.interval)", "is_method": true, "class_name": "DateParameter", "function_description": "Provides the next date in an enumeration, stepping forward from a given date by the object's configured daily interval. Useful for generating a series of dates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 418, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, datetime.datetime):\n            value = value.date()\n\n        delta = (value - self.start).days % self.interval\n        return value - datetime.timedelta(days=delta)", "is_method": true, "class_name": "DateParameter", "function_description": "This method normalizes a given date, aligning it backwards to the nearest interval starting from a defined base date. It ensures dates conform to a specific periodic structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_add_months", "line_number": 441, "body": "def _add_months(self, date, months):\n        \"\"\"\n        Add ``months`` months to ``date``.\n\n        Unfortunately we can't use timedeltas to add months because timedelta counts in days\n        and there's no foolproof way to add N months in days without counting the number of\n        days per month.\n        \"\"\"\n        year = date.year + (date.month + months - 1) // 12\n        month = (date.month + months - 1) % 12 + 1\n        return datetime.date(year=year, month=month, day=1)", "is_method": true, "class_name": "MonthParameter", "function_description": "This utility method calculates a future date by adding a specified number of months to a given date. It returns the first day of the resulting month."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 453, "body": "def next_in_enumeration(self, value):\n        return self._add_months(value, self.interval)", "is_method": true, "class_name": "MonthParameter", "function_description": "This method calculates the next month in an enumeration sequence by adding the configured interval to a given month value. It helps iterate through months for time-based operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 456, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, date_interval.Month):\n            value = value.date_a\n\n        months_since_start = (value.year - self.start.year) * 12 + (value.month - self.start.month)\n        months_since_start -= months_since_start % self.interval\n\n        return self._add_months(self.start, months_since_start)", "is_method": true, "class_name": "MonthParameter", "function_description": "The function normalizes a month value by snapping it to a specific interval from a start date, ensuring consistent month-based calculations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 480, "body": "def next_in_enumeration(self, value):\n        return value.replace(year=value.year + self.interval)", "is_method": true, "class_name": "YearParameter", "function_description": "Calculates the next year in an enumeration sequence by incrementing a given date's year by the `YearParameter`'s defined interval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 483, "body": "def normalize(self, value):\n        if value is None:\n            return None\n\n        if isinstance(value, date_interval.Year):\n            value = value.date_a\n\n        delta = (value.year - self.start.year) % self.interval\n        return datetime.date(year=value.year - delta, month=1, day=1)", "is_method": true, "class_name": "YearParameter", "function_description": "The `normalize` method within `YearParameter` standardizes a date's year to the beginning of a defined interval. It's useful for consistent period-based data processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 520, "body": "def parse(self, s):\n        \"\"\"\n        Parses a string to a :py:class:`~datetime.datetime`.\n        \"\"\"\n        return datetime.datetime.strptime(s, self.date_format)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Converts a string representation of a date and time into a `datetime.datetime` object. It provides a standardized parsing utility based on the class's configured format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 526, "body": "def serialize(self, dt):\n        \"\"\"\n        Converts the date to a string using the :py:attr:`~_DatetimeParameterBase.date_format`.\n        \"\"\"\n        if dt is None:\n            return str(dt)\n        return dt.strftime(self.date_format)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Serializes a datetime object into a string representation. It uses the class's specified date format for consistent output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_convert_to_dt", "line_number": 535, "body": "def _convert_to_dt(dt):\n        if not isinstance(dt, datetime.datetime):\n            dt = datetime.datetime.combine(dt, datetime.time.min)\n        return dt", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Ensures an input date or datetime object is a `datetime.datetime` instance. It converts `datetime.date` objects to the start of that day, standardizing datetime values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 540, "body": "def normalize(self, dt):\n        \"\"\"\n        Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at\n        :py:attr:`~_DatetimeParameterBase.start`.\n        \"\"\"\n        if dt is None:\n            return None\n\n        dt = self._convert_to_dt(dt)\n\n        dt = dt.replace(microsecond=0)  # remove microseconds, to avoid float rounding issues.\n        delta = (dt - self.start).total_seconds()\n        granularity = (self._timedelta * self.interval).total_seconds()\n        return dt - datetime.timedelta(seconds=delta % granularity)", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "This method normalizes a datetime by aligning it to a predefined time grid. It clamps the given datetime to the nearest multiple of the configured interval relative to a start time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 555, "body": "def next_in_enumeration(self, value):\n        return value + self._timedelta * self.interval", "is_method": true, "class_name": "_DatetimeParameterBase", "function_description": "Calculates the next datetime in a sequence by adding a scaled time interval. This method helps enumerate datetime values for scheduling or time-series generation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 587, "body": "def parse(self, s):\n        try:\n            value = datetime.datetime.strptime(s, self.deprecated_date_format)\n            warnings.warn(\n                'Using \"H\" between hours and minutes is deprecated, omit it instead.',\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return value\n        except ValueError:\n            return super(DateMinuteParameter, self).parse(s)", "is_method": true, "class_name": "DateMinuteParameter", "function_description": "Parses a string into a datetime object, prioritizing a deprecated date format and issuing a warning. If parsing fails with the deprecated format, it delegates to the superclass's parse method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 620, "body": "def parse(self, s):\n        \"\"\"\n        Parses an ``int`` from the string using ``int()``.\n        \"\"\"\n        return int(s)", "is_method": true, "class_name": "IntParameter", "function_description": "Parses a string input and converts it into an integer value. This method provides the core capability to interpret string representations as integer parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "next_in_enumeration", "line_number": 626, "body": "def next_in_enumeration(self, value):\n        return value + 1", "is_method": true, "class_name": "IntParameter", "function_description": "Calculates the next integer in an ordered sequence, providing the subsequent value for enumeration or progression."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 635, "body": "def parse(self, s):\n        \"\"\"\n        Parses a ``float`` from the string using ``float()``.\n        \"\"\"\n        return float(s)", "is_method": true, "class_name": "FloatParameter", "function_description": "Parses a string representation into a floating-point number. This method is essential for converting textual parameter inputs into their numerical float values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 682, "body": "def parse(self, val):\n        \"\"\"\n        Parses a ``bool`` from the string, matching 'true' or 'false' ignoring case.\n        \"\"\"\n        s = str(val).lower()\n        if s == \"true\":\n            return True\n        elif s == \"false\":\n            return False\n        else:\n            raise ValueError(\"cannot interpret '{}' as boolean\".format(val))", "is_method": true, "class_name": "BoolParameter", "function_description": "Provides a method to parse a string value into a boolean. It recognizes 'true' and 'false' case-insensitively, raising an error for invalid inputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 694, "body": "def normalize(self, value):\n        try:\n            return self.parse(value)\n        except ValueError:\n            return None", "is_method": true, "class_name": "BoolParameter", "function_description": "Normalizes an input value into a boolean representation using the class's parsing logic. It returns `None` if the conversion encounters an invalid format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parser_kwargs", "line_number": 700, "body": "def _parser_kwargs(self, *args, **kwargs):\n        parser_kwargs = super(BoolParameter, self)._parser_kwargs(*args, **kwargs)\n        if self.parsing == self.IMPLICIT_PARSING:\n            parser_kwargs[\"action\"] = \"store_true\"\n        elif self.parsing == self.EXPLICIT_PARSING:\n            parser_kwargs[\"nargs\"] = \"?\"\n            parser_kwargs[\"const\"] = True\n        else:\n            raise ValueError(\"unknown parsing value '{}'\".format(self.parsing))\n        return parser_kwargs", "is_method": true, "class_name": "BoolParameter", "function_description": "Configures argument parser keyword arguments for a boolean parameter. It defines how the parameter is parsed, supporting both implicit flag-like behavior and explicit value assignment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 722, "body": "def parse(self, s):\n        \"\"\"\n        Parses a :py:class:`~luigi.date_interval.DateInterval` from the input.\n\n        see :py:mod:`luigi.date_interval`\n          for details on the parsing of DateIntervals.\n        \"\"\"\n        # TODO: can we use xml.utils.iso8601 or something similar?\n\n        from luigi import date_interval as d\n\n        for cls in [d.Year, d.Month, d.Week, d.Date, d.Custom]:\n            i = cls.parse(s)\n            if i:\n                return i\n\n        raise ValueError('Invalid date interval - could not be parsed')", "is_method": true, "class_name": "DateIntervalParameter", "function_description": "Parses a string input into a specific DateInterval object, attempting to match various defined interval types. This provides a robust way to interpret diverse date interval specifications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_apply_regex", "line_number": 753, "body": "def _apply_regex(self, regex, input):\n        import re\n        re_match = re.match(regex, input)\n        if re_match and any(re_match.groups()):\n            kwargs = {}\n            has_val = False\n            for k, v in re_match.groupdict(default=\"0\").items():\n                val = int(v)\n                if val > -1:\n                    has_val = True\n                    kwargs[k] = val\n            if has_val:\n                return datetime.timedelta(**kwargs)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Parses an input string using a regular expression to extract time components. It then constructs a `datetime.timedelta` object from these extracted values, providing a duration from text."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parseIso8601", "line_number": 767, "body": "def _parseIso8601(self, input):\n        def field(key):\n            return r\"(?P<%s>\\d+)%s\" % (key, key[0].upper())\n\n        def optional_field(key):\n            return \"(%s)?\" % field(key)\n\n        # A little loose: ISO 8601 does not allow weeks in combination with other fields, but this regex does (as does python timedelta)\n        regex = \"P(%s|%s(T%s)?)\" % (field(\"weeks\"), optional_field(\"days\"),\n                                    \"\".join([optional_field(key) for key in [\"hours\", \"minutes\", \"seconds\"]]))\n        return self._apply_regex(regex, input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Parses an ISO 8601 duration string, extracting time components like weeks, days, hours, minutes, and seconds. It converts a textual time duration into structured fields."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_parseSimple", "line_number": 779, "body": "def _parseSimple(self, input):\n        keys = [\"weeks\", \"days\", \"hours\", \"minutes\", \"seconds\"]\n        # Give the digits a regex group name from the keys, then look for text with the first letter of the key,\n        # optionally followed by the rest of the word, with final char (the \"s\") optional\n        regex = \"\".join([r\"((?P<%s>\\d+) ?%s(%s)?(%s)? ?)?\" % (k, k[0], k[1:-1], k[-1]) for k in keys])\n        return self._apply_regex(regex, input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Prepares a regular expression pattern to extract specific time components (weeks, days, hours, minutes, seconds) from a simple input string. It then applies this pattern to parse the time delta."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 786, "body": "def parse(self, input):\n        \"\"\"\n        Parses a time delta from the input.\n\n        See :py:class:`TimeDeltaParameter` for details on supported formats.\n        \"\"\"\n        result = self._parseIso8601(input)\n        if not result:\n            result = self._parseSimple(input)\n        if result is not None:\n            return result\n        else:\n            raise ParameterException(\"Invalid time delta - could not parse %s\" % input)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "This method parses an input string into a time delta object, supporting various formats. It provides a service to convert string representations into a usable time duration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 800, "body": "def serialize(self, x):\n        \"\"\"\n        Converts datetime.timedelta to a string\n\n        :param x: the value to serialize.\n        \"\"\"\n        weeks = x.days // 7\n        days = x.days % 7\n        hours = x.seconds // 3600\n        minutes = (x.seconds % 3600) // 60\n        seconds = (x.seconds % 3600) % 60\n        result = \"{} w {} d {} h {} m {} s\".format(weeks, days, hours, minutes, seconds)\n        return result", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Converts a `datetime.timedelta` object into a human-readable string representation of its duration. This method serializes timedelta values for display, logging, or storage purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "_warn_on_wrong_param_type", "line_number": 814, "body": "def _warn_on_wrong_param_type(self, param_name, param_value):\n        if self.__class__ != TimeDeltaParameter:\n            return\n        if not isinstance(param_value, datetime.timedelta):\n            warnings.warn('Parameter \"{}\" with value \"{}\" is not of type timedelta.'.format(param_name, param_value))", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Issues a warning if a given parameter's value, specific to the `TimeDeltaParameter` class, is not an instance of `datetime.timedelta`. This helps users identify and correct type mismatches."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 840, "body": "def parse(self, input):\n        \"\"\"\n        Parse a task_famly using the :class:`~luigi.task_register.Register`\n        \"\"\"\n        return task_register.Register.get_task_cls(input)", "is_method": true, "class_name": "TaskParameter", "function_description": "This method resolves a given input, likely a task family name, into its corresponding Luigi task class object. It leverages the global task registration system to look up and retrieve the correct class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 846, "body": "def serialize(self, cls):\n        \"\"\"\n        Converts the :py:class:`luigi.task.Task` (sub) class to its family name.\n        \"\"\"\n        return cls.get_task_family()", "is_method": true, "class_name": "TaskParameter", "function_description": "Converts a Luigi task class into its unique family name string. This provides a standardized identifier for the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 882, "body": "def parse(self, s):\n        try:\n            return self._enum[s]\n        except KeyError:\n            raise ValueError('Invalid enum value - could not be parsed')", "is_method": true, "class_name": "EnumParameter", "function_description": "Parses a string into its corresponding enumeration member, ensuring the input is a valid enum value. This method provides robust conversion from string to enum."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 888, "body": "def serialize(self, e):\n        return e.name", "is_method": true, "class_name": "EnumParameter", "function_description": "Converts an enum member into its string name, providing a simple way to serialize or represent enum values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 925, "body": "def parse(self, s):\n        values = [] if s == '' else s.split(self._sep)\n\n        for i, v in enumerate(values):\n            try:\n                values[i] = self._enum[v]\n            except KeyError:\n                raise ValueError('Invalid enum value \"{}\" index {} - could not be parsed'.format(v, i))\n\n        return tuple(values)", "is_method": true, "class_name": "EnumListParameter", "function_description": "Converts a string representation of enum names into a tuple of actual enum members. It validates each name against the defined enum, raising an error for invalid values."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 936, "body": "def serialize(self, enum_values):\n        return self._sep.join([e.name for e in enum_values])", "is_method": true, "class_name": "EnumListParameter", "function_description": "Converts a list of enum members into a single string by joining their names with a separator, suitable for storage or display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "default", "line_number": 945, "body": "def default(self, obj):\n        if isinstance(obj, FrozenOrderedDict):\n            return obj.get_wrapped()\n        return json.JSONEncoder.default(self, obj)", "is_method": true, "class_name": "_DictParamEncoder", "function_description": "Customizes JSON encoding to properly serialize `FrozenOrderedDict` objects. It unwraps these specialized dictionaries before default JSON serialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 984, "body": "def normalize(self, value):\n        \"\"\"\n        Ensure that dictionary parameter is converted to a FrozenOrderedDict so it can be hashed.\n        \"\"\"\n        return recursively_freeze(value)", "is_method": true, "class_name": "DictParameter", "function_description": "Converts dictionary parameters into a hashable `FrozenOrderedDict` for consistent use within the `DictParameter` class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 990, "body": "def parse(self, source):\n        \"\"\"\n        Parses an immutable and ordered ``dict`` from a JSON string using standard JSON library.\n\n        We need to use an immutable dictionary, to create a hashable parameter and also preserve the internal structure\n        of parsing. The traversal order of standard ``dict`` is undefined, which can result various string\n        representations of this parameter, and therefore a different task id for the task containing this parameter.\n        This is because task id contains the hash of parameters' JSON representation.\n\n        :param s: String to be parse\n        \"\"\"\n        # TOML based config convert params to python types itself.\n        if not isinstance(source, str):\n            return source\n        return json.loads(source, object_pairs_hook=FrozenOrderedDict)", "is_method": true, "class_name": "DictParameter", "function_description": "This method parses a JSON string into an immutable, ordered dictionary. This ensures the parameter is hashable and maintains a consistent string representation for generating unique task identifiers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 1006, "body": "def serialize(self, x):\n        return json.dumps(x, cls=_DictParamEncoder)", "is_method": true, "class_name": "DictParameter", "function_description": "Serializes Python dictionary parameters into a JSON string, enabling their storage, transmission, or logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 1041, "body": "def normalize(self, x):\n        \"\"\"\n        Ensure that struct is recursively converted to a tuple so it can be hashed.\n\n        :param str x: the value to parse.\n        :return: the normalized (hashable/immutable) value.\n        \"\"\"\n        return recursively_freeze(x)", "is_method": true, "class_name": "ListParameter", "function_description": "This method converts an input value into a hashable, immutable form. It recursively freezes mutable structures, enabling their use as dictionary keys or set elements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1050, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        i = json.loads(x, object_pairs_hook=FrozenOrderedDict)\n        if i is None:\n            return None\n        return list(i)", "is_method": true, "class_name": "ListParameter", "function_description": "This method of the `ListParameter` class converts a JSON string representation into a Python list. It enables parsing of individual string values into structured list objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "serialize", "line_number": 1062, "body": "def serialize(self, x):\n        \"\"\"\n        Opposite of :py:meth:`parse`.\n\n        Converts the value ``x`` to a string.\n\n        :param x: the value to serialize.\n        \"\"\"\n        return json.dumps(x, cls=_DictParamEncoder)", "is_method": true, "class_name": "ListParameter", "function_description": "Converts a Python object into its string representation using JSON, serving as the inverse operation of parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1102, "body": "def parse(self, x):\n        \"\"\"\n        Parse an individual value from the input.\n\n        :param str x: the value to parse.\n        :return: the parsed value.\n        \"\"\"\n        # Since the result of json.dumps(tuple) differs from a tuple string, we must handle either case.\n        # A tuple string may come from a config file or from cli execution.\n\n        # t = ((1, 2), (3, 4))\n        # t_str = '((1,2),(3,4))'\n        # t_json_str = json.dumps(t)\n        # t_json_str == '[[1, 2], [3, 4]]'\n        # json.loads(t_json_str) == t\n        # json.loads(t_str) == ValueError: No JSON object could be decoded\n\n        # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).\n        # ast.literal_eval(t_str) == t\n        try:\n            # loop required to parse tuple of tuples\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))", "is_method": true, "class_name": "TupleParameter", "function_description": "Parses a string input into a Python tuple. It handles both JSON-encoded list representations and direct string representations of tuples, useful for config files or CLI arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1194, "body": "def parse(self, s):\n        value = self._var_type(s)\n        if (self._left_op(self._min_value, value) and self._right_op(value, self._max_value)):\n            return value\n        else:\n            raise ValueError(\n                \"{s} is not in the set of {permitted_range}\".format(\n                    s=s, permitted_range=self._permitted_range))", "is_method": true, "class_name": "NumericalParameter", "function_description": "Provides a validated numerical representation of a string input. It ensures the parsed value adheres to the `NumericalParameter`'s defined range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "parse", "line_number": 1250, "body": "def parse(self, s):\n        var = self._var_type(s)\n        return self.normalize(var)", "is_method": true, "class_name": "ChoiceParameter", "function_description": "The `parse` method converts an input string into its specified variable type for a choice parameter. It then normalizes the value, ensuring it represents a valid choice."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "normalize", "line_number": 1254, "body": "def normalize(self, var):\n        if var in self._choices:\n            return var\n        else:\n            raise ValueError(\"{var} is not a valid choice from {choices}\".format(\n                var=var, choices=self._choices))", "is_method": true, "class_name": "ChoiceParameter", "function_description": "This method of `ChoiceParameter` validates if the provided variable is one of its predefined valid choices. It ensures input conformity, raising a ValueError for invalid options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "field", "line_number": 768, "body": "def field(key):\n            return r\"(?P<%s>\\d+)%s\" % (key, key[0].upper())", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Generates a regular expression pattern for a named capture group to extract a numeric time unit value. This pattern is a building block for parsing time delta strings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/parameter.py", "function": "optional_field", "line_number": 771, "body": "def optional_field(key):\n            return \"(%s)?\" % field(key)", "is_method": true, "class_name": "TimeDeltaParameter", "function_description": "Generates a string pattern that makes a specified field optional within a larger pattern. It wraps the base field's pattern with optional grouping syntax for parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_upgrade_schema", "line_number": 242, "body": "def _upgrade_schema(engine):\n    \"\"\"\n    Ensure the database schema is up to date with the codebase.\n\n    :param engine: SQLAlchemy engine of the underlying database.\n    \"\"\"\n    inspector = reflection.Inspector.from_engine(engine)\n    with engine.connect() as conn:\n\n        # Upgrade 1.  Add task_id column and index to tasks\n        if 'task_id' not in [x['name'] for x in inspector.get_columns('tasks')]:\n            logger.warning('Upgrading DbTaskHistory schema: Adding tasks.task_id')\n            conn.execute('ALTER TABLE tasks ADD COLUMN task_id VARCHAR(200)')\n            conn.execute('CREATE INDEX ix_task_id ON tasks (task_id)')\n\n        # Upgrade 2. Alter value column to be TEXT, note that this is idempotent so no if-guard\n        if 'mysql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY COLUMN value TEXT')\n        elif 'oracle' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY value TEXT')\n        elif 'mssql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TEXT')\n        elif 'postgresql' in engine.dialect.name:\n            if str([x for x in inspector.get_columns('task_parameters')\n                    if x['name'] == 'value'][0]['type']) != 'TEXT':\n                conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TYPE TEXT')\n        elif 'sqlite' in engine.dialect.name:\n            # SQLite does not support changing column types. A database file will need\n            # to be used to pickup this migration change.\n            for i in conn.execute('PRAGMA table_info(task_parameters);').fetchall():\n                if i['name'] == 'value' and i['type'] != 'TEXT':\n                    logger.warning(\n                        'SQLite can not change column types. Please use a new database '\n                        'to pickup column type changes.'\n                    )\n        else:\n            logger.warning(\n                'SQLAlcheny dialect {} could not be migrated to the TEXT type'.format(\n                    engine.dialect\n                )\n            )", "is_method": false, "function_description": "Ensures the database schema is up-to-date by applying necessary migrations, such as adding columns or modifying types, for various database systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_session", "line_number": 65, "body": "def _session(self, session=None):\n        if session:\n            yield session\n        else:\n            session = self.session_factory()\n            try:\n                yield session\n            except BaseException:\n                session.rollback()\n                raise\n            else:\n                session.commit()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Manages the lifecycle of a database session for `DbTaskHistory` operations, ensuring proper transaction commit or rollback."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_scheduled", "line_number": 88, "body": "def task_scheduled(self, task):\n        htask = self._get_task(task, status=PENDING)\n        self._add_task_event(htask, TaskEvent(event_name=PENDING, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Records a task as scheduled, marking it with a pending status. It logs an event to track the task's entry into the history system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_finished", "line_number": 92, "body": "def task_finished(self, task, successful):\n        event_name = DONE if successful else FAILED\n        htask = self._get_task(task, status=event_name)\n        self._add_task_event(htask, TaskEvent(event_name=event_name, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Records the final status of a task in the history, marking it as either successful or failed. It logs this completion event with a timestamp for historical tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "task_started", "line_number": 97, "body": "def task_started(self, task, worker_host):\n        htask = self._get_task(task, status=RUNNING, host=worker_host)\n        self._add_task_event(htask, TaskEvent(event_name=RUNNING, ts=datetime.datetime.now()))", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Logs that a specified task has begun execution on a worker host, updating its status to running in the task history."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_get_task", "line_number": 101, "body": "def _get_task(self, task, status, host=None):\n        if task.id in self.tasks:\n            htask = self.tasks[task.id]\n            htask.status = status\n            if host:\n                htask.host = host\n        else:\n            htask = self.tasks[task.id] = task_history.StoredTask(task, status, host)\n        return htask", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Manages the history of a task. It retrieves an existing record, updating its status and host, or creates a new one if it doesn't exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_add_task_event", "line_number": 111, "body": "def _add_task_event(self, task, event):\n        for (task_record, session) in self._find_or_create_task(task):\n            task_record.events.append(event)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Records a new event for a specific task within the database history. It ensures the task record exists, creating it if not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "_find_or_create_task", "line_number": 115, "body": "def _find_or_create_task(self, task):\n        with self._session() as session:\n            if task.record_id is not None:\n                logger.debug(\"Finding task with record_id [%d]\", task.record_id)\n                task_record = session.query(TaskRecord).get(task.record_id)\n                if not task_record:\n                    raise Exception(\"Task with record_id, but no matching Task record!\")\n                yield (task_record, session)\n            else:\n                task_record = TaskRecord(task_id=task._task.id, name=task.task_family, host=task.host)\n                for k, v in task.parameters.items():\n                    task_record.parameters[k] = TaskParameter(name=k, value=v)\n                session.add(task_record)\n                yield (task_record, session)\n            if task.host:\n                task_record.host = task.host\n        task.record_id = task_record.id", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Helper method that finds an existing task record or creates a new one in the database. It yields the record and session, ensuring task persistence and assigning its database ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_by_parameters", "line_number": 133, "body": "def find_all_by_parameters(self, task_name, session=None, **task_params):\n        \"\"\"\n        Find tasks with the given task_name and the same parameters as the kwargs.\n        \"\"\"\n        with self._session(session) as session:\n            query = session.query(TaskRecord).join(TaskEvent).filter(TaskRecord.name == task_name)\n            for k, v in task_params.items():\n                alias = sqlalchemy.orm.aliased(TaskParameter)\n                query = query.join(alias).filter(alias.name == k, alias.value == v)\n\n            tasks = query.order_by(TaskEvent.ts)\n            for task in tasks:\n                # Sanity check\n                assert all(k in task.parameters and v == str(task.parameters[k].value) for k, v in task_params.items())\n\n                yield task", "is_method": true, "class_name": "DbTaskHistory", "function_description": "This `DbTaskHistory` method retrieves all historical task records. It filters tasks by name and exact parameter values, enabling precise searches for past executions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_by_name", "line_number": 150, "body": "def find_all_by_name(self, task_name, session=None):\n        \"\"\"\n        Find all tasks with the given task_name.\n        \"\"\"\n        return self.find_all_by_parameters(task_name, session)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Retrieves all historical task records that match a specified task name from the database. It provides a way to query past occurrences of a particular task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_latest_runs", "line_number": 156, "body": "def find_latest_runs(self, session=None):\n        \"\"\"\n        Return tasks that have been updated in the past 24 hours.\n        \"\"\"\n        with self._session(session) as session:\n            yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n            return session.query(TaskRecord).\\\n                join(TaskEvent).\\\n                filter(TaskEvent.ts >= yesterday).\\\n                group_by(TaskRecord.id, TaskEvent.event_name, TaskEvent.ts).\\\n                order_by(TaskEvent.ts.desc()).\\\n                all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Retrieves task records that show activity or updates within the last 24 hours. It helps identify recently changed tasks for monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_runs", "line_number": 169, "body": "def find_all_runs(self, session=None):\n        \"\"\"\n        Return all tasks that have been updated.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Retrieves all historical task records from the database. It provides a complete list of past task executions managed by the DbTaskHistory system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_all_events", "line_number": 176, "body": "def find_all_events(self, session=None):\n        \"\"\"\n        Return all running/failed/done events.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskEvent).all()", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Retrieves all task events (running, failed, or done) from the database. It provides a complete history of task executions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "find_task_by_id", "line_number": 183, "body": "def find_task_by_id(self, id, session=None):\n        \"\"\"\n        Find task with the given record ID.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).get(id)", "is_method": true, "class_name": "DbTaskHistory", "function_description": "Retrieves a specific task record from the database using its unique ID. It serves as a lookup utility within the task history management system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 200, "body": "def __repr__(self):\n        return \"TaskParameter(task_id=%d, name=%s, value=%s)\" % (self.task_id, self.name, self.value)", "is_method": true, "class_name": "TaskParameter", "function_description": "Generates a developer-friendly string representation of a TaskParameter object, displaying its task ID, name, and value for debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 214, "body": "def __repr__(self):\n        return \"TaskEvent(task_id=%s, event_name=%s, ts=%s\" % (self.task_id, self.event_name, self.ts)", "is_method": true, "class_name": "TaskEvent", "function_description": "Provides a developer-friendly string representation of a `TaskEvent` object. This string includes the task ID, event name, and timestamp, aiding debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/db_task_history.py", "function": "__repr__", "line_number": 238, "body": "def __repr__(self):\n        return \"TaskRecord(name=%s, host=%s)\" % (self.name, self.host)", "is_method": true, "class_name": "TaskRecord", "function_description": "Provides a clear, unambiguous string representation of the TaskRecord object. This is useful for debugging and development."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline.py", "function": "luigi_run", "line_number": 8, "body": "def luigi_run(argv=sys.argv[1:]):\n    run_with_retcodes(argv)", "is_method": false, "function_description": "Serves as the main entry point to execute Luigi tasks from the command line, processing input arguments for workflow execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline.py", "function": "luigid", "line_number": 12, "body": "def luigid(argv=sys.argv[1:]):\n    import luigi.server\n    import luigi.process\n    import luigi.configuration\n    parser = argparse.ArgumentParser(description=u'Central luigi server')\n    parser.add_argument(u'--background', help=u'Run in background mode', action='store_true')\n    parser.add_argument(u'--pidfile', help=u'Write pidfile')\n    parser.add_argument(u'--logdir', help=u'log directory')\n    parser.add_argument(u'--state-path', help=u'Pickled state file')\n    parser.add_argument(u'--address', help=u'Listening interface')\n    parser.add_argument(u'--unix-socket', help=u'Unix socket path')\n    parser.add_argument(u'--port', default=8082, help=u'Listening port')\n\n    opts = parser.parse_args(argv)\n\n    if opts.state_path:\n        config = luigi.configuration.get_config()\n        config.set('scheduler', 'state_path', opts.state_path)\n\n    DaemonLogging.setup(opts)\n    if opts.background:\n        luigi.process.daemonize(luigi.server.run, api_port=opts.port,\n                                address=opts.address, pidfile=opts.pidfile,\n                                logdir=opts.logdir, unix_socket=opts.unix_socket)\n    else:\n        luigi.server.run(api_port=opts.port, address=opts.address, unix_socket=opts.unix_socket)", "is_method": false, "function_description": "Initiates and manages the Luigi central scheduler server. It provides options to run as a background daemon and configure network settings via command-line arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move_to_final_destination", "line_number": 39, "body": "def move_to_final_destination(self):\n        os.rename(self.tmp_path, self.path)", "is_method": true, "class_name": "atomic_file", "function_description": "Finalizes an atomic file write by moving the temporary file to its permanent destination. This ensures data integrity and consistency during file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "generate_tmp_path", "line_number": 42, "body": "def generate_tmp_path(self, path):\n        return path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)", "is_method": true, "class_name": "atomic_file", "function_description": "This method generates a unique temporary file path by appending a random suffix to a given path. It provides a secure intermediate path for atomic file operations, ensuring data integrity during writes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "copy", "line_number": 53, "body": "def copy(self, old_path, new_path, raise_if_exists=False):\n        if raise_if_exists and os.path.exists(new_path):\n            raise RuntimeError('Destination exists: %s' % new_path)\n        d = os.path.dirname(new_path)\n        if d and not os.path.exists(d):\n            self.mkdir(d)\n        shutil.copy(old_path, new_path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Copies a file from an old path to a new path on the local file system. It ensures the destination directory exists and can prevent overwriting existing files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "exists", "line_number": 61, "body": "def exists(self, path):\n        return os.path.exists(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Verifies if a given file or directory path exists on the local filesystem. This capability is essential for managing file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "mkdir", "line_number": 64, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise FileAlreadyExists()\n            elif not self.isdir(path):\n                raise NotADirectory()\n            else:\n                return\n\n        if parents:\n            try:\n                os.makedirs(path)\n            except OSError as err:\n                # somebody already created the path\n                if err.errno != errno.EEXIST:\n                    raise\n        else:\n            if not os.path.exists(os.path.dirname(path)):\n                raise MissingParentDirectory()\n            os.mkdir(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Creates a new directory on the local file system at the given path. It can optionally create parent directories and handles cases where the directory already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "isdir", "line_number": 85, "body": "def isdir(self, path):\n        return os.path.isdir(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Checks if a given path on the local file system points to an existing directory. This provides a fundamental file system path validation service."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "listdir", "line_number": 88, "body": "def listdir(self, path):\n        for dir_, _, files in os.walk(path):\n            assert dir_.startswith(path)\n            for name in files:\n                yield os.path.join(dir_, name)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Provides a recursive generator that yields the full path of every file located within a specified directory and its subdirectories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "remove", "line_number": 94, "body": "def remove(self, path, recursive=True):\n        if recursive and self.isdir(path):\n            shutil.rmtree(path)\n        else:\n            os.remove(path)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Removes a file or directory from the local file system. It can recursively delete entire directories if specified."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move", "line_number": 100, "body": "def move(self, old_path, new_path, raise_if_exists=False):\n        \"\"\"\n        Move file atomically. If source and destination are located\n        on different filesystems, atomicity is approximated\n        but cannot be guaranteed.\n        \"\"\"\n        if raise_if_exists and os.path.exists(new_path):\n            raise FileAlreadyExists('Destination exists: %s' % new_path)\n        d = os.path.dirname(new_path)\n        if d and not os.path.exists(d):\n            self.mkdir(d)\n        try:\n            os.rename(old_path, new_path)\n        except OSError as err:\n            if err.errno == errno.EXDEV:\n                new_path_tmp = '%s-%09d' % (new_path, random.randint(0, 999999999))\n                shutil.copy(old_path, new_path_tmp)\n                os.rename(new_path_tmp, new_path)\n                os.remove(old_path)\n            else:\n                raise err", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Moves a file from a source path to a destination on the local file system, attempting an atomic operation. It handles cross-filesystem transfers and can prevent overwrites."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "rename_dont_move", "line_number": 122, "body": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Rename ``path`` to ``dest``, but don't move it into the ``dest``\n        folder (if it is a folder). This method is just a wrapper around the\n        ``move`` method of LocalTarget.\n        \"\"\"\n        self.move(path, dest, raise_if_exists=True)", "is_method": true, "class_name": "LocalFileSystem", "function_description": "Renames a specified file or directory on the local file system. It ensures the destination is a new name, not a target directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "makedirs", "line_number": 146, "body": "def makedirs(self):\n        \"\"\"\n        Create all parent folders if they do not exist.\n        \"\"\"\n        normpath = os.path.normpath(self.path)\n        parentfolder = os.path.dirname(normpath)\n        if parentfolder:\n            try:\n                os.makedirs(parentfolder)\n            except OSError:\n                pass", "is_method": true, "class_name": "LocalTarget", "function_description": "Creates all necessary parent directories for the local target's path if they do not already exist. This ensures the file system is prepared for writing or saving operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "open", "line_number": 158, "body": "def open(self, mode='r'):\n        rwmode = mode.replace('b', '').replace('t', '')\n        if rwmode == 'w':\n            self.makedirs()\n            return self.format.pipe_writer(atomic_file(self.path))\n\n        elif rwmode == 'r':\n            fileobj = FileWrapper(io.BufferedReader(io.FileIO(self.path, mode)))\n            return self.format.pipe_reader(fileobj)\n\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "LocalTarget", "function_description": "Provides a robust file-like object to read from or write to the local path. It handles directory creation and ensures atomic write operations for data integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move", "line_number": 171, "body": "def move(self, new_path, raise_if_exists=False):\n        self.fs.move(self.path, new_path, raise_if_exists=raise_if_exists)", "is_method": true, "class_name": "LocalTarget", "function_description": "Enables relocation of the file or directory represented by this object to a new path on the local file system, with an option to prevent overwriting existing targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "move_dir", "line_number": 174, "body": "def move_dir(self, new_path):\n        self.move(new_path)", "is_method": true, "class_name": "LocalTarget", "function_description": "Provides the capability to relocate the directory managed by this LocalTarget instance to a specified new path on the local filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "remove", "line_number": 177, "body": "def remove(self):\n        self.fs.remove(self.path)", "is_method": true, "class_name": "LocalTarget", "function_description": "Provides the capability to remove the file or directory represented by this LocalTarget instance from the local filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "copy", "line_number": 180, "body": "def copy(self, new_path, raise_if_exists=False):\n        self.fs.copy(self.path, new_path, raise_if_exists)", "is_method": true, "class_name": "LocalTarget", "function_description": "Provides the capability to duplicate the file or directory represented by this LocalTarget instance to a new path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "fn", "line_number": 184, "body": "def fn(self):\n        warnings.warn(\"Use LocalTarget.path to reference filename\", DeprecationWarning, stacklevel=2)\n        return self.path", "is_method": true, "class_name": "LocalTarget", "function_description": "Provides the local target's file path, though this method is deprecated. Users are advised to directly access the `path` attribute instead."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/local_target.py", "function": "__del__", "line_number": 188, "body": "def __del__(self):\n        if self.is_tmp and self.exists():\n            self.remove()", "is_method": true, "class_name": "LocalTarget", "function_description": "This destructor automatically removes temporary local files or directories associated with the `LocalTarget` object upon its destruction, ensuring clean resource management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "get_instance", "line_number": 38, "body": "def get_instance(cls):\n        \"\"\" Singleton getter \"\"\"\n        return cls._instance", "is_method": true, "class_name": "CmdlineParser", "function_description": "Provides the single, globally accessible instance of the `CmdlineParser` class, enforcing the singleton pattern for consistent access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "global_instance", "line_number": 44, "body": "def global_instance(cls, cmdline_args, allow_override=False):\n        \"\"\"\n        Meant to be used as a context manager.\n        \"\"\"\n        orig_value = cls._instance\n        assert (orig_value is None) or allow_override\n        new_value = None\n        try:\n            new_value = CmdlineParser(cmdline_args)\n            cls._instance = new_value\n            yield new_value\n        finally:\n            assert cls._instance is new_value\n            cls._instance = orig_value", "is_method": true, "class_name": "CmdlineParser", "function_description": "Provides a context manager to temporarily set and manage the global `CmdlineParser` instance. It allows scoped configuration, ensuring the original instance is restored automatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_build_parser", "line_number": 81, "body": "def _build_parser(root_task=None, help_all=False):\n        parser = argparse.ArgumentParser(add_help=False)\n\n        # Unfortunately, we have to set it as optional to argparse, so we can\n        # parse out stuff like `--module` before we call for `--help`.\n        parser.add_argument('root_task',\n                            nargs='?',\n                            help='Task family to run. Is not optional.',\n                            metavar='Required root task',\n                            )\n\n        for task_name, is_without_section, param_name, param_obj in Register.get_all_params():\n            is_the_root_task = task_name == root_task\n            help = param_obj.description if any((is_the_root_task, help_all, param_obj.always_in_help)) else argparse.SUPPRESS\n            flag_name_underscores = param_name if is_without_section else task_name + '_' + param_name\n            global_flag_name = '--' + flag_name_underscores.replace('_', '-')\n            parser.add_argument(global_flag_name,\n                                help=help,\n                                **param_obj._parser_kwargs(param_name, task_name)\n                                )\n            if is_the_root_task:\n                local_flag_name = '--' + param_name.replace('_', '-')\n                parser.add_argument(local_flag_name,\n                                    help=help,\n                                    **param_obj._parser_kwargs(param_name)\n                                    )\n\n        return parser", "is_method": true, "class_name": "CmdlineParser", "function_description": "This internal method of `CmdlineParser` dynamically builds and configures an `argparse.ArgumentParser` instance. It registers a root task and various global and local command-line arguments from a central registry, preparing it for parsing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "get_task_obj", "line_number": 110, "body": "def get_task_obj(self):\n        \"\"\"\n        Get the task object\n        \"\"\"\n        return self._get_task_cls()(**self._get_task_kwargs())", "is_method": true, "class_name": "CmdlineParser", "function_description": "This method instantiates and returns a task object, dynamically configured from command-line arguments processed by the parser. It provides a ready-to-use object for subsequent operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_get_task_cls", "line_number": 116, "body": "def _get_task_cls(self):\n        \"\"\"\n        Get the task class\n        \"\"\"\n        return Register.get_task_cls(self.known_args.root_task)", "is_method": true, "class_name": "CmdlineParser", "function_description": "Retrieves the appropriate task class based on parsed command-line arguments. This enables the command-line parser to dynamically identify and prepare the correct task for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_get_task_kwargs", "line_number": 122, "body": "def _get_task_kwargs(self):\n        \"\"\"\n        Get the local task arguments as a dictionary. The return value is in\n        the form ``dict(my_param='my_value', ...)``\n        \"\"\"\n        res = {}\n        for (param_name, param_obj) in self._get_task_cls().get_params():\n            attr = getattr(self.known_args, param_name)\n            if attr:\n                res.update(((param_name, param_obj.parse(attr)),))\n\n        return res", "is_method": true, "class_name": "CmdlineParser", "function_description": "This CmdlineParser method extracts and parses task-specific arguments from command-line input. It returns a dictionary suitable for configuring or executing a task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_attempt_load_module", "line_number": 136, "body": "def _attempt_load_module(known_args):\n        \"\"\"\n        Load the --module parameter\n        \"\"\"\n        module = known_args.core_module\n        if module:\n            __import__(module)", "is_method": true, "class_name": "CmdlineParser", "function_description": "Loads a Python module specified by command-line arguments. It allows dynamic extension of the application's capabilities."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/cmdline_parser.py", "function": "_possibly_exit_with_help", "line_number": 145, "body": "def _possibly_exit_with_help(parser, known_args):\n        \"\"\"\n        Check if the user passed --help[-all], if so, print a message and exit.\n        \"\"\"\n        if known_args.core_help or known_args.core_help_all:\n            parser.print_help()\n            sys.exit()", "is_method": true, "class_name": "CmdlineParser", "function_description": "Allows the command-line parser to display help information and exit the application when a help flag is provided by the user."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/metrics.py", "function": "get", "line_number": 13, "body": "def get(cls, which):\n        if which == MetricsCollectors.none:\n            return NoMetricsCollector()\n        elif which == MetricsCollectors.datadog:\n            from luigi.contrib.datadog_metric import DatadogMetricsCollector\n            return DatadogMetricsCollector()\n        elif which == MetricsCollectors.prometheus:\n            from luigi.contrib.prometheus_metric import PrometheusMetricsCollector\n            return PrometheusMetricsCollector()\n        else:\n            raise ValueError(\"MetricsCollectors value ' {0} ' isn't supported\", which)", "is_method": true, "class_name": "MetricsCollectors", "function_description": "It serves as a factory method for retrieving appropriate metrics collector objects for various backend systems, centralizing their instantiation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "get_default_format", "line_number": 514, "body": "def get_default_format():\n    return Text", "is_method": false, "function_description": "Returns the system's default text format. It provides a standard format for operations requiring one."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 36, "body": "def __getattr__(self, name):\n        # forward calls to 'write', 'close' and other methods not defined below\n        return getattr(self._subpipe, name)", "is_method": true, "class_name": "FileWrapper", "function_description": "This method enables the `FileWrapper` to act as a proxy, transparently forwarding attribute and method calls to its underlying `_subpipe` object. It allows the wrapper to mimic the behavior of the wrapped object for operations like writing or closing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 40, "body": "def __enter__(self, *args, **kwargs):\n        # instead of returning whatever is returned by __enter__ on the subpipe\n        # this returns self, so whatever custom injected methods are still available\n        # this might cause problems with custom file_objects, but seems to work\n        # fine with standard python `file` objects which is the only default use\n        return self", "is_method": true, "class_name": "FileWrapper", "function_description": "Enables the `FileWrapper` instance to be used as a context manager in `with` statements. It returns `self` to ensure any custom injected methods remain accessible within the context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 47, "body": "def __exit__(self, *args, **kwargs):\n        return self._subpipe.__exit__(*args, **kwargs)", "is_method": true, "class_name": "FileWrapper", "function_description": "Enables `FileWrapper` to be used as a context manager, ensuring the proper cleanup of its encapsulated underlying resource when exiting a `with` statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 50, "body": "def __iter__(self):\n        return iter(self._subpipe)", "is_method": true, "class_name": "FileWrapper", "function_description": "This method makes instances of `FileWrapper` iterable. It allows the `FileWrapper` object to be used directly in `for` loops by delegating iteration to its internal `_subpipe`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "create_subprocess", "line_number": 91, "body": "def create_subprocess(self, command):\n        \"\"\"\n        http://www.chiark.greenend.org.uk/ucgi/~cjwatson/blosxom/2009-07-02-python-sigpipe.html\n        \"\"\"\n\n        def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n\n        return subprocess.Popen(command,\n                                stdin=self._input_pipe,\n                                stdout=subprocess.PIPE,\n                                preexec_fn=subprocess_setup,\n                                close_fds=True)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Launches a new external subprocess, connecting its input to a managed pipe and capturing its output. It configures SIGPIPE for robust interaction with external programs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_finish", "line_number": 107, "body": "def _finish(self):\n        # Need to close this before input_pipe to get all SIGPIPE messages correctly\n        self._process.stdout.close()\n        if not self._original_input and os.path.exists(self._tmp_file):\n            os.remove(self._tmp_file)\n\n        if self._input_pipe is not None:\n            self._input_pipe.close()\n\n        self._process.wait()  # deadlock?\n        if self._process.returncode not in (0, 141, 128 - 141):\n            # 141 == 128 + 13 == 128 + SIGPIPE - normally processes exit with 128 + {reiceived SIG}\n            # 128 - 141 == -13 == -SIGPIPE, sometimes python receives -13 for some subprocesses\n            raise RuntimeError('Error reading from pipe. Subcommand exited with non-zero exit status %s.' % self._process.returncode)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "This method finalizes the interaction with a wrapped subprocess. It cleans up associated resources, closes pipes, waits for the process to complete, and validates its exit code, raising an error for unexpected termination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "close", "line_number": 122, "body": "def close(self):\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Initiates the shutdown and cleanup procedure for the wrapped input pipe process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 125, "body": "def __del__(self):\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Ensures proper resource cleanup by calling `_finish()` when an `InputPipeProcessWrapper` object is destroyed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 128, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Enables the `InputPipeProcessWrapper` instance to be used as a context manager. It returns the instance itself upon entry to a `with` block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_abort", "line_number": 131, "body": "def _abort(self):\n        \"\"\"\n        Call _finish, but eat the exception (if any).\n        \"\"\"\n        try:\n            self._finish()\n        except KeyboardInterrupt:\n            raise\n        except BaseException:\n            pass", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "This method attempts to finalize the process. It silently handles most exceptions during finalization, ensuring the process can terminate without propagating internal errors, except for user interruptions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 142, "body": "def __exit__(self, type, value, traceback):\n        if type:\n            self._abort()\n        else:\n            self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "This method manages the termination of the `InputPipeProcessWrapper` within a `with` statement, ensuring proper cleanup by calling `_abort()` on error or `_finish()` on successful completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 148, "body": "def __getattr__(self, name):\n        if name in ['_process', '_input_pipe']:\n            raise AttributeError(name)\n        try:\n            return getattr(self._process.stdout, name)\n        except AttributeError:\n            return getattr(self._input_pipe, name)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Enables `InputPipeProcessWrapper` to delegate attribute access. It first proxies requests to the wrapped process's standard output, then falls back to its input pipe for seamless interaction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 156, "body": "def __iter__(self):\n        for line in self._process.stdout:\n            yield line\n        self._finish()", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Enables direct iteration over the wrapped process's standard output, providing lines one by one. It ensures proper finalization after all output is consumed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "readable", "line_number": 161, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Indicates that the wrapped input pipe is always in a readable state, signaling its availability for data consumption."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writable", "line_number": 164, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Indicates that this input pipe process wrapper is not writable, signifying it's designed only for reading data. It provides a flag for external components to query its write capability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "seekable", "line_number": 167, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Reports that the wrapped input pipe is not seekable. This signals that the pipe only supports sequential reading, not random access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 184, "body": "def write(self, *args, **kwargs):\n        self._process.stdin.write(*args, **kwargs)\n        self._flushcount += 1\n        if self._flushcount == self.WRITES_BEFORE_FLUSH:\n            self._process.stdin.flush()\n            self._flushcount = 0", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Provides a buffered mechanism to write data to the standard input of an encapsulated child process, flushing periodically for efficient communication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writeLine", "line_number": 191, "body": "def writeLine(self, line):\n        assert '\\n' not in line\n        self.write(line + '\\n')", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Writes a given string as a complete line of text to the output pipe. It automatically appends a newline character for proper line termination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_finish", "line_number": 195, "body": "def _finish(self):\n        \"\"\"\n        Closes and waits for subprocess to exit.\n        \"\"\"\n        if self._process.returncode is None:\n            self._process.stdin.flush()\n            self._process.stdin.close()\n            self._process.wait()\n            self.closed = True", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Ensures the wrapped subprocess cleanly exits. It flushes and closes the subprocess's input stream, then waits for its termination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 205, "body": "def __del__(self):\n        if not self.closed:\n            self.abort()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "This destructor method automatically aborts the wrapped process if the output pipe was not explicitly closed, ensuring resource cleanup upon object deletion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 209, "body": "def __exit__(self, type, value, traceback):\n        if type is None:\n            self.close()\n        else:\n            self.abort()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "This context manager method ensures proper resource cleanup when exiting a `with` statement. It calls `close()` on normal completion or `abort()` if an exception occurred.\nThis context manager method ensures proper resource cleanup when exiting a `with` statement. It calls `close()` on normal completion or `abort()` if an exception occurred."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 215, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Enables the `OutputPipeProcessWrapper` object to be used as a context manager, providing the instance itself for operations within a `with` statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "close", "line_number": 218, "body": "def close(self):\n        self._finish()\n        if self._process.returncode == 0:\n            if self._output_pipe is not None:\n                self._output_pipe.close()\n        else:\n            raise RuntimeError('Error when executing command %s' % self._command)", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Finalizes the wrapped process and closes its output pipe upon successful completion. It raises an error if the managed process terminated with a non-zero exit code."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "abort", "line_number": 226, "body": "def abort(self):\n        self._finish()", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Terminates the process managed by this wrapper, ensuring any necessary cleanup or finalization routines are executed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 229, "body": "def __getattr__(self, name):\n        if name in ['_process', '_output_pipe']:\n            raise AttributeError(name)\n        try:\n            return getattr(self._process.stdin, name)\n        except AttributeError:\n            return getattr(self._output_pipe, name)", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Delegates attribute access to the wrapped process's standard input or its output pipe. This provides a unified interface for interacting with the process's I/O streams."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "readable", "line_number": 237, "body": "def readable(self):\n        return False", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Signals that the wrapped output pipe is not designed for reading. This method consistently returns False."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writable", "line_number": 240, "body": "def writable(self):\n        return True", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Indicates that the wrapped output pipe is always writable, signaling its capability for write operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "seekable", "line_number": 243, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "OutputPipeProcessWrapper", "function_description": "Declares that the wrapped output stream is not seekable, meaning it does not support random access operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 256, "body": "def __getattr__(self, name):\n        if name == '_stream':\n            raise AttributeError(name)\n        return getattr(self._stream, name)", "is_method": true, "class_name": "BaseWrapper", "function_description": "Enables the `BaseWrapper` to act as a proxy. It delegates attribute lookups to its internal `_stream` object, excluding direct `_stream` access.\nEnables the `BaseWrapper` to act as a proxy. It delegates attribute lookups to its internal `_stream` object, excluding direct `_stream` access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 261, "body": "def __enter__(self):\n        self._stream.__enter__()\n        return self", "is_method": true, "class_name": "BaseWrapper", "function_description": "Enables `BaseWrapper` instances to function as context managers, proxying the `__enter__` call to its wrapped stream object for proper resource setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 265, "body": "def __exit__(self, *args):\n        self._stream.__exit__(*args)", "is_method": true, "class_name": "BaseWrapper", "function_description": "This method enables `BaseWrapper` to function as a context manager. It ensures proper resource cleanup by delegating the exit call to the wrapped `_stream` object upon exiting a `with` statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__iter__", "line_number": 268, "body": "def __iter__(self):\n        try:\n            for line in self._stream:\n                yield line\n        finally:\n            self.close()", "is_method": true, "class_name": "BaseWrapper", "function_description": "Enables line-by-line iteration over the wrapped stream. It ensures the underlying stream is automatically closed upon completion or interruption of iteration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "read", "line_number": 288, "body": "def read(self, n=-1):\n        b = self._stream.read(n)\n\n        if self.newline == b'':\n            return b\n\n        if self.newline is None:\n            newline = b'\\n'\n\n        return re.sub(b'(\\n|\\r\\n|\\r)', newline, b)", "is_method": true, "class_name": "NewlineWrapper", "function_description": "Reads bytes from an underlying stream, converting various newline character sequences into a single, specified format. This standardizes newline representation for consistent data processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writelines", "line_number": 299, "body": "def writelines(self, lines):\n        if self.newline is None or self.newline == '':\n            newline = os.linesep.encode('ascii')\n        else:\n            newline = self.newline\n\n        self._stream.writelines(\n            (re.sub(b'(\\n|\\r\\n|\\r)', newline, line) for line in lines)\n        )", "is_method": true, "class_name": "NewlineWrapper", "function_description": "Writes a list of lines to an underlying stream. It standardizes all newline characters within each line to a configured or OS-default newline sequence before writing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 309, "body": "def write(self, b):\n        if self.newline is None or self.newline == '':\n            newline = os.linesep.encode('ascii')\n        else:\n            newline = self.newline\n\n        self._stream.write(re.sub(b'(\\n|\\r\\n|\\r)', newline, b))", "is_method": true, "class_name": "NewlineWrapper", "function_description": "This method writes byte data to an underlying stream, ensuring all newline characters are standardized to a consistent, configured sequence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "write", "line_number": 328, "body": "def write(self, b):\n        self._stream.write(self._convert(b))", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Writes data to the wrapped stream, transparently converting mixed Unicode and byte inputs into a consistent format for output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "writelines", "line_number": 331, "body": "def writelines(self, lines):\n        self._stream.writelines((self._convert(line) for line in lines))", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Writes multiple lines to the wrapped stream, automatically converting each line to handle mixed Unicode and byte content consistently."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "_convert", "line_number": 334, "body": "def _convert(self, b):\n        if isinstance(b, str):\n            b = b.encode(self.encoding)\n            warnings.warn('Writing unicode to byte stream', stacklevel=2)\n        return b", "is_method": true, "class_name": "MixedUnicodeBytesWrapper", "function_description": "Converts Unicode strings to bytes using the wrapper's encoding, issuing a warning. This ensures all input is in byte format, handling mixed Unicode and byte streams."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__rshift__", "line_number": 354, "body": "def __rshift__(a, b):\n        return ChainFormat(a, b)", "is_method": true, "class_name": "Format", "function_description": "Facilitates the creation of a `ChainFormat` by combining two format elements using the right shift operator."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 387, "body": "def pipe_reader(self, input_pipe):\n        for x in reversed(self.args):\n            input_pipe = x.pipe_reader(input_pipe)\n        return input_pipe", "is_method": true, "class_name": "ChainFormat", "function_description": "Orchestrates a processing pipeline by chaining `pipe_reader` operations from its components. It transforms an input pipe through a sequence of these chained operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 392, "body": "def pipe_writer(self, output_pipe):\n        for x in reversed(self.args):\n            output_pipe = x.pipe_writer(output_pipe)\n        return output_pipe", "is_method": true, "class_name": "ChainFormat", "function_description": "Constructs a composite output pipeline by recursively chaining `pipe_writer` methods from its arguments in reverse order. This prepares a sequence of operations for data flow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__exit__", "line_number": 400, "body": "def __exit__(self, *args):\n        # io.TextIOWrapper close the file on __exit__, let the underlying file decide\n        if not self.closed and self.writable():\n            super(TextWrapper, self).flush()\n\n        self._stream.__exit__(*args)", "is_method": true, "class_name": "TextWrapper", "function_description": "This method handles the cleanup when exiting a `with` statement. It flushes any buffered data and delegates the exit responsibility to the underlying stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__del__", "line_number": 407, "body": "def __del__(self, *args):\n        # io.TextIOWrapper close the file on __del__, let the underlying file decide\n        if not self.closed and self.writable():\n            super(TextWrapper, self).flush()\n\n        try:\n            self._stream.__del__(*args)\n        except AttributeError:\n            pass", "is_method": true, "class_name": "TextWrapper", "function_description": "This method ensures proper finalization and resource cleanup of the underlying text stream when the `TextWrapper` object is destroyed. It delegates the responsibility for closing the stream to the wrapped object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__getattr__", "line_number": 424, "body": "def __getattr__(self, name):\n        if name == '_stream':\n            raise AttributeError(name)\n        return getattr(self._stream, name)", "is_method": true, "class_name": "TextWrapper", "function_description": "Provides attribute delegation for `TextWrapper` instances, allowing them to act as proxies for an internal `_stream` object. It prevents direct external access to the `_stream` attribute itself."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "__enter__", "line_number": 429, "body": "def __enter__(self):\n        self._stream.__enter__()\n        return self", "is_method": true, "class_name": "TextWrapper", "function_description": "Prepares the TextWrapper object and its underlying stream for use within a 'with' statement. It enables the TextWrapper to act as a context manager."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 435, "body": "def pipe_reader(self, input_pipe):\n        return input_pipe", "is_method": true, "class_name": "NopFormat", "function_description": "This method of NopFormat simply returns the input pipe without any modification. It acts as a pass-through or identity function."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 448, "body": "def pipe_reader(self, input_pipe):\n        return self.wrapper_cls(input_pipe, *self.args, **self.kwargs)", "is_method": true, "class_name": "WrappedFormat", "function_description": "Instantiates the `WrappedFormat`'s designated wrapper class with an input pipe and stored arguments. It provides a convenient factory for pipe-driven object creation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 451, "body": "def pipe_writer(self, output_pipe):\n        return self.wrapper_cls(output_pipe, *self.args, **self.kwargs)", "is_method": true, "class_name": "WrappedFormat", "function_description": "Creates and returns an instance of a pre-configured writer, tailored to a specific output pipe. This facilitates applying a consistent format to data streams."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 483, "body": "def pipe_reader(self, input_pipe):\n        return InputPipeProcessWrapper(['gunzip'], input_pipe)", "is_method": true, "class_name": "GzipFormat", "function_description": "Provides a pipe reader that transparently decompresses gzipped data using the 'gunzip' command from the given input pipe. This allows other functions to consume uncompressed data directly.\nProvides a pipe reader that transparently decompresses gzipped data using the 'gunzip' command from the given input pipe. This allows other functions to consume uncompressed data directly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 486, "body": "def pipe_writer(self, output_pipe):\n        args = ['gzip']\n        if self.compression_level is not None:\n            args.append('-' + str(int(self.compression_level)))\n        return OutputPipeProcessWrapper(args, output_pipe)", "is_method": true, "class_name": "GzipFormat", "function_description": "This method provides a pipe writer that automatically compresses data using `gzip` at a specified compression level. It enables writing compressed data directly to an output stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_reader", "line_number": 498, "body": "def pipe_reader(self, input_pipe):\n        return InputPipeProcessWrapper(['bzcat'], input_pipe)", "is_method": true, "class_name": "Bzip2Format", "function_description": "This method creates a process wrapper to decompress Bzip2 data from an input pipe using the `bzcat` command. It enables reading compressed streams as uncompressed data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "pipe_writer", "line_number": 501, "body": "def pipe_writer(self, output_pipe):\n        return OutputPipeProcessWrapper(['bzip2'], output_pipe)", "is_method": true, "class_name": "Bzip2Format", "function_description": "Provides a specialized writer that pipes uncompressed data through an external bzip2 compression process. This enables direct output of bzip2 compressed data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/format.py", "function": "subprocess_setup", "line_number": 96, "body": "def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)", "is_method": true, "class_name": "InputPipeProcessWrapper", "function_description": "Ensures subprocesses handle the SIGPIPE signal correctly by restoring its default behavior. This facilitates reliable inter-process communication within the pipe processing system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "getpcmd", "line_number": 30, "body": "def getpcmd(pid):\n    \"\"\"\n    Returns command of process.\n\n    :param pid:\n    \"\"\"\n    if os.name == \"nt\":\n        # Use wmic command instead of ps on Windows.\n        cmd = 'wmic path win32_process where ProcessID=%s get Commandline 2> nul' % (pid, )\n        with os.popen(cmd, 'r') as p:\n            lines = [line for line in p.readlines() if line.strip(\"\\r\\n \") != \"\"]\n            if lines:\n                _, val = lines\n                return val\n    elif sys.platform == \"darwin\":\n        # Use pgrep instead of /proc on macOS.\n        pidfile = \".%d.pid\" % (pid, )\n        with open(pidfile, 'w') as f:\n            f.write(str(pid))\n        try:\n            p = Popen(['pgrep', '-lf', '-F', pidfile], stdout=PIPE)\n            stdout, _ = p.communicate()\n            line = stdout.decode('utf8').strip()\n            if line:\n                _, scmd = line.split(' ', 1)\n                return scmd\n        finally:\n            os.unlink(pidfile)\n    else:\n        # Use the /proc filesystem\n        # At least on android there have been some issues with not all\n        # process infos being readable. In these cases using the `ps` command\n        # worked. See the pull request at\n        # https://github.com/spotify/luigi/pull/1876\n        try:\n            with open('/proc/{0}/cmdline'.format(pid), 'r') as fh:\n                return fh.read().replace('\\0', ' ').rstrip()\n        except IOError:\n            # the system may not allow reading the command line\n            # of a process owned by another user\n            pass\n\n    # Fallback instead of None, for e.g. Cygwin where -o is an \"unknown option\" for the ps command:\n    return '[PROCESS_WITH_PID={}]'.format(pid)", "is_method": false, "function_description": "Retrieves the full command line used to launch a process given its process ID. It provides robust, OS-specific implementations for Windows, macOS, and Linux systems."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "get_info", "line_number": 76, "body": "def get_info(pid_dir, my_pid=None):\n    # Check the name and pid of this process\n    if my_pid is None:\n        my_pid = os.getpid()\n\n    my_cmd = getpcmd(my_pid)\n    cmd_hash = my_cmd.encode('utf8')\n    pid_file = os.path.join(pid_dir, hashlib.md5(cmd_hash).hexdigest()) + '.pid'\n\n    return my_pid, my_cmd, pid_file", "is_method": false, "function_description": "Retrieves a process's ID and command line, and generates a unique PID file path based on its command. This aids in process management and tracking active instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "acquire_for", "line_number": 88, "body": "def acquire_for(pid_dir, num_available=1, kill_signal=None):\n    \"\"\"\n    Makes sure the process is only run once at the same time with the same name.\n\n    Notice that we since we check the process name, different parameters to the same\n    command can spawn multiple processes at the same time, i.e. running\n    \"/usr/bin/my_process\" does not prevent anyone from launching\n    \"/usr/bin/my_process --foo bar\".\n    \"\"\"\n\n    my_pid, my_cmd, pid_file = get_info(pid_dir)\n\n    # Create a pid file if it does not exist\n    try:\n        os.mkdir(pid_dir)\n        os.chmod(pid_dir, 0o777)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise\n        pass\n\n    # Let variable \"pids\" be all pids who exist in the .pid-file who are still\n    # about running the same command.\n    pids = {pid for pid in _read_pids_file(pid_file) if getpcmd(pid) == my_cmd}\n\n    if kill_signal is not None:\n        for pid in pids:\n            os.kill(pid, kill_signal)\n        print('Sent kill signal to Pids: {}'.format(pids))\n        # We allow for the killer to progress, yet we don't want these to stack\n        # up! So we only allow it once.\n        num_available += 1\n\n    if len(pids) >= num_available:\n        # We are already running under a different pid\n        print('Pid(s) {} already running'.format(pids))\n        if kill_signal is not None:\n            print('Note: There have (probably) been 1 other \"--take-lock\"'\n                  ' process which continued to run! Probably no need to run'\n                  ' this one as well.')\n        return False\n\n    _write_pids_file(pid_file, pids | {my_pid})\n\n    return True", "is_method": false, "function_description": "Provides a basic process lock, ensuring only a specified number of instances of a command-identified process run concurrently. It supports optional termination of existing instances."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "_read_pids_file", "line_number": 135, "body": "def _read_pids_file(pid_file):\n    # First setup a python 2 vs 3 compatibility\n    # http://stackoverflow.com/a/21368622/621449\n    try:\n        FileNotFoundError  # noqa: F823\n    except NameError:\n        # Should only happen on python 2\n        FileNotFoundError = IOError\n    # If the file happen to not exist, simply return\n    # an empty set()\n    try:\n        with open(pid_file, 'r') as f:\n            return {int(pid_str.strip()) for pid_str in f if pid_str.strip()}\n    except FileNotFoundError:\n        return set()", "is_method": false, "function_description": "Reads process IDs (PIDs) from a specified file, returning them as a set. If the file does not exist, it returns an empty set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/lock.py", "function": "_write_pids_file", "line_number": 152, "body": "def _write_pids_file(pid_file, pids_set):\n    with open(pid_file, 'w') as f:\n        f.writelines('{}\\n'.format(pid) for pid in pids_set)\n\n    # Make the .pid-file writable by all (when the os allows for it)\n    if os.name != 'nt':\n        s = os.stat(pid_file)\n        if os.getuid() == s.st_uid:\n            os.chmod(pid_file, s.st_mode | 0o777)", "is_method": false, "function_description": "This function writes a set of process IDs to a specified file. It then modifies the file permissions to ensure it is widely accessible for other processes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "recursively_freeze", "line_number": 50, "body": "def recursively_freeze(value):\n    \"\"\"\n    Recursively walks ``Mapping``s and ``list``s and converts them to ``FrozenOrderedDict`` and ``tuples``, respectively.\n    \"\"\"\n    if isinstance(value, Mapping):\n        return FrozenOrderedDict(((k, recursively_freeze(v)) for k, v in value.items()))\n    elif isinstance(value, list) or isinstance(value, tuple):\n        return tuple(recursively_freeze(v) for v in value)\n    return value", "is_method": false, "function_description": "Recursively transforms mutable mappings and lists into their immutable `FrozenOrderedDict` and `tuple` equivalents. This provides a hashable, frozen representation of complex data structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__getitem__", "line_number": 26, "body": "def __getitem__(self, key):\n        return self.__dict[key]", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides read-only access to the value associated with a given key in the frozen, ordered dictionary."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__iter__", "line_number": 29, "body": "def __iter__(self):\n        return iter(self.__dict)", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Enables direct iteration over the keys of the FrozenOrderedDict instance, allowing it to be used in loops."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__len__", "line_number": 32, "body": "def __len__(self):\n        return len(self.__dict)", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides the count of elements in the FrozenOrderedDict, enabling the use of the standard `len()` function."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__repr__", "line_number": 35, "body": "def __repr__(self):\n        # We should use short representation for beautiful console output\n        return repr(dict(self.__dict))", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides a concise string representation of the FrozenOrderedDict instance, primarily for developer debugging and console output, showing its underlying dictionary content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "__hash__", "line_number": 39, "body": "def __hash__(self):\n        if self.__hash is None:\n            hashes = map(hash, self.items())\n            self.__hash = functools.reduce(operator.xor, hashes, 0)\n\n        return self.__hash", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides a hash value for the FrozenOrderedDict, allowing instances to be used as dictionary keys or set elements. It efficiently computes and caches this value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/freezing.py", "function": "get_wrapped", "line_number": 46, "body": "def get_wrapped(self):\n        return self.__dict", "is_method": true, "class_name": "FrozenOrderedDict", "function_description": "Provides access to the internal dictionary wrapped by the FrozenOrderedDict instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "copy", "line_number": 38, "body": "def copy(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Copies the contents of a single file path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data()[path]\n        self.get_all_data()[dest] = contents", "is_method": true, "class_name": "MockFileSystem", "function_description": "Copies the contents of a specified file path to a new destination within the mock file system, optionally preventing overwrites."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "get_all_data", "line_number": 47, "body": "def get_all_data(self):\n        # This starts a server in the background, so we don't want to do it in the global scope\n        if MockFileSystem._data is None:\n            MockFileSystem._data = multiprocessing.Manager().dict()\n        return MockFileSystem._data", "is_method": true, "class_name": "MockFileSystem", "function_description": "Provides access to the `MockFileSystem`'s shared, multiprocessing-safe data store. It ensures the central data dictionary is initialized upon first access, enabling concurrent data management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "get_data", "line_number": 53, "body": "def get_data(self, fn):\n        return self.get_all_data()[fn]", "is_method": true, "class_name": "MockFileSystem", "function_description": "Retrieves the content associated with a given filename from the mock file system's internal store. It provides direct access to simulated file data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "exists", "line_number": 56, "body": "def exists(self, path):\n        return MockTarget(path).exists()", "is_method": true, "class_name": "MockFileSystem", "function_description": "Checks if a file or directory exists at the given path within this mock file system. This method is useful for testing file system interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "remove", "line_number": 59, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        \"\"\"\n        Removes the given mockfile. skip_trash doesn't have any meaning.\n        \"\"\"\n        if recursive:\n            to_delete = []\n            for s in self.get_all_data().keys():\n                if s.startswith(path):\n                    to_delete.append(s)\n            for s in to_delete:\n                self.get_all_data().pop(s)\n        else:\n            self.get_all_data().pop(path)", "is_method": true, "class_name": "MockFileSystem", "function_description": "Removes specified files or directories from the mock file system. It can delete directory contents recursively."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "move", "line_number": 73, "body": "def move(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Moves a single file from path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data().pop(path)\n        self.get_all_data()[dest] = contents", "is_method": true, "class_name": "MockFileSystem", "function_description": "Moves a file from a specified source path to a destination path within the mock file system. It optionally prevents overwriting existing destinations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "listdir", "line_number": 82, "body": "def listdir(self, path):\n        \"\"\"\n        listdir does a prefix match of self.get_all_data(), but doesn't yet support globs.\n        \"\"\"\n        return [s for s in self.get_all_data().keys()\n                if s.startswith(path)]", "is_method": true, "class_name": "MockFileSystem", "function_description": "This method simulates listing directory contents within the mock file system. It returns all entries (file or directory paths) that begin with the specified path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "isdir", "line_number": 89, "body": "def isdir(self, path):\n        return any(self.listdir(path))", "is_method": true, "class_name": "MockFileSystem", "function_description": "Determines if a given path within the mock file system is considered a directory. It indicates whether the path contains any listable entries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "clear", "line_number": 98, "body": "def clear(self):\n        self.get_all_data().clear()", "is_method": true, "class_name": "MockFileSystem", "function_description": "Clears all in-memory data, effectively emptying the entire mock file system. This resets its state for testing or reinitialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "exists", "line_number": 110, "body": "def exists(self,):\n        return self.path in self.fs.get_all_data()", "is_method": true, "class_name": "MockTarget", "function_description": "Checks if the target's path is present within the mock file system's stored data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "move", "line_number": 113, "body": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        Call MockFileSystem's move command\n        \"\"\"\n        self.fs.move(self.path, path, raise_if_exists)", "is_method": true, "class_name": "MockTarget", "function_description": "Moves the mock target's represented file or directory to a new location within its associated mock file system. This method wraps the underlying file system's move command."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "rename", "line_number": 119, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Call move to rename self\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "MockTarget", "function_description": "Provides a rename capability for the `MockTarget` instance by leveraging its underlying move operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "open", "line_number": 125, "body": "def open(self, mode='r'):\n        fn = self.path\n        mock_target = self\n\n        class Buffer(BytesIO):\n            # Just to be able to do writing + reading from the same buffer\n\n            _write_line = True\n\n            def set_wrapper(self, wrapper):\n                self.wrapper = wrapper\n\n            def write(self, data):\n                if mock_target._mirror_on_stderr:\n                    if self._write_line:\n                        sys.stderr.write(fn + \": \")\n                    if bytes:\n                        sys.stderr.write(data.decode('utf8'))\n                    else:\n                        sys.stderr.write(data)\n                    if (data[-1]) == '\\n':\n                        self._write_line = True\n                    else:\n                        self._write_line = False\n                super(Buffer, self).write(data)\n\n            def close(self):\n                if mode[0] == 'w':\n                    try:\n                        mock_target.wrapper.flush()\n                    except AttributeError:\n                        pass\n                    mock_target.fs.get_all_data()[fn] = self.getvalue()\n                super(Buffer, self).close()\n\n            def __exit__(self, exc_type, exc_val, exc_tb):\n                if not exc_type:\n                    self.close()\n\n            def __enter__(self):\n                return self\n\n            def readable(self):\n                return mode[0] == 'r'\n\n            def writeable(self):\n                return mode[0] == 'w'\n\n            def seekable(self):\n                return False\n\n        if mode[0] == 'w':\n            wrapper = self.format.pipe_writer(Buffer())\n            wrapper.set_wrapper(wrapper)\n            return wrapper\n        else:\n            return self.format.pipe_reader(Buffer(self.fs.get_all_data()[fn]))", "is_method": true, "class_name": "MockTarget", "function_description": "Provides a file-like interface for a mock in-memory file system. It enables reading from or writing to virtual files, primarily used for testing operations without actual disk I/O."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "set_wrapper", "line_number": 134, "body": "def set_wrapper(self, wrapper):\n                self.wrapper = wrapper", "is_method": true, "class_name": "Buffer", "function_description": "Assigns a specified object to act as the wrapper for this buffer instance, enabling external control or interaction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "write", "line_number": 137, "body": "def write(self, data):\n                if mock_target._mirror_on_stderr:\n                    if self._write_line:\n                        sys.stderr.write(fn + \": \")\n                    if bytes:\n                        sys.stderr.write(data.decode('utf8'))\n                    else:\n                        sys.stderr.write(data)\n                    if (data[-1]) == '\\n':\n                        self._write_line = True\n                    else:\n                        self._write_line = False\n                super(Buffer, self).write(data)", "is_method": true, "class_name": "Buffer", "function_description": "This method writes provided data to the buffer. It also conditionally mirrors the data to standard error for monitoring or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "close", "line_number": 151, "body": "def close(self):\n                if mode[0] == 'w':\n                    try:\n                        mock_target.wrapper.flush()\n                    except AttributeError:\n                        pass\n                    mock_target.fs.get_all_data()[fn] = self.getvalue()\n                super(Buffer, self).close()", "is_method": true, "class_name": "Buffer", "function_description": "This `Buffer` method finalizes operations. If in write mode, it ensures the buffer's accumulated data is persisted to the mock target's file system before performing standard resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "__exit__", "line_number": 160, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n                if not exc_type:\n                    self.close()", "is_method": true, "class_name": "Buffer", "function_description": "Ensures the `Buffer` resource is automatically closed when exiting a `with` statement, but only if no exception occurred within the block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "__enter__", "line_number": 164, "body": "def __enter__(self):\n                return self", "is_method": true, "class_name": "Buffer", "function_description": "Enables the Buffer object to function as a context manager. It prepares the buffer for use within a `with` statement block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "readable", "line_number": 167, "body": "def readable(self):\n                return mode[0] == 'r'", "is_method": true, "class_name": "Buffer", "function_description": "Determines if the buffer is currently in a readable mode, indicating whether read operations are permitted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "writeable", "line_number": 170, "body": "def writeable(self):\n                return mode[0] == 'w'", "is_method": true, "class_name": "Buffer", "function_description": "Determines if the buffer object is currently in a writable mode. This enables other parts of the system to check if writing operations are permitted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/mock.py", "function": "seekable", "line_number": 173, "body": "def seekable(self):\n                return False", "is_method": true, "class_name": "Buffer", "function_description": "This method indicates that the buffer does not support random access to its data. It informs callers that operations like `seek()` are not available."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "_update_data", "line_number": 39, "body": "def _update_data(data, new_data):\n        if not new_data:\n            return data\n        if not data:\n            return new_data\n        for section, content in new_data.items():\n            if section not in data:\n                data[section] = dict()\n            data[section].update(content)\n        return data", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Merges two nested dictionaries, updating existing sections and adding new ones from the `new_data` into `data`. It is a core utility for combining TOML configuration settings.\nMerges two nested dictionaries, updating existing sections and adding new ones from the `new_data` into `data`. It is a core utility for combining TOML configuration settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "read", "line_number": 50, "body": "def read(self, config_paths):\n        self.data = dict()\n        for path in config_paths:\n            if os.path.isfile(path):\n                self.data = self._update_data(self.data, toml.load(path))\n\n        # freeze dict params\n        for section, content in self.data.items():\n            for key, value in content.items():\n                if isinstance(value, dict):\n                    self.data[section][key] = recursively_freeze(value)\n\n        return self.data", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Reads and merges content from multiple TOML configuration files into a single dictionary. It then ensures that all nested dictionary parameters within this configuration are frozen for immutability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "get", "line_number": 64, "body": "def get(self, section, option, default=NO_DEFAULT, **kwargs):\n        try:\n            return self.data[section][option]\n        except KeyError:\n            if default is self.NO_DEFAULT:\n                raise\n            return default", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Retrieves a configuration option from a specified section of the parsed TOML data. It provides a default value if the option is not found, preventing errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getboolean", "line_number": 72, "body": "def getboolean(self, section, option, default=NO_DEFAULT):\n        return self.get(section, option, default)", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Retrieves a configuration option as a boolean from a TOML section. It provides a default value if the option is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getint", "line_number": 75, "body": "def getint(self, section, option, default=NO_DEFAULT):\n        return self.get(section, option, default)", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Retrieves an integer configuration value from a specified section and option in a TOML file, providing an optional default fallback."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "getintdict", "line_number": 81, "body": "def getintdict(self, section):\n        return self.data.get(section, {})", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Retrieves a specified section from the parsed TOML configuration data. It safely returns an empty dictionary if the section is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "set", "line_number": 84, "body": "def set(self, section, option, value=None):\n        if section not in self.data:\n            self.data[section] = {}\n        self.data[section][option] = value", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Allows updating or adding a configuration option and its value within a specified TOML section. It ensures the section exists before setting the option."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "has_option", "line_number": 89, "body": "def has_option(self, section, option):\n        return section in self.data and option in self.data[section]", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "Verifies if a specific configuration option exists within a designated section of the parsed TOML data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/toml_parser.py", "function": "__getitem__", "line_number": 92, "body": "def __getitem__(self, name):\n        return self.data[name]", "is_method": true, "class_name": "LuigiTomlParser", "function_description": "This method enables dictionary-like access to the parsed TOML data. It allows users to retrieve specific configuration values or sections by key."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "_check_parser", "line_number": 43, "body": "def _check_parser(parser_class, parser):\n    if not parser_class.enabled:\n        msg = (\n            \"Parser not installed yet. \"\n            \"Please, install luigi with required parser:\\n\"\n            \"pip install luigi[{parser}]\"\n        )\n        raise ImportError(msg.format(parser=parser))", "is_method": false, "function_description": "Validates if a specified parser is enabled for use. If not, it raises an ImportError with instructions on how to install the required parser."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "get_config", "line_number": 53, "body": "def get_config(parser=PARSER):\n    \"\"\"Get configs singleton for parser\n    \"\"\"\n    parser_class = PARSERS[parser]\n    _check_parser(parser_class, parser)\n    return parser_class.instance()", "is_method": false, "function_description": "Retrieves the singleton configuration instance for a specified parser type. It provides a globally accessible and unique configuration object for various system components."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/core.py", "function": "add_config_path", "line_number": 61, "body": "def add_config_path(path):\n    \"\"\"Select config parser by file extension and add path into parser.\n    \"\"\"\n    if not os.path.isfile(path):\n        warnings.warn(\"Config file does not exist: {path}\".format(path=path))\n        return False\n\n    # select parser by file extension\n    _base, ext = os.path.splitext(path)\n    if ext and ext[1:] in PARSERS:\n        parser = ext[1:]\n    else:\n        parser = PARSER\n    parser_class = PARSERS[parser]\n\n    _check_parser(parser_class, parser)\n    if parser != PARSER:\n        msg = (\n            \"Config for {added} parser added, but used {used} parser. \"\n            \"Set up right parser via env var: \"\n            \"export LUIGI_CONFIG_PARSER={added}\"\n        )\n        warnings.warn(msg.format(added=parser, used=PARSER))\n\n    # add config path to parser\n    parser_class.add_config_path(path)\n    return True", "is_method": false, "function_description": "This function adds a given configuration file path to the appropriate parser, automatically selected by file extension. It provides a warning if the chosen parser differs from the default."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_get", "line_number": 62, "body": "def before_get(self, parser, section, option, value, defaults):\n        return self._interpolate_env(option, section, value)", "is_method": true, "class_name": "EnvironmentInterpolation", "function_description": "As a hook, this method of `EnvironmentInterpolation` processes a configuration value by substituting environment variables before it is retrieved."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "_interpolate_env", "line_number": 65, "body": "def _interpolate_env(self, option, section, value):\n        rawval = value\n        parts = []\n        while value:\n            match = self._ENVRE.search(value)\n            if match is None:\n                parts.append(value)\n                break\n            envvar = match.groups()[0]\n            try:\n                envval = os.environ[envvar]\n            except KeyError:\n                raise InterpolationMissingEnvvarError(\n                    option, section, rawval, envvar)\n            start, end = match.span()\n            parts.append(value[:start])\n            parts.append(envval)\n            value = value[end:]\n        return \"\".join(parts)", "is_method": true, "class_name": "EnvironmentInterpolation", "function_description": "Interpolates a string by replacing environment variable placeholders with their actual values. Useful for dynamically configuring settings based on the execution environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_get", "line_number": 96, "body": "def before_get(self, parser, section, option, value, defaults):\n        for interp in self._interpolations:\n            value = interp.before_get(parser, section, option, value, defaults)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "The `before_get` method in `CombinedInterpolation` applies a sequence of pre-retrieval transformations to a configuration value. It iteratively modifies the value by chaining multiple registered interpolation strategies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_read", "line_number": 101, "body": "def before_read(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_read(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "Applies a sequence of `before_read` transformations from registered interpolation strategies. This method preprocesses a configuration value before it is read."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_set", "line_number": 106, "body": "def before_set(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_set(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "This method sequentially applies multiple preprocessing transformations to a value using a chain of interpolation objects. It allows various pre-setting modifications before a configuration option is stored."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "before_write", "line_number": 111, "body": "def before_write(self, parser, section, option, value):\n        for interp in self._interpolations:\n            value = interp.before_write(parser, section, option, value)\n        return value", "is_method": true, "class_name": "CombinedInterpolation", "function_description": "This method chains multiple interpolation steps to transform a value using configured interpolators. It prepares the value by applying a sequence of modifications before it is written."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "reload", "line_number": 130, "body": "def reload(cls):\n        # Warn about deprecated old-style config paths.\n        deprecated_paths = [p for p in cls._config_paths if os.path.basename(p) == 'client.cfg' and os.path.exists(p)]\n        if deprecated_paths:\n            warnings.warn(\"Luigi configuration files named 'client.cfg' are deprecated if favor of 'luigi.cfg'. \" +\n                          \"Found: {paths!r}\".format(paths=deprecated_paths),\n                          DeprecationWarning)\n\n        return cls.instance().read(cls._config_paths)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Reloads the Luigi system configuration by re-reading all specified configuration files. It also warns about deprecated configuration file names during the process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "_get_with_default", "line_number": 140, "body": "def _get_with_default(self, method, section, option, default, expected_type=None, **kwargs):\n        \"\"\"\n        Gets the value of the section/option using method.\n\n        Returns default if value is not found.\n\n        Raises an exception if the default value is not None and doesn't match the expected_type.\n        \"\"\"\n        try:\n            try:\n                # Underscore-style is the recommended configuration style\n                option = option.replace('-', '_')\n                return method(self, section, option, **kwargs)\n            except (NoOptionError, NoSectionError):\n                # Support dash-style option names (with deprecation warning).\n                option_alias = option.replace('_', '-')\n                value = method(self, section, option_alias, **kwargs)\n                warn = 'Configuration [{s}] {o} (with dashes) should be avoided. Please use underscores: {u}.'.format(\n                    s=section, o=option_alias, u=option)\n                warnings.warn(warn, DeprecationWarning)\n                return value\n        except (NoOptionError, NoSectionError):\n            if default is LuigiConfigParser.NO_DEFAULT:\n                raise\n            if expected_type is not None and default is not None and \\\n               not isinstance(default, expected_type):\n                raise\n            return default", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves a configuration option from the parser, providing a default value if not found. It handles missing options/sections and supports both underscore and dash-style option names."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "has_option", "line_number": 169, "body": "def has_option(self, section, option):\n        \"\"\"modified has_option\n        Check for the existence of a given option in a given section. If the\n        specified 'section' is None or an empty string, DEFAULT is assumed. If\n        the specified 'section' does not exist, returns False.\n        \"\"\"\n\n        # Underscore-style is the recommended configuration style\n        option = option.replace('-', '_')\n        if ConfigParser.has_option(self, section, option):\n            return True\n\n        # Support dash-style option names (with deprecation warning).\n        option_alias = option.replace('_', '-')\n        if ConfigParser.has_option(self, section, option_alias):\n            warn = 'Configuration [{s}] {o} (with dashes) should be avoided. Please use underscores: {u}.'.format(\n                s=section, o=option_alias, u=option)\n            warnings.warn(warn, DeprecationWarning)\n            return True\n\n        return False", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Checks if a configuration option exists within a specified section, supporting both underscore and dash-style naming conventions. This method helps verify the presence of configuration settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "get", "line_number": 191, "body": "def get(self, section, option, default=NO_DEFAULT, **kwargs):\n        return self._get_with_default(ConfigParser.get, section, option, default, **kwargs)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves a configuration option's value from a specified section within the Luigi configuration, providing a default if the option is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getboolean", "line_number": 194, "body": "def getboolean(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getboolean, section, option, default, bool)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves a boolean value for a specified configuration option from a section, using a provided default if not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getint", "line_number": 197, "body": "def getint(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getint, section, option, default, int)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves an integer value from the configuration for a given section and option, providing a default if not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getfloat", "line_number": 200, "body": "def getfloat(self, section, option, default=NO_DEFAULT):\n        return self._get_with_default(ConfigParser.getfloat, section, option, default, float)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves a floating-point number from a specified configuration section and option. It safely returns a default value if the option is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "getintdict", "line_number": 203, "body": "def getintdict(self, section):\n        try:\n            # Exclude keys from [DEFAULT] section because in general they do not hold int values\n            return dict((key, int(value)) for key, value in self.items(section)\n                        if key not in {k for k, _ in self.items('DEFAULT')})\n        except NoSectionError:\n            return {}", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Retrieves a specified configuration section as a dictionary, converting all its values to integers. It excludes keys from the DEFAULT section and returns an empty dict if the section is not found."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/cfg_parser.py", "function": "set", "line_number": 211, "body": "def set(self, section, option, value=None):\n        if not ConfigParser.has_section(self, section):\n            ConfigParser.add_section(self, section)\n\n        return ConfigParser.set(self, section, option, value)", "is_method": true, "class_name": "LuigiConfigParser", "function_description": "Sets a configuration option within a specified section, automatically adding the section if it doesn't already exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "instance", "line_number": 25, "body": "def instance(cls, *args, **kwargs):\n        \"\"\" Singleton getter \"\"\"\n        if cls._instance is None:\n            cls._instance = cls(*args, **kwargs)\n            loaded = cls._instance.reload()\n            logging.getLogger('luigi-interface').info('Loaded %r', loaded)\n\n        return cls._instance", "is_method": true, "class_name": "BaseParser", "function_description": "Provides the singleton instance of the class. It ensures only one object is created, initializing and reloading it upon first access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "add_config_path", "line_number": 35, "body": "def add_config_path(cls, path):\n        cls._config_paths.append(path)\n        cls.reload()", "is_method": true, "class_name": "BaseParser", "function_description": "Registers a new path for configuration files, immediately triggering a reload. This extends the locations where the parser searches for and loads its settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/configuration/base_parser.py", "function": "reload", "line_number": 40, "body": "def reload(cls):\n        return cls.instance().read(cls._config_paths)", "is_method": true, "class_name": "BaseParser", "function_description": "This method refreshes the configuration of the `BaseParser` instance. It ensures the parser operates with the most current settings by re-reading them."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_constrain_glob", "line_number": 489, "body": "def _constrain_glob(glob, paths, limit=5):\n    \"\"\"\n    Tweaks glob into a list of more specific globs that together still cover paths and not too much extra.\n\n    Saves us minutes long listings for long dataset histories.\n\n    Specifically, in this implementation the leftmost occurrences of \"[0-9]\"\n    give rise to a few separate globs that each specialize the expression to\n    digits that actually occur in paths.\n    \"\"\"\n\n    def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)\n\n    current = {glob: paths}\n    while True:\n        pos = list(current.keys())[0].find('[0-9]')\n        if pos == -1:\n            # no wildcard expressions left to specialize in the glob\n            return list(current.keys())\n        char_sets = {}\n        for g, p in current.items():\n            char_sets[g] = sorted({path[pos] for path in p})\n        if sum(len(s) for s in char_sets.values()) > limit:\n            return [g.replace('[0-9]', digit_set_wildcard(char_sets[g]), 1) for g in current]\n        for g, s in char_sets.items():\n            for c in s:\n                new_glob = g.replace('[0-9]', c, 1)\n                new_paths = list(filter(lambda p: p[pos] == c, current[g]))\n                current[new_glob] = new_paths\n            del current[g]", "is_method": false, "function_description": "Refines a generic glob pattern into multiple specific globs based on actual file paths. It optimizes file system searches by reducing the scope of matching, improving performance for large datasets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "most_common", "line_number": 529, "body": "def most_common(items):\n    [(element, counter)] = Counter(items).most_common(1)\n    return element, counter", "is_method": false, "function_description": "Determines the most frequently occurring element in a given collection. It returns this element along with the number of times it appears, useful for frequency analysis."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_get_per_location_glob", "line_number": 534, "body": "def _get_per_location_glob(tasks, outputs, regexes):\n    \"\"\"\n    Builds a glob listing existing output paths.\n\n    Esoteric reverse engineering, but worth it given that (compared to an\n    equivalent contiguousness guarantee by naive complete() checks)\n    requests to the filesystem are cut by orders of magnitude, and users\n    don't even have to retrofit existing tasks anyhow.\n    \"\"\"\n    paths = [o.path for o in outputs]\n    # naive, because some matches could be confused by numbers earlier\n    # in path, e.g. /foo/fifa2000k/bar/2000-12-31/00\n    matches = [r.search(p) for r, p in zip(regexes, paths)]\n\n    for m, p, t in zip(matches, paths, tasks):\n        if m is None:\n            raise NotImplementedError(\"Couldn't deduce datehour representation in output path %r of task %s\" % (p, t))\n\n    n_groups = len(matches[0].groups())\n    # the most common position of every group is likely\n    # to be conclusive hit or miss\n    positions = [most_common((m.start(i), m.end(i)) for m in matches)[0] for i in range(1, n_groups + 1)]\n\n    glob = list(paths[0])  # FIXME sanity check that it's the same for all paths\n    for start, end in positions:\n        glob = glob[:start] + ['[0-9]'] * (end - start) + glob[end:]\n    # chop off the last path item\n    # (wouldn't need to if `hadoop fs -ls -d` equivalent were available)\n    return ''.join(glob).rsplit('/', 1)[0]", "is_method": false, "function_description": "Infers a glob pattern by reverse-engineering common numerical segments (e.g., dates/hours) within provided output paths. This pattern can efficiently check for the existence of similar outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_get_filesystems_and_globs", "line_number": 565, "body": "def _get_filesystems_and_globs(datetime_to_task, datetime_to_re):\n    \"\"\"\n    Yields a (filesystem, glob) tuple per every output location of task.\n\n    The task can have one or several FileSystemTarget outputs.\n\n    For convenience, the task can be a luigi.WrapperTask,\n    in which case outputs of all its dependencies are considered.\n    \"\"\"\n    # probe some scattered datetimes unlikely to all occur in paths, other than by being sincere datetime parameter's representations\n    # TODO limit to [self.start, self.stop) so messages are less confusing? Done trivially it can kill correctness\n    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]\n    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]\n    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]\n    sample_outputs = [flatten_output(t) for t in sample_tasks]\n\n    for o, t in zip(sample_outputs, sample_tasks):\n        if len(o) != len(sample_outputs[0]):\n            raise NotImplementedError(\"Outputs must be consistent over time, sorry; was %r for %r and %r for %r\" % (o, t, sample_outputs[0], sample_tasks[0]))\n            # TODO fall back on requiring last couple of days? to avoid astonishing blocking when changes like that are deployed\n            # erm, actually it's not hard to test entire hours_back..hours_forward and split into consistent subranges FIXME?\n        for target in o:\n            if not isinstance(target, FileSystemTarget):\n                raise NotImplementedError(\"Output targets must be instances of FileSystemTarget; was %r for %r\" % (target, t))\n\n    for o in zip(*sample_outputs):  # transposed, so here we're iterating over logical outputs, not datetimes\n        glob = _get_per_location_glob(sample_tasks, o, regexes)\n        yield o[0].fs, glob", "is_method": false, "function_description": "Yields filesystem and glob patterns for Luigi task outputs. It determines these patterns by examining how outputs of time-parameterized tasks vary across different datetimes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_list_existing", "line_number": 595, "body": "def _list_existing(filesystem, glob, paths):\n    \"\"\"\n    Get all the paths that do in fact exist. Returns a set of all existing paths.\n\n    Takes a luigi.target.FileSystem object, a str which represents a glob and\n    a list of strings representing paths.\n    \"\"\"\n    globs = _constrain_glob(glob, paths)\n    time_start = time.time()\n    listing = []\n    for g in sorted(globs):\n        logger.debug('Listing %s', g)\n        if filesystem.exists(g):\n            listing.extend(filesystem.listdir(g))\n    logger.debug('%d %s listings took %f s to return %d items',\n                 len(globs), filesystem.__class__.__name__, time.time() - time_start, len(listing))\n    return set(listing)", "is_method": false, "function_description": "Retrieves a unique set of all existing paths and their contents on a specified filesystem, constrained by input glob patterns. It efficiently lists only confirmed existing directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "infer_bulk_complete_from_fs", "line_number": 614, "body": "def infer_bulk_complete_from_fs(datetimes, datetime_to_task, datetime_to_re):\n    \"\"\"\n    Efficiently determines missing datetimes by filesystem listing.\n\n    The current implementation works for the common case of a task writing\n    output to a ``FileSystemTarget`` whose path is built using strftime with\n    format like '...%Y...%m...%d...%H...', without custom ``complete()`` or\n    ``exists()``.\n\n    (Eventually Luigi could have ranges of completion as first-class citizens.\n    Then this listing business could be factored away/be provided for\n    explicitly in target API or some kind of a history server.)\n    \"\"\"\n    filesystems_and_globs_by_location = _get_filesystems_and_globs(datetime_to_task, datetime_to_re)\n    paths_by_datetime = [[o.path for o in flatten_output(datetime_to_task(d))] for d in datetimes]\n    listing = set()\n    for (f, g), p in zip(filesystems_and_globs_by_location, zip(*paths_by_datetime)):  # transposed, so here we're iterating over logical outputs, not datetimes\n        listing |= _list_existing(f, g, p)\n\n    # quickly learn everything that's missing\n    missing_datetimes = []\n    for d, p in zip(datetimes, paths_by_datetime):\n        if not set(p) <= listing:\n            missing_datetimes.append(d)\n\n    return missing_datetimes", "is_method": false, "function_description": "Efficiently identifies datetimes for which associated task outputs are missing on the filesystem. It provides a bulk check of task completion without running tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_emit_metrics", "line_number": 164, "body": "def _emit_metrics(self, missing_datetimes, finite_start, finite_stop):\n        \"\"\"\n        For consistent metrics one should consider the entire range, but\n        it is open (infinite) if stop or start is None.\n\n        Hence make do with metrics respective to the finite simplification.\n        \"\"\"\n        datetimes = self.finite_datetimes(\n            finite_start if self.start is None else min(finite_start, self.parameter_to_datetime(self.start)),\n            finite_stop if self.stop is None else max(finite_stop, self.parameter_to_datetime(self.stop)))\n\n        delay_in_jobs = len(datetimes) - datetimes.index(missing_datetimes[0]) if datetimes and missing_datetimes else 0\n        self.trigger_event(RangeEvent.DELAY, self.of_cls.task_family, delay_in_jobs)\n\n        expected_count = len(datetimes)\n        complete_count = expected_count - len(missing_datetimes)\n        self.trigger_event(RangeEvent.COMPLETE_COUNT, self.of_cls.task_family, complete_count)\n        self.trigger_event(RangeEvent.COMPLETE_FRACTION, self.of_cls.task_family, float(complete_count) / expected_count if expected_count else 1)", "is_method": true, "class_name": "RangeBase", "function_description": "Calculates and emits key metrics like delay, completion count, and fraction for a specified datetime range. This provides observability into the processing status and progress within that range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 183, "body": "def _format_datetime(self, dt):\n        return self.datetime_to_parameter(dt)", "is_method": true, "class_name": "RangeBase", "function_description": "Converts a datetime object into the `RangeBase` class's specific parameter format by delegating to `datetime_to_parameter`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_range", "line_number": 186, "body": "def _format_range(self, datetimes):\n        param_first = self._format_datetime(datetimes[0])\n        param_last = self._format_datetime(datetimes[-1])\n        return '[%s, %s]' % (param_first, param_last)", "is_method": true, "class_name": "RangeBase", "function_description": "Generates a string representation of a time range using the first and last datetimes from a provided list. The output is formatted as `[start, end]`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_instantiate_task_cls", "line_number": 191, "body": "def _instantiate_task_cls(self, param):\n        return self.of(**self._task_parameters(param))", "is_method": true, "class_name": "RangeBase", "function_description": "Creates a new task object. It processes input parameters to configure and instantiate the task using an internal factory method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_param_name", "line_number": 195, "body": "def _param_name(self):\n        if self.param_name is None:\n            return next(x[0] for x in self.of.get_params() if x[1].positional)\n        else:\n            return self.param_name", "is_method": true, "class_name": "RangeBase", "function_description": "Provides a parameter name for the range. It uses the stored name, or infers the name of the first positional parameter from an associated object if none is set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_task_parameters", "line_number": 201, "body": "def _task_parameters(self, param):\n        kwargs = dict(**self.of_params)\n        kwargs[self._param_name] = param\n        return kwargs", "is_method": true, "class_name": "RangeBase", "function_description": "Provides a dictionary of task parameters by combining a base set with a specific, designated parameter value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "requires", "line_number": 206, "body": "def requires(self):\n        # cache because we anticipate a fair amount of computation\n        if hasattr(self, '_cached_requires'):\n            return self._cached_requires\n\n        if not self.start and not self.stop:\n            raise ParameterException(\"At least one of start and stop needs to be specified\")\n        if not self.start and not self.reverse:\n            raise ParameterException(\"Either start needs to be specified or reverse needs to be True\")\n        if self.start and self.stop and self.start > self.stop:\n            raise ParameterException(\"Can't have start > stop\")\n        # TODO check overridden complete() and exists()\n\n        now = datetime.utcfromtimestamp(time.time() if self.now is None else self.now)\n\n        moving_start = self.moving_start(now)\n        finite_start = moving_start if self.start is None else max(self.parameter_to_datetime(self.start), moving_start)\n        moving_stop = self.moving_stop(now)\n        finite_stop = moving_stop if self.stop is None else min(self.parameter_to_datetime(self.stop), moving_stop)\n\n        datetimes = self.finite_datetimes(finite_start, finite_stop) if finite_start <= finite_stop else []\n\n        if datetimes:\n            logger.debug('Actually checking if range %s of %s is complete',\n                         self._format_range(datetimes), self.of_cls.task_family)\n            missing_datetimes = sorted(self._missing_datetimes(datetimes))\n            logger.debug('Range %s lacked %d of expected %d %s instances',\n                         self._format_range(datetimes), len(missing_datetimes), len(datetimes), self.of_cls.task_family)\n        else:\n            missing_datetimes = []\n            logger.debug('Empty range. No %s instances expected', self.of_cls.task_family)\n\n        self._emit_metrics(missing_datetimes, finite_start, finite_stop)\n\n        if self.reverse:\n            required_datetimes = missing_datetimes[-self.task_limit:]\n        else:\n            required_datetimes = missing_datetimes[:self.task_limit]\n        if required_datetimes:\n            logger.debug('Requiring %d missing %s instances in range %s',\n                         len(required_datetimes), self.of_cls.task_family, self._format_range(required_datetimes))\n        if self.reverse:\n            required_datetimes.reverse()  # TODO priorities, so that within the batch tasks are ordered too\n\n        self._cached_requires = [self._instantiate_task_cls(self.datetime_to_parameter(d)) for d in required_datetimes]\n        return self._cached_requires", "is_method": true, "class_name": "RangeBase", "function_description": "Determines and returns a list of required tasks corresponding to incomplete or missing time-based intervals within a specified range. It supports dependency management for sequential workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 253, "body": "def missing_datetimes(self, finite_datetimes):\n        \"\"\"\n        Override in subclasses to do bulk checks.\n\n        Returns a sorted list.\n\n        This is a conservative base implementation that brutally checks completeness, instance by instance.\n\n        Inadvisable as it may be slow.\n        \"\"\"\n        return [d for d in finite_datetimes if not self._instantiate_task_cls(self.datetime_to_parameter(d)).complete()]", "is_method": true, "class_name": "RangeBase", "function_description": "Identifies datetimes within a given list that lack associated complete tasks. This base method is slow and should be overridden by subclasses for efficient bulk checks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_missing_datetimes", "line_number": 265, "body": "def _missing_datetimes(self, finite_datetimes):\n        \"\"\"\n        Backward compatible wrapper. Will be deleted eventually (stated on Dec 2015)\n        \"\"\"\n        try:\n            return self.missing_datetimes(finite_datetimes)\n        except TypeError as ex:\n            if 'missing_datetimes()' in repr(ex):\n                warnings.warn('In your Range* subclass, missing_datetimes() should only take 1 argument (see latest docs)')\n                return self.missing_datetimes(self.of_cls, finite_datetimes)\n            else:\n                raise", "is_method": true, "class_name": "RangeBase", "function_description": "Provides backward compatibility for the `missing_datetimes` method by adapting to changes in its argument signature, ensuring older subclasses continue to function."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 302, "body": "def datetime_to_parameter(self, dt):\n        return dt.date()", "is_method": true, "class_name": "RangeDailyBase", "function_description": "A method of `RangeDailyBase` that converts a datetime object to its date component. This is useful for normalizing datetimes to a specific day for daily-based operations or parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 305, "body": "def parameter_to_datetime(self, p):\n        return datetime(p.year, p.month, p.day)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "This method converts a date-like object into a `datetime` object, using its year, month, and day. It standardizes input for date-specific operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 308, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt.date())", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Generates a dictionary of parameters for a daily-ranged task. It extracts the date from a given datetime object to define task-specific settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 314, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, dt.day)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Extracts and converts a designated date parameter from a dictionary into a `datetime` object, normalized to year, month, and day. This standardizes daily-ranged task parameters.\nExtracts and converts a designated date parameter from a dictionary into a `datetime` object, normalized to year, month, and day. This standardizes daily-ranged task parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 321, "body": "def moving_start(self, now):\n        return now - timedelta(days=self.days_back)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Calculates the start of a historical time window by subtracting a configured number of days from a given point in time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 324, "body": "def moving_stop(self, now):\n        return now + timedelta(days=self.days_forward)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Calculates the end point of a future daily range. It determines the stop time by adding a configured number of days to a given start timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 327, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to turn of day.\n        \"\"\"\n        date_start = datetime(finite_start.year, finite_start.month, finite_start.day)\n        dates = []\n        for i in itertools.count():\n            t = date_start + timedelta(days=i)\n            if t >= finite_stop:\n                return dates\n            if t >= finite_start:\n                dates.append(t)", "is_method": true, "class_name": "RangeDailyBase", "function_description": "Generates a list of datetimes representing the start of each day within a specified inclusive start and exclusive end range. Useful for daily interval processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 371, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Converts a given datetime into a dictionary of parameters, likely for tasks operating within specific hourly ranges. It provides the necessary configuration for time-based operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 377, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        return p[self._param_name]", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Provides a method to extract a specific datetime-related parameter from a dictionary of task parameters. This helps other functions consistently access the relevant time range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 383, "body": "def moving_start(self, now):\n        return now - timedelta(hours=self.hours_back)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "Provides the start datetime for a historical period. It calculates this by going back a configured number of hours from a given 'now' timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 386, "body": "def moving_stop(self, now):\n        return now + timedelta(hours=self.hours_forward)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "This method determines a future \"stop\" time by adding a configured number of hours to a given current timestamp. It defines the dynamic end of an hourly time range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 389, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to whole hours.\n        \"\"\"\n        datehour_start = datetime(finite_start.year, finite_start.month, finite_start.day, finite_start.hour)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(hours=i)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "This method generates a sequence of datetime objects, each representing the beginning of a whole hour, within a specified start and end time range. It provides discrete hourly points for time-series operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 402, "body": "def _format_datetime(self, dt):\n        return luigi.DateHourParameter().serialize(dt)", "is_method": true, "class_name": "RangeHourlyBase", "function_description": "A utility method that formats a datetime object into a specific string representation required by Luigi's hourly task parameters. This ensures consistent date-hour formatting for task dependencies."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 441, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Converts a datetime object into a dictionary of task-specific parameters. This method is useful for configuring tasks that operate over defined time ranges."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 447, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Converts a designated parameter from an input dictionary into a datetime object, extracting the year, month, day, hour, and minute. This standardizes time values for minute-based range operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 454, "body": "def moving_start(self, now):\n        return now - timedelta(minutes=self.minutes_back)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Calculates a past timestamp by subtracting a predefined number of minutes from a given `now` timestamp. It defines the start of a backward-looking time window."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 457, "body": "def moving_stop(self, now):\n        return now + timedelta(minutes=self.minutes_forward)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "This method calculates a future timestamp by adding a predefined minute duration to a given time. It determines the end point for a forward-looking time range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 460, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to a whole number of minutes intervals.\n        \"\"\"\n        # Validate that the minutes_interval can divide 60 and it is greater than 0 and lesser than 60\n        if not (0 < self.minutes_interval < 60):\n            raise ParameterException('minutes-interval must be within 0..60')\n        if 60 % self.minutes_interval != 0:\n            raise ParameterException('minutes-interval does not evenly divide 60')\n        # start of a complete interval, e.g. 20:13 and the interval is 5 -> 20:10\n        start_minute = int(finite_start.minute/self.minutes_interval)*self.minutes_interval\n        datehour_start = datetime(\n            year=finite_start.year,\n            month=finite_start.month,\n            day=finite_start.day,\n            hour=finite_start.hour,\n            minute=start_minute)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(minutes=i*self.minutes_interval)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "Generates a sequence of datetime objects at fixed, minute-based intervals within a specified time range. Each point is aligned to the interval's start time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 485, "body": "def _format_datetime(self, dt):\n        return luigi.DateMinuteParameter().serialize(dt)", "is_method": true, "class_name": "RangeByMinutesBase", "function_description": "This method formats a datetime object into a specific string representation using Luigi's `DateMinuteParameter` serialization. It provides a standardized way to represent time for Luigi tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "digit_set_wildcard", "line_number": 500, "body": "def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)", "is_method": false, "function_description": "Generates a human-readable regular expression character set from input characters. It optimizes for contiguous ranges using a hyphen (e.g., `[1-5]`) for brevity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameter", "line_number": 670, "body": "def datetime_to_parameter(self, dt):\n        return date(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Converts any datetime object into a date object representing the first day of its month. This is useful for normalizing specific dates to a monthly parameter within a monthly range system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameter_to_datetime", "line_number": 673, "body": "def parameter_to_datetime(self, p):\n        return datetime(p.year, p.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "This method converts an object containing year and month attributes into a `datetime` object, specifically representing the first day of that month. It standardizes monthly parameters for use within the `RangeMonthly` class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "datetime_to_parameters", "line_number": 676, "body": "def datetime_to_parameters(self, dt):\n        \"\"\"\n        Given a date-time, will produce a dictionary of of-params combined with the ranged task parameter\n        \"\"\"\n        return self._task_parameters(dt.date())", "is_method": true, "class_name": "RangeMonthly", "function_description": "Generates task-specific parameters from a given datetime, suitable for configuring operations within a monthly range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "parameters_to_datetime", "line_number": 682, "body": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "This method converts a specific date parameter from a dictionary into a datetime object representing the first day of that month. It standardizes date values for monthly-scoped operations or filtering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_format_datetime", "line_number": 689, "body": "def _format_datetime(self, dt):\n        return dt.strftime('%Y-%m')", "is_method": true, "class_name": "RangeMonthly", "function_description": "Formats a datetime object into a 'YYYY-MM' string. This provides a consistent monthly identifier for use within the RangeMonthly class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_start", "line_number": 692, "body": "def moving_start(self, now):\n        return self._align(now) - relativedelta(months=self.months_back)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Calculates the start date for a historical monthly range. It aligns the current date and subtracts a configured number of months."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "moving_stop", "line_number": 695, "body": "def moving_stop(self, now):\n        return self._align(now) + relativedelta(months=self.months_forward)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Calculates the future end date of a monthly range. It determines this by aligning a given timestamp and advancing it by a configured number of months."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "_align", "line_number": 698, "body": "def _align(self, dt):\n        return datetime(dt.year, dt.month, 1)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Aligns a datetime object to the first day of its respective month. This method is crucial for operations requiring a standardized monthly start point."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "finite_datetimes", "line_number": 701, "body": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        Simply returns the points in time that correspond to turn of month.\n        \"\"\"\n        start_date = self._align(finite_start)\n        aligned_stop = self._align(finite_stop)\n        dates = []\n        for m in itertools.count():\n            t = start_date + relativedelta(months=m)\n            if t >= aligned_stop:\n                return dates\n            if t >= finite_start:\n                dates.append(t)", "is_method": true, "class_name": "RangeMonthly", "function_description": "Generates a sequence of month-beginning datetimes within a specified finite start and stop period. This is useful for time-series analysis requiring monthly intervals."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 730, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, map(self.datetime_to_parameter, finite_datetimes))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d)'))", "is_method": true, "class_name": "RangeDaily", "function_description": "Identifies datetimes from an input list that are missing when compared to a comprehensive set. This set is either programmatically derived or inferred from an external source like a file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 758, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            # TODO: Why is there a list() here but not for the RangeDaily??\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, list(map(self.datetime_to_parameter, finite_datetimes)))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d).*(%H)'))", "is_method": true, "class_name": "RangeHourly", "function_description": "Identifies datetimes from a given list that are missing from an expected complete hourly sequence. It uses a bulk completion mechanism or infers completeness from file system patterns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/range.py", "function": "missing_datetimes", "line_number": 787, "body": "def missing_datetimes(self, finite_datetimes):\n        try:\n            cls_with_params = functools.partial(self.of, **self.of_params)\n            complete_parameters = self.of.bulk_complete.__func__(cls_with_params, map(self.datetime_to_parameter, finite_datetimes))\n            return set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters))\n        except NotImplementedError:\n            return infer_bulk_complete_from_fs(\n                finite_datetimes,\n                lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d)),\n                lambda d: d.strftime('(%Y).*(%m).*(%d).*(%H).*(%M)'))", "is_method": true, "class_name": "RangeByMinutes", "function_description": "Identifies datetimes from a given set that are missing or incomplete according to the system's definition of available data. It's used to verify data presence or integrity within a time range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "get_task_requires", "line_number": 54, "body": "def get_task_requires(task):\n    return set(flatten(task.requires()))", "is_method": false, "function_description": "Retrieves all unique and flattened dependencies required by a given task. This is useful for understanding task prerequisites in a workflow system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "dfs_paths", "line_number": 58, "body": "def dfs_paths(start_task, goal_task_family, path=None):\n    if path is None:\n        path = [start_task]\n    if start_task.task_family == goal_task_family or goal_task_family is None:\n        for item in path:\n            yield item\n    for next in get_task_requires(start_task) - set(path):\n        for t in dfs_paths(next, goal_task_family, path + [next]):\n            yield t", "is_method": false, "function_description": "This function identifies dependency paths within a task graph. It yields sequences of tasks, tracing from a start task towards a specified goal task family."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "find_deps", "line_number": 76, "body": "def find_deps(task, upstream_task_family):\n    '''\n    Finds all dependencies that start with the given task and have a path\n    to upstream_task_family\n\n    Returns all deps on all paths between task and upstream\n    '''\n    return {t for t in dfs_paths(task, upstream_task_family)}", "is_method": false, "function_description": "Identifies all tasks on any dependency path from a given starting task to a specified upstream task family. This service helps map complex dependency chains."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "find_deps_cli", "line_number": 86, "body": "def find_deps_cli():\n    '''\n    Finds all tasks on all paths from provided CLI task\n    '''\n    cmdline_args = sys.argv[1:]\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        return find_deps(cp.get_task_obj(), upstream().family)", "is_method": false, "function_description": "Provides a command-line interface to find all upstream task dependencies. It parses CLI arguments to identify a starting task and determine its complete dependency graph."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "get_task_output_description", "line_number": 95, "body": "def get_task_output_description(task_output):\n    '''\n    Returns a task's output as a string\n    '''\n    output_description = \"n/a\"\n\n    if isinstance(task_output, RemoteTarget):\n        output_description = \"[SSH] {0}:{1}\".format(task_output._fs.remote_context.host, task_output.path)\n    elif isinstance(task_output, S3Target):\n        output_description = \"[S3] {0}\".format(task_output.path)\n    elif isinstance(task_output, FileSystemTarget):\n        output_description = \"[FileSystem] {0}\".format(task_output.path)\n    elif isinstance(task_output, PostgresTarget):\n        output_description = \"[DB] {0}:{1}\".format(task_output.host, task_output.table)\n    else:\n        output_description = \"to be determined\"\n\n    return output_description", "is_method": false, "function_description": "Provides a concise string description for different types of task output locations, such as SSH, S3, file system, or database targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps.py", "function": "main", "line_number": 115, "body": "def main():\n    deps = find_deps_cli()\n    for task in deps:\n        task_output = task.output()\n\n        if isinstance(task_output, dict):\n            output_descriptions = [get_task_output_description(output) for label, output in task_output.items()]\n        elif isinstance(task_output, Iterable):\n            output_descriptions = [get_task_output_description(output) for output in task_output]\n        else:\n            output_descriptions = [get_task_output_description(task_output)]\n\n        print(\"   TASK: {0}\".format(task))\n        for desc in output_descriptions:\n            print(\"                       : {0}\".format(desc))", "is_method": false, "function_description": "This function serves as a main entry point to discover CLI dependencies and tasks. For each task, it retrieves and prints a structured description of all its generated outputs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "main", "line_number": 55, "body": "def main():\n    parser = argparse.ArgumentParser(\n        \"luigi-grep is used to search for workflows using the luigi scheduler's json api\")\n    parser.add_argument(\n        \"--scheduler-host\", default=\"localhost\", help=\"hostname of the luigi scheduler\")\n    parser.add_argument(\n        \"--scheduler-port\", default=\"8082\", help=\"port of the luigi scheduler\")\n    parser.add_argument(\"--prefix\", help=\"prefix of a task query to search for\", default=None)\n    parser.add_argument(\"--status\", help=\"search for jobs with the given status\", default=None)\n\n    args = parser.parse_args()\n    grep = LuigiGrep(args.scheduler_host, args.scheduler_port)\n\n    results = []\n    if args.prefix:\n        results = grep.prefix_search(args.prefix)\n    elif args.status:\n        results = grep.status_search(args.status)\n\n    for job in results:\n        print(\"{name}: {status}, Dependencies:\".format(name=job['name'], status=job['status']))\n        for status, jobs in job['deps_by_status'].items():\n            print(\"  status={status}\".format(status=status))\n            for job in jobs:\n                print(\"    {job}\".format(job=job))", "is_method": false, "function_description": "This function serves as the command-line entry point for luigi-grep. It searches and displays Luigi workflows by task prefix or status using the scheduler's API."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "graph_url", "line_number": 17, "body": "def graph_url(self):\n        return \"http://{0}:{1}/api/graph\".format(self._host, self._port)", "is_method": true, "class_name": "LuigiGrep", "function_description": "This method constructs and returns the API URL for accessing the job dependency graph. It enables other components to retrieve the graphical representation of pipeline tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "_fetch_json", "line_number": 20, "body": "def _fetch_json(self):\n        \"\"\"Returns the json representation of the dep graph\"\"\"\n        print(\"Fetching from url: \" + self.graph_url)\n        resp = urlopen(self.graph_url).read()\n        return json.loads(resp.decode('utf-8'))", "is_method": true, "class_name": "LuigiGrep", "function_description": "As a method of `LuigiGrep`, it retrieves the JSON representation of a dependency graph by fetching and parsing data from a URL."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "_build_results", "line_number": 26, "body": "def _build_results(self, jobs, job):\n        job_info = jobs[job]\n        deps = job_info['deps']\n        deps_status = defaultdict(list)\n        for j in deps:\n            if j in jobs:\n                deps_status[jobs[j]['status']].append(j)\n            else:\n                deps_status['UNKNOWN'].append(j)\n        return {\"name\": job, \"status\": job_info['status'], \"deps_by_status\": deps_status}", "is_method": true, "class_name": "LuigiGrep", "function_description": "This method creates a structured dictionary detailing a specific job's status and its dependencies, categorized by their respective statuses. It provides a quick overview of a job's immediate context within a pipeline."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "prefix_search", "line_number": 37, "body": "def prefix_search(self, job_name_prefix):\n        \"\"\"Searches for jobs matching the given ``job_name_prefix``.\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            if job.startswith(job_name_prefix):\n                yield self._build_results(jobs, job)", "is_method": true, "class_name": "LuigiGrep", "function_description": "As part of LuigiGrep, this method searches for and yields Luigi job names that begin with a specified prefix. It provides a core utility for filtering and finding specific jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/luigi_grep.py", "function": "status_search", "line_number": 45, "body": "def status_search(self, status):\n        \"\"\"Searches for jobs matching the given ``status``.\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            job_info = jobs[job]\n            if job_info['status'].lower() == status.lower():\n                yield self._build_results(jobs, job)", "is_method": true, "class_name": "LuigiGrep", "function_description": "This method of LuigiGrep searches for and yields information about jobs matching a specified status. It provides a way to filter Luigi job execution data by their current state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps_tree.py", "function": "print_tree", "line_number": 41, "body": "def print_tree(task, indent='', last=True):\n    '''\n    Return a string representation of the tasks, their statuses/parameters in a dependency tree format\n    '''\n    # dont bother printing out warnings about tasks with no output\n    with warnings.catch_warnings():\n        warnings.filterwarnings(action='ignore', message='Task .* without outputs has no custom complete\\\\(\\\\) method')\n        is_task_complete = task.complete()\n    is_complete = (bcolors.OKGREEN + 'COMPLETE' if is_task_complete else bcolors.OKBLUE + 'PENDING') + bcolors.ENDC\n    name = task.__class__.__name__\n    params = task.to_str_params(only_significant=True)\n    result = '\\n' + indent\n    if(last):\n        result += '\u2514\u2500--'\n        indent += '    '\n    else:\n        result += '|---'\n        indent += '|   '\n    result += '[{0}-{1} ({2})]'.format(name, params, is_complete)\n    children = flatten(task.requires())\n    for index, child in enumerate(children):\n        result += print_tree(child, indent, (index+1) == len(children))\n    return result", "is_method": false, "function_description": "Generates a string representation of a task and its dependencies, displaying their completion status and parameters. It provides a clear, hierarchical tree format for visualizing workflow progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/tools/deps_tree.py", "function": "main", "line_number": 66, "body": "def main():\n    cmdline_args = sys.argv[1:]\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        task = cp.get_task_obj()\n        print(print_tree(task))", "is_method": false, "function_description": "This function serves as the program's entry point, parsing command-line arguments to configure a task object. It then prints a hierarchical representation of this task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "_do_work_on_compute_node", "line_number": 42, "body": "def _do_work_on_compute_node(work_dir, tarball=True):\n\n    if tarball:\n        # Extract the necessary dependencies\n        # This can create a lot of I/O overhead when running many SGEJobTasks,\n        # so is optional if the luigi project is accessible from the cluster node\n        _extract_packages_archive(work_dir)\n\n    # Open up the pickle file with the work to be done\n    os.chdir(work_dir)\n    with open(\"job-instance.pickle\", \"r\") as f:\n        job = pickle.load(f)\n\n    # Do the work contained\n    job.work()", "is_method": false, "function_description": "Prepares the execution environment and runs a specified, serialized job on a compute node. It's a core utility for distributed task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "_extract_packages_archive", "line_number": 59, "body": "def _extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)", "is_method": false, "function_description": "Extracts a 'packages.tar' archive from a specified working directory. It then updates `sys.path` to ensure the extracted packages become discoverable by Python."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge_runner.py", "function": "main", "line_number": 77, "body": "def main(args=sys.argv):\n    \"\"\"Run the work() method from the class instance in the file \"job-instance.pickle\".\n    \"\"\"\n    try:\n        tarball = \"--no-tarball\" not in args\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to sge_runner.py must be a directory that exists\"\n        project_dir = args[2]\n        sys.path.append(project_dir)\n        _do_work_on_compute_node(work_dir, tarball)\n    except Exception as e:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(e)\n        raise", "is_method": false, "function_description": "Serves as the main entry point to execute a predefined job, running a `work()` method loaded from a pickled instance. It prepares the execution environment on a compute node."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_parse_qstat_state", "line_number": 110, "body": "def _parse_qstat_state(qstat_out, job_id):\n    \"\"\"Parse \"state\" column from `qstat` output for given job_id\n\n    Returns state for the *first* job matching job_id. Returns 'u' if\n    `qstat` output is empty or job_id is not found.\n\n    \"\"\"\n    if qstat_out.strip() == '':\n        return 'u'\n    lines = qstat_out.split('\\n')\n    # skip past header\n    while not lines.pop(0).startswith('---'):\n        pass\n    for line in lines:\n        if line:\n            job, prior, name, user, state = line.strip().split()[0:5]\n            if int(job) == int(job_id):\n                return state\n    return 'u'", "is_method": false, "function_description": "Provides the execution state of a specific job by parsing raw `qstat` command output. It returns 'u' if the job is absent or the output is empty."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_parse_qsub_job_id", "line_number": 131, "body": "def _parse_qsub_job_id(qsub_out):\n    \"\"\"Parse job id from qsub output string.\n\n    Assume format:\n\n        \"Your job <job_id> (\"<job_name>\") has been submitted\"\n\n    \"\"\"\n    return int(qsub_out.split()[2])", "is_method": false, "function_description": "Parses a `qsub` command's output string to extract and return the integer job ID, enabling identification of submitted jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_build_qsub_command", "line_number": 142, "body": "def _build_qsub_command(cmd, job_name, outfile, errfile, pe, n_cpu):\n    \"\"\"Submit shell command to SGE queue via `qsub`\"\"\"\n    qsub_template = \"\"\"echo {cmd} | qsub -o \":{outfile}\" -e \":{errfile}\" -V -r y -pe {pe} {n_cpu} -N {job_name}\"\"\"\n    return qsub_template.format(\n        cmd=cmd, job_name=job_name, outfile=outfile, errfile=errfile,\n        pe=pe, n_cpu=n_cpu)", "is_method": false, "function_description": "Constructs a complete `qsub` command string for submitting a shell command to a Sun Grid Engine (SGE) queue, configuring job parameters. It provides the necessary command for automated job submission on an SGE cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_fetch_task_failures", "line_number": 216, "body": "def _fetch_task_failures(self):\n        if not os.path.exists(self.errfile):\n            logger.info('No error file')\n            return []\n        with open(self.errfile, \"r\") as f:\n            errors = f.readlines()\n        if errors == []:\n            return errors\n        if errors[0].strip() == 'stdin: is not a tty':  # SGE complains when we submit through a pipe\n            errors.pop(0)\n        return errors", "is_method": true, "class_name": "SGEJobTask", "function_description": "Retrieves and filters error messages from the SGE job's error file, providing insights into task execution failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_init_local", "line_number": 228, "body": "def _init_local(self):\n\n        # Set up temp folder in shared directory (trim to max filename length)\n        base_tmp_dir = self.shared_tmp_dir\n        random_id = '%016x' % random.getrandbits(64)\n        folder_name = self.task_id + '-' + random_id\n        self.tmp_dir = os.path.join(base_tmp_dir, folder_name)\n        max_filename_length = os.fstatvfs(0).f_namemax\n        self.tmp_dir = self.tmp_dir[:max_filename_length]\n        logger.info(\"Tmp dir: %s\", self.tmp_dir)\n        os.makedirs(self.tmp_dir)\n\n        # Dump the code to be run into a pickle file\n        logging.debug(\"Dumping pickled class\")\n        self._dump(self.tmp_dir)\n\n        if not self.no_tarball:\n            # Make sure that all the class's dependencies are tarred and available\n            # This is not necessary if luigi is importable from the cluster node\n            logging.debug(\"Tarballing dependencies\")\n            # Grab luigi and the module containing the code to be run\n            packages = [luigi] + [__import__(self.__module__, None, None, 'dummy')]\n            create_packages_archive(packages, os.path.join(self.tmp_dir, \"packages.tar\"))", "is_method": true, "class_name": "SGEJobTask", "function_description": "This method initializes the local environment for a grid engine job. It creates a temporary working directory and prepares the task's code and dependencies for remote execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "run", "line_number": 252, "body": "def run(self):\n        if self.run_locally:\n            self.work()\n        else:\n            self._init_local()\n            self._run_job()", "is_method": true, "class_name": "SGEJobTask", "function_description": "Initiates the execution of the job task. It either runs the task directly if configured for local execution or submits it to the job scheduler for remote processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_dump", "line_number": 270, "body": "def _dump(self, out_dir=''):\n        \"\"\"Dump instance to file.\"\"\"\n        with self.no_unpicklable_properties():\n            self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace('(c__main__', \"(c\" + module_name)\n                with open(self.job_file, \"w\") as f:\n                    f.write(d)\n            else:\n                with open(self.job_file, \"wb\") as f:\n                    pickle.dump(self, f)", "is_method": true, "class_name": "SGEJobTask", "function_description": "This method serializes the `SGEJobTask` instance, saving its state to a pickle file. This allows the job task object to be persisted for later resumption or inspection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_run_job", "line_number": 284, "body": "def _run_job(self):\n\n        # Build a qsub argument that will run sge_runner.py on the directory we've specified\n        runner_path = sge_runner.__file__\n        if runner_path.endswith(\"pyc\"):\n            runner_path = runner_path[:-3] + \"py\"\n        job_str = 'python {0} \"{1}\" \"{2}\"'.format(\n            runner_path, self.tmp_dir, os.getcwd())  # enclose tmp_dir in quotes to protect from special escape chars\n        if self.no_tarball:\n            job_str += ' \"--no-tarball\"'\n\n        # Build qsub submit command\n        self.outfile = os.path.join(self.tmp_dir, 'job.out')\n        self.errfile = os.path.join(self.tmp_dir, 'job.err')\n        submit_cmd = _build_qsub_command(job_str, self.task_family, self.outfile,\n                                         self.errfile, self.parallel_env, self.n_cpu)\n        logger.debug('qsub command: \\n' + submit_cmd)\n\n        # Submit the job and grab job ID\n        output = subprocess.check_output(submit_cmd, shell=True)\n        self.job_id = _parse_qsub_job_id(output)\n        logger.debug(\"Submitted job to qsub with response:\\n\" + output)\n\n        self._track_job()\n\n        # Now delete the temporaries, if they're there.\n        if (self.tmp_dir and os.path.exists(self.tmp_dir) and not self.dont_remove_tmp_dir):\n            logger.info('Removing temporary directory %s' % self.tmp_dir)\n            subprocess.call([\"rm\", \"-rf\", self.tmp_dir])", "is_method": true, "class_name": "SGEJobTask", "function_description": "This method submits the SGEJobTask as a job to the Sun Grid Engine (SGE) cluster. It handles command construction, job submission, tracking, and temporary file cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "_track_job", "line_number": 314, "body": "def _track_job(self):\n        while True:\n            # Sleep for a little bit\n            time.sleep(self.poll_time)\n\n            # See what the job's up to\n            # ASSUMPTION\n            qstat_out = subprocess.check_output(['qstat'])\n            sge_status = _parse_qstat_state(qstat_out, self.job_id)\n            if sge_status == 'r':\n                logger.info('Job is running...')\n            elif sge_status == 'qw':\n                logger.info('Job is pending...')\n            elif 'E' in sge_status:\n                logger.error('Job has FAILED:\\n' + '\\n'.join(self._fetch_task_failures()))\n                break\n            elif sge_status == 't' or sge_status == 'u':\n                # Then the job could either be failed or done.\n                errors = self._fetch_task_failures()\n                if not errors:\n                    logger.info('Job is done')\n                else:\n                    logger.error('Job has FAILED:\\n' + '\\n'.join(errors))\n                break\n            else:\n                logger.info('Job status is UNKNOWN!')\n                logger.info('Status is : %s' % sge_status)\n                raise Exception(\"job status isn't one of ['r', 'qw', 'E*', 't', 'u']: %s\" % sge_status)", "is_method": true, "class_name": "SGEJobTask", "function_description": "This method continuously monitors an SGE job by polling its status via `qstat`. It provides real-time updates on progress and logs the job's final completion or failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sge.py", "function": "run", "line_number": 352, "body": "def run(self):\n        self.work()", "is_method": true, "class_name": "LocalSGEJobTask", "function_description": "Executes the core functionality defined for this local SGE job task. It serves as the primary entry point for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcp.py", "function": "get_authenticate_kwargs", "line_number": 15, "body": "def get_authenticate_kwargs(oauth_credentials=None, http_=None):\n    \"\"\"Returns a dictionary with keyword arguments for use with discovery\n\n    Prioritizes oauth_credentials or a http client provided by the user\n    If none provided, falls back to default credentials provided by google's command line\n    utilities. If that also fails, tries using httplib2.Http()\n\n    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client\n    \"\"\"\n    if oauth_credentials:\n        authenticate_kwargs = {\n            \"credentials\": oauth_credentials\n        }\n    elif http_:\n        authenticate_kwargs = {\n            \"http\": http_\n        }\n    else:\n        # neither http_ or credentials provided\n        try:\n            # try default credentials\n            credentials, _ = google.auth.default()\n            authenticate_kwargs = {\n                \"credentials\": credentials\n            }\n        except google.auth.exceptions.DefaultCredentialsError:\n            # try http using httplib2\n            authenticate_kwargs = {\n                \"http\": httplib2.Http()\n            }\n\n    return authenticate_kwargs", "is_method": false, "function_description": "This function provides a dictionary of keyword arguments for authenticating Google API clients. It prioritizes user-provided credentials or HTTP client, falling back to default Google credentials or a generic HTTP client."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "engine", "line_number": 193, "body": "def engine(self):\n        \"\"\"\n        Return an engine instance, creating it if it doesn't exist.\n\n        Recreate the engine connection if it wasn't originally created\n        by the current process.\n        \"\"\"\n        pid = os.getpid()\n        conn = SQLAlchemyTarget._engine_dict.get(self.connection_string)\n        if not conn or conn.pid != pid:\n            # create and reset connection\n            engine = sqlalchemy.create_engine(\n                self.connection_string,\n                connect_args=self.connect_args,\n                echo=self.echo\n            )\n            SQLAlchemyTarget._engine_dict[self.connection_string] = self.Connection(engine, pid)\n        return SQLAlchemyTarget._engine_dict[self.connection_string].engine", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Provides a process-specific SQLAlchemy engine instance, creating or re-establishing it as needed. It ensures a valid database connection for the current process, especially in multi-threaded or multi-process environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "touch", "line_number": 212, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n        \"\"\"\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n\n        table = self.marker_table_bound\n        id_exists = self.exists()\n        with self.engine.begin() as conn:\n            if not id_exists:\n                ins = table.insert().values(update_id=self.update_id, target_table=self.target_table,\n                                            inserted=datetime.datetime.now())\n            else:\n                ins = table.update().where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                           table.c.target_table == self.target_table)).\\\n                    values(update_id=self.update_id, target_table=self.target_table,\n                           inserted=datetime.datetime.now())\n            conn.execute(ins)\n        assert self.exists()", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Records the completion of a database update for a target table. It inserts or updates a timestamped entry in a marker table to track successful operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "exists", "line_number": 233, "body": "def exists(self):\n        row = None\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n        with self.engine.begin() as conn:\n            table = self.marker_table_bound\n            s = sqlalchemy.select([table]).where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                                 table.c.target_table == self.target_table)).limit(1)\n            row = conn.execute(s).fetchone()\n        return row is not None", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Determines if a specific target table and update ID combination has been marked as processed in the database. It indicates whether a prior operation has completed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "create_marker_table", "line_number": 244, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        if self.marker_table is None:\n            self.marker_table = luigi.configuration.get_config().get('sqlalchemy', 'marker-table', 'table_updates')\n\n        engine = self.engine\n\n        with engine.begin() as con:\n            metadata = sqlalchemy.MetaData()\n            if not con.dialect.has_table(con, self.marker_table):\n                self.marker_table_bound = sqlalchemy.Table(\n                    self.marker_table, metadata,\n                    sqlalchemy.Column(\"update_id\", sqlalchemy.String(128), primary_key=True),\n                    sqlalchemy.Column(\"target_table\", sqlalchemy.String(128)),\n                    sqlalchemy.Column(\"inserted\", sqlalchemy.DateTime, default=datetime.datetime.now()))\n                metadata.create_all(engine)\n            else:\n                metadata.reflect(only=[self.marker_table], bind=engine)\n                self.marker_table_bound = metadata.tables[self.marker_table]", "is_method": true, "class_name": "SQLAlchemyTarget", "function_description": "Ensures a dedicated marker table exists in the database for tracking updates. If not present, it creates this table with a predefined schema for the `SQLAlchemyTarget`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "create_table", "line_number": 320, "body": "def create_table(self, engine):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types specified in columns.\n        If the table exists, then it binds to the existing table.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        :param engine: The sqlalchemy engine instance\n        :type engine: object\n        \"\"\"\n        def construct_sqla_columns(columns):\n            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]\n            return retval\n\n        needs_setup = (len(self.columns) == 0) or (False in [len(c) == 2 for c in self.columns]) if not self.reflect else False\n        if needs_setup:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        else:\n            # if columns is specified as (name, type) tuples\n            with engine.begin() as con:\n\n                if self.schema:\n                    metadata = sqlalchemy.MetaData(schema=self.schema)\n                else:\n                    metadata = sqlalchemy.MetaData()\n\n                try:\n                    if not con.dialect.has_table(con, self.table, self.schema or None):\n                        sqla_columns = construct_sqla_columns(self.columns)\n                        self.table_bound = sqlalchemy.Table(self.table, metadata, *sqla_columns)\n                        metadata.create_all(engine)\n                    else:\n                        full_table = '.'.join([self.schema, self.table]) if self.schema else self.table\n                        metadata.reflect(only=[self.table], bind=engine)\n                        self.table_bound = metadata.tables[full_table]\n                except Exception as e:\n                    self._logger.exception(self.table + str(e))", "is_method": true, "class_name": "CopyToTable", "function_description": "Prepares the target database table for data insertion. It either creates a new table based on column definitions or binds to an existing table if already present in the database."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "update_id", "line_number": 361, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides a unique identifier for the current table insertion operation, useful for tracking or correlating data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "output", "line_number": 367, "body": "def output(self):\n        return SQLAlchemyTarget(\n            connection_string=self.connection_string,\n            target_table=self.table,\n            update_id=self.update_id(),\n            connect_args=self.connect_args,\n            echo=self.echo)", "is_method": true, "class_name": "CopyToTable", "function_description": "This method creates and configures an `SQLAlchemyTarget` object. It provides the necessary database table destination for data copying operations from the `CopyToTable` instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "rows", "line_number": 375, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n\n        This method can be overridden for custom file types or formats.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip(\"\\n\").split(self.column_separator)", "is_method": true, "class_name": "CopyToTable", "function_description": "This method reads an input file and yields each line as a parsed list of column values. It provides the default mechanism for extracting rows, allowing custom file format handling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "run", "line_number": 385, "body": "def run(self):\n        self._logger.info(\"Running task copy to table for update id %s for table %s\" % (self.update_id(), self.table))\n        output = self.output()\n        engine = output.engine\n        self.create_table(engine)\n        with engine.begin() as conn:\n            rows = iter(self.rows())\n            ins_rows = [dict(zip((\"_\" + c.key for c in self.table_bound.c), row))\n                        for row in itertools.islice(rows, self.chunk_size)]\n            while ins_rows:\n                self.copy(conn, ins_rows, self.table_bound)\n                ins_rows = [dict(zip((\"_\" + c.key for c in self.table_bound.c), row))\n                            for row in itertools.islice(rows, self.chunk_size)]\n                self._logger.info(\"Finished inserting %d rows into SQLAlchemy target\" % len(ins_rows))\n        output.touch()\n        self._logger.info(\"Finished inserting rows into SQLAlchemy target\")", "is_method": true, "class_name": "CopyToTable", "function_description": "This method copies data in chunks from a source into a specified database table. It ensures the table exists and uses a transaction for efficient, robust data transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "copy", "line_number": 402, "body": "def copy(self, conn, ins_rows, table_bound):\n        \"\"\"\n        This method does the actual insertion of the rows of data given by ins_rows into the\n        database. A task that needs row updates instead of insertions should overload this method.\n        :param conn: The sqlalchemy connection object\n        :param ins_rows: The dictionary of rows with the keys in the format _<column_name>. For example\n        if you have a table with a column name \"property\", then the key in the dictionary\n        would be \"_property\". This format is consistent with the bindparam usage in sqlalchemy.\n        :param table_bound: The object referring to the table\n        :return:\n        \"\"\"\n        bound_cols = dict((c, sqlalchemy.bindparam(\"_\" + c.key)) for c in table_bound.columns)\n        ins = table_bound.insert().values(bound_cols)\n        conn.execute(ins, ins_rows)", "is_method": true, "class_name": "CopyToTable", "function_description": "The `CopyToTable` method `copy` inserts a collection of rows into a specified database table. It provides the core capability for adding new data records using SQLAlchemy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sqla.py", "function": "construct_sqla_columns", "line_number": 332, "body": "def construct_sqla_columns(columns):\n            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]\n            return retval", "is_method": true, "class_name": "CopyToTable", "function_description": "Converts raw column definition tuples into SQLAlchemy Column objects. It provides the necessary schema elements for defining tables programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "do_work_on_compute_node", "line_number": 34, "body": "def do_work_on_compute_node(work_dir):\n    # Extract the necessary dependencies\n    extract_packages_archive(work_dir)\n\n    # Open up the pickle file with the work to be done\n    os.chdir(work_dir)\n    with open(\"job-instance.pickle\", \"r\") as pickle_file_handle:\n        job = pickle.load(pickle_file_handle)\n\n    # Do the work contained\n    job.work()", "is_method": false, "function_description": "Executes a pre-defined job on a compute node. It sets up the environment, extracts dependencies, and loads/runs a serialized \"job\" object, enabling remote or batch task processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "extract_packages_archive", "line_number": 47, "body": "def extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)", "is_method": false, "function_description": "Extracts a `packages.tar` archive from a specified directory into that directory. It makes the extracted package contents importable by Python programs within that environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf_runner.py", "function": "main", "line_number": 65, "body": "def main(args=sys.argv):\n    \"\"\"Run the work() method from the class instance in the file \"job-instance.pickle\".\n    \"\"\"\n    try:\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to lsf_runner.py must be a directory that exists\"\n        do_work_on_compute_node(work_dir)\n    except Exception as exc:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(exc)\n        raise", "is_method": false, "function_description": "This function serves as the main entry point for a computational job. It prepares the execution environment and runs the `work()` method loaded from a pickled job instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "marker_index_document_id", "line_number": 159, "body": "def marker_index_document_id(self):\n        \"\"\"\n        Generate an id for the indicator document.\n        \"\"\"\n        params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n        return hashlib.sha1(params.encode('utf-8')).hexdigest()", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Generates a unique, deterministic ID for an Elasticsearch marker document. This ID is used to track specific update states within an index and document type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "touch", "line_number": 166, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n\n        The document id would be sufficient but,\n        for documentation,\n        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.\n        \"\"\"\n        self.create_marker_index()\n        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n                      id=self.marker_index_document_id(), body={\n                          'update_id': self.update_id,\n                          'target_index': self.index,\n                          'target_doc_type': self.doc_type,\n                          'date': datetime.datetime.now()})\n        self.es.indices.flush(index=self.marker_index)\n        self.ensure_hist_size()", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Records the completion of a data update operation by creating a timestamped marker document in a dedicated Elasticsearch index. This serves as an auditable log of updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "exists", "line_number": 184, "body": "def exists(self):\n        \"\"\"\n        Test, if this task has been run.\n        \"\"\"\n        try:\n            self.es.get(index=self.marker_index, doc_type=self.marker_doc_type, id=self.marker_index_document_id())\n            return True\n        except elasticsearch.NotFoundError:\n            logger.debug('Marker document not found.')\n        except elasticsearch.ElasticsearchException as err:\n            logger.warn(err)\n        return False", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "The `exists` method determines if a specific marker document is present in Elasticsearch. This signals whether an associated task has completed its execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "create_marker_index", "line_number": 197, "body": "def create_marker_index(self):\n        \"\"\"\n        Create the index that will keep track of the tasks if necessary.\n        \"\"\"\n        if not self.es.indices.exists(index=self.marker_index):\n            self.es.indices.create(index=self.marker_index)", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "Creates an Elasticsearch index if it doesn't already exist. This index is used to track and manage the status of tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "ensure_hist_size", "line_number": 204, "body": "def ensure_hist_size(self):\n        \"\"\"\n        Shrink the history of updates for\n        a `index/doc_type` combination down to `self.marker_index_hist_size`.\n        \"\"\"\n        if self.marker_index_hist_size == 0:\n            return\n        result = self.es.search(index=self.marker_index,\n                                doc_type=self.marker_doc_type,\n                                body={'query': {\n                                    'term': {'target_index': self.index}}},\n                                sort=('date:desc',))\n\n        for i, hit in enumerate(result.get('hits').get('hits'), start=1):\n            if i > self.marker_index_hist_size:\n                marker_document_id = hit.get('_id')\n                self.es.delete(id=marker_document_id, index=self.marker_index,\n                               doc_type=self.marker_doc_type)\n        self.es.indices.flush(index=self.marker_index)", "is_method": true, "class_name": "ElasticsearchTarget", "function_description": "This method manages an Elasticsearch index's historical update records. It prunes older marker documents to maintain a defined maximum history size for a specific target index."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "host", "line_number": 254, "body": "def host(self):\n        \"\"\"\n        ES hostname.\n        \"\"\"\n        return 'localhost'", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method provides the default hostname for connecting to an Elasticsearch instance, which is 'localhost'. It serves as a static configuration setting within the CopyToIndex process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "port", "line_number": 261, "body": "def port(self):\n        \"\"\"\n        ES port.\n        \"\"\"\n        return 9200", "is_method": true, "class_name": "CopyToIndex", "function_description": "Provides the standard port number for Elasticsearch, likely for connection or configuration purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "doc_type", "line_number": 286, "body": "def doc_type(self):\n        \"\"\"\n        The target doc_type.\n        \"\"\"\n        return 'default'", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method provides the default document type for documents being copied to an index. It ensures a consistent default type for indexing operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "settings", "line_number": 300, "body": "def settings(self):\n        \"\"\"\n        Settings to be used at index creation time.\n        \"\"\"\n        return {'settings': {}}", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns a default empty dictionary of settings for index creation. It serves as a placeholder to be overridden for custom index configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "chunk_size", "line_number": 307, "body": "def chunk_size(self):\n        \"\"\"\n        Single API call for this number of docs.\n        \"\"\"\n        return 2000", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method defines the optimal number of documents to process within a single API call for indexing operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "raise_on_error", "line_number": 314, "body": "def raise_on_error(self):\n        \"\"\"\n        Whether to fail fast.\n        \"\"\"\n        return True", "is_method": true, "class_name": "CopyToIndex", "function_description": "Indicates whether operations should fail immediately upon encountering an error, enabling a 'fail-fast' strategy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "purge_existing_index", "line_number": 321, "body": "def purge_existing_index(self):\n        \"\"\"\n        Whether to delete the `index` completely before any indexing.\n        \"\"\"\n        return False", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns a configuration flag indicating that the existing index will not be purged before new indexing operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "marker_index_hist_size", "line_number": 328, "body": "def marker_index_hist_size(self):\n        \"\"\"\n        Number of event log entries in the marker index. 0: unlimited.\n        \"\"\"\n        return 0", "is_method": true, "class_name": "CopyToIndex", "function_description": "Reports the current size limit for event log entries in the marker index, which is set to unlimited."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "docs", "line_number": 348, "body": "def docs(self):\n        \"\"\"\n        Return the documents to be indexed.\n\n        Beside the user defined fields, the document may contain an `_index`, `_type` and `_id`.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method provides an iterable stream of documents, reading each as a line from the configured input source. It prepares documents for indexing operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "_docs", "line_number": 360, "body": "def _docs(self):\n        \"\"\"\n        Since `self.docs` may yield documents that do not explicitly contain `_index` or `_type`,\n        add those attributes here, if necessary.\n        \"\"\"\n        iterdocs = iter(self.docs())\n        first = next(iterdocs)\n        needs_parsing = False\n        if isinstance(first, str):\n            needs_parsing = True\n        elif isinstance(first, dict):\n            pass\n        else:\n            raise RuntimeError('Document must be either JSON strings or dict.')\n        for doc in itertools.chain([first], iterdocs):\n            if needs_parsing:\n                doc = json.loads(doc)\n            if '_index' not in doc:\n                doc['_index'] = self.index\n            if '_type' not in doc:\n                doc['_type'] = self.doc_type\n            yield doc", "is_method": true, "class_name": "CopyToIndex", "function_description": "Prepares documents for indexing by converting them to dictionaries and ensuring they contain `_index` and `_type` fields, parsing JSON strings as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "_init_connection", "line_number": 383, "body": "def _init_connection(self):\n        return elasticsearch.Elasticsearch(\n            connection_class=Urllib3HttpConnection,\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            timeout=self.timeout,\n            **self.extra_elasticsearch_args\n        )", "is_method": true, "class_name": "CopyToIndex", "function_description": "Provides a configured Elasticsearch client connection for the `CopyToIndex` class. It initializes the necessary client object to interact with an Elasticsearch cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "create_index", "line_number": 393, "body": "def create_index(self):\n        \"\"\"\n        Override to provide code for creating the target index.\n\n        By default it will be created without any special settings or mappings.\n        \"\"\"\n        es = self._init_connection()\n        if not es.indices.exists(index=self.index):\n            es.indices.create(index=self.index, body=self.settings)", "is_method": true, "class_name": "CopyToIndex", "function_description": "Creates the target Elasticsearch index if it doesn't exist, using the class's configured settings. This prepares the index for subsequent data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "delete_index", "line_number": 403, "body": "def delete_index(self):\n        \"\"\"\n        Delete the index, if it exists.\n        \"\"\"\n        es = self._init_connection()\n        if es.indices.exists(index=self.index):\n            es.indices.delete(index=self.index)", "is_method": true, "class_name": "CopyToIndex", "function_description": "Deletes the associated index if it exists, providing a way to clear or reset the index managed by the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "update_id", "line_number": 411, "body": "def update_id(self):\n        \"\"\"\n        This id will be a unique identifier for this indexing task.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToIndex", "function_description": "Retrieves the unique identifier associated with this indexing task. Other components can use this ID to reference or track the specific indexing operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "output", "line_number": 417, "body": "def output(self):\n        \"\"\"\n        Returns a ElasticsearchTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return ElasticsearchTarget(\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            index=self.index,\n            doc_type=self.doc_type,\n            update_id=self.update_id(),\n            marker_index_hist_size=self.marker_index_hist_size,\n            timeout=self.timeout,\n            extra_elasticsearch_args=self.extra_elasticsearch_args\n        )", "is_method": true, "class_name": "CopyToIndex", "function_description": "Returns an `ElasticsearchTarget` object that precisely defines the destination index and connection details for data being copied. It specifies where the dataset will be inserted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/esindex.py", "function": "run", "line_number": 435, "body": "def run(self):\n        \"\"\"\n        Run task, namely:\n\n        * purge existing index, if requested (`purge_existing_index`),\n        * create the index, if missing,\n        * apply mappings, if given,\n        * set refresh interval to -1 (disable) for performance reasons,\n        * bulk index in batches of size `chunk_size` (2000),\n        * set refresh interval to 1s,\n        * refresh Elasticsearch,\n        * create entry in marker index.\n        \"\"\"\n        if self.purge_existing_index:\n            self.delete_index()\n        self.create_index()\n        es = self._init_connection()\n        if self.mapping:\n            es.indices.put_mapping(index=self.index, doc_type=self.doc_type,\n                                   body=self.mapping)\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n                                index=self.index)\n\n        bulk(es, self._docs(), chunk_size=self.chunk_size,\n             raise_on_error=self.raise_on_error)\n\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n                                index=self.index)\n        es.indices.refresh()\n        self.output().touch()", "is_method": true, "class_name": "CopyToIndex", "function_description": "This method orchestrates the complete process of copying data into an Elasticsearch index. It handles index management, bulk document indexing, and performance optimization for efficient data loading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_soql_fields", "line_number": 39, "body": "def get_soql_fields(soql):\n    \"\"\"\n    Gets queried columns names.\n    \"\"\"\n    soql_fields = re.search('(?<=select)(?s)(.*)(?=from)', soql, re.IGNORECASE)     # get fields\n    soql_fields = re.sub(' ', '', soql_fields.group())                              # remove extra spaces\n    soql_fields = re.sub('\\t', '', soql_fields)                                     # remove tabs\n    fields = re.split(',|\\n|\\r|', soql_fields)                                      # split on commas and newlines\n    fields = [field for field in fields if field != '']                             # remove empty strings\n    return fields", "is_method": false, "function_description": "This function extracts and returns the list of selected field names from a given SOQL (Salesforce Object Query Language) query string. It provides a way to programmatically identify what data columns a SOQL query is requesting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "ensure_utf", "line_number": 51, "body": "def ensure_utf(value):\n    return value.encode(\"utf-8\") if isinstance(value, unicode) else value", "is_method": false, "function_description": "Ensures a given value, particularly a Python 2 unicode string, is returned as UTF-8 encoded bytes. It passes through other types unchanged, standardizing text representation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "parse_results", "line_number": 55, "body": "def parse_results(fields, data):\n    \"\"\"\n    Traverses ordered dictionary, calls _traverse_results() to recursively read into the dictionary depth of data\n    \"\"\"\n    master = []\n\n    for record in data['records']:  # for each 'record' in response\n        row = [None] * len(fields)  # create null list the length of number of columns\n        for obj, value in record.items():  # for each obj in record\n            if not isinstance(value, (dict, list, tuple)):  # if not data structure\n                if obj in fields:\n                    row[fields.index(obj)] = ensure_utf(value)\n\n            elif isinstance(value, dict) and obj != 'attributes':  # traverse down into object\n                path = obj\n                _traverse_results(value, fields, row, path)\n\n        master.append(row)\n    return master", "is_method": false, "function_description": "Extracts specified fields from a list of structured records. This function transforms potentially nested data into a flat, tabular format, preparing it for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_traverse_results", "line_number": 76, "body": "def _traverse_results(value, fields, row, path):\n    \"\"\"\n    Helper method for parse_results().\n\n    Traverses through ordered dict and recursively calls itself when encountering a dictionary\n    \"\"\"\n    for f, v in value.items():  # for each item in obj\n        field_name = '{path}.{name}'.format(path=path, name=f) if path else f\n\n        if not isinstance(v, (dict, list, tuple)):  # if not data structure\n            if field_name in fields:\n                row[fields.index(field_name)] = ensure_utf(v)\n\n        elif isinstance(v, dict) and f != 'attributes':  # it is a dict\n            _traverse_results(v, fields, row, field_name)", "is_method": false, "function_description": "This helper function recursively traverses a nested dictionary to extract specified scalar values. It populates a flat row with these values, effectively flattening hierarchical data into a structured format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "use_sandbox", "line_number": 118, "body": "def use_sandbox(self):\n        \"\"\"\n        Override to specify use of SF sandbox.\n        True iff we should be uploading to a sandbox environment instead of the production organization.\n        \"\"\"\n        return False", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Indicates whether Salesforce operations should target a sandbox environment or the production organization. Subclasses can override this method to specify sandbox usage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "sandbox_name", "line_number": 126, "body": "def sandbox_name(self):\n        \"\"\"Override to specify the sandbox name if it is intended to be used.\"\"\"\n        return None", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Enables subclasses to override and specify the Salesforce sandbox name to be used for operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "soql", "line_number": 132, "body": "def soql(self):\n        \"\"\"Override to return the raw string SOQL or the path to it.\"\"\"\n        return None", "is_method": true, "class_name": "QuerySalesforce", "function_description": "Not Implemented\nThis method `soql` in the `QuerySalesforce` class is intended to be overridden by subclasses. Its purpose is to define *how* a SOQL query string or its path is supplied, not to execute a query itself. The current implementation returns `None`, indicating it's a placeholder or abstract method that must be implemented to provide actual query details. It has no functional value as is. Therefore, \"Not Implemented\" is the appropriate description."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "is_soql_file", "line_number": 137, "body": "def is_soql_file(self):\n        \"\"\"Override to True if soql property is a file path.\"\"\"\n        return False", "is_method": true, "class_name": "QuerySalesforce", "function_description": "This method indicates whether the `soql` property of the `QuerySalesforce` class is a file path. It defaults to `False`, signifying the `soql` property is a direct query string."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "content_type", "line_number": 142, "body": "def content_type(self):\n        \"\"\"\n        Override to use a different content type. Salesforce allows XML, CSV, ZIP_CSV, or ZIP_XML. Defaults to CSV.\n        \"\"\"\n        return \"CSV\"", "is_method": true, "class_name": "QuerySalesforce", "function_description": "This method defines the default content type, \"CSV\", for data exchange with Salesforce. It can be overridden in subclasses to specify other formats like XML."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "run", "line_number": 148, "body": "def run(self):\n        if self.use_sandbox and not self.sandbox_name:\n            raise Exception(\"Parameter sf_sandbox_name must be provided when uploading to a Salesforce Sandbox\")\n\n        sf = SalesforceAPI(salesforce().username,\n                           salesforce().password,\n                           salesforce().security_token,\n                           salesforce().sb_security_token,\n                           self.sandbox_name)\n\n        job_id = sf.create_operation_job('query', self.object_name, content_type=self.content_type)\n        logger.info(\"Started query job %s in salesforce for object %s\" % (job_id, self.object_name))\n\n        batch_id = ''\n        msg = ''\n        try:\n            if self.is_soql_file:\n                with open(self.soql, 'r') as infile:\n                    self.soql = infile.read()\n\n            batch_id = sf.create_batch(job_id, self.soql, self.content_type)\n            logger.info(\"Creating new batch %s to query: %s for job: %s.\" % (batch_id, self.object_name, job_id))\n            status = sf.block_on_batch(job_id, batch_id)\n            if status['state'].lower() == 'failed':\n                msg = \"Batch failed with message: %s\" % status['state_message']\n                logger.error(msg)\n                # don't raise exception if it's b/c of an included relationship\n                # normal query will execute (with relationship) after bulk job is closed\n                if 'foreign key relationships not supported' not in status['state_message'].lower():\n                    raise Exception(msg)\n            else:\n                result_ids = sf.get_batch_result_ids(job_id, batch_id)\n\n                # If there's only one result, just download it, otherwise we need to merge the resulting downloads\n                if len(result_ids) == 1:\n                    data = sf.get_batch_result(job_id, batch_id, result_ids[0])\n                    with open(self.output().path, 'wb') as outfile:\n                        outfile.write(data)\n                else:\n                    # Download each file to disk, and then merge into one.\n                    # Preferring to do it this way so as to minimize memory consumption.\n                    for i, result_id in enumerate(result_ids):\n                        logger.info(\"Downloading batch result %s for batch: %s and job: %s\" % (result_id, batch_id, job_id))\n                        with open(\"%s.%d\" % (self.output().path, i), 'wb') as outfile:\n                            outfile.write(sf.get_batch_result(job_id, batch_id, result_id))\n\n                    logger.info(\"Merging results of batch %s\" % batch_id)\n                    self.merge_batch_results(result_ids)\n        finally:\n            logger.info(\"Closing job %s\" % job_id)\n            sf.close_job(job_id)\n\n        if 'state_message' in status and 'foreign key relationships not supported' in status['state_message'].lower():\n            logger.info(\"Retrying with REST API query\")\n            data_file = sf.query_all(self.soql)\n\n            reader = csv.reader(data_file)\n            with open(self.output().path, 'wb') as outfile:\n                writer = csv.writer(outfile, dialect='excel')\n                for row in reader:\n                    writer.writerow(row)", "is_method": true, "class_name": "QuerySalesforce", "function_description": "This method queries data from Salesforce using SOQL, handling both bulk and REST API operations. It retrieves and consolidates query results, writing them to a specified output file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "merge_batch_results", "line_number": 210, "body": "def merge_batch_results(self, result_ids):\n        \"\"\"\n        Merges the resulting files of a multi-result batch bulk query.\n        \"\"\"\n        outfile = open(self.output().path, 'w')\n\n        if self.content_type.lower() == 'csv':\n            for i, result_id in enumerate(result_ids):\n                with open(\"%s.%d\" % (self.output().path, i), 'r') as f:\n                    header = f.readline()\n                    if i == 0:\n                        outfile.write(header)\n                    for line in f:\n                        outfile.write(line)\n        else:\n            raise Exception(\"Batch result merging not implemented for %s\" % self.content_type)\n\n        outfile.close()", "is_method": true, "class_name": "QuerySalesforce", "function_description": "This method consolidates multiple individual result files from a Salesforce bulk query into a single output file. It specifically merges CSV data, ensuring a unified header."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "start_session", "line_number": 253, "body": "def start_session(self):\n        \"\"\"\n        Starts a Salesforce session and determines which SF instance to use for future requests.\n        \"\"\"\n        if self.has_active_session():\n            raise Exception(\"Session already in progress.\")\n\n        response = requests.post(self._get_login_url(),\n                                 headers=self._get_login_headers(),\n                                 data=self._get_login_xml())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        for e in root.iter(\"%ssessionId\" % self.SOAP_NS):\n            if self.session_id:\n                raise Exception(\"Invalid login attempt.  Multiple session ids found.\")\n            self.session_id = e.text\n\n        for e in root.iter(\"%sserverUrl\" % self.SOAP_NS):\n            if self.server_url:\n                raise Exception(\"Invalid login attempt.  Multiple server urls found.\")\n            self.server_url = e.text\n\n        if not self.has_active_session():\n            raise Exception(\"Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s].\" %\n                            (self.session_id, self.server_url))\n        self.hostname = urlsplit(self.server_url).hostname", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Establishes an authenticated session with Salesforce, securing the session ID and server URL required for future API calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "has_active_session", "line_number": 281, "body": "def has_active_session(self):\n        return self.session_id and self.server_url", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Determines if the Salesforce API client has an active session by checking for valid session ID and server URL."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query", "line_number": 284, "body": "def query(self, query, **kwargs):\n        \"\"\"\n        Return the result of a Salesforce SOQL query as a dict decoded from the Salesforce response JSON payload.\n\n        :param query: the SOQL query to send to Salesforce, e.g. \"SELECT id from Lead WHERE email = 'a@b.com'\"\n        \"\"\"\n        params = {'q': query}\n        response = requests.get(self._get_norm_query_url(),\n                                headers=self._get_rest_headers(),\n                                params=params,\n                                **kwargs)\n        if response.status_code != requests.codes.ok:\n            raise Exception(response.content)\n\n        return response.json()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This `SalesforceAPI` method executes SOQL queries against Salesforce, returning the data as a Python dictionary. It provides a direct way to programmatically retrieve information from Salesforce."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query_more", "line_number": 300, "body": "def query_more(self, next_records_identifier, identifier_is_url=False, **kwargs):\n        \"\"\"\n        Retrieves more results from a query that returned more results\n        than the batch maximum. Returns a dict decoded from the Salesforce\n        response JSON payload.\n\n        :param next_records_identifier: either the Id of the next Salesforce\n                                     object in the result, or a URL to the\n                                     next record in the result.\n        :param identifier_is_url: True if `next_records_identifier` should be\n                               treated as a URL, False if\n                               `next_records_identifer` should be treated as\n                               an Id.\n        \"\"\"\n        if identifier_is_url:\n            # Don't use `self.base_url` here because the full URI is provided\n            url = (u'https://{instance}{next_record_url}'\n                   .format(instance=self.hostname,\n                           next_record_url=next_records_identifier))\n        else:\n            url = self._get_norm_query_url() + '{next_record_id}'\n            url = url.format(next_record_id=next_records_identifier)\n        response = requests.get(url, headers=self._get_rest_headers(), **kwargs)\n\n        response.raise_for_status()\n\n        return response.json()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This method retrieves subsequent batches of data from a Salesforce query that returned more results than the API's batch maximum. It enables pagination for large query results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "query_all", "line_number": 328, "body": "def query_all(self, query, **kwargs):\n        \"\"\"\n        Returns the full set of results for the `query`. This is a\n        convenience wrapper around `query(...)` and `query_more(...)`.\n        The returned dict is the decoded JSON payload from the final call to\n        Salesforce, but with the `totalSize` field representing the full\n        number of results retrieved and the `records` list representing the\n        full list of records retrieved.\n\n        :param query: the SOQL query to send to Salesforce, e.g.\n                   `SELECT Id FROM Lead WHERE Email = \"waldo@somewhere.com\"`\n        \"\"\"\n        # Make the initial query to Salesforce\n        response = self.query(query, **kwargs)\n\n        # get fields\n        fields = get_soql_fields(query)\n\n        # put fields and first page of results into a temp list to be written to TempFile\n        tmp_list = [fields]\n        tmp_list.extend(parse_results(fields, response))\n\n        tmp_dir = luigi.configuration.get_config().get('salesforce', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(mode='a+b', dir=tmp_dir)\n\n        writer = csv.writer(tmp_file)\n        writer.writerows(tmp_list)\n\n        # The number of results might have exceeded the Salesforce batch limit\n        # so check whether there are more results and retrieve them if so.\n\n        length = len(response['records'])\n        while not response['done']:\n            response = self.query_more(response['nextRecordsUrl'], identifier_is_url=True, **kwargs)\n\n            writer.writerows(parse_results(fields, response))\n            length += len(response['records'])\n            if not length % 10000:\n                logger.info('Requested {0} lines...'.format(length))\n\n        logger.info('Requested a total of {0} lines.'.format(length))\n\n        tmp_file.seek(0)\n        return tmp_file", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Retrieves all paginated results for a Salesforce SOQL query, writing the complete dataset to a temporary CSV file. It provides a convenient way to extract large amounts of Salesforce data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "restful", "line_number": 374, "body": "def restful(self, path, params):\n        \"\"\"\n        Allows you to make a direct REST call if you know the path\n        Arguments:\n        :param path: The path of the request. Example: sobjects/User/ABC123/password'\n        :param params: dict of parameters to pass to the path\n        \"\"\"\n\n        url = self._get_norm_base_url() + path\n        response = requests.get(url, headers=self._get_rest_headers(), params=params)\n\n        if response.status_code != 200:\n            raise Exception(response)\n        json_result = response.json(object_pairs_hook=OrderedDict)\n        if len(json_result) == 0:\n            return None\n        else:\n            return json_result", "is_method": true, "class_name": "SalesforceAPI", "function_description": "The `restful` method provides a direct interface for making generic REST API calls to any specified Salesforce endpoint, returning the JSON response."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "create_operation_job", "line_number": 393, "body": "def create_operation_job(self, operation, obj, external_id_field_name=None, content_type=None):\n        \"\"\"\n        Creates a new SF job that for doing any operation (insert, upsert, update, delete, query)\n\n        :param operation: delete, insert, query, upsert, update, hardDelete. Must be lowercase.\n        :param obj: Parent SF object\n        :param external_id_field_name: Optional.\n        \"\"\"\n        if not self.has_active_session():\n            self.start_session()\n\n        response = requests.post(self._get_create_job_url(),\n                                 headers=self._get_create_job_headers(),\n                                 data=self._get_create_job_xml(operation, obj, external_id_field_name, content_type))\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        job_id = root.find('%sid' % self.API_NS).text\n        return job_id", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This method creates a new Salesforce job for performing various data operations like insert, update, delete, upsert, or query on specified objects. It returns the ID of the created Salesforce job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_job_details", "line_number": 413, "body": "def get_job_details(self, job_id):\n        \"\"\"\n        Gets all details for existing job\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: job info as xml\n        \"\"\"\n        response = requests.get(self._get_job_details_url(job_id))\n\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This `SalesforceAPI` method retrieves all details for an existing job from Salesforce using its ID. It provides current status and information about the Salesforce operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "abort_job", "line_number": 426, "body": "def abort_job(self, job_id):\n        \"\"\"\n        Abort an existing job. When a job is aborted, no more records are processed.\n        Changes to data may already have been committed and aren't rolled back.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: abort response as xml\n        \"\"\"\n        response = requests.post(self._get_abort_job_url(job_id),\n                                 headers=self._get_abort_job_headers(),\n                                 data=self._get_abort_job_xml())\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This method allows users to prematurely terminate an ongoing Salesforce job. It stops further record processing, but already committed changes are not rolled back."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "close_job", "line_number": 441, "body": "def close_job(self, job_id):\n        \"\"\"\n        Closes job\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: close response as xml\n        \"\"\"\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not close job without valid job_id and an active session.\")\n\n        response = requests.post(self._get_close_job_url(job_id),\n                                 headers=self._get_close_job_headers(),\n                                 data=self._get_close_job_xml())\n        response.raise_for_status()\n\n        return response", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This method terminates a specified operation or batch job in Salesforce. It sends an API request to finalize its status, providing a critical lifecycle management capability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "create_batch", "line_number": 458, "body": "def create_batch(self, job_id, data, file_type):\n        \"\"\"\n        Creates a batch with either a string of data or a file containing data.\n\n        If a file is provided, this will pull the contents of the file_target into memory when running.\n        That shouldn't be a problem for any files that meet the Salesforce single batch upload\n        size limit (10MB) and is done to ensure compressed files can be uploaded properly.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param data:\n\n        :return: Returns batch_id\n        \"\"\"\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not create a batch without a valid job_id and an active session.\")\n\n        headers = self._get_create_batch_content_headers(file_type)\n        headers['Content-Length'] = str(len(data))\n\n        response = requests.post(self._get_create_batch_url(job_id),\n                                 headers=headers,\n                                 data=data)\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        batch_id = root.find('%sid' % self.API_NS).text\n        return batch_id", "is_method": true, "class_name": "SalesforceAPI", "function_description": "The `create_batch` method of `SalesforceAPI` uploads a collection of data to Salesforce for a specified job. It facilitates bulk data operations by returning a unique batch identifier."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "block_on_batch", "line_number": 486, "body": "def block_on_batch(self, job_id, batch_id, sleep_time_seconds=5, max_wait_time_seconds=-1):\n        \"\"\"\n        Blocks until @batch_id is completed or failed.\n        :param job_id:\n        :param batch_id:\n        :param sleep_time_seconds:\n        :param max_wait_time_seconds:\n        \"\"\"\n        if not job_id or not batch_id or not self.has_active_session():\n            raise Exception(\"Can not block on a batch without a valid batch_id, job_id and an active session.\")\n\n        start_time = time.time()\n        status = {}\n        while max_wait_time_seconds < 0 or time.time() - start_time < max_wait_time_seconds:\n            status = self._get_batch_info(job_id, batch_id)\n            logger.info(\"Batch %s Job %s in state %s.  %s records processed.  %s records failed.\" %\n                        (batch_id, job_id, status['state'], status['num_processed'], status['num_failed']))\n            if status['state'].lower() in [\"completed\", \"failed\"]:\n                return status\n            time.sleep(sleep_time_seconds)\n\n        raise Exception(\"Batch did not complete in %s seconds.  Final status was: %s\" % (sleep_time_seconds, status))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This method blocks program execution until a specified Salesforce batch job, identified by its job and batch IDs, completes or fails. It provides a synchronous wait mechanism for asynchronous Salesforce operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_results", "line_number": 509, "body": "def get_batch_results(self, job_id, batch_id):\n        \"\"\"\n        DEPRECATED: Use `get_batch_result_ids`\n        \"\"\"\n        warnings.warn(\"get_batch_results is deprecated and only returns one batch result. Please use get_batch_result_ids\")\n        return self.get_batch_result_ids(job_id, batch_id)[0]", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This deprecated method retrieves a single batch result for a Salesforce job. It advises using `get_batch_result_ids` for complete batch results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_result_ids", "line_number": 516, "body": "def get_batch_result_ids(self, job_id, batch_id):\n        \"\"\"\n        Get result IDs of a batch that has completed processing.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param batch_id: batch_id as returned by 'create_batch(...)'\n        :return: list of batch result IDs to be used in 'get_batch_result(...)'\n        \"\"\"\n        response = requests.get(self._get_batch_results_url(job_id, batch_id),\n                                headers=self._get_batch_info_headers())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        result_ids = [r.text for r in root.findall('%sresult' % self.API_NS)]\n\n        return result_ids", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Retrieves identifiers for processed results within a Salesforce bulk API batch. These IDs are crucial for subsequent retrieval of detailed batch result data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "get_batch_result", "line_number": 533, "body": "def get_batch_result(self, job_id, batch_id, result_id):\n        \"\"\"\n        Gets result back from Salesforce as whatever type was originally sent in create_batch (xml, or csv).\n        :param job_id:\n        :param batch_id:\n        :param result_id:\n\n        \"\"\"\n        response = requests.get(self._get_batch_result_url(job_id, batch_id, result_id),\n                                headers=self._get_session_headers())\n        response.raise_for_status()\n\n        return response.content", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Retrieves the raw content of a specific batch result from a Salesforce job. It delivers the result in the original data format, such as XML or CSV."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info", "line_number": 547, "body": "def _get_batch_info(self, job_id, batch_id):\n        response = requests.get(self._get_batch_info_url(job_id, batch_id),\n                                headers=self._get_batch_info_headers())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n\n        result = {\n            \"state\": root.find('%sstate' % self.API_NS).text,\n            \"num_processed\": root.find('%snumberRecordsProcessed' % self.API_NS).text,\n            \"num_failed\": root.find('%snumberRecordsFailed' % self.API_NS).text,\n        }\n        if root.find('%sstateMessage' % self.API_NS) is not None:\n            result['state_message'] = root.find('%sstateMessage' % self.API_NS).text\n        return result", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Retrieves the status and processing statistics (processed/failed records) for a specific Salesforce batch within a job. This allows monitoring of asynchronous operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_url", "line_number": 563, "body": "def _get_login_url(self):\n        server = \"login\" if not self.sandbox_name else \"test\"\n        return \"https://%s.salesforce.com/services/Soap/u/%s\" % (server, self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the appropriate Salesforce API login URL. It adapts for either production or sandbox environments, essential for authentication and connection setup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_base_url", "line_number": 567, "body": "def _get_base_url(self):\n        return \"https://%s/services\" % self.hostname", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Constructs the base URL for Salesforce API services. It provides the essential starting point for API requests made by other methods in the class."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_bulk_base_url", "line_number": 570, "body": "def _get_bulk_base_url(self):\n        # Expands on Base Url for Bulk\n        return \"%s/async/%s\" % (self._get_base_url(), self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Constructs and returns the base URL for Salesforce Bulk API operations. This internal helper method provides the necessary foundation for bulk data interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_base_url", "line_number": 574, "body": "def _get_norm_base_url(self):\n        # Expands on Base Url for Norm\n        return \"%s/data/v%s\" % (self._get_base_url(), self.API_VERSION)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the properly formatted base URL for Salesforce data API requests. It incorporates the API version for specific endpoint access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_query_url", "line_number": 578, "body": "def _get_norm_query_url(self):\n        # Expands on Norm Base Url\n        return \"%s/query\" % self._get_norm_base_url()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the complete URL endpoint for executing queries against the Salesforce API. It constructs this by appending \"/query\" to the base URL."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_url", "line_number": 582, "body": "def _get_create_job_url(self):\n        # Expands on Bulk url\n        return \"%s/job\" % (self._get_bulk_base_url())", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the URL endpoint for creating new bulk API jobs within Salesforce. This facilitates initiating large-scale data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_job_id_url", "line_number": 586, "body": "def _get_job_id_url(self, job_id):\n        # Expands on Job Creation url\n        return \"%s/%s\" % (self._get_create_job_url(), job_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates a specific Salesforce API URL for a given job ID. It's used internally to construct API endpoints for job-related operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_batch_url", "line_number": 602, "body": "def _get_create_batch_url(self, job_id):\n        # Expands on basic Job Id url\n        return \"%s/batch\" % (self._get_job_id_url(job_id))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This helper method provides the specific Salesforce API endpoint URL for creating new batches associated with a given job ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info_url", "line_number": 606, "body": "def _get_batch_info_url(self, job_id, batch_id):\n        # Expands on Batch Creation url\n        return \"%s/%s\" % (self._get_create_batch_url(job_id), batch_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the API endpoint URL to retrieve detailed information about a specific batch within a Salesforce job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_results_url", "line_number": 610, "body": "def _get_batch_results_url(self, job_id, batch_id):\n        # Expands on Batch Info url\n        return \"%s/result\" % (self._get_batch_info_url(job_id, batch_id))", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the specific API URL required to retrieve results for a Salesforce batch job. This provides the endpoint to access processed data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_result_url", "line_number": 614, "body": "def _get_batch_result_url(self, job_id, batch_id, result_id):\n        # Expands on Batch Results url\n        return \"%s/%s\" % (self._get_batch_results_url(job_id, batch_id), result_id)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the specific API URL to access a particular result within a given Salesforce batch job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_headers", "line_number": 618, "body": "def _get_login_headers(self):\n        headers = {\n            'Content-Type': \"text/xml; charset=UTF-8\",\n            'SOAPAction': 'login'\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the standard HTTP headers necessary for performing a login request to the Salesforce SOAP API."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_session_headers", "line_number": 625, "body": "def _get_session_headers(self):\n        headers = {\n            'X-SFDC-Session': self.session_id\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides standard Salesforce API session headers. This enables other methods to make authenticated requests using the stored session ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_norm_session_headers", "line_number": 631, "body": "def _get_norm_session_headers(self):\n        headers = {\n            'Authorization': 'Bearer %s' % self.session_id\n        }\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "This internal helper method generates standard HTTP authorization headers for Salesforce API requests. It constructs a bearer token using the stored session ID."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_rest_headers", "line_number": 637, "body": "def _get_rest_headers(self):\n        headers = self._get_norm_session_headers()\n        headers['Content-Type'] = 'application/json'\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides HTTP headers for Salesforce REST API calls. It ensures the 'Content-Type' is set to 'application/json' for proper request formatting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_job_headers", "line_number": 642, "body": "def _get_job_headers(self):\n        headers = self._get_session_headers()\n        headers['Content-Type'] = \"application/xml; charset=UTF-8\"\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides HTTP headers for Salesforce API job operations, specifically setting the Content-Type for XML data exchange."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_headers", "line_number": 647, "body": "def _get_create_job_headers(self):\n        return self._get_job_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the necessary HTTP headers for initiating a job creation request within the Salesforce API. It specifically retrieves headers tailored for creating Salesforce jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_abort_job_headers", "line_number": 650, "body": "def _get_abort_job_headers(self):\n        return self._get_job_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the necessary HTTP headers to abort a Salesforce job, reusing standard job headers for the operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_batch_content_headers", "line_number": 656, "body": "def _get_create_batch_content_headers(self, content_type):\n        headers = self._get_session_headers()\n        content_type = 'text/csv' if content_type.lower() == 'csv' else 'application/xml'\n        headers['Content-Type'] = \"%s; charset=UTF-8\" % content_type\n        return headers", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides specific HTTP headers required for Salesforce batch content creation, dynamically setting the Content-Type (CSV or XML) and character set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_batch_info_headers", "line_number": 662, "body": "def _get_batch_info_headers(self):\n        return self._get_session_headers()", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the necessary HTTP headers for Salesforce batch information requests. It ensures consistent session-level authentication for batch operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_login_xml", "line_number": 665, "body": "def _get_login_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n            <env:Envelope xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n                xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xmlns:env=\"http://schemas.xmlsoap.org/soap/envelope/\">\n              <env:Body>\n                <n1:login xmlns:n1=\"urn:partner.soap.sforce.com\">\n                  <n1:username>%s</n1:username>\n                  <n1:password>%s%s</n1:password>\n                </n1:login>\n              </env:Body>\n            </env:Envelope>\n        \"\"\" % (self.username, self.password, self.security_token if self.sandbox_name is None else self.sb_security_token)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the SOAP XML payload required for authenticating with the Salesforce API, incorporating user credentials and security token."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_create_job_xml", "line_number": 679, "body": "def _get_create_job_xml(self, operation, obj, external_id_field_name, content_type):\n        external_id_field_name_element = \"\" if not external_id_field_name else \\\n            \"\\n<externalIdFieldName>%s</externalIdFieldName>\" % external_id_field_name\n\n        # Note: \"Unable to parse job\" error may be caused by reordering fields.\n        #       ExternalIdFieldName element must be before contentType element.\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n                <operation>%s</operation>\n                <object>%s</object>\n                %s\n                <contentType>%s</contentType>\n            </jobInfo>\n        \"\"\" % (operation, obj, external_id_field_name_element, content_type)", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the XML request payload to create a new job for bulk data operations in the Salesforce API. It specifies the operation, object, and content type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_abort_job_xml", "line_number": 694, "body": "def _get_abort_job_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n              <state>Aborted</state>\n            </jobInfo>\n        \"\"\"", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Provides the specific XML payload required to set a Salesforce job's state to 'Aborted'. Useful for constructing requests to cancel ongoing bulk API operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/salesforce.py", "function": "_get_close_job_xml", "line_number": 701, "body": "def _get_close_job_xml(self):\n        return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <jobInfo xmlns=\"http://www.force.com/2009/06/asyncapi/dataload\">\n              <state>Closed</state>\n            </jobInfo>\n        \"\"\"", "is_method": true, "class_name": "SalesforceAPI", "function_description": "Generates the XML payload to set a Salesforce bulk data job's state to 'Closed'."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_daily_snapshot.py", "function": "latest", "line_number": 51, "body": "def latest(cls, *args, **kwargs):\n        \"\"\"This is cached so that requires() is deterministic.\"\"\"\n        date = kwargs.pop(\"date\", datetime.date.today())\n        lookback = kwargs.pop(\"lookback\", 14)\n        # hashing kwargs deterministically would be hard. Let's just lookup by equality\n        key = (cls, args, kwargs, lookback, date)\n        for k, v in ExternalDailySnapshot.__cache:\n            if k == key:\n                return v\n        val = cls.__latest(date, lookback, args, kwargs)\n        ExternalDailySnapshot.__cache.append((key, val))\n        return val", "is_method": true, "class_name": "ExternalDailySnapshot", "function_description": "Retrieves the latest external daily snapshot data. It utilizes a cache to provide deterministic results and avoid recomputation for given date and lookback parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_daily_snapshot.py", "function": "__latest", "line_number": 65, "body": "def __latest(cls, date, lookback, args, kwargs):\n        assert lookback > 0\n        t = None\n        for i in range(lookback):\n            d = date - datetime.timedelta(i)\n            t = cls(date=d, *args, **kwargs)\n            if t.complete():\n                return t\n        logger.debug(\"Could not find last dump for %s (looked back %d days)\",\n                     cls.__name__, lookback)\n        return t", "is_method": true, "class_name": "ExternalDailySnapshot", "function_description": "This internal method for `ExternalDailySnapshot` finds the most recent complete daily snapshot. It searches backward from a given date within a specified historical period."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "_get_jars", "line_number": 81, "body": "def _get_jars(self, path):\n        return [os.path.join(path, j) for j in os.listdir(path)\n                if j.endswith('.jar')]", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "This method provides a list of absolute paths for all JAR files found within a specified directory. It's useful for collecting necessary Java dependencies for Scalding job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scala_jars", "line_number": 85, "body": "def get_scala_jars(self, include_compiler=False):\n        lib_dir = os.path.join(self.scala_home, 'lib')\n        jars = [os.path.join(lib_dir, 'scala-library.jar')]\n\n        # additional jar for scala 2.10 only\n        reflect = os.path.join(lib_dir, 'scala-reflect.jar')\n        if os.path.exists(reflect):\n            jars.append(reflect)\n\n        if include_compiler:\n            jars.append(os.path.join(lib_dir, 'scala-compiler.jar'))\n\n        return jars", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "This method retrieves a list of essential Scala JAR file paths (library, reflect, and optionally compiler) needed for running or compiling Scala/Scalding jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scalding_jars", "line_number": 99, "body": "def get_scalding_jars(self):\n        lib_dir = os.path.join(self.scalding_home, 'lib')\n        return self._get_jars(lib_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Provides the necessary Scalding JAR files by scanning its 'lib' directory. This enables the proper execution and deployment of Scalding-based applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_scalding_core", "line_number": 103, "body": "def get_scalding_core(self):\n        lib_dir = os.path.join(self.scalding_home, 'lib')\n        for j in os.listdir(lib_dir):\n            if j.startswith('scalding-core-'):\n                p = os.path.join(lib_dir, j)\n                logger.debug('Found scalding-core: %s', p)\n                return p\n        raise luigi.contrib.hadoop.HadoopJobError('Could not find scalding-core.')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Locates the `scalding-core` library file within the Scalding installation directory. This method is crucial for ensuring the proper execution of Scalding jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_provided_jars", "line_number": 112, "body": "def get_provided_jars(self):\n        return self._get_jars(self.provided_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "This method retrieves a list of provided JAR files required for running a Scalding job. It aggregates pre-configured external dependencies for job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_libjars", "line_number": 115, "body": "def get_libjars(self):\n        return self._get_jars(self.libjars_dir)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Retrieves the collection of library JAR files required for the Scalding job execution. This method provides the necessary dependencies for the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_tmp_job_jar", "line_number": 118, "body": "def get_tmp_job_jar(self, source):\n        job_name = os.path.basename(os.path.splitext(source)[0])\n        return os.path.join(self.tmp_dir.path, job_name + '.jar')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Provides the temporary file path where a compiled Scalding job's JAR artifact will be stored or located. It ensures consistent temporary storage for job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_build_dir", "line_number": 122, "body": "def get_build_dir(self, source):\n        build_dir = os.path.join(self.tmp_dir.path, 'build')\n        return build_dir", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "This method provides the standardized path to a temporary build directory within the Scalding job runner's working space. It ensures consistent location for compiled artifacts or intermediate files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "get_job_class", "line_number": 126, "body": "def get_job_class(self, source):\n        # find name of the job class\n        # usually the one that matches file name or last class that extends Job\n        job_name = os.path.splitext(os.path.basename(source))[0]\n        package = None\n        job_class = None\n        for line in open(source).readlines():\n            p = re.search(r'package\\s+([^\\s\\(]+)', line)\n            if p:\n                package = p.groups()[0]\n            p = re.search(r'class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)\n            if p:\n                job_class = p.groups()[0]\n                if job_class == job_name:\n                    break\n        if job_class:\n            if package:\n                job_class = package + '.' + job_class\n            logger.debug('Found scalding job class: %s', job_class)\n            return job_class\n        else:\n            raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Identifies the fully qualified class name of a Scalding job by parsing its source file. This enables the runner to locate and execute the specified job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "build_job_jar", "line_number": 149, "body": "def build_job_jar(self, job):\n        job_jar = job.jar()\n        if job_jar:\n            if not os.path.exists(job_jar):\n                logger.error(\"Can't find jar: %s, full path %s\", job_jar, os.path.abspath(job_jar))\n                raise Exception(\"job jar does not exist\")\n            if not job.job_class():\n                logger.error(\"Undefined job_class()\")\n                raise Exception(\"Undefined job_class()\")\n            return job_jar\n\n        job_src = job.source()\n        if not job_src:\n            logger.error(\"Both source() and jar() undefined\")\n            raise Exception(\"Both source() and jar() undefined\")\n        if not os.path.exists(job_src):\n            logger.error(\"Can't find source: %s, full path %s\", job_src, os.path.abspath(job_src))\n            raise Exception(\"job source does not exist\")\n\n        job_src = job.source()\n        job_jar = self.get_tmp_job_jar(job_src)\n\n        build_dir = self.get_build_dir(job_src)\n        if not os.path.exists(build_dir):\n            os.makedirs(build_dir)\n\n        classpath = ':'.join(filter(None,\n                                    self.get_scalding_jars() +\n                                    self.get_provided_jars() +\n                                    self.get_libjars() +\n                                    job.extra_jars()))\n        scala_cp = ':'.join(self.get_scala_jars(include_compiler=True))\n\n        # compile scala source\n        arglist = ['java', '-cp', scala_cp, 'scala.tools.nsc.Main',\n                   '-classpath', classpath,\n                   '-d', build_dir, job_src]\n        logger.info('Compiling scala source: %s', subprocess.list2cmdline(arglist))\n        subprocess.check_call(arglist)\n\n        # build job jar file\n        arglist = ['jar', 'cf', job_jar, '-C', build_dir, '.']\n        logger.info('Building job jar: %s', subprocess.list2cmdline(arglist))\n        subprocess.check_call(arglist)\n        return job_jar", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Provides a runnable JAR file for a Scalding job by either validating an existing one or compiling its Scala source code into a new JAR. This ensures the job is prepared for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "run_job", "line_number": 195, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        job_jar = self.build_job_jar(job)\n        jars = [job_jar] + self.get_libjars() + job.extra_jars()\n        scalding_core = self.get_scalding_core()\n        libjars = ','.join(filter(None, jars))\n        arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', scalding_core, '-libjars', libjars]\n        arglist += ['-D%s' % c for c in job.jobconfs()]\n\n        job_class = job.job_class() or self.get_job_class(job.source())\n        arglist += [job_class, '--hdfs']\n\n        # scalding does not parse argument with '=' properly\n        arglist += ['--name', job.task_id.replace('=', ':')]\n\n        (tmp_files, job_args) = luigi.contrib.hadoop_jar.fix_paths(job)\n        arglist += job_args\n\n        env = os.environ.copy()\n        jars.append(scalding_core)\n        hadoop_cp = ':'.join(filter(None, jars))\n        env['HADOOP_CLASSPATH'] = hadoop_cp\n        logger.info(\"Submitting Hadoop job: HADOOP_CLASSPATH=%s %s\",\n                    hadoop_cp, subprocess.list2cmdline(arglist))\n        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url, env=env)\n\n        for a, b in tmp_files:\n            a.move(b)", "is_method": true, "class_name": "ScaldingJobRunner", "function_description": "Executes a Scalding job on a Hadoop cluster by constructing the necessary command and environment. It then submits and tracks the job's execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "relpath", "line_number": 243, "body": "def relpath(self, current_file, rel_path):\n        \"\"\"\n        Compute path given current file and relative path.\n        \"\"\"\n        script_dir = os.path.dirname(os.path.abspath(current_file))\n        rel_path = os.path.abspath(os.path.join(script_dir, rel_path))\n        return rel_path", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "This method computes an absolute path by resolving a given relative path against the directory of a specified current file. It helps locate resources reliably within a project's structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "extra_jars", "line_number": 267, "body": "def extra_jars(self):\n        \"\"\"\n        Extra jars for building and running this Scalding Job.\n        \"\"\"\n        return []", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Specifies additional JAR file dependencies required for building and running this Scalding job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "job_class", "line_number": 273, "body": "def job_class(self):\n        \"\"\"\n        optional main job class for this Scalding Job.\n        \"\"\"\n        return None", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Retrieves the optional main job class associated with this Scalding Job task, returning None if no class is specified."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "job_runner", "line_number": 279, "body": "def job_runner(self):\n        return ScaldingJobRunner()", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Provides a ScaldingJobRunner instance. This enables the execution of Scalding jobs within the system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "atomic_output", "line_number": 282, "body": "def atomic_output(self):\n        \"\"\"\n        If True, then rewrite output arguments to be temp locations and\n        atomically move them into place after the job finishes.\n        \"\"\"\n        return True", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Enables atomic output handling for Scalding jobs, ensuring outputs are written to temporary locations and moved into place after completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "requires", "line_number": 289, "body": "def requires(self):\n        return {}", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "Indicates this Scalding job task has no external dependencies, acting as a root or independent task in a workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/scalding.py", "function": "args", "line_number": 298, "body": "def args(self):\n        \"\"\"\n        Returns an array of args to pass to the job.\n        \"\"\"\n        arglist = []\n        for k, v in self.requires_hadoop().items():\n            arglist.append('--' + k)\n            arglist.extend([t.output().path for t in flatten(v)])\n        arglist.extend(['--output', self.output()])\n        arglist.extend(self.job_args())\n        return arglist", "is_method": true, "class_name": "ScaldingJobTask", "function_description": "This method assembles all command-line arguments for a Scalding job. It includes necessary input dependencies, output paths, and custom job parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "_init_kubernetes", "line_number": 72, "body": "def _init_kubernetes(self):\n        self.__logger = logger\n        self.__logger.debug(\"Kubernetes auth method: \" + self.auth_method)\n        if self.auth_method == \"kubeconfig\":\n            self.__kube_api = HTTPClient(KubeConfig.from_file(self.kubeconfig_path))\n        elif self.auth_method == \"service-account\":\n            self.__kube_api = HTTPClient(KubeConfig.from_service_account())\n        else:\n            raise ValueError(\"Illegal auth_method\")\n        self.job_uuid = str(uuid.uuid4().hex)\n        now = datetime.utcnow()\n        self.uu_name = \"%s-%s-%s\" % (self.name, now.strftime('%Y%m%d%H%M%S'), self.job_uuid[:16])", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Configures the Kubernetes API client for interaction with the cluster and creates unique identifiers for the job, based on the specified authentication method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "auth_method", "line_number": 86, "body": "def auth_method(self):\n        \"\"\"\n        This can be set to ``kubeconfig`` or ``service-account``.\n        It defaults to ``kubeconfig``.\n\n        For more details, please refer to:\n\n        - kubeconfig: http://kubernetes.io/docs/user-guide/kubeconfig-file\n        - service-account: http://kubernetes.io/docs/user-guide/service-accounts\n        \"\"\"\n        return self.kubernetes_config.auth_method", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides the authentication method used by the Kubernetes job task to connect to the cluster, typically 'kubeconfig' or 'service-account'."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubeconfig_path", "line_number": 99, "body": "def kubeconfig_path(self):\n        \"\"\"\n        Path to kubeconfig file used for cluster authentication.\n        It defaults to \"~/.kube/config\", which is the default location\n        when using minikube (http://kubernetes.io/docs/getting-started-guides/minikube).\n        When auth_method is ``service-account`` this property is ignored.\n\n        **WARNING**: For Python versions < 3.5 kubeconfig must point to a Kubernetes API\n        hostname, and NOT to an IP address.\n\n        For more details, please refer to:\n        http://kubernetes.io/docs/user-guide/kubeconfig-file\n        \"\"\"\n        return self.kubernetes_config.kubeconfig_path", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides the path to the `kubeconfig` file. This path is used for authenticating the task with a Kubernetes cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubernetes_namespace", "line_number": 115, "body": "def kubernetes_namespace(self):\n        \"\"\"\n        Namespace in Kubernetes where the job will run.\n        It defaults to the default namespace in Kubernetes\n\n        For more details, please refer to:\n        https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\n        \"\"\"\n        return self.kubernetes_config.kubernetes_namespace", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "This method provides the Kubernetes namespace where the job will be executed. It defines the job's operational scope within the cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "labels", "line_number": 134, "body": "def labels(self):\n        \"\"\"\n        Return custom labels for kubernetes job.\n        example::\n        ``{\"run_dt\": datetime.date.today().strftime('%F')}``\n        \"\"\"\n        return {}", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides a hook for subclasses to define and return custom key-value labels for a Kubernetes job, enabling metadata attachment for organization and filtering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "max_retrials", "line_number": 170, "body": "def max_retrials(self):\n        \"\"\"\n        Maximum number of retrials in case of failure.\n        \"\"\"\n        return self.kubernetes_config.max_retrials", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Retrieves the maximum number of retrials configured for this Kubernetes job task. This value determines how many times the job will attempt to rerun upon failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "backoff_limit", "line_number": 177, "body": "def backoff_limit(self):\n        \"\"\"\n        Maximum number of retries before considering the job as failed.\n        See: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy\n        \"\"\"\n        return 6", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Provides the maximum number of retries for a Kubernetes job before it is marked as failed. This value configures the job's backoff policy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "kubernetes_config", "line_number": 207, "body": "def kubernetes_config(self):\n        if not self._kubernetes_config:\n            self._kubernetes_config = kubernetes()\n        return self._kubernetes_config", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Retrieves or lazily initializes the Kubernetes configuration for the task. It ensures a single configuration object is consistently available for Kubernetes operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "poll_interval", "line_number": 213, "body": "def poll_interval(self):\n        \"\"\"How often to poll Kubernetes for job status, in seconds.\"\"\"\n        return self.__DEFAULT_POLL_INTERVAL", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Returns the default time interval, in seconds, for how often Kubernetes job status should be polled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "pod_creation_wait_interal", "line_number": 218, "body": "def pod_creation_wait_interal(self):\n        \"\"\"Delay for initial pod creation for just submitted job in seconds\"\"\"\n        return self.__DEFAULT_POD_CREATION_INTERVAL", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "This method provides the default time interval in seconds to wait for a Kubernetes pod to be created after a job submission. It specifies the initial delay before checking the pod's status."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__track_job", "line_number": 222, "body": "def __track_job(self):\n        \"\"\"Poll job status while active\"\"\"\n        while not self.__verify_job_has_started():\n            time.sleep(self.poll_interval)\n            self.__logger.debug(\"Waiting for Kubernetes job \" + self.uu_name + \" to start\")\n        self.__print_kubectl_hints()\n\n        status = self.__get_job_status()\n        while status == \"RUNNING\":\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name + \" is running\")\n            time.sleep(self.poll_interval)\n            status = self.__get_job_status()\n\n        assert status != \"FAILED\", \"Kubernetes job \" + self.uu_name + \" failed\"\n\n        # status == \"SUCCEEDED\"\n        self.__logger.info(\"Kubernetes job \" + self.uu_name + \" succeeded\")\n        self.signal_complete()", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Continuously monitors the status of an associated Kubernetes job. It polls the job's progress from start until it successfully completes or fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_pods", "line_number": 252, "body": "def __get_pods(self):\n        pod_objs = Pod.objects(self.__kube_api, namespace=self.kubernetes_namespace) \\\n            .filter(selector=\"job-name=\" + self.uu_name) \\\n            .response['items']\n        return [Pod(self.__kube_api, p) for p in pod_objs]", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Retrieves Kubernetes Pods belonging to the current Kubernetes job task, identified by its unique name. This provides a way to access and manage the job's associated pod resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_job", "line_number": 258, "body": "def __get_job(self):\n        jobs = Job.objects(self.__kube_api, namespace=self.kubernetes_namespace) \\\n            .filter(selector=\"luigi_task_id=\" + self.job_uuid) \\\n            .response['items']\n        assert len(jobs) == 1, \"Kubernetes job \" + self.uu_name + \" not found\"\n        return Job(self.__kube_api, jobs[0])", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "This method retrieves the specific Kubernetes job associated with the current task, identified by its unique Luigi task ID. It ensures a unique job is found, providing access to its status and details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__print_pod_logs", "line_number": 265, "body": "def __print_pod_logs(self):\n        for pod in self.__get_pods():\n            logs = pod.logs(timestamps=True).strip()\n            self.__logger.info(\"Fetching logs from \" + pod.name)\n            if len(logs) > 0:\n                for line in logs.split('\\n'):\n                    self.__logger.info(line)", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "As a private helper for `KubernetesJobTask`, this method retrieves and outputs all logs from associated Kubernetes pods to the logger for monitoring or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__print_kubectl_hints", "line_number": 273, "body": "def __print_kubectl_hints(self):\n        self.__logger.info(\"To stream Pod logs, use:\")\n        for pod in self.__get_pods():\n            self.__logger.info(\"`kubectl logs -f pod/%s -n %s`\" % (pod.name, pod.namespace))", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Outputs helpful `kubectl` commands for streaming logs from associated Kubernetes pods. This assists users in monitoring and debugging their Kubernetes job tasks.\nOutputs helpful `kubectl` commands for streaming logs from associated Kubernetes pods. This assists users in monitoring and debugging their Kubernetes job tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__verify_job_has_started", "line_number": 278, "body": "def __verify_job_has_started(self):\n        \"\"\"Asserts that the job has successfully started\"\"\"\n        # Verify that the job started\n        self.__get_job()\n\n        # Verify that the pod started\n        pods = self.__get_pods()\n        if not pods:\n            self.__logger.debug(\n                'No pods found for %s, waiting for cluster state to match the job definition' % self.uu_name\n            )\n            time.sleep(self.pod_creation_wait_interal)\n            pods = self.__get_pods()\n\n        assert len(pods) > 0, \"No pod scheduled by \" + self.uu_name\n        for pod in pods:\n            status = pod.obj['status']\n            for cont_stats in status.get('containerStatuses', []):\n                if 'terminated' in cont_stats['state']:\n                    t = cont_stats['state']['terminated']\n                    err_msg = \"Pod %s %s (exit code %d). Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, t['reason'], t['exitCode'], pod.name)\n                    assert t['exitCode'] == 0, err_msg\n\n                if 'waiting' in cont_stats['state']:\n                    wr = cont_stats['state']['waiting']['reason']\n                    assert wr == 'ContainerCreating', \"Pod %s %s. Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, wr, pod.name)\n\n            for cond in status.get('conditions', []):\n                if 'message' in cond:\n                    if cond['reason'] == 'ContainersNotReady':\n                        return False\n                    assert cond['status'] != 'False', \\\n                        \"[ERROR] %s - %s\" % (cond['reason'], cond['message'])\n        return True", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Verifies a Kubernetes job and its associated pods have successfully started. It checks pod existence and container statuses to ensure the job is initializing without errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__get_job_status", "line_number": 315, "body": "def __get_job_status(self):\n        \"\"\"Return the Kubernetes job status\"\"\"\n        # Figure out status and return it\n        job = self.__get_job()\n\n        if \"succeeded\" in job.obj[\"status\"] and job.obj[\"status\"][\"succeeded\"] > 0:\n            job.scale(replicas=0)\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if self.delete_on_success:\n                self.__delete_job_cascade(job)\n            return \"SUCCEEDED\"\n\n        if \"failed\" in job.obj[\"status\"]:\n            failed_cnt = job.obj[\"status\"][\"failed\"]\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name\n                                + \" status.failed: \" + str(failed_cnt))\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if failed_cnt > self.max_retrials:\n                job.scale(replicas=0)  # avoid more retrials\n                return \"FAILED\"\n        return \"RUNNING\"", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Determines the current execution status of an associated Kubernetes job. It simplifies the status to SUCCEEDED, FAILED, or RUNNING, and manages post-completion actions like logging and scaling down."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "__delete_job_cascade", "line_number": 339, "body": "def __delete_job_cascade(self, job):\n        delete_options_cascade = {\n            \"kind\": \"DeleteOptions\",\n            \"apiVersion\": \"v1\",\n            \"propagationPolicy\": \"Background\"\n        }\n        r = self.__kube_api.delete(json=delete_options_cascade, **job.api_kwargs())\n        if r.status_code != 200:\n            self.__kube_api.raise_for_status(r)", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "Deletes a specified Kubernetes job. It also cascades the deletion to associated resources, ensuring background cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/kubernetes.py", "function": "run", "line_number": 349, "body": "def run(self):\n        self._init_kubernetes()\n        # Render job\n        job_json = {\n            \"apiVersion\": \"batch/v1\",\n            \"kind\": \"Job\",\n            \"metadata\": {\n                \"name\": self.uu_name,\n                \"labels\": {\n                    \"spawned_by\": \"luigi\",\n                    \"luigi_task_id\": self.job_uuid\n                }\n            },\n            \"spec\": {\n                \"backoffLimit\": self.backoff_limit,\n                \"template\": {\n                    \"metadata\": {\n                        \"name\": self.uu_name,\n                        \"labels\": {}\n                    },\n                    \"spec\": self.spec_schema\n                }\n            }\n        }\n        if self.kubernetes_namespace is not None:\n            job_json['metadata']['namespace'] = self.kubernetes_namespace\n        if self.active_deadline_seconds is not None:\n            job_json['spec']['activeDeadlineSeconds'] = \\\n                self.active_deadline_seconds\n        # Update user labels\n        job_json['metadata']['labels'].update(self.labels)\n        job_json['spec']['template']['metadata']['labels'].update(self.labels)\n\n        # Add default restartPolicy if not specified\n        if \"restartPolicy\" not in self.spec_schema:\n            job_json[\"spec\"][\"template\"][\"spec\"][\"restartPolicy\"] = \"Never\"\n        # Submit job\n        self.__logger.info(\"Submitting Kubernetes Job: \" + self.uu_name)\n        job = Job(self.__kube_api, job_json)\n        job.create()\n        # Track the Job (wait while active)\n        self.__logger.info(\"Start tracking Kubernetes Job: \" + self.uu_name)\n        self.__track_job()", "is_method": true, "class_name": "KubernetesJobTask", "function_description": "The `run` method of `KubernetesJobTask` creates and submits a Kubernetes Job based on the task's configuration. It defines the job's specifications and tracks its execution on the cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "load_hive_cmd", "line_number": 47, "body": "def load_hive_cmd():\n    return luigi.configuration.get_config().get('hive', 'command', 'hive').split(' ')", "is_method": false, "function_description": "Retrieves the configured Hive command-line arguments from Luigi's global settings. It provides the command as a list suitable for execution by other tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_hive_syntax", "line_number": 51, "body": "def get_hive_syntax():\n    return luigi.configuration.get_config().get('hive', 'release', 'cdh4')", "is_method": false, "function_description": "Retrieves the configured Hive release version from Luigi's global configuration. This specifies the expected Hive syntax for operations, defaulting to 'cdh4'."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_hive_warehouse_location", "line_number": 55, "body": "def get_hive_warehouse_location():\n    return luigi.configuration.get_config().get('hive', 'warehouse_location', '/user/hive/warehouse')", "is_method": false, "function_description": "Provides the Hive data warehouse path by retrieving it from Luigi configuration. Defaults to '/user/hive/warehouse' if not set."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_ignored_file_masks", "line_number": 59, "body": "def get_ignored_file_masks():\n    return luigi.configuration.get_config().get('hive', 'ignored_file_masks', None)", "is_method": false, "function_description": "Retrieves file masks configured to be ignored within the 'hive' section from Luigi's global configuration. This allows the system to identify and skip specific files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive", "line_number": 63, "body": "def run_hive(args, check_return_code=True):\n    \"\"\"\n    Runs the `hive` from the command line, passing in the given args, and\n    returning stdout.\n\n    With the apache release of Hive, so of the table existence checks\n    (which are done using DESCRIBE do not exit with a return code of 0\n    so we need an option to ignore the return code and just return stdout for parsing\n    \"\"\"\n    cmd = load_hive_cmd() + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout.decode('utf-8')", "is_method": false, "function_description": "Executes a Hive command-line operation and captures its standard output. It offers an option to bypass return code checks, useful for automating Hive interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive_cmd", "line_number": 81, "body": "def run_hive_cmd(hivecmd, check_return_code=True):\n    \"\"\"\n    Runs the given hive query and returns stdout.\n    \"\"\"\n    return run_hive(['-e', hivecmd], check_return_code)", "is_method": false, "function_description": "Executes a given Hive query or command and returns its standard output. This function is useful for programmatically interacting with Hive."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_hive_script", "line_number": 88, "body": "def run_hive_script(script):\n    \"\"\"\n    Runs the contents of the given script in hive and returns stdout.\n    \"\"\"\n    if not os.path.isfile(script):\n        raise RuntimeError(\"Hive script: {0} does not exist.\".format(script))\n    return run_hive(['-f', script])", "is_method": false, "function_description": "This function executes a specified Hive script file using the Hive engine. It returns the standard output generated by the script's execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_is_ordered_dict", "line_number": 97, "body": "def _is_ordered_dict(dikt):\n    if isinstance(dikt, collections.OrderedDict):\n        return True\n\n    if sys.version_info >= (3, 7):\n        return isinstance(dikt, dict)\n\n    return False", "is_method": false, "function_description": "Checks if a given dictionary object maintains insertion order. It accounts for Python 3.7+ where standard dictionaries are insertion-ordered by default."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_validate_partition", "line_number": 107, "body": "def _validate_partition(partition):\n    \"\"\"\n    If partition is set and its size is more than one and not ordered,\n    then we're unable to restore its path in the warehouse\n    \"\"\"\n    if (\n            partition\n            and len(partition) > 1\n            and not _is_ordered_dict(partition)\n    ):\n        raise ValueError('Unable to restore table/partition location')", "is_method": false, "function_description": "Validates if a partition has the necessary structure (ordered if multi-element) for successful path restoration in a data warehouse."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_default_client", "line_number": 326, "body": "def get_default_client():\n    syntax = get_hive_syntax()\n    if syntax == \"apache\":\n        return ApacheHiveCommandClient()\n    elif syntax == \"metastore\":\n        return MetastoreClient()\n    elif syntax == 'warehouse':\n        return WarehouseHiveClient()\n    else:\n        return HiveCommandClient()", "is_method": false, "function_description": "This function provides the appropriate default Hive client instance based on the detected Hive syntax configuration. It serves as a utility to dynamically select the correct client for interacting with Hive."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 156, "body": "def table_location(self, table, database='default', partition=None):\n        cmd = \"use {0}; describe formatted {1}\".format(database, table)\n        if partition is not None:\n            cmd += \" PARTITION ({0})\".format(self.partition_spec(partition))\n\n        stdout = run_hive_cmd(cmd)\n\n        for line in stdout.split(\"\\n\"):\n            if \"Location:\" in line:\n                return line.split(\"\\t\")[1]", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Retrieves the storage location (e.g., HDFS path) of a specified Hive table or a particular partition within it. This enables direct access to the table's underlying data files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 167, "body": "def table_exists(self, table, database='default', partition=None):\n        if partition is None:\n            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n\n            return stdout and table.lower() in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Verifies the existence of a Hive table in a specified database. It can also check if a particular partition exists within that table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 181, "body": "def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table))\n        if not describe or \"does not exist\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Retrieves the detailed column schema (name, type, comments) for a specified Hive table. Provides programmatic access to inspect table structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 187, "body": "def partition_spec(self, partition):\n        \"\"\"\n        Turns a dict into the a Hive partition specification string.\n        \"\"\"\n        return ','.join([\"`{0}`='{1}'\".format(k, v) for (k, v) in\n                         sorted(partition.items(), key=operator.itemgetter(0))])", "is_method": true, "class_name": "HiveCommandClient", "function_description": "Generates a Hive partition specification string from a dictionary of key-value pairs. This enables proper targeting of data partitions in Hive commands."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 201, "body": "def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table), False)\n        if not describe or \"Table not found\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]", "is_method": true, "class_name": "ApacheHiveCommandClient", "function_description": "Retrieves the schema (columns, types, comments) for a specified table from an Apache Hive database. This provides essential metadata for understanding table structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 210, "body": "def table_location(self, table, database='default', partition=None):\n        with HiveThriftContext() as client:\n            if partition is not None:\n                try:\n                    import hive_metastore.ttypes\n                    partition_str = self.partition_spec(partition)\n                    thrift_table = client.get_partition_by_name(database, table, partition_str)\n                except hive_metastore.ttypes.NoSuchObjectException:\n                    return ''\n            else:\n                thrift_table = client.get_table(database, table)\n            return thrift_table.sd.location", "is_method": true, "class_name": "MetastoreClient", "function_description": "Retrieves the storage location for a specified Hive table or its partition from the Metastore. This enables direct access to the underlying data files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 223, "body": "def table_exists(self, table, database='default', partition=None):\n        with HiveThriftContext() as client:\n            if partition is None:\n                return table in client.get_all_tables(database)\n            else:\n                return partition in self._existing_partitions(table, database, client)", "is_method": true, "class_name": "MetastoreClient", "function_description": "Checks if a specified table exists within a database in the metastore. It can also verify the existence of a particular partition for that table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_existing_partitions", "line_number": 230, "body": "def _existing_partitions(self, table, database, client):\n        def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def\n\n        # -1 is max_parts, the # of partition names to return (-1 = unlimited)\n        partition_strings = client.get_partition_names(database, table, -1)\n        return [_parse_partition_string(existing_partition) for existing_partition in partition_strings]", "is_method": true, "class_name": "MetastoreClient", "function_description": "Retrieves and parses all existing partition definitions for a specified table from the metastore client, returning them as structured dictionaries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_schema", "line_number": 242, "body": "def table_schema(self, table, database='default'):\n        with HiveThriftContext() as client:\n            return [(field_schema.name, field_schema.type) for field_schema in client.get_schema(database, table)]", "is_method": true, "class_name": "MetastoreClient", "function_description": "Retrieves the column names and data types (schema) for a specified table from the Metastore. This provides essential metadata for data cataloging and query generation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 246, "body": "def partition_spec(self, partition):\n        return \"/\".join(\"%s=%s\" % (k, v) for (k, v) in sorted(partition.items(), key=operator.itemgetter(0)))", "is_method": true, "class_name": "MetastoreClient", "function_description": "The MetastoreClient's `partition_spec` method provides a standardized, path-like string representation for data partitions. It formats partition key-value pairs into a sorted string (e.g., \"key=value/key2=value2\")."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "__enter__", "line_number": 255, "body": "def __enter__(self):\n        try:\n            from thrift.transport import TSocket\n            from thrift.transport import TTransport\n            from thrift.protocol import TBinaryProtocol\n            # Note that this will only work with a CDH release.\n            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.\n            # If using the Apache release of Hive this import will fail.\n            from hive_metastore import ThriftHiveMetastore\n            config = luigi.configuration.get_config()\n            host = config.get('hive', 'metastore_host')\n            port = config.getint('hive', 'metastore_port')\n            transport = TSocket.TSocket(host, port)\n            transport = TTransport.TBufferedTransport(transport)\n            protocol = TBinaryProtocol.TBinaryProtocol(transport)\n            transport.open()\n            self.transport = transport\n            return ThriftHiveMetastore.Client(protocol)\n        except ImportError as e:\n            raise Exception('Could not import Hive thrift library:' + str(e))", "is_method": true, "class_name": "HiveThriftContext", "function_description": "Establishes a Thrift client connection to the Hive Metastore, providing programmatic access to Hive metadata within a context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "__exit__", "line_number": 276, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.transport.close()", "is_method": true, "class_name": "HiveThriftContext", "function_description": "Manages resource cleanup for the Hive Thrift connection. It ensures the underlying transport is closed when the context manager is exited.\nManages resource cleanup for the Hive Thrift connection. It ensures the underlying transport is closed when the context manager is exited, preventing resource leaks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_location", "line_number": 292, "body": "def table_location(self, table, database='default', partition=None):\n        return os.path.join(\n            self.warehouse_location,\n            database + '.db',\n            table,\n            self.partition_spec(partition)\n        )", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Calculates the file system path for a given Hive table, including its database and optional partition. Essential for locating table data within the warehouse."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "table_exists", "line_number": 300, "body": "def table_exists(self, table, database='default', partition=None):\n        \"\"\"\n        The table/partition is considered existing if corresponding path in hdfs exists\n        and contains file except those which match pattern set in  `ignored_file_masks`\n        \"\"\"\n        path = self.table_location(table, database, partition)\n        if self.hdfs_client.exists(path):\n            ignored_files = get_ignored_file_masks()\n            if ignored_files is None:\n                return True\n\n            filenames = self.hdfs_client.listdir(path)\n            pattern = re.compile(ignored_files)\n            for filename in filenames:\n                if not pattern.match(filename):\n                    return True\n\n        return False", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Verifies if a Hive table or partition physically exists on HDFS. It checks the corresponding HDFS path and ensures it contains at least one valid data file, excluding specified ignored file patterns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "partition_spec", "line_number": 319, "body": "def partition_spec(self, partition):\n        _validate_partition(partition)\n        return '/'.join([\n            '{}={}'.format(k, v) for (k, v) in (partition or {}).items()\n        ])", "is_method": true, "class_name": "WarehouseHiveClient", "function_description": "Generates a standard Hive partition path string from a dictionary of key-value pairs. This is useful for constructing file paths or SQL queries in a Hive environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hiverc", "line_number": 356, "body": "def hiverc(self):\n        \"\"\"\n        Location of an rc file to run before the query\n        if hiverc-location key is specified in luigi.cfg, will default to the value there\n        otherwise returns None.\n\n        Returning a list of rc files will load all of them in order.\n        \"\"\"\n        return luigi.configuration.get_config().get('hive', 'hiverc-location', default=None)", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Provides the location of a Hive resource configuration file (`.hiverc`) from Luigi settings, which is run before executing Hive queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hivevars", "line_number": 366, "body": "def hivevars(self):\n        \"\"\"\n        Returns a dict of key=value settings to be passed along\n        to the hive command line via --hivevar.\n        This option can be used as a separated namespace for script local variables.\n        See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VariableSubstitution\n        \"\"\"\n        return {}", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Provides a customizable dictionary of key-value settings to be passed as variables to Hive command-line queries. It serves as a dedicated namespace for script-local variables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "hiveconfs", "line_number": 375, "body": "def hiveconfs(self):\n        \"\"\"\n        Returns a dict of key=value settings to be passed along\n        to the hive command line via --hiveconf. By default, sets\n        mapred.job.name to task_id and if not None, sets:\n\n        * mapred.reduce.tasks (n_reduce_tasks)\n        * mapred.fairscheduler.pool (pool) or mapred.job.queue.name (pool)\n        * hive.exec.reducers.bytes.per.reducer (bytes_per_reducer)\n        * hive.exec.reducers.max (reducers_max)\n        \"\"\"\n        jcs = {}\n        jcs['mapred.job.name'] = \"'\" + self.task_id + \"'\"\n        if self.n_reduce_tasks is not None:\n            jcs['mapred.reduce.tasks'] = self.n_reduce_tasks\n        if self.pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = luigi.configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs['mapred.fairscheduler.pool'] = self.pool\n            elif scheduler_type == 'capacity':\n                jcs['mapred.job.queue.name'] = self.pool\n        if self.bytes_per_reducer is not None:\n            jcs['hive.exec.reducers.bytes.per.reducer'] = self.bytes_per_reducer\n        if self.reducers_max is not None:\n            jcs['hive.exec.reducers.max'] = self.reducers_max\n        return jcs", "is_method": true, "class_name": "HiveQueryTask", "function_description": "This method generates a dictionary of custom Hive configuration settings. It allows users to control specific parameters for a Hive query task, such as job name, reducer counts, and scheduler pool."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "job_runner", "line_number": 403, "body": "def job_runner(self):\n        return HiveQueryRunner()", "is_method": true, "class_name": "HiveQueryTask", "function_description": "Provides an instance of a HiveQueryRunner. This enables the task to execute Hive queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "prepare_outputs", "line_number": 412, "body": "def prepare_outputs(self, job):\n        \"\"\"\n        Called before job is started.\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail\n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Ensures all file system output directories are created before a job starts, preventing execution failures. It handles potential race conditions during directory creation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "get_arglist", "line_number": 431, "body": "def get_arglist(self, f_name, job):\n        arglist = load_hive_cmd() + ['-f', f_name]\n        hiverc = job.hiverc()\n        if hiverc:\n            if isinstance(hiverc, str):\n                hiverc = [hiverc]\n            for rcfile in hiverc:\n                arglist += ['-i', rcfile]\n        hiveconfs = job.hiveconfs()\n        if hiveconfs:\n            for k, v in hiveconfs.items():\n                arglist += ['--hiveconf', '{0}={1}'.format(k, v)]\n        hivevars = job.hivevars()\n        if hivevars:\n            for k, v in hivevars.items():\n                arglist += ['--hivevar', '{0}={1}'.format(k, v)]\n        logger.info(arglist)\n        return arglist", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Constructs a complete command-line argument list for executing a Hive script. It incorporates the script file, resource configurations, and custom Hive properties or variables from a job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "run_job", "line_number": 450, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        self.prepare_outputs(job)\n        with tempfile.NamedTemporaryFile() as f:\n            query = job.query()\n            if isinstance(query, str):\n                query = query.encode('utf8')\n            f.write(query)\n            f.flush()\n            arglist = self.get_arglist(f.name, job)\n            return luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url)", "is_method": true, "class_name": "HiveQueryRunner", "function_description": "Executes a given job by preparing its Hive query and running it as a tracked Hadoop job using Luigi's integration for Hive query execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "exists", "line_number": 488, "body": "def exists(self):\n        \"\"\"\n        returns `True` if the partition/table exists\n        \"\"\"\n        try:\n            logger.debug(\n                \"Checking Hive table '{d}.{t}' for partition {p}\".format(\n                    d=self.database,\n                    t=self.table,\n                    p=str(self.partition or {})\n                )\n            )\n\n            return self.client.table_exists(self.table, self.database, self.partition)\n        except HiveCommandError:\n            if self.fail_missing_table:\n                raise\n            else:\n                if self.client.table_exists(self.table, self.database):\n                    # a real error occurred\n                    raise\n                else:\n                    # oh the table just doesn't exist\n                    return False", "is_method": true, "class_name": "HivePartitionTarget", "function_description": "Checks if the target Hive table or its specified partition exists. It returns True if present, False if not found, and handles related command errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "path", "line_number": 514, "body": "def path(self):\n        \"\"\"\n        Returns the path for this HiveTablePartitionTarget's data.\n        \"\"\"\n        location = self.client.table_location(self.table, self.database, self.partition)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location", "is_method": true, "class_name": "HivePartitionTarget", "function_description": "Provides the exact storage path for the data of a specific Hive table partition. Essential for processes needing direct access to partition data files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "output", "line_number": 550, "body": "def output(self):\n        return HivePartitionTarget(\n            database=self.database,\n            table=self.table,\n            partition=self.partition,\n        )", "is_method": true, "class_name": "ExternalHiveTask", "function_description": "This method provides a `HivePartitionTarget` object, defining the specific Hive database, table, and partition where the external task's output is directed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hive.py", "function": "_parse_partition_string", "line_number": 231, "body": "def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def", "is_method": true, "class_name": "MetastoreClient", "function_description": "Parses a specific string format into a dictionary of partition definitions. This internal helper method converts 'key=value/key2=value2' strings for metadata processing within the MetastoreClient."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "touch", "line_number": 71, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n\n        connection.execute_non_query(\n            \"\"\"IF NOT EXISTS(SELECT 1\n                            FROM {marker_table}\n                            WHERE update_id = %(update_id)s)\n                    INSERT INTO {marker_table} (update_id, target_table)\n                        VALUES (%(update_id)s, %(table)s)\n                ELSE\n                    UPDATE t\n                    SET target_table = %(table)s\n                        , inserted = GETDATE()\n                    FROM {marker_table} t\n                    WHERE update_id = %(update_id)s\n              \"\"\".format(marker_table=self.marker_table),\n            {\"update_id\": self.update_id, \"table\": self.table})\n\n        # make sure update is properly marked\n        assert self.exists(connection)", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Records the successful completion of a data update operation for a target table within a dedicated MSSQL marker table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "exists", "line_number": 102, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n        try:\n            row = connection.execute_row(\"\"\"SELECT 1 FROM {marker_table}\n                                            WHERE update_id = %s\n                                    \"\"\".format(marker_table=self.marker_table),\n                                         (self.update_id,))\n        except _mssql.MSSQLDatabaseException as e:\n            # Error number for table doesn't exist\n            if e.number == 208:\n                row = None\n            else:\n                raise\n\n        return row is not None", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Verifies if a specific update marker exists in the MS SQL database's marker table. This indicates if a previous operation has been recorded."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "connect", "line_number": 119, "body": "def connect(self):\n        \"\"\"\n        Create a SQL Server connection and return a connection object\n        \"\"\"\n        connection = _mssql.connect(user=self.user,\n                                    password=self.password,\n                                    server=self.host,\n                                    port=self.port,\n                                    database=self.database)\n        return connection", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Establishes a connection to an MS SQL Server database. It provides a ready-to-use connection object for subsequent database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mssqldb.py", "function": "create_marker_table", "line_number": 130, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n        Use a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        try:\n            connection.execute_non_query(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT    NOT NULL IDENTITY(1,1),\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      DATETIME DEFAULT(GETDATE()),\n                        PRIMARY KEY (update_id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except _mssql.MSSQLDatabaseException as e:\n            # Table already exists code\n            if e.number == 2714:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "MSSqlTarget", "function_description": "Ensures a dedicated marker table exists in the MS SQL database. This table tracks update IDs and their insertion times for target tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "is_error_5xx", "line_number": 73, "body": "def is_error_5xx(err):\n    return isinstance(err, errors.HttpError) and err.resp.status >= 500", "is_method": false, "function_description": "Determines if an error object represents a server-side HTTP error (5xx status code). Useful for robust error handling and specific retry logic."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_wait_for_consistency", "line_number": 84, "body": "def _wait_for_consistency(checker):\n    \"\"\"Eventual consistency: wait until GCS reports something is true.\n\n    This is necessary for e.g. create/delete where the operation might return,\n    but won't be reflected for a bit.\n    \"\"\"\n    for _ in range(EVENTUAL_CONSISTENCY_MAX_SLEEPS):\n        if checker():\n            return\n\n        time.sleep(EVENTUAL_CONSISTENCY_SLEEP_INTERVAL)\n\n    logger.warning('Exceeded wait for eventual GCS consistency - this may be a'\n                   'bug in the library or something is terribly wrong.')", "is_method": false, "function_description": "Waits for a provided condition to become true, handling eventual consistency in distributed systems like GCS. It polls the condition until met or a timeout occurs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_path_to_bucket_and_key", "line_number": 141, "body": "def _path_to_bucket_and_key(self, path):\n        (scheme, netloc, path, _, _) = urlsplit(path)\n        assert scheme == 'gs'\n        path_without_initial_slash = path[1:]\n        return netloc, path_without_initial_slash", "is_method": true, "class_name": "GCSClient", "function_description": "Parses a GCS URL string into its corresponding bucket name and object key. This internal helper facilitates GCS operations requiring separate bucket and key parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_is_root", "line_number": 147, "body": "def _is_root(self, key):\n        return len(key) == 0 or key == '/'", "is_method": true, "class_name": "GCSClient", "function_description": "Determines if a given storage key represents the root path of a Google Cloud Storage bucket. It helps identify operations or paths at the top level."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_add_path_delimiter", "line_number": 150, "body": "def _add_path_delimiter(self, key):\n        return key if key[-1:] == '/' else key + '/'", "is_method": true, "class_name": "GCSClient", "function_description": "Ensures a given string, likely a storage path or prefix, ends with a forward slash. This standardizes paths for consistent directory-like operations within Google Cloud Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_obj_exists", "line_number": 154, "body": "def _obj_exists(self, bucket, obj):\n        try:\n            self.client.objects().get(bucket=bucket, object=obj).execute()\n        except errors.HttpError as ex:\n            if ex.resp['status'] == '404':\n                return False\n            raise\n        else:\n            return True", "is_method": true, "class_name": "GCSClient", "function_description": "Checks if a specified object exists within a given Google Cloud Storage bucket, returning true if found, false otherwise."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_list_iter", "line_number": 164, "body": "def _list_iter(self, bucket, prefix):\n        request = self.client.objects().list(bucket=bucket, prefix=prefix)\n        response = request.execute()\n\n        while response is not None:\n            for it in response.get('items', []):\n                yield it\n\n            request = self.client.objects().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "GCSClient", "function_description": "Provides an efficient iterator to list all objects within a Google Cloud Storage bucket. It handles pagination automatically for comprehensive retrieval of items matching a specified prefix."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "_do_put", "line_number": 179, "body": "def _do_put(self, media, dest_path):\n        bucket, obj = self._path_to_bucket_and_key(dest_path)\n\n        request = self.client.objects().insert(bucket=bucket, name=obj, media_body=media)\n        if not media.resumable():\n            return request.execute()\n\n        response = None\n        while response is None:\n            status, response = request.next_chunk()\n            if status:\n                logger.debug('Upload progress: %.2f%%', 100 * status.progress())\n\n        _wait_for_consistency(lambda: self._obj_exists(bucket, obj))\n        return response", "is_method": true, "class_name": "GCSClient", "function_description": "Uploads data to a specified path in Google Cloud Storage, handling both direct and resumable transfers. It ensures the uploaded object is consistent and available."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "exists", "line_number": 195, "body": "def exists(self, path):\n        bucket, obj = self._path_to_bucket_and_key(path)\n        if self._obj_exists(bucket, obj):\n            return True\n\n        return self.isdir(path)", "is_method": true, "class_name": "GCSClient", "function_description": "Verifies if a given path exists on Google Cloud Storage. It checks for the presence of both objects (files) and directory prefixes, useful for validating resource availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "isdir", "line_number": 202, "body": "def isdir(self, path):\n        bucket, obj = self._path_to_bucket_and_key(path)\n        if self._is_root(obj):\n            try:\n                self.client.buckets().get(bucket=bucket).execute()\n            except errors.HttpError as ex:\n                if ex.resp['status'] == '404':\n                    return False\n                raise\n\n        obj = self._add_path_delimiter(obj)\n        if self._obj_exists(bucket, obj):\n            return True\n\n        # Any objects with this prefix\n        resp = self.client.objects().list(bucket=bucket, prefix=obj, maxResults=20).execute()\n        lst = next(iter(resp.get('items', [])), None)\n        return bool(lst)", "is_method": true, "class_name": "GCSClient", "function_description": "Determines if a given path represents an existing directory or bucket in Google Cloud Storage. It accounts for GCS's object-based directory simulation by checking for object prefixes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "remove", "line_number": 221, "body": "def remove(self, path, recursive=True):\n        (bucket, obj) = self._path_to_bucket_and_key(path)\n\n        if self._is_root(obj):\n            raise InvalidDeleteException(\n                'Cannot delete root of bucket at path {}'.format(path))\n\n        if self._obj_exists(bucket, obj):\n            self.client.objects().delete(bucket=bucket, object=obj).execute()\n            _wait_for_consistency(lambda: not self._obj_exists(bucket, obj))\n            return True\n\n        if self.isdir(path):\n            if not recursive:\n                raise InvalidDeleteException(\n                    'Path {} is a directory. Must use recursive delete'.format(path))\n\n            req = http.BatchHttpRequest(batch_uri=GCS_BATCH_URI)\n            for it in self._list_iter(bucket, self._add_path_delimiter(obj)):\n                req.add(self.client.objects().delete(bucket=bucket, object=it['name']))\n            req.execute()\n\n            _wait_for_consistency(lambda: not self.isdir(path))\n            return True\n\n        return False", "is_method": true, "class_name": "GCSClient", "function_description": "Deletes a file or recursively removes a directory from Google Cloud Storage. It ensures data consistency after deletion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put", "line_number": 248, "body": "def put(self, filename, dest_path, mimetype=None, chunksize=None):\n        chunksize = chunksize or self.chunksize\n        resumable = os.path.getsize(filename) > 0\n\n        mimetype = mimetype or mimetypes.guess_type(dest_path)[0] or DEFAULT_MIMETYPE\n        media = http.MediaFileUpload(filename, mimetype=mimetype, chunksize=chunksize, resumable=resumable)\n\n        self._do_put(media, dest_path)", "is_method": true, "class_name": "GCSClient", "function_description": "This method uploads a local file to a specified destination path in Google Cloud Storage, supporting MIME type detection and resumable uploads for efficient data transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put_multiple", "line_number": 260, "body": "def put_multiple(self, filepaths, remote_directory, mimetype=None, chunksize=None, num_process=1):\n        if isinstance(filepaths, str):\n            raise ValueError(\n                'filenames must be a list of strings. If you want to put a single file, '\n                'use the `put(self, filename, ...)` method'\n            )\n\n        put_kwargs_list = [\n            {\n                'filename': filepath,\n                'dest_path': os.path.join(remote_directory, os.path.basename(filepath)),\n                'mimetype': mimetype,\n                'chunksize': chunksize,\n            }\n            for filepath in filepaths\n        ]\n\n        if num_process > 1:\n            from multiprocessing import Pool\n            from contextlib import closing\n            with closing(Pool(num_process)) as p:\n                return p.map(self._forward_args_to_put, put_kwargs_list)\n        else:\n            for put_kwargs in put_kwargs_list:\n                self._forward_args_to_put(put_kwargs)", "is_method": true, "class_name": "GCSClient", "function_description": "This method uploads multiple local files from a list to a specified directory in Google Cloud Storage. It can perform these uploads in parallel for improved efficiency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "put_string", "line_number": 286, "body": "def put_string(self, contents, dest_path, mimetype=None):\n        mimetype = mimetype or mimetypes.guess_type(dest_path)[0] or DEFAULT_MIMETYPE\n        assert isinstance(mimetype, str)\n        if not isinstance(contents, bytes):\n            contents = contents.encode(\"utf-8\")\n        media = http.MediaIoBaseUpload(BytesIO(contents), mimetype, resumable=bool(contents))\n        self._do_put(media, dest_path)", "is_method": true, "class_name": "GCSClient", "function_description": "Provides a service to upload string or byte content to a destination path in Google Cloud Storage. It automatically handles content encoding and MIME type detection for the uploaded data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "mkdir", "line_number": 294, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            elif not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            else:\n                return\n\n        self.put_string(b\"\", self._add_path_delimiter(path), mimetype='text/plain')", "is_method": true, "class_name": "GCSClient", "function_description": "Provides `mkdir` functionality for Google Cloud Storage. It creates a directory at the specified path, safely handling pre-existing directories or raising exceptions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "copy", "line_number": 305, "body": "def copy(self, source_path, destination_path):\n        src_bucket, src_obj = self._path_to_bucket_and_key(source_path)\n        dest_bucket, dest_obj = self._path_to_bucket_and_key(destination_path)\n\n        if self.isdir(source_path):\n            src_prefix = self._add_path_delimiter(src_obj)\n            dest_prefix = self._add_path_delimiter(dest_obj)\n\n            source_path = self._add_path_delimiter(source_path)\n            copied_objs = []\n            for obj in self.listdir(source_path):\n                suffix = obj[len(source_path):]\n\n                self.client.objects().copy(\n                    sourceBucket=src_bucket,\n                    sourceObject=src_prefix + suffix,\n                    destinationBucket=dest_bucket,\n                    destinationObject=dest_prefix + suffix,\n                    body={}).execute()\n                copied_objs.append(dest_prefix + suffix)\n\n            _wait_for_consistency(\n                lambda: all(self._obj_exists(dest_bucket, obj)\n                            for obj in copied_objs))\n        else:\n            self.client.objects().copy(\n                sourceBucket=src_bucket,\n                sourceObject=src_obj,\n                destinationBucket=dest_bucket,\n                destinationObject=dest_obj,\n                body={}).execute()\n            _wait_for_consistency(lambda: self._obj_exists(dest_bucket, dest_obj))", "is_method": true, "class_name": "GCSClient", "function_description": "Provides the capability to copy single files or entire directories, including their contents, between locations within Google Cloud Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "rename", "line_number": 338, "body": "def rename(self, *args, **kwargs):\n        \"\"\"\n        Alias for ``move()``\n        \"\"\"\n        self.move(*args, **kwargs)", "is_method": true, "class_name": "GCSClient", "function_description": "Provides an alias for the `move()` method, enabling the renaming of objects within Google Cloud Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "move", "line_number": 344, "body": "def move(self, source_path, destination_path):\n        \"\"\"\n        Rename/move an object from one GCS location to another.\n        \"\"\"\n        self.copy(source_path, destination_path)\n        self.remove(source_path)", "is_method": true, "class_name": "GCSClient", "function_description": "Moves an object from a source path to a destination path within Google Cloud Storage, effectively renaming or relocating it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "listdir", "line_number": 351, "body": "def listdir(self, path):\n        \"\"\"\n        Get an iterable with GCS folder contents.\n        Iterable contains paths relative to queried path.\n        \"\"\"\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        obj_prefix = self._add_path_delimiter(obj)\n        if self._is_root(obj_prefix):\n            obj_prefix = ''\n\n        obj_prefix_len = len(obj_prefix)\n        for it in self._list_iter(bucket, obj_prefix):\n            yield self._add_path_delimiter(path) + it['name'][obj_prefix_len:]", "is_method": true, "class_name": "GCSClient", "function_description": "This GCSClient method retrieves an iterable of contents for a specified Google Cloud Storage \"folder.\" It provides paths relative to the queried location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "list_wildcard", "line_number": 366, "body": "def list_wildcard(self, wildcard_path):\n        \"\"\"Yields full object URIs matching the given wildcard.\n\n        Currently only the '*' wildcard after the last path delimiter is supported.\n\n        (If we need \"full\" wildcard functionality we should bring in gsutil dependency with its\n        https://github.com/GoogleCloudPlatform/gsutil/blob/master/gslib/wildcard_iterator.py...)\n        \"\"\"\n        path, wildcard_obj = wildcard_path.rsplit('/', 1)\n        assert '*' not in path, \"The '*' wildcard character is only supported after the last '/'\"\n        wildcard_parts = wildcard_obj.split('*')\n        assert len(wildcard_parts) == 2, \"Only one '*' wildcard is supported\"\n\n        for it in self.listdir(path):\n            if it.startswith(path + '/' + wildcard_parts[0]) and it.endswith(wildcard_parts[1]) and \\\n                   len(it) >= len(path + '/' + wildcard_parts[0]) + len(wildcard_parts[1]):\n                yield it", "is_method": true, "class_name": "GCSClient", "function_description": "This method of the GCSClient lists Google Cloud Storage objects by matching a simple wildcard pattern. It supports a single '*' character only at the end of the object name within a specified path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "download", "line_number": 385, "body": "def download(self, path, chunksize=None, chunk_callback=lambda _: False):\n        \"\"\"Downloads the object contents to local file system.\n\n        Optionally stops after the first chunk for which chunk_callback returns True.\n        \"\"\"\n        chunksize = chunksize or self.chunksize\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        with tempfile.NamedTemporaryFile(delete=False) as fp:\n            # We can't return the tempfile reference because of a bug in python: http://bugs.python.org/issue18879\n            return_fp = _DeleteOnCloseFile(fp.name, 'r')\n\n            # Special case empty files because chunk-based downloading doesn't work.\n            result = self.client.objects().get(bucket=bucket, object=obj).execute()\n            if int(result['size']) == 0:\n                return return_fp\n\n            request = self.client.objects().get_media(bucket=bucket, object=obj)\n            downloader = http.MediaIoBaseDownload(fp, request, chunksize=chunksize)\n\n            done = False\n            while not done:\n                _, done = downloader.next_chunk()\n                if chunk_callback(fp):\n                    done = True\n\n        return return_fp", "is_method": true, "class_name": "GCSClient", "function_description": "Downloads a Google Cloud Storage object to a local file, supporting chunked transfers and early termination via a callback function."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "close", "line_number": 415, "body": "def close(self):\n        super(_DeleteOnCloseFile, self).close()\n        try:\n            os.remove(self.name)\n        except OSError:\n            # Catch a potential threading race condition and also allow this\n            # method to be called multiple times.\n            pass", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Closes the file and ensures its automatic deletion from the file system upon completion, robustly handling concurrent access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "readable", "line_number": 424, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that the file object is always readable, allowing consumers to confidently attempt read operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "writable", "line_number": 427, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that the file associated with this object is not writable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "seekable", "line_number": 430, "body": "def seekable(self):\n        return True", "is_method": true, "class_name": "_DeleteOnCloseFile", "function_description": "Indicates that this file-like object supports seeking operations. It signals that consumers can reposition the file pointer within the stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "move_to_final_destination", "line_number": 443, "body": "def move_to_final_destination(self):\n        self.gcs_client.put(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicGCSFile", "function_description": "Finalizes an atomic Google Cloud Storage file operation by moving data from a temporary staging path to its permanent destination."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "open", "line_number": 458, "body": "def open(self, mode='r'):\n        if mode == 'r':\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(self.fs.download(self.path))))\n        elif mode == 'w':\n            return self.format.pipe_writer(AtomicGCSFile(self.path, self.fs))\n        else:\n            raise ValueError(\"Unsupported open mode '{}'\".format(mode))", "is_method": true, "class_name": "GCSTarget", "function_description": "This method provides an interface to open a Google Cloud Storage (GCS) file for reading or writing. It returns a file-like object, integrating with a specified format and supporting atomic writes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/gcs.py", "function": "exists", "line_number": 512, "body": "def exists(self):\n        flag_target = self.path + self.flag\n        return self.fs.exists(flag_target)", "is_method": true, "class_name": "GCSFlagTarget", "function_description": "Verifies if the specific flag target object exists within Google Cloud Storage. It confirms the presence of a designated marker file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "dataset", "line_number": 106, "body": "def dataset(self):\n        return BQDataset(project_id=self.project_id, dataset_id=self.dataset_id, location=self.location)", "is_method": true, "class_name": "BQTable", "function_description": "Returns the `BQDataset` object corresponding to the table's dataset. This provides direct access to the dataset context for further operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "uri", "line_number": 110, "body": "def uri(self):\n        return \"bq://\" + self.project_id + \"/\" + \\\n               self.dataset.dataset_id + \"/\" + self.table_id", "is_method": true, "class_name": "BQTable", "function_description": "Constructs and returns a unique URI for the BigQuery table. This standardized string identifies the table by its project, dataset, and table IDs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "dataset_exists", "line_number": 131, "body": "def dataset_exists(self, dataset):\n        \"\"\"Returns whether the given dataset exists.\n        If regional location is specified for the dataset, that is also checked\n        to be compatible with the remote dataset, otherwise an exception is thrown.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        try:\n            response = self.client.datasets().get(projectId=dataset.project_id,\n                                                  datasetId=dataset.dataset_id).execute()\n            if dataset.location is not None:\n                fetched_location = response.get('location')\n                if dataset.location != fetched_location:\n                    raise Exception('''Dataset already exists with regional location {}. Can't use {}.'''.format(\n                        fetched_location if fetched_location is not None else 'unspecified',\n                        dataset.location))\n\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "is_method": true, "class_name": "BigQueryClient", "function_description": "Checks if a specified BigQuery dataset exists remotely. It also validates that the provided dataset's location matches the remote one, raising an error if they conflict."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "table_exists", "line_number": 157, "body": "def table_exists(self, table):\n        \"\"\"Returns whether the given table exists.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n        if not self.dataset_exists(table.dataset):\n            return False\n\n        try:\n            self.client.tables().get(projectId=table.project_id,\n                                     datasetId=table.dataset_id,\n                                     tableId=table.table_id).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "is_method": true, "class_name": "BigQueryClient", "function_description": "Determines if a given BigQuery table exists. This method ensures table availability before performing data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "make_dataset", "line_number": 177, "body": "def make_dataset(self, dataset, raise_if_exists=False, body=None):\n        \"\"\"Creates a new dataset with the default permissions.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :param raise_if_exists: whether to raise an exception if the dataset already exists.\n           :raises luigi.target.FileAlreadyExists: if raise_if_exists=True and the dataset exists\n        \"\"\"\n\n        if body is None:\n            body = {}\n\n        try:\n            # Construct a message body in the format required by\n            # https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.datasets.html#insert\n            body['datasetReference'] = {\n                'projectId': dataset.project_id,\n                'datasetId': dataset.dataset_id\n            }\n            if dataset.location is not None:\n                body['location'] = dataset.location\n            self.client.datasets().insert(projectId=dataset.project_id, body=body).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 409:\n                if raise_if_exists:\n                    raise luigi.target.FileAlreadyExists()\n            else:\n                raise", "is_method": true, "class_name": "BigQueryClient", "function_description": "Creates a new dataset within Google BigQuery. It can optionally raise an error if a dataset with the same ID already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "delete_dataset", "line_number": 206, "body": "def delete_dataset(self, dataset, delete_nonempty=True):\n        \"\"\"Deletes a dataset (and optionally any tables in it), if it exists.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :param delete_nonempty: if true, will delete any tables before deleting the dataset\n        \"\"\"\n\n        if not self.dataset_exists(dataset):\n            return\n\n        self.client.datasets().delete(projectId=dataset.project_id,\n                                      datasetId=dataset.dataset_id,\n                                      deleteContents=delete_nonempty).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Deletes a specified BigQuery dataset. It can optionally remove all tables within the dataset before deletion, ensuring comprehensive cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "delete_table", "line_number": 221, "body": "def delete_table(self, table):\n        \"\"\"Deletes a table, if it exists.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n\n        if not self.table_exists(table):\n            return\n\n        self.client.tables().delete(projectId=table.project_id,\n                                    datasetId=table.dataset_id,\n                                    tableId=table.table_id).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "This BigQueryClient method deletes a specified BigQuery table. It first verifies the table's existence, ensuring a safe removal operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "list_datasets", "line_number": 235, "body": "def list_datasets(self, project_id):\n        \"\"\"Returns the list of datasets in a given project.\n\n           :param project_id:\n           :type project_id: str\n        \"\"\"\n\n        request = self.client.datasets().list(projectId=project_id,\n                                              maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for ds in response.get('datasets', []):\n                yield ds['datasetReference']['datasetId']\n\n            request = self.client.datasets().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "This BigQueryClient method lists all BigQuery dataset IDs available within a specified Google Cloud project, allowing discovery and enumeration of data resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "list_tables", "line_number": 256, "body": "def list_tables(self, dataset):\n        \"\"\"Returns the list of tables in a given dataset.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        request = self.client.tables().list(projectId=dataset.project_id,\n                                            datasetId=dataset.dataset_id,\n                                            maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for t in response.get('tables', []):\n                yield t['tableReference']['tableId']\n\n            request = self.client.tables().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "Provides an iterable list of all table IDs present in a specified BigQuery dataset, handling pagination to ensure completeness."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "get_view", "line_number": 278, "body": "def get_view(self, table):\n        \"\"\"Returns the SQL query for a view, or None if it doesn't exist or is not a view.\n\n        :param table: The table containing the view.\n        :type table: BQTable\n        \"\"\"\n\n        request = self.client.tables().get(projectId=table.project_id,\n                                           datasetId=table.dataset_id,\n                                           tableId=table.table_id)\n\n        try:\n            response = request.execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return None\n            raise\n\n        return response['view']['query'] if 'view' in response else None", "is_method": true, "class_name": "BigQueryClient", "function_description": "Retrieves the SQL query of a specified BigQuery view. It returns None if the given table is not a view or does not exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "update_view", "line_number": 298, "body": "def update_view(self, table, view):\n        \"\"\"Updates the SQL query for a view.\n\n        If the output table exists, it is replaced with the supplied view query. Otherwise a new\n        table is created with this view.\n\n        :param table: The table to contain the view.\n        :type table: BQTable\n        :param view: The SQL query for the view.\n        :type view: str\n        \"\"\"\n\n        body = {\n            'tableReference': {\n                'projectId': table.project_id,\n                'datasetId': table.dataset_id,\n                'tableId': table.table_id\n            },\n            'view': {\n                'query': view\n            }\n        }\n\n        if self.table_exists(table):\n            self.client.tables().update(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        tableId=table.table_id,\n                                        body=body).execute()\n        else:\n            self.client.tables().insert(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        body=body).execute()", "is_method": true, "class_name": "BigQueryClient", "function_description": "This method updates an existing BigQuery view's SQL query or creates a new view for the specified table. It automates BigQuery view definition and redefinition."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run_job", "line_number": 331, "body": "def run_job(self, project_id, body, dataset=None):\n        \"\"\"Runs a BigQuery \"job\". See the documentation for the format of body.\n\n           .. note::\n               You probably don't need to use this directly. Use the tasks defined below.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :return: the job id of the job.\n           :rtype: str\n           :raises luigi.contrib.BigQueryExecutionError: if the job fails.\n        \"\"\"\n\n        if dataset and not self.dataset_exists(dataset):\n            self.make_dataset(dataset)\n\n        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()\n        job_id = new_job['jobReference']['jobId']\n        logger.info('Started import job %s:%s', project_id, job_id)\n        while True:\n            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)\n            if status['status']['state'] == 'DONE':\n                if status['status'].get('errorResult'):\n                    raise BigQueryExecutionError(job_id, status['status']['errorResult'])\n                return job_id\n\n            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)\n            time.sleep(5)", "is_method": true, "class_name": "BigQueryClient", "function_description": "Executes a specified BigQuery job, continuously polling its status until completion or error, and returns the job ID. This is a core internal method for managing BigQuery operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "copy", "line_number": 360, "body": "def copy(self,\n             source_table,\n             dest_table,\n             create_disposition=CreateDisposition.CREATE_IF_NEEDED,\n             write_disposition=WriteDisposition.WRITE_TRUNCATE):\n        \"\"\"Copies (or appends) a table to another table.\n\n            :param source_table:\n            :type source_table: BQTable\n            :param dest_table:\n            :type dest_table: BQTable\n            :param create_disposition: whether to create the table if needed\n            :type create_disposition: CreateDisposition\n            :param write_disposition: whether to append/truncate/fail if the table exists\n            :type write_disposition: WriteDisposition\n        \"\"\"\n\n        job = {\n            \"configuration\": {\n                \"copy\": {\n                    \"sourceTable\": {\n                        \"projectId\": source_table.project_id,\n                        \"datasetId\": source_table.dataset_id,\n                        \"tableId\": source_table.table_id,\n                    },\n                    \"destinationTable\": {\n                        \"projectId\": dest_table.project_id,\n                        \"datasetId\": dest_table.dataset_id,\n                        \"tableId\": dest_table.table_id,\n                    },\n                    \"createDisposition\": create_disposition,\n                    \"writeDisposition\": write_disposition,\n                }\n            }\n        }\n\n        self.run_job(dest_table.project_id, job, dataset=dest_table.dataset)", "is_method": true, "class_name": "BigQueryClient", "function_description": "Enables copying data between BigQuery tables, offering control over destination table creation and how existing data is handled (append/overwrite)."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "from_bqtable", "line_number": 405, "body": "def from_bqtable(cls, table, client=None):\n        \"\"\"A constructor that takes a :py:class:`BQTable`.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n        return cls(table.project_id, table.dataset_id, table.table_id, client=client)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "This class method constructs a BigQueryTarget instance directly from a BQTable object, extracting its project, dataset, and table identifiers. It serves as an alternative constructor."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "exists", "line_number": 413, "body": "def exists(self):\n        return self.client.table_exists(self.table)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "Verifies if the BigQuery table associated with this target object currently exists in BigQuery. This capability is crucial for managing BigQuery resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "__str__", "line_number": 416, "body": "def __str__(self):\n        return str(self.table)", "is_method": true, "class_name": "BigQueryTarget", "function_description": "Provides a human-readable string representation of the `BigQueryTarget` object, which is the string form of its associated BigQuery table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "bulk_complete", "line_number": 430, "body": "def bulk_complete(cls, parameter_tuples):\n        # Instantiate the tasks to inspect them\n        tasks_with_params = [(cls(p), p) for p in parameter_tuples]\n        if not tasks_with_params:\n            return\n\n        # Grab the set of BigQuery datasets we are interested in\n        datasets = {t.output().table.dataset for t, p in tasks_with_params}\n        logger.info('Checking datasets %s for available tables', datasets)\n\n        # Query the available tables for all datasets\n        client = tasks_with_params[0][0].output().client\n        available_datasets = filter(client.dataset_exists, datasets)\n        available_tables = {d: set(client.list_tables(d)) for d in available_datasets}\n\n        # Return parameter_tuples belonging to available tables\n        for t, p in tasks_with_params:\n            table = t.output().table\n            if table.table_id in available_tables.get(table.dataset, []):\n                yield p", "is_method": true, "class_name": "MixinBigQueryBulkComplete", "function_description": "Determines which BigQuery tasks are complete by checking for the existence of their output tables. It yields parameters for tasks whose tables are already present in BigQuery."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "source_format", "line_number": 455, "body": "def source_format(self):\n        \"\"\"The source format to use (see :py:class:`SourceFormat`).\"\"\"\n        return SourceFormat.NEWLINE_DELIMITED_JSON", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "This method specifies the required source data format for a BigQuery load task. It indicates that the input data should be in newline-delimited JSON format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "encoding", "line_number": 460, "body": "def encoding(self):\n        \"\"\"The encoding of the data that is going to be loaded (see :py:class:`Encoding`).\"\"\"\n        return Encoding.UTF_8", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Provides the character encoding for data intended to be loaded into BigQuery, specifically indicating UTF-8."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "write_disposition", "line_number": 465, "body": "def write_disposition(self):\n        \"\"\"What to do if the table already exists. By default this will fail the job.\n\n           See :py:class:`WriteDisposition`\"\"\"\n        return WriteDisposition.WRITE_EMPTY", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Defines the BigQuery write disposition, specifying what the load task does if the destination table exists. It configures the job to fail if the table is not empty."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "schema", "line_number": 472, "body": "def schema(self):\n        \"\"\"Schema in the format defined at https://cloud.google.com/bigquery/docs/reference/v2/jobs#configuration.load.schema.\n\n        If the value is falsy, it is omitted and inferred by BigQuery.\"\"\"\n        return []", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Configures the BigQuery load task to use automatic schema inference. This method provides an empty schema, instructing BigQuery to infer the data structure during the load operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "max_bad_records", "line_number": 479, "body": "def max_bad_records(self):\n        \"\"\" The maximum number of bad records that BigQuery can ignore when reading data.\n\n        If the number of bad records exceeds this value, an invalid error is returned in the job result.\"\"\"\n        return 0", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "This method specifies the maximum number of corrupted records BigQuery can ignore during a data load operation. Exceeding this limit causes the job to fail, ensuring data quality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "field_delimiter", "line_number": 486, "body": "def field_delimiter(self):\n        \"\"\"The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.\"\"\"\n        return FieldDelimiter.COMMA", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the character used to delimit fields in a CSV file. It provides the default comma delimiter for BigQuery load tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "source_uris", "line_number": 490, "body": "def source_uris(self):\n        \"\"\"The fully-qualified URIs that point to your data in Google Cloud Storage.\n\n        Each URI can contain one '*' wildcard character and it must come after the 'bucket' name.\"\"\"\n        return [x.path for x in luigi.task.flatten(self.input())]", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "This method provides the fully-qualified Google Cloud Storage URIs from which data will be loaded. It compiles the list of source paths for the BigQuery load operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "skip_leading_rows", "line_number": 497, "body": "def skip_leading_rows(self):\n        \"\"\"The number of rows at the top of a CSV file that BigQuery will skip when loading the data.\n\n        The default value is 0. This property is useful if you have header rows in the file that should be skipped.\"\"\"\n        return 0", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Provides the default number of leading CSV rows for BigQuery to skip when loading data. This is useful for handling header rows in files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "allow_jagged_rows", "line_number": 504, "body": "def allow_jagged_rows(self):\n        \"\"\"Accept rows that are missing trailing optional columns. The missing values are treated as nulls.\n\n        If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,\n\n        an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Returns the configuration for BigQuery load jobs regarding jagged CSV rows. It indicates that rows missing trailing columns are explicitly not allowed and treated as bad records."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "ignore_unknown_values", "line_number": 513, "body": "def ignore_unknown_values(self):\n        \"\"\"Indicates if BigQuery should allow extra values that are not represented in the table schema.\n\n        If true, the extra values are ignored. If false, records with extra columns are treated as bad records,\n\n        and if there are too many bad records, an invalid error is returned in the job result. The default value is false.\n\n        The sourceFormat property determines what BigQuery treats as an extra value:\n\n        CSV: Trailing columns JSON: Named values that don't match any column names\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Determines if BigQuery load operations should reject records with values not defined in the table schema. By returning `False`, it enforces strict schema adherence during data ingestion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "allow_quoted_new_lines", "line_number": 526, "body": "def allow_quoted_new_lines(self):\n        \"\"\"\tIndicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file. The default value is false.\"\"\"\n        return False", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "Declares that BigQuery load tasks should disallow newlines within quoted CSV fields, matching the default BigQuery behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 530, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        bq_client = output.client\n\n        source_uris = self.source_uris()\n        assert all(x.startswith('gs://') for x in source_uris)\n\n        job = {\n            'configuration': {\n                'load': {\n                    'destinationTable': {\n                        'projectId': output.table.project_id,\n                        'datasetId': output.table.dataset_id,\n                        'tableId': output.table.table_id,\n                    },\n                    'encoding': self.encoding,\n                    'sourceFormat': self.source_format,\n                    'writeDisposition': self.write_disposition,\n                    'sourceUris': source_uris,\n                    'maxBadRecords': self.max_bad_records,\n                    'ignoreUnknownValues': self.ignore_unknown_values\n                }\n            }\n        }\n\n        if self.source_format == SourceFormat.CSV:\n            job['configuration']['load']['fieldDelimiter'] = self.field_delimiter\n            job['configuration']['load']['skipLeadingRows'] = self.skip_leading_rows\n            job['configuration']['load']['allowJaggedRows'] = self.allow_jagged_rows\n            job['configuration']['load']['allowQuotedNewlines'] = self.allow_quoted_new_lines\n\n        if self.schema:\n            job['configuration']['load']['schema'] = {'fields': self.schema}\n        else:\n            job['configuration']['load']['autodetect'] = True\n\n        bq_client.run_job(output.table.project_id, job, dataset=output.table.dataset)", "is_method": true, "class_name": "BigQueryLoadTask", "function_description": "The `run` method initiates a BigQuery load job, importing data from Google Cloud Storage into the specified BigQuery table. It supports various load options and schema definition or autodiscovery."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "write_disposition", "line_number": 574, "body": "def write_disposition(self):\n        \"\"\"What to do if the table already exists. By default this will fail the job.\n\n           See :py:class:`WriteDisposition`\"\"\"\n        return WriteDisposition.WRITE_TRUNCATE", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "This method configures the write disposition for a BigQuery task. It specifies that the target table will be truncated if it already exists when writing new data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "create_disposition", "line_number": 581, "body": "def create_disposition(self):\n        \"\"\"Whether to create the table or not. See :py:class:`CreateDisposition`\"\"\"\n        return CreateDisposition.CREATE_IF_NEEDED", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "This method specifies the default table creation disposition for a BigQuery query task, ensuring the target table is created if it's needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "flatten_results", "line_number": 586, "body": "def flatten_results(self):\n        \"\"\"Flattens all nested and repeated fields in the query results.\n        allowLargeResults must be true if this is set to False.\"\"\"\n        return True", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Enables the BigQuery query task to automatically flatten all nested and repeated fields within its results, providing a simpler data structure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "query_mode", "line_number": 597, "body": "def query_mode(self):\n        \"\"\"The query mode. See :py:class:`QueryMode`.\"\"\"\n        return QueryMode.INTERACTIVE", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Exposes the query execution mode for BigQuery tasks, ensuring it is always set to interactive."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "use_legacy_sql", "line_number": 608, "body": "def use_legacy_sql(self):\n        \"\"\"Whether to use legacy SQL\n        \"\"\"\n        return True", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Specifies that BigQuery queries executed by this task must use legacy SQL."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 613, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        query = self.query\n        assert query, 'No query was provided'\n\n        bq_client = output.client\n\n        logger.info('Launching Query')\n        logger.info('Query destination: %s (%s)', output, self.write_disposition)\n        logger.info('Query SQL: %s', query)\n\n        job = {\n            'configuration': {\n                'query': {\n                    'query': query,\n                    'priority': self.query_mode,\n                    'destinationTable': {\n                        'projectId': output.table.project_id,\n                        'datasetId': output.table.dataset_id,\n                        'tableId': output.table.table_id,\n                    },\n                    'allowLargeResults': True,\n                    'createDisposition': self.create_disposition,\n                    'writeDisposition': self.write_disposition,\n                    'flattenResults': self.flatten_results,\n                    'userDefinedFunctionResources': [{\"resourceUri\": v} for v in self.udf_resource_uris],\n                    'useLegacySql': self.use_legacy_sql,\n                }\n            }\n        }\n\n        bq_client.run_job(output.table.project_id, job, dataset=output.table.dataset)", "is_method": true, "class_name": "BigQueryRunQueryTask", "function_description": "Runs a SQL query against Google BigQuery, writing the results to a designated BigQuery table. It configures the job with specified BigQuery options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "complete", "line_number": 664, "body": "def complete(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        if not output.exists():\n            return False\n\n        existing_view = output.client.get_view(output.table)\n        return existing_view == self.view", "is_method": true, "class_name": "BigQueryCreateViewTask", "function_description": "Verifies if the BigQuery view created by this task exists and its current definition matches the task's specified view. It serves as a completion status check for the view creation process."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 674, "body": "def run(self):\n        output = self.output()\n        assert isinstance(output, BigQueryTarget), 'Output must be a BigQueryTarget, not %s' % (output)\n\n        view = self.view\n        assert view, 'No view was provided'\n\n        logger.info('Create view')\n        logger.info('Destination: %s', output)\n        logger.info('View SQL: %s', view)\n\n        output.client.update_view(output.table, view)", "is_method": true, "class_name": "BigQueryCreateViewTask", "function_description": "Creates or updates a BigQuery view using the provided SQL definition. It enables programmatic management of BigQuery views."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "destination_uris", "line_number": 704, "body": "def destination_uris(self):\n        \"\"\"\n        The fully-qualified URIs that point to your data in Google Cloud\n        Storage. Each URI can contain one '*' wildcard character and it must\n        come after the 'bucket' name.\n\n        Wildcarded destinationUris in GCSQueryTarget might not be resolved\n        correctly and result in incomplete data. If a GCSQueryTarget is used to\n        pass wildcarded destinationUris be sure to overwrite this property to\n        suppress the warning.\n        \"\"\"\n        return [x.path for x in luigi.task.flatten(self.output())]", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "This method provides the Google Cloud Storage URIs where the BigQuery extraction task's output data is located. It returns a list of paths for accessing the extracted data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "print_header", "line_number": 718, "body": "def print_header(self):\n        \"\"\"Whether to print the header or not.\"\"\"\n        return PrintHeader.TRUE", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Configures the BigQuery extraction task to always include a header in its output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "field_delimiter", "line_number": 723, "body": "def field_delimiter(self):\n        \"\"\"\n        The separator for fields in a CSV file. The separator can be any\n        ISO-8859-1 single-byte character.\n        \"\"\"\n        return FieldDelimiter.COMMA", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "This method specifies the default field separator for CSV files generated during a BigQuery data extraction task. It defines the character used to delimit fields in the output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "destination_format", "line_number": 731, "body": "def destination_format(self):\n        \"\"\"\n        The destination format to use (see :py:class:`DestinationFormat`).\n        \"\"\"\n        return DestinationFormat.CSV", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "Provides the destination format for data extracted from BigQuery. This method specifies that the extracted data will be in CSV format."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "compression", "line_number": 738, "body": "def compression(self):\n        \"\"\"Whether to use compression.\"\"\"\n        return Compression.NONE", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "This method indicates the compression configuration for the BigQuery extract task, which is always set to no compression."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery.py", "function": "run", "line_number": 742, "body": "def run(self):\n        input = luigi.task.flatten(self.input())[0]\n        assert (\n            isinstance(input, BigQueryTarget) or\n            (len(input) == 1 and isinstance(input[0], BigQueryTarget))), \\\n            'Input must be exactly one BigQueryTarget, not %s' % (input)\n        bq_client = input.client\n\n        destination_uris = self.destination_uris\n        assert all(x.startswith('gs://') for x in destination_uris)\n\n        logger.info('Launching Extract Job')\n        logger.info('Extract source: %s', input)\n        logger.info('Extract destination: %s', destination_uris)\n\n        job = {\n            'configuration': {\n                'extract': {\n                    'sourceTable': {\n                        'projectId': input.table.project_id,\n                        'datasetId': input.table.dataset_id,\n                        'tableId': input.table.table_id\n                    },\n                    'destinationUris': destination_uris,\n                    'destinationFormat': self.destination_format,\n                    'compression': self.compression\n                }\n            }\n        }\n\n        if self.destination_format == 'CSV':\n            # \"Only exports to CSV may specify a field delimiter.\"\n            job['configuration']['extract']['printHeader'] = self.print_header\n            job['configuration']['extract']['fieldDelimiter'] = \\\n                self.field_delimiter\n\n        bq_client.run_job(\n            input.table.project_id,\n            job,\n            dataset=input.table.dataset)", "is_method": true, "class_name": "BigQueryExtractTask", "function_description": "This method of `BigQueryExtractTask` initiates a BigQuery extract job. It exports data from a specified BigQuery table to Google Cloud Storage URIs, with configurable format and compression."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "generate_latest", "line_number": 41, "body": "def generate_latest(self):\n        return generate_latest(self.registry)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Serves the most recent Prometheus metrics collected by the instance. It formats them for direct scraping by a Prometheus monitoring server."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_started", "line_number": 44, "body": "def handle_task_started(self, task):\n        self.task_started_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Registers a task's start by incrementing a Prometheus counter, categorized by its family. This enables tracking the total count of initiated tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_failed", "line_number": 48, "body": "def handle_task_failed(self, task):\n        self.task_failed_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Updates Prometheus metrics to track task failures. It increments a counter for failed tasks and records their execution time, providing insights into task reliability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_disabled", "line_number": 52, "body": "def handle_task_disabled(self, task, config):\n        self.task_disabled_counter.labels(family=task.family).inc()\n        self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "This method updates Prometheus metrics to record when a task is disabled. It increments a counter for disabled tasks and sets their last reported execution duration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "handle_task_done", "line_number": 56, "body": "def handle_task_done(self, task):\n        self.task_done_counter.labels(family=task.family).inc()\n        # time_running can be `None` if task was already complete\n        if task.time_running is not None:\n            self.task_execution_time.labels(family=task.family).set(task.updated - task.time_running)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "Updates Prometheus metrics upon task completion, incrementing a task-done counter and recording the execution duration. This enables monitoring task performance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/prometheus_metric.py", "function": "configure_http_handler", "line_number": 62, "body": "def configure_http_handler(self, http_handler):\n        http_handler.set_header('Content-Type', CONTENT_TYPE_LATEST)", "is_method": true, "class_name": "PrometheusMetricsCollector", "function_description": "This method configures an HTTP handler to correctly serve Prometheus metrics by setting the appropriate `Content-Type` header, ensuring data is ingested properly."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "track_job", "line_number": 74, "body": "def track_job(job_id):\n    \"\"\"\n    Tracking is done by requesting each job and then searching for whether the job\n    has one of the following states:\n    - \"RUN\",\n    - \"PEND\",\n    - \"SSUSP\",\n    - \"EXIT\"\n    based on the LSF documentation\n    \"\"\"\n    cmd = \"bjobs -noheader -o stat {}\".format(job_id)\n    track_job_proc = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, shell=True)\n    status = track_job_proc.communicate()[0].strip('\\n')\n    return status", "is_method": false, "function_description": "Retrieves the current execution status of a specified job ID. This function provides a monitoring capability to determine if a job is running, pending, suspended, or exited."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "kill_job", "line_number": 91, "body": "def kill_job(job_id):\n    \"\"\"\n    Kill a running LSF job\n    \"\"\"\n    subprocess.call(['bkill', job_id])", "is_method": false, "function_description": "Terminates a specified job running on an LSF cluster. It provides the capability to forcefully stop ongoing computational tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "fetch_task_failures", "line_number": 119, "body": "def fetch_task_failures(self):\n        \"\"\"\n        Read in the error file from bsub\n        \"\"\"\n        error_file = os.path.join(self.tmp_dir, \"job.err\")\n        if os.path.isfile(error_file):\n            with open(error_file, \"r\") as f_err:\n                errors = f_err.readlines()\n        else:\n            errors = ''\n        return errors", "is_method": true, "class_name": "LSFJobTask", "function_description": "This method retrieves error messages from the LSF job's standard error file. It provides access to failure information for the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "fetch_task_output", "line_number": 131, "body": "def fetch_task_output(self):\n        \"\"\"\n        Read in the output file\n        \"\"\"\n        # Read in the output file\n        if os.path.isfile(os.path.join(self.tmp_dir, \"job.out\")):\n            with open(os.path.join(self.tmp_dir, \"job.out\"), \"r\") as f_out:\n                outputs = f_out.readlines()\n        else:\n            outputs = ''\n        return outputs", "is_method": true, "class_name": "LSFJobTask", "function_description": "Retrieves the standard output (stdout) generated by the LSF job task. It reads the job's designated output file, providing access to its execution logs and results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_init_local", "line_number": 143, "body": "def _init_local(self):\n\n        base_tmp_dir = self.shared_tmp_dir\n\n        random_id = '%016x' % random.getrandbits(64)\n        task_name = random_id + self.task_id\n        # If any parameters are directories, if we don't\n        # replace the separators on *nix, it'll create a weird nested directory\n        task_name = task_name.replace(\"/\", \"::\")\n\n        # Max filename length\n        max_filename_length = os.fstatvfs(0).f_namemax\n        self.tmp_dir = os.path.join(base_tmp_dir, task_name[:max_filename_length])\n\n        LOGGER.info(\"Tmp dir: %s\", self.tmp_dir)\n        os.makedirs(self.tmp_dir)\n\n        # Dump the code to be run into a pickle file\n        LOGGER.debug(\"Dumping pickled class\")\n        self._dump(self.tmp_dir)\n\n        # Make sure that all the class's dependencies are tarred and available\n        LOGGER.debug(\"Tarballing dependencies\")\n        # Grab luigi and the module containing the code to be run\n        packages = [luigi, __import__(self.__module__, None, None, 'dummy')]\n        create_packages_archive(packages, os.path.join(self.tmp_dir, \"packages.tar\"))\n\n        # Now, pass onto the class's specified init_local() method.\n        self.init_local()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Prepares the local environment for an LSF job task by creating a unique temporary directory, serializing the task instance, and packaging its Python dependencies for remote execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "run", "line_number": 181, "body": "def run(self):\n        \"\"\"\n        The procedure:\n        - Pickle the class\n        - Tarball the dependencies\n        - Construct a bsub argument that runs a generic runner function with the path to the pickled class\n        - Runner function loads the class from pickle\n        - Runner class untars the dependencies\n        - Runner function hits the button on the class's work() method\n        \"\"\"\n        self._init_local()\n        self._run_job()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Submits and executes the current task as a job on an LSF cluster. It handles class serialization, dependency packaging, and remote execution of the task's core work."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_dump", "line_number": 205, "body": "def _dump(self, out_dir=''):\n        \"\"\"\n        Dump instance to file.\n        \"\"\"\n        self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n        if self.__module__ == '__main__':\n            dump_inst = pickle.dumps(self)\n            module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n            dump_inst = dump_inst.replace('(c__main__', \"(c\" + module_name)\n            open(self.job_file, \"w\").write(dump_inst)\n\n        else:\n            pickle.dump(self, open(self.job_file, \"w\"))", "is_method": true, "class_name": "LSFJobTask", "function_description": "Persists the LSFJobTask instance's state to a pickle file. This enables saving job configurations for later retrieval or transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_run_job", "line_number": 219, "body": "def _run_job(self):\n        \"\"\"\n        Build a bsub argument that will run lsf_runner.py on the directory we've specified.\n        \"\"\"\n\n        args = []\n\n        if isinstance(self.output(), list):\n            log_output = os.path.split(self.output()[0].path)\n        else:\n            log_output = os.path.split(self.output().path)\n\n        args += [\"bsub\", \"-q\", self.queue_flag]\n        args += [\"-n\", str(self.n_cpu_flag)]\n        args += [\"-M\", str(self.memory_flag)]\n        args += [\"-R\", \"rusage[%s]\" % self.resource_flag]\n        args += [\"-W\", str(self.runtime_flag)]\n        if self.job_name_flag:\n            args += [\"-J\", str(self.job_name_flag)]\n        args += [\"-o\", os.path.join(log_output[0], \"job.out\")]\n        args += [\"-e\", os.path.join(log_output[0], \"job.err\")]\n        if self.extra_bsub_args:\n            args += self.extra_bsub_args.split()\n\n        # Find where the runner file is\n        runner_path = os.path.abspath(lsf_runner.__file__)\n\n        args += [runner_path]\n        args += [self.tmp_dir]\n\n        # That should do it. Let the world know what we're doing.\n        LOGGER.info(\"### LSF SUBMISSION ARGS: %s\",\n                    \" \".join([str(a) for a in args]))\n\n        # Submit the job\n        run_job_proc = subprocess.Popen(\n            [str(a) for a in args],\n            stdin=subprocess.PIPE, stdout=subprocess.PIPE, cwd=self.tmp_dir)\n        output = run_job_proc.communicate()[0]\n\n        # ASSUMPTION\n        # The result will be of the format\n        # Job <123> is submitted ot queue <myqueue>\n        # So get the number in those first brackets.\n        # I cannot think of a better workaround that leaves logic on the Task side of things.\n        LOGGER.info(\"### JOB SUBMISSION OUTPUT: %s\", str(output))\n        self.job_id = int(output.split(\"<\")[1].split(\">\")[0])\n        LOGGER.info(\n            \"Job %ssubmitted as job %s\",\n            self.job_name_flag + ' ',\n            str(self.job_id)\n        )\n\n        self._track_job()\n\n        # If we want to save the job temporaries, then do so\n        # We'll move them to be next to the job output\n        if self.save_job_info:\n            LOGGER.info(\"Saving up temporary bits\")\n\n            # dest_dir = self.output().path\n            shutil.move(self.tmp_dir, \"/\".join(log_output[0:-1]))\n\n        # Now delete the temporaries, if they're there.\n        self._finish()", "is_method": true, "class_name": "LSFJobTask", "function_description": "Submits a configured computational job to the LSF cluster by building and executing the `bsub` command. It extracts the job ID and initiates tracking for the submitted job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_track_job", "line_number": 285, "body": "def _track_job(self):\n        time0 = 0\n        while True:\n            # Sleep for a little bit\n            time.sleep(self.poll_time)\n\n            # See what the job's up to\n            # ASSUMPTION\n            lsf_status = track_job(self.job_id)\n            if lsf_status == \"RUN\":\n                self.job_status = RUNNING\n                LOGGER.info(\"Job is running...\")\n                if time0 == 0:\n                    time0 = int(round(time.time()))\n            elif lsf_status == \"PEND\":\n                self.job_status = PENDING\n                LOGGER.info(\"Job is pending...\")\n            elif lsf_status == \"DONE\" or lsf_status == \"EXIT\":\n                # Then the job could either be failed or done.\n                errors = self.fetch_task_failures()\n                if not errors:\n                    self.job_status = DONE\n                    LOGGER.info(\"Job is done\")\n                    time1 = int(round(time.time()))\n\n                    # Return a near estimate of the run time to with +/- the\n                    # self.poll_time\n                    job_name = str(self.job_id)\n                    if self.job_name_flag:\n                        job_name = \"%s %s\" % (self.job_name_flag, job_name)\n                    LOGGER.info(\n                        \"### JOB COMPLETED: %s in %s seconds\",\n                        job_name,\n                        str(time1-time0)\n                    )\n                else:\n                    self.job_status = FAILED\n                    LOGGER.error(\"Job has FAILED\")\n                    LOGGER.error(\"\\n\\n\")\n                    LOGGER.error(\"Traceback: \")\n                    for error in errors:\n                        LOGGER.error(error)\n                break\n            elif lsf_status == \"SSUSP\":\n                self.job_status = PENDING\n                LOGGER.info(\"Job is suspended (basically, pending)...\")\n\n            else:\n                self.job_status = UNKNOWN\n                LOGGER.info(\"Job status is UNKNOWN!\")\n                LOGGER.info(\"Status is : %s\", lsf_status)\n                break", "is_method": true, "class_name": "LSFJobTask", "function_description": "Continuously monitors an associated LSF job's status by polling the LSF system. It updates the internal job status and logs progress until the job completes or fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "_finish", "line_number": 338, "body": "def _finish(self):\n        LOGGER.info(\"Cleaning up temporary bits\")\n        if self.tmp_dir and os.path.exists(self.tmp_dir):\n            LOGGER.info('Removing directory %s', self.tmp_dir)\n            shutil.rmtree(self.tmp_dir)", "is_method": true, "class_name": "LSFJobTask", "function_description": "Cleans up temporary resources associated with the LSF job task. It specifically removes the task's temporary working directory if it exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/lsf.py", "function": "run", "line_number": 354, "body": "def run(self):\n        self.init_local()\n        self.work()", "is_method": true, "class_name": "LocalLSFJobTask", "function_description": "Executes the Local LSF Job Task by first initializing local resources, then performing the task's core work."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/target.py", "function": "_make_method", "line_number": 55, "body": "def _make_method(cls, method_name):\n        def new_method(self, *args, **kwargs):\n            return self._chained_call(method_name, *args, **kwargs)\n        return new_method", "is_method": true, "class_name": "CascadingClient", "function_description": "Dynamically generates methods for `CascadingClient` instances. These methods automatically forward their calls and arguments to a central `_chained_call` handler for fluent API patterns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/target.py", "function": "_chained_call", "line_number": 60, "body": "def _chained_call(self, method_name, *args, **kwargs):\n        for i in range(len(self.clients)):\n            client = self.clients[i]\n            try:\n                result = getattr(client, method_name)(*args, **kwargs)\n                return result\n            except luigi.target.FileSystemException:\n                # For exceptions that are semantical, we must throw along\n                raise\n            except BaseException:\n                is_last_iteration = (i + 1) >= len(self.clients)\n                if is_last_iteration:\n                    raise\n                else:\n                    logger.warning('The %s failed to %s, using fallback class %s',\n                                   client.__class__.__name__, method_name, self.clients[i + 1].__class__.__name__)", "is_method": true, "class_name": "CascadingClient", "function_description": "This method attempts to execute a specific operation on a sequence of client objects. It provides resilient, cascading execution by falling back to the next client if the current one fails, until success or all clients are exhausted."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "get_dataproc_client", "line_number": 26, "body": "def get_dataproc_client():\n    return _dataproc_client", "is_method": false, "function_description": "Provides access to the Dataproc client instance. This client is used to interact with and manage Dataproc clusters and jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "set_dataproc_client", "line_number": 30, "body": "def set_dataproc_client(client):\n    global _dataproc_client\n    _dataproc_client = client", "is_method": false, "function_description": "Configures the global Dataproc client instance. This allows other parts of the application to access a pre-set client for Dataproc operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_job", "line_number": 53, "body": "def submit_job(self, job_config):\n        self._job = self.dataproc_client.projects().regions().jobs()\\\n            .submit(projectId=self.gcloud_project_id, region=self.dataproc_region, body=job_config).execute()\n        self._job_id = self._job['reference']['jobId']\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Submits a job configuration to Google Cloud Dataproc, initiating its execution and storing the job's details for tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_spark_job", "line_number": 59, "body": "def submit_spark_job(self, jars, main_class, job_args=None):\n\n        if job_args is None:\n            job_args = []\n\n        job_config = {\"job\": {\n            \"placement\": {\n                \"clusterName\": self.dataproc_cluster_name\n            },\n            \"sparkJob\": {\n                \"args\": job_args,\n                \"mainClass\": main_class,\n                \"jarFileUris\": jars\n            }\n        }}\n        self.submit_job(job_config)\n        self._job_name = os.path.basename(self._job['sparkJob']['mainClass'])\n        logger.info(\"Submitted new dataproc job:{} id:{}\".format(self._job_name, self._job_id))\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Submits a Spark job to the configured Google Cloud Dataproc cluster. It sets up the job's main class, JARs, and arguments for execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "submit_pyspark_job", "line_number": 79, "body": "def submit_pyspark_job(self, job_file, extra_files=list(), job_args=None):\n\n        if job_args is None:\n            job_args = []\n\n        job_config = {\"job\": {\n            \"placement\": {\n                \"clusterName\": self.dataproc_cluster_name\n            },\n            \"pysparkJob\": {\n                \"mainPythonFileUri\": job_file,\n                \"pythonFileUris\": extra_files,\n                \"args\": job_args\n            }\n        }}\n        self.submit_job(job_config)\n        self._job_name = os.path.basename(self._job['pysparkJob']['mainPythonFileUri'])\n        logger.info(\"Submitted new dataproc job:{} id:{}\".format(self._job_name, self._job_id))\n        return self._job", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Submits a PySpark job to a specified Google Cloud Dataproc cluster. It enables the execution of Python-based big data processing applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "wait_for_job", "line_number": 99, "body": "def wait_for_job(self):\n        if self._job is None:\n            raise Exception(\"You must submit a job before you can wait for it\")\n        while True:\n            job_result = self.dataproc_client.projects().regions().jobs()\\\n                .get(projectId=self.gcloud_project_id, region=self.dataproc_region, jobId=self._job_id).execute()\n            status = job_result['status']['state']\n            logger.info(\"Current dataproc status: {} job:{} id:{}\".format(status, self._job_name, self._job_id))\n            if status == 'DONE':\n                break\n            if status == 'ERROR':\n                raise Exception(job_result['status']['details'])\n            time.sleep(5)", "is_method": true, "class_name": "DataprocBaseTask", "function_description": "Waits for a submitted Google Cloud Dataproc job to finish. It monitors the job's status, ensuring successful completion or raising an exception upon failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 122, "body": "def run(self):\n        self.submit_spark_job(main_class=self.main_class,\n                              jars=self.jars.split(\",\") if self.jars else [],\n                              job_args=self.job_args.split(\",\") if self.job_args else [])\n        self.wait_for_job()", "is_method": true, "class_name": "DataprocSparkTask", "function_description": "This method submits a configured Spark job to Dataproc and waits for its completion. It provides a complete orchestration of the Spark task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 137, "body": "def run(self):\n        self.submit_pyspark_job(job_file=self.job_file,\n                                extra_files=self.extra_files.split(\",\") if self.extra_files else [],\n                                job_args=self.job_args.split(\",\") if self.job_args else [])\n        self.wait_for_job()", "is_method": true, "class_name": "DataprocPysparkTask", "function_description": "Executes a configured PySpark job on a Dataproc cluster. It then waits for the submitted job to complete."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "_get_cluster_status", "line_number": 158, "body": "def _get_cluster_status(self):\n        return self.dataproc_client.projects().regions().clusters()\\\n            .get(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name)\\\n            .execute()", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "Queries the Google Cloud Dataproc API to retrieve the current status and detailed information of a specified cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "complete", "line_number": 163, "body": "def complete(self):\n        try:\n            self._get_cluster_status()\n            return True  # No (404) error so the cluster already exists\n        except HttpError as e:\n            if e.resp.status == 404:\n                return False  # We got a 404 so the cluster doesn't exist yet\n            else:\n                raise e", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "Determines if the Dataproc cluster associated with the task already exists. It serves as a pre-check to prevent redundant cluster creation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 173, "body": "def run(self):\n        base_uri = \"https://www.googleapis.com/compute/v1/projects/{}\".format(self.gcloud_project_id)\n        software_config = {\"imageVersion\": self.image_version} if self.image_version else {}\n\n        cluster_conf = {\n            \"clusterName\": self.dataproc_cluster_name,\n            \"projectId\": self.gcloud_project_id,\n            \"config\": {\n                \"configBucket\": \"\",\n                \"gceClusterConfig\": {\n                    \"networkUri\": base_uri + \"/global/networks/\" + self.gcloud_network,\n                    \"zoneUri\": base_uri + \"/zones/\" + self.gcloud_zone,\n                    \"serviceAccountScopes\": [\n                        \"https://www.googleapis.com/auth/cloud-platform\"\n                    ]\n                },\n                \"masterConfig\": {\n                    \"numInstances\": 1,\n                    \"machineTypeUri\": base_uri + \"/zones/\" + self.gcloud_zone + \"/machineTypes/\" + self.master_node_type,\n                    \"diskConfig\": {\n                        \"bootDiskSizeGb\": self.master_disk_size,\n                        \"numLocalSsds\": 0\n                    }\n                },\n                \"workerConfig\": {\n                    \"numInstances\": self.worker_normal_count,\n                    \"machineTypeUri\": base_uri + \"/zones/\" + self.gcloud_zone + \"/machineTypes/\" + self.worker_node_type,\n                    \"diskConfig\": {\n                        \"bootDiskSizeGb\": self.worker_disk_size,\n                        \"numLocalSsds\": 0\n                    }\n                },\n                \"secondaryWorkerConfig\": {\n                    \"numInstances\": self.worker_preemptible_count,\n                    \"isPreemptible\": True\n                },\n                \"softwareConfig\": software_config\n            }\n        }\n\n        self.dataproc_client.projects().regions().clusters()\\\n            .create(projectId=self.gcloud_project_id, region=self.dataproc_region, body=cluster_conf).execute()\n\n        while True:\n            time.sleep(10)\n            cluster_status = self._get_cluster_status()\n            status = cluster_status['status']['state']\n            logger.info(\"Creating new dataproc cluster: {} status: {}\".format(self.dataproc_cluster_name, status))\n            if status == 'RUNNING':\n                break\n            if status == 'ERROR':\n                raise Exception(cluster_status['status']['details'])", "is_method": true, "class_name": "CreateDataprocClusterTask", "function_description": "This method provisions a new Google Cloud Dataproc cluster with specified configurations. It then waits for the cluster to become fully operational, automating the setup of big data processing environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "_get_cluster_status", "line_number": 236, "body": "def _get_cluster_status(self):\n        try:\n            return self.dataproc_client.projects().regions().clusters()\\\n                .get(projectId=self.gcloud_project_id, region=self.dataproc_region,\n                     clusterName=self.dataproc_cluster_name, fields=\"status\")\\\n                .execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                return None  # We got a 404 so the cluster doesn't exist\n            else:\n                raise e", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Queries the Google Cloud Dataproc API to retrieve a cluster's status. It returns None if the specified cluster does not exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "complete", "line_number": 248, "body": "def complete(self): return self._get_cluster_status() is None", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Checks if the Dataproc cluster associated with the task has been successfully deleted, indicating the completion of the deletion operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dataproc.py", "function": "run", "line_number": 250, "body": "def run(self):\n        self.dataproc_client.projects().regions().clusters()\\\n            .delete(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name).execute()\n\n        while True:\n            time.sleep(10)\n            status = self._get_cluster_status()\n            if status is None:\n                logger.info(\"Finished shutting down cluster: {}\".format(self.dataproc_cluster_name))\n                break\n            logger.info(\"Shutting down cluster: {} current status: {}\".format(self.dataproc_cluster_name, status['status']['state']))", "is_method": true, "class_name": "DeleteDataprocClusterTask", "function_description": "Initiates and monitors the complete deletion of a specified Google Cloud Dataproc cluster. It ensures the cluster is fully deprovisioned before completing the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "marker_key", "line_number": 76, "body": "def marker_key(self):\n        \"\"\"\n        Generate a key for the indicator hash.\n        \"\"\"\n        return '%s:%s' % (self.marker_prefix, self.update_id)", "is_method": true, "class_name": "RedisTarget", "function_description": "Generates a unique string key for the Redis target instance. This key identifies markers or status updates within a Redis hash."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "touch", "line_number": 82, "body": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n\n        We index the parameters `update_id` and `date`.\n        \"\"\"\n        marker_key = self.marker_key()\n        self.redis_client.hset(marker_key, 'update_id', self.update_id)\n        self.redis_client.hset(marker_key, 'date', datetime.datetime.now().isoformat())\n\n        if self.expire is not None:\n            self.redis_client.expire(marker_key, self.expire)", "is_method": true, "class_name": "RedisTarget", "function_description": "Marks a data update as complete in Redis by recording its ID and a timestamp. Optionally sets an expiration time for this completion marker."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redis_store.py", "function": "exists", "line_number": 95, "body": "def exists(self):\n        \"\"\"\n        Test, if this task has been run.\n        \"\"\"\n        return self.redis_client.exists(self.marker_key()) == 1", "is_method": true, "class_name": "RedisTarget", "function_description": "Checks if a task's completion marker exists in Redis. This determines if the task has been run previously."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__str__", "line_number": 60, "body": "def __str__(self):\n        return \"Command '%s' on host %s returned non-zero exit status %d\" % (\n            self.cmd, self.host, self.returncode)", "is_method": true, "class_name": "RemoteCalledProcessError", "function_description": "Generates a descriptive error message for failed remote command executions. It specifies the command, host, and non-zero exit status for easy debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__repr__", "line_number": 77, "body": "def __repr__(self):\n        return '%s(%r, %r, %r, %r, %r)' % (\n            type(self).__name__, self.host, self.username, self.key_file, self.connect_timeout, self.port)", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides an unambiguous string representation of a `RemoteContext` object. This representation is primarily for debugging and allows object reconstruction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__eq__", "line_number": 81, "body": "def __eq__(self, other):\n        return repr(self) == repr(other)", "is_method": true, "class_name": "RemoteContext", "function_description": "Defines equality between two RemoteContext objects by comparing their string representations, allowing for direct comparison using the `==` operator."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__hash__", "line_number": 84, "body": "def __hash__(self):\n        return hash(repr(self))", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides a hash value for `RemoteContext` instances, enabling them to be used as dictionary keys or set elements. The hash is based on the object's `repr()` string."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_host_ref", "line_number": 87, "body": "def _host_ref(self):\n        if self.username:\n            return \"{0}@{1}\".format(self.username, self.host)\n        else:\n            return self.host", "is_method": true, "class_name": "RemoteContext", "function_description": "This method generates a formatted string representing the remote host, optionally including the username if available. It's useful for displaying connection details or constructing remote access commands."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_prepare_cmd", "line_number": 93, "body": "def _prepare_cmd(self, cmd):\n        connection_cmd = [\"ssh\", self._host_ref(), \"-o\", \"ControlMaster=no\"]\n        if self.sshpass:\n            connection_cmd = [\"sshpass\", \"-e\"] + connection_cmd\n        else:\n            connection_cmd += [\"-o\", \"BatchMode=yes\"]  # no password prompts etc\n        if self.port:\n            connection_cmd.extend([\"-p\", self.port])\n\n        if self.connect_timeout is not None:\n            connection_cmd += ['-o', 'ConnectTimeout=%d' % self.connect_timeout]\n\n        if self.no_host_key_check:\n            connection_cmd += ['-o', 'UserKnownHostsFile=/dev/null',\n                               '-o', 'StrictHostKeyChecking=no']\n\n        if self.key_file:\n            connection_cmd.extend([\"-i\", self.key_file])\n\n        if self.tty:\n            connection_cmd.append('-t')\n        return connection_cmd + cmd", "is_method": true, "class_name": "RemoteContext", "function_description": "Helper method that constructs a complete SSH command, incorporating host, port, authentication, timeout, and other connection options. It prepares the command for remote execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "Popen", "line_number": 116, "body": "def Popen(self, cmd, **kwargs):\n        \"\"\"\n        Remote Popen.\n        \"\"\"\n        prefixed_cmd = self._prepare_cmd(cmd)\n        return subprocess.Popen(prefixed_cmd, **kwargs)", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides the ability to execute shell commands within a remote context. It automatically prepares the command for remote execution before running it via a subprocess."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "check_output", "line_number": 123, "body": "def check_output(self, cmd):\n        \"\"\"\n        Execute a shell command remotely and return the output.\n\n        Simplified version of Popen when you only want the output as a string and detect any errors.\n        \"\"\"\n        p = self.Popen(cmd, stdout=subprocess.PIPE)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise RemoteCalledProcessError(p.returncode, cmd, self.host, output=output)\n        return output", "is_method": true, "class_name": "RemoteContext", "function_description": "Executes a shell command on a remote host and retrieves its standard output. It ensures command success by raising an error if the command fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "tunnel", "line_number": 136, "body": "def tunnel(self, local_port, remote_port=None, remote_host=\"localhost\"):\n        \"\"\"\n        Open a tunnel between localhost:local_port and remote_host:remote_port via the host specified by this context.\n\n        Remember to close() the returned \"tunnel\" object in order to clean up\n        after yourself when you are done with the tunnel.\n        \"\"\"\n        tunnel_host = \"{0}:{1}:{2}\".format(local_port, remote_host, remote_port)\n        proc = self.Popen(\n            # cat so we can shut down gracefully by closing stdin\n            [\"-L\", tunnel_host, \"echo -n ready && cat\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n        )\n        # make sure to get the data so we know the connection is established\n        ready = proc.stdout.read(5)\n        assert ready == b\"ready\", \"Didn't get ready from remote echo\"\n        yield  # user code executed here\n        proc.communicate()\n        assert proc.returncode == 0, \"Tunnel process did an unclean exit (returncode %s)\" % (proc.returncode,)", "is_method": true, "class_name": "RemoteContext", "function_description": "Provides a service to establish an SSH local port forwarding tunnel. It connects a specified local port to a remote host and port via the context's remote server, allowing temporary network access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "exists", "line_number": 163, "body": "def exists(self, path):\n        \"\"\"\n        Return `True` if file or directory at `path` exist, False otherwise.\n        \"\"\"\n        try:\n            self.remote_context.check_output([\"test\", \"-e\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Verifies the existence of a file or directory on the remote filesystem. This capability is essential for managing remote resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "listdir", "line_number": 176, "body": "def listdir(self, path):\n        while path.endswith('/'):\n            path = path[:-1]\n\n        path = path or '.'\n        listing = self.remote_context.check_output([\"find\", \"-L\", path, \"-type\", \"f\"]).splitlines()\n        return [v.decode('utf-8') for v in listing]", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Lists all regular files recursively within a specified path on the remote file system. It enables enumeration of file paths for remote operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "isdir", "line_number": 184, "body": "def isdir(self, path):\n        \"\"\"\n        Return `True` if directory at `path` exist, False otherwise.\n        \"\"\"\n        try:\n            self.remote_context.check_output([\"test\", \"-d\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Determines if a specified path points to an existing directory on the remote file system. This enables other functions to verify remote directory presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "remove", "line_number": 197, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove file or directory at location `path`.\n        \"\"\"\n        if recursive:\n            cmd = [\"rm\", \"-r\", path]\n        else:\n            cmd = [\"rm\", path]\n\n        self.remote_context.check_output(cmd)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Removes a file or directory from the remote file system. It supports recursive deletion for directory structures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "mkdir", "line_number": 208, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            elif not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            else:\n                return\n\n        if parents:\n            cmd = ['mkdir', '-p', path]\n        else:\n            cmd = ['mkdir', path, '2>&1']\n\n        try:\n            self.remote_context.check_output(cmd)\n        except subprocess.CalledProcessError as e:\n            if b'no such file' in e.output.lower():\n                raise luigi.target.MissingParentDirectory()\n            raise", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Creates a directory on the remote file system. It supports creating parent directories and handles cases where the path already exists, raising specific errors as needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "_scp", "line_number": 229, "body": "def _scp(self, src, dest):\n        cmd = [\"scp\", \"-q\", \"-C\", \"-o\", \"ControlMaster=no\"]\n        if self.remote_context.sshpass:\n            cmd = [\"sshpass\", \"-e\"] + cmd\n        else:\n            cmd.append(\"-B\")\n        if self.remote_context.no_host_key_check:\n            cmd.extend(['-o', 'UserKnownHostsFile=/dev/null',\n                        '-o', 'StrictHostKeyChecking=no'])\n        if self.remote_context.key_file:\n            cmd.extend([\"-i\", self.remote_context.key_file])\n        if self.remote_context.port:\n            cmd.extend([\"-P\", self.remote_context.port])\n        if os.path.isdir(src):\n            cmd.extend([\"-r\"])\n        cmd.extend([src, dest])\n        p = subprocess.Popen(cmd)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise subprocess.CalledProcessError(p.returncode, cmd, output=output)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This internal helper method of `RemoteFileSystem` securely transfers files or directories using the `scp` command. It dynamically configures SSH options for reliable remote operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "put", "line_number": 250, "body": "def put(self, local_path, path):\n        # create parent folder if not exists\n        normpath = posixpath.normpath(path)\n        folder = os.path.dirname(normpath)\n        if folder and not self.exists(folder):\n            self.remote_context.check_output(['mkdir', '-p', folder])\n\n        tmp_path = path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n        self._scp(local_path, \"%s:%s\" % (self.remote_context._host_ref(), tmp_path))\n        self.remote_context.check_output(['mv', tmp_path, path])", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Transfers a file from the local machine to a specified path on the remote file system. It ensures necessary parent directories exist and performs an atomic upload."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "get", "line_number": 261, "body": "def get(self, path, local_path):\n        # Create folder if it does not exist\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder:\n            try:\n                os.makedirs(folder)\n            except OSError:\n                pass\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n        self._scp(\"%s:%s\" % (self.remote_context._host_ref(), path), tmp_local_path)\n        os.rename(tmp_local_path, local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Downloads a specified file from the remote file system to a local path. It handles necessary directory creation and ensures atomic transfer for reliability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "__del__", "line_number": 292, "body": "def __del__(self):\n        super(AtomicRemoteFileWriter, self).__del__()\n\n        try:\n            if self.fs.exists(self.__tmp_path):\n                self.fs.remote_context.check_output(['rm', self.__tmp_path])\n        except Exception:\n            # Don't propagate the exception; bad things can happen.\n            logger.exception('Failed to delete in-flight file')", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "This destructor ensures the cleanup of any leftover temporary files on the remote file system. It automatically removes incomplete or abandoned temporary files created during atomic write operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "close", "line_number": 302, "body": "def close(self):\n        super(AtomicRemoteFileWriter, self).close()\n        self.fs.remote_context.check_output(['mv', self.__tmp_path, self.path])", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Finalizes an atomic remote file write by atomically moving the temporary remote file to its final destination. This ensures the complete file appears at its destination only upon successful closure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "tmp_path", "line_number": 307, "body": "def tmp_path(self):\n        return self.__tmp_path", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "Returns the temporary file path used by the `AtomicRemoteFileWriter` for staging files during an atomic write operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "fs", "line_number": 311, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "AtomicRemoteFileWriter", "function_description": "This method returns the underlying filesystem object that the AtomicRemoteFileWriter uses for managing remote file operations. It provides access to the file system interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "fs", "line_number": 330, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "RemoteTarget", "function_description": "Returns the file system object associated with this remote target. It provides access to the underlying storage for remote operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "open", "line_number": 333, "body": "def open(self, mode='r'):\n        if mode == 'w':\n            file_writer = AtomicRemoteFileWriter(self.fs, self.path)\n            if self.format:\n                return self.format.pipe_writer(file_writer)\n            else:\n                return file_writer\n        elif mode == 'r':\n            file_reader = luigi.format.InputPipeProcessWrapper(\n                self.fs.remote_context._prepare_cmd([\"cat\", self.path]))\n            if self.format:\n                return self.format.pipe_reader(file_reader)\n            else:\n                return file_reader\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Provides an interface to open the remote target for reading or writing data, returning a file-like object. It handles remote file system interaction and optional data formatting."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "put", "line_number": 350, "body": "def put(self, local_path):\n        self.fs.put(local_path, self.path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Uploads a file from a local path to the remote target's location. This method provides the capability to transfer local data to a remote filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ssh.py", "function": "get", "line_number": 353, "body": "def get(self, local_path):\n        self.fs.get(self.path, local_path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Downloads the remote file associated with this target to a specified local path. It facilitates retrieving remote data for local processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "configuration_section", "line_number": 48, "body": "def configuration_section(self):\n        \"\"\"\n        Override to change the configuration section used\n        to obtain default credentials.\n        \"\"\"\n        return 'redshift'", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides the default configuration section name ('redshift') used for obtaining credentials. This method can be overridden to specify a different section."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_access_key_id", "line_number": 56, "body": "def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_access_key_id')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides the configured AWS access key ID, essential for authenticating with Amazon Web Services."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_secret_access_key", "line_number": 63, "body": "def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return self._get_configuration_attribute('aws_secret_access_key')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "This method retrieves the AWS secret access key. It provides a standard way to access this sensitive credential for AWS service authentication or configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_account_id", "line_number": 70, "body": "def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_account_id')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides the configured AWS account ID. This method allows other components to identify the specific AWS environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_arn_role_name", "line_number": 77, "body": "def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return self._get_configuration_attribute('aws_arn_role_name')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides the configured AWS ARN role name, enabling the class to assume specific AWS roles for authentication and authorization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "aws_session_token", "line_number": 84, "body": "def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return self._get_configuration_attribute('aws_session_token')", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Retrieves the AWS session token, providing temporary credentials for authenticating with AWS services."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "_get_configuration_attribute", "line_number": 90, "body": "def _get_configuration_attribute(self, attribute):\n        config = luigi.configuration.get_config()\n\n        value = config.get(self.configuration_section, attribute, default=None)\n\n        if not value:\n            value = os.environ.get(attribute.upper(), None)\n\n        return value", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "This method retrieves a configuration attribute, first from Luigi's configuration and then from environment variables. It provides a robust way to access settings for credential management within the `_CredentialsMixin`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "_credentials", "line_number": 100, "body": "def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")", "is_method": true, "class_name": "_CredentialsMixin", "function_description": "Provides a formatted AWS credential string for authentication, raising an error if required credentials are not configured."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "table_constraints", "line_number": 225, "body": "def table_constraints(self):\n        \"\"\"\n        Add extra table constraints, for example:\n\n        PRIMARY KEY (MY_FIELD, MY_FIELD_2)\n        UNIQUE KEY (MY_FIELD_3)\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Not Implemented\nChain of Thought:\nThe function `table_constraints` is a method within the `S3CopyToTable` class.\n1.  **Analyze the code:** It simply `return ''`.\n2.  **Analyze the docstring:** The docstring explains its *purpose*: \"Add extra table constraints, for example: PRIMARY KEY (MY_FIELD, MY_FIELD_2) UNIQUE KEY (MY_FIELD_3)\".\n3.  **Reconcile code and docstring:** The docstring describes a functionality that the *current implementation* (returning `''`) does not provide. This is a common pattern for methods designed to be overridden in subclasses, acting as a placeholder or hook for custom behavior.\n4.  **Apply \"Not Implemented\" rule:** The rule states: \"Return exactly 'Not Implemented' for placeholder functions, not implemented functions, or functions with no retrieval value\". Since the current implementation returns an empty string, it provides \"no retrieval value\" in terms of actual constraints, and it serves as a placeholder for where constraints *would* be added if the method were overridden. Therefore, it fits the criteria for \"Not Implemented\".\nNot Implemented"}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "do_truncate_table", "line_number": 235, "body": "def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Provides a configuration flag indicating whether the target table should be truncated before copying new data from S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "do_prune", "line_number": 241, "body": "def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method determines if table data pruning is fully configured and enabled for the copy operation. It validates that all or none of the pruning parameters are set, returning True if enabled."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "truncate_table", "line_number": 268, "body": "def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Empties the specified database table by executing a TRUNCATE SQL command. It prepares the table for a fresh load of data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "prune", "line_number": 276, "body": "def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Deletes outdated records from a configured database table. It removes rows where a specified date column is greater than or equal to a defined prune date."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "create_schema", "line_number": 284, "body": "def create_schema(self, connection):\n        \"\"\"\n        Will create the schema in the database\n        \"\"\"\n        if '.' not in self.table:\n            return\n\n        query = 'CREATE SCHEMA IF NOT EXISTS {schema_name};'.format(schema_name=self.table.split('.')[0])\n        connection.cursor().execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Creates the database schema if it does not already exist, using the schema name derived from the target table. This prepares the database for data ingestion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "create_table", "line_number": 294, "body": "def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ', ' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        elif len(self.columns[0]) == 3:\n            # if columns is specified as (name, type, encoding) tuples\n            # possible column encodings: https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n            coldefs = ','.join(\n                '{name} {type} ENCODE {encoding}'.format(\n                    name=name,\n                    type=type,\n                    encoding=encoding) for name, type, encoding in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ',' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Creates the target database table for data ingestion. It constructs and executes the `CREATE TABLE` SQL statement using provided column definitions and table properties."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 360, "body": "def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        if self.enable_metadata_columns:\n            self.post_copy_metacolumns(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method orchestrates the complete data transfer from an S3 path to a database table. It manages connection, initiates the copy, performs post-copy actions, and updates task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy", "line_number": 387, "body": "def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if self.columns and len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Transfers data from an S3 file into a specified Redshift table. It constructs and executes the necessary Redshift `COPY` command for this data ingestion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 411, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Returns a RedshiftTarget object that formally represents the dataset inserted into a Redshift table. This allows other components to interact with the copied data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "does_schema_exist", "line_number": 425, "body": "def does_schema_exist(self, connection):\n        \"\"\"\n        Determine whether the schema already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as schema_exists \"\n                     \"from pg_namespace \"\n                     \"where nspname = lower(%s) limit 1\")\n        else:\n            return True\n\n        cursor = connection.cursor()\n        try:\n            schema = self.table.split('.')[0]\n            cursor.execute(query, [schema])\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Verifies the existence of a database schema, derived from the full table name, within the connected database system. This ensures a valid destination for data loading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "does_table_exist", "line_number": 446, "body": "def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Checks if the target database table exists. It serves as a crucial pre-check before initiating S3 data transfer operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "init_copy", "line_number": 467, "body": "def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_schema_exist(connection):\n            logger.info(\"Creating schema for %s\", self.table)\n            self.create_schema(connection)\n\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            self.create_table(connection)\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "Prepares the target database table for an S3 data copy operation. It ensures schema/table existence, and optionally truncates or prunes old data before the copy."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "post_copy", "line_number": 490, "body": "def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method handles subsequent SQL operations after an S3-to-table data copy. It ensures data cleansing or final insertion into production tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "post_copy_metacolums", "line_number": 498, "body": "def post_copy_metacolums(self, cursor):\n        \"\"\"\n        Performs post-copy to fill metadata columns.\n        \"\"\"\n        logger.info('Executing post copy metadata queries')\n        for query in self.metadata_queries:\n            cursor.execute(query)", "is_method": true, "class_name": "S3CopyToTable", "function_description": "This method executes predefined SQL queries to populate metadata columns in a database table. It serves as a post-copy step to enrich data after loading, likely from S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "jsonpath", "line_number": 532, "body": "def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''", "is_method": true, "class_name": "S3CopyJSONToTable", "function_description": "Provides the configured JSONPath schema location for S3 data-to-table copy operations. It returns an empty string, indicating no external JSONPath file is utilized."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "copy", "line_number": 549, "body": "def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))", "is_method": true, "class_name": "S3CopyJSONToTable", "function_description": "Performs bulk data loading, copying JSON files from a given S3 path directly into a specified Redshift database table. This enables efficient ingestion of S3-hosted JSON data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 591, "body": "def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()", "is_method": true, "class_name": "RedshiftManifestTask", "function_description": "Generates a Redshift-compatible manifest file by listing all files across specified S3 folders. This manifest is then written to the task's configured output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "update_id", "line_number": 649, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Provides a unique identifier for the current task instance. This \"update ID\" helps track specific operations or log entries, particularly for Redshift session management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 656, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "Provides a standardized way to define the Redshift database table that represents the task's output or a dataset it interacts with. It encapsulates connection details and the target table for a specific task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 671, "body": "def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)", "is_method": true, "class_name": "KillOpenRedshiftSessions", "function_description": "The `run` method in `KillOpenRedshiftSessions` terminates all active user sessions for a specified Redshift database. It excludes system sessions and its own, managing potential connection resets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 725, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )", "is_method": true, "class_name": "RedshiftQuery", "function_description": "This method returns a RedshiftTarget object. It specifies the destination for the executed Redshift query, providing connection and table details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "unload_options", "line_number": 762, "body": "def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "This method defines the standard UNLOAD options for a Redshift task, including delimiter, quoting, compression, and overwrite settings. It ensures consistent data export behavior."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "unload_query", "line_number": 769, "body": "def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials '{credentials}' \"\n                \"{unload_options};\")", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Generates a templated SQL UNLOAD command for exporting query results from Redshift to a specified S3 path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "run", "line_number": 777, "body": "def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            credentials=self._credentials())\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n\n        cursor = connection.cursor()\n        cursor.execute(unload_query)\n        logger.info(cursor.statusmessage)\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Executes a Redshift UNLOAD command, exporting data from the database (based on a query) to a specified S3 path. It manages the database connection and task completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/redshift.py", "function": "output", "line_number": 799, "body": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )", "is_method": true, "class_name": "RedshiftUnloadTask", "function_description": "Returns a RedshiftTarget object detailing the destination for the data unloaded by this Redshift task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "percentage_progress", "line_number": 47, "body": "def percentage_progress(self):\n        \"\"\"\n        :return: percentage of query overall progress\n        \"\"\"\n        return self._status.get('stats', {}).get('progressPercentage', 0.1)", "is_method": true, "class_name": "PrestoClient", "function_description": "Reports the current overall progress of a Presto query as a percentage. This method provides real-time status for monitoring query execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "info_uri", "line_number": 54, "body": "def info_uri(self):\n        \"\"\"\n        :return: query UI link\n        \"\"\"\n        return self._status.get('infoUri')", "is_method": true, "class_name": "PrestoClient", "function_description": "Provides the URI (Uniform Resource Identifier) for the Presto query user interface."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "execute", "line_number": 60, "body": "def execute(self, query, parameters=None, mode=None):\n        \"\"\"\n\n        :param query: query to run\n        :param parameters: parameters should be injected in the query\n        :param mode: \"fetch\" - yields rows, \"watch\" - yields log entries\n        :return:\n        \"\"\"\n        class Mode(Enum):\n            watch = 'watch'\n            fetch = 'fetch'\n\n        _mode = Mode(mode) if mode else Mode.watch\n\n        with closing(self._connection.cursor()) as cursor:\n            cursor.execute(query, parameters)\n            status = self._status\n            while status:\n                sleep(self.sleep_time)\n                status = cursor.poll()\n                if status:\n                    if _mode == Mode.watch:\n                        yield status\n                    self._status = status\n\n            if _mode == Mode.fetch:\n                for row in cursor.fetchall():\n                    yield row", "is_method": true, "class_name": "PrestoClient", "function_description": "Executes a Presto query, allowing the caller to either watch real-time query status updates or fetch the complete result set row by row."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "__new__", "line_number": 97, "body": "def __new__(cls, name, bases, attrs):\n        def _client(self):\n            def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val\n\n            connection = Connection(**dict(_kwargs()))\n            return PrestoClient(connection=connection)\n\n        attrs.update({\n            '_client': property(_client)\n        })\n        return super(cls, WithPrestoClient).__new__(cls, name, bases, attrs)", "is_method": true, "class_name": "WithPrestoClient", "function_description": "Equips classes inheriting from `WithPrestoClient` with a `_client` property. This property provides a configured Presto database client, built from class attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_count_query", "line_number": 135, "body": "def _count_query(self):\n        partition = OrderedDict(self.partition or {1: 1})\n\n        def _clauses():\n            for k in partition.keys():\n                yield '{} = %s'.format(k)\n\n        clauses = ' AND '.join(_clauses())\n\n        query = 'SELECT COUNT(*) AS cnt FROM {}.{}.{} WHERE {} LIMIT 1'.format(\n            self.catalog,\n            self.database,\n            self.table,\n            clauses\n        )\n        params = list(partition.values())\n        return query, params", "is_method": true, "class_name": "PrestoTarget", "function_description": "Constructs a Presto SQL query and its parameters to count records within a specific table partition. It dynamically builds the WHERE clause based on the defined partition key-value pairs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "count", "line_number": 168, "body": "def count(self):\n        if not self._count:\n            '''\n            replace to\n            self._count, *_ = next(self._client.execute(*self.count_query, 'fetch'))\n            after py2 deprecation\n            '''\n            self._count = next(self._client.execute(*self._count_query, mode='fetch'))[0]\n        return self._count", "is_method": true, "class_name": "PrestoTarget", "function_description": "This method retrieves the total row count for the Presto target. It efficiently caches the count after the first execution to prevent redundant queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "exists", "line_number": 178, "body": "def exists(self):\n        \"\"\"\n\n        :return: `True` if given table exists and there are any rows in a given partition\n                 `False` if no rows in the partition exists or table is absent\n        \"\"\"\n        try:\n            return self.count() > 0\n        except DatabaseError as exception:\n            if self._table_doesnot_exist(exception):\n                return False\n        except Exception:\n            raise", "is_method": true, "class_name": "PrestoTarget", "function_description": "Determines if the specific Presto table or partition exists and holds data. It returns True if rows are present, False if not or the table is absent."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "host", "line_number": 201, "body": "def host(self):\n        return presto().host", "is_method": true, "class_name": "PrestoTask", "function_description": "Retrieves the host address for the Presto service. This method provides the network location for connecting to Presto within a task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "port", "line_number": 205, "body": "def port(self):\n        return presto().port", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides the configured port number for the Presto server connection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "user", "line_number": 209, "body": "def user(self):\n        return presto().user", "is_method": true, "class_name": "PrestoTask", "function_description": "Retrieves the current user associated with the Presto task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "username", "line_number": 213, "body": "def username(self):\n        return self.user", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides the username associated with the PrestoTask instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "schema", "line_number": 217, "body": "def schema(self):\n        return self.database", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides the database (schema) name associated with this Presto task, indicating the scope of its operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "password", "line_number": 221, "body": "def password(self):\n        return presto().password", "is_method": true, "class_name": "PrestoTask", "function_description": "Retrieves the password required for authenticating with the Presto database, typically for task execution within the PrestoTask context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "catalog", "line_number": 225, "body": "def catalog(self):\n        return presto().catalog", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides access to the Presto database catalog. This enables interaction with its metadata, such as schemas and tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "poll_interval", "line_number": 229, "body": "def poll_interval(self):\n        return presto().poll_interval", "is_method": true, "class_name": "PrestoTask", "function_description": "Retrieves the configured polling interval for Presto tasks, enabling consumers to know how frequently to check for updates."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "source", "line_number": 233, "body": "def source(self):\n        return 'pyhive'", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides the fixed data source identifier 'pyhive' for the Presto task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "protocol", "line_number": 241, "body": "def protocol(self):\n        return 'https' if self.password else presto().protocol", "is_method": true, "class_name": "PrestoTask", "function_description": "Determines the communication protocol for a Presto task. It uses HTTPS if a password is configured, otherwise it defaults to the standard Presto protocol."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "requests_kwargs", "line_number": 253, "body": "def requests_kwargs(self):\n        return {\n            'verify': False\n        }", "is_method": true, "class_name": "PrestoTask", "function_description": "This method provides a default set of keyword arguments for HTTP requests, primarily disabling SSL certificate verification for network communication within the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_maybe_set_tracking_url", "line_number": 260, "body": "def _maybe_set_tracking_url(self):\n        if not self._tracking_url_set:\n            self.set_tracking_url(self._client.info_uri)\n            self._tracking_url_set = True", "is_method": true, "class_name": "PrestoTask", "function_description": "Conditionally sets the Presto task's tracking URL using the client's information URI. This method ensures the URL is initialized only once."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_set_progress", "line_number": 265, "body": "def _set_progress(self):\n        self.set_progress_percentage(self._client.percentage_progress)", "is_method": true, "class_name": "PrestoTask", "function_description": "Synchronizes the task's progress percentage with the current progress reported by its internal client. This updates the task's status based on the underlying operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "run", "line_number": 268, "body": "def run(self):\n        for _ in self._client.execute(self.query):\n            self._maybe_set_tracking_url()\n            self._set_progress()", "is_method": true, "class_name": "PrestoTask", "function_description": "This method executes the Presto query associated with the task. It also updates the task's tracking URL and progress throughout its execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "output", "line_number": 273, "body": "def output(self):\n        return PrestoTarget(\n            client=self._client,\n            catalog=self.catalog,\n            database=self.database,\n            table=self.table,\n            partition=self.partition,\n        )", "is_method": true, "class_name": "PrestoTask", "function_description": "Provides a `PrestoTarget` object, specifying the exact Presto catalog, database, table, and partition where this task's output will be directed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_client", "line_number": 98, "body": "def _client(self):\n            def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val\n\n            connection = Connection(**dict(_kwargs()))\n            return PrestoClient(connection=connection)", "is_method": true, "class_name": "WithPrestoClient", "function_description": "Creates a configured Presto client instance. It dynamically derives connection parameters from the `WithPrestoClient` object's attributes to initialize the client."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_clauses", "line_number": 138, "body": "def _clauses():\n            for k in partition.keys():\n                yield '{} = %s'.format(k)", "is_method": true, "class_name": "PrestoTarget", "function_description": "Provides SQL-formatted equality clauses for partition keys, suitable for constructing WHERE conditions in Presto queries."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/presto.py", "function": "_kwargs", "line_number": 99, "body": "def _kwargs():\n                \"\"\"\n                replace to\n                ```\n                (_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n                ```\n                after py2-deprecation\n                \"\"\"\n                args = inspect.getargspec(Cursor.__init__)[0][1:]\n                for parameter in args:\n                    val = getattr(self, parameter)\n                    if val:\n                        yield parameter, val", "is_method": true, "class_name": "WithPrestoClient", "function_description": "This method dynamically collects non-empty instance attributes, formatting them as keyword arguments for `Cursor` object initialization. It provides a flexible way to configure database cursors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_connect", "line_number": 66, "body": "def _connect(self):\n        \"\"\"\n        Log in to ftp.\n        \"\"\"\n        if self.sftp:\n            self._sftp_connect()\n        else:\n            self._ftp_connect()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Establishes a connection to the remote file system. It handles both SFTP and FTP protocols based on the system's configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_connect", "line_number": 75, "body": "def _sftp_connect(self):\n        try:\n            import pysftp\n        except ImportError:\n            logger.warning('Please install pysftp to use SFTP.')\n\n        self.conn = pysftp.Connection(self.host, username=self.username, password=self.password,\n                                      port=self.port, **self.pysftp_conn_kwargs)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This private method establishes an SFTP connection to a remote host using the pysftp library. It prepares the underlying connection object for the RemoteFileSystem class to perform secure file transfers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_connect", "line_number": 84, "body": "def _ftp_connect(self):\n        if self.tls:\n            self.conn = ftplib.FTP_TLS()\n        else:\n            self.conn = ftplib.FTP()\n        self.conn.connect(self.host, self.port, timeout=self.timeout)\n        self.conn.login(self.username, self.password)\n        if self.tls:\n            self.conn.prot_p()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This private method establishes and authenticates a connection to an FTP or FTP_TLS server. It prepares the file system for subsequent remote operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_close", "line_number": 94, "body": "def _close(self):\n        \"\"\"\n        Close ftp connection.\n        \"\"\"\n        if self.sftp:\n            self._sftp_close()\n        else:\n            self._ftp_close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This method closes the active connection to the remote file system. It handles both SFTP and FTP connections for proper resource release."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_close", "line_number": 103, "body": "def _sftp_close(self):\n        self.conn.close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Closes the underlying SFTP connection, releasing resources for the remote file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_close", "line_number": 106, "body": "def _ftp_close(self):\n        self.conn.quit()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Provides the capability to close the active FTP connection. It gracefully terminates the network session, releasing associated resources for the remote file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "exists", "line_number": 109, "body": "def exists(self, path, mtime=None):\n        \"\"\"\n        Return `True` if file or directory at `path` exist, False otherwise.\n\n        Additional check on modified time when mtime is passed in.\n\n        Return False if the file's modified time is older mtime.\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            exists = self._sftp_exists(path, mtime)\n        else:\n            exists = self._ftp_exists(path, mtime)\n\n        self._close()\n\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Determines if a file or directory exists on the remote file system. It can optionally verify if the file's modification time is newer than a specified timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_exists", "line_number": 128, "body": "def _sftp_exists(self, path, mtime):\n        exists = False\n        if mtime:\n            exists = self.conn.stat(path).st_mtime > mtime\n        elif self.conn.exists(path):\n            exists = True\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Determines if a path exists on the remote SFTP system. It can also verify if an existing path's modification time is newer than a given timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_exists", "line_number": 136, "body": "def _ftp_exists(self, path, mtime):\n        dirname, fn = os.path.split(path)\n\n        files = self.conn.nlst(dirname)\n\n        exists = False\n        if path in files or fn in files:\n            if mtime:\n                mdtm = self.conn.sendcmd('MDTM ' + path)\n                modified = datetime.datetime.strptime(mdtm[4:], \"%Y%m%d%H%M%S\")\n                exists = modified > mtime\n            else:\n                exists = True\n        return exists", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Checks if a file exists on the remote FTP server. It can also verify if the remote file's modification time is newer than a specified timestamp."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "remove", "line_number": 151, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove file or directory at location ``path``.\n\n        :param path: a path within the FileSystem to remove.\n        :type path: str\n        :param recursive: if the path is a directory, recursively remove the directory and\n                          all of its descendants. Defaults to ``True``.\n        :type recursive: bool\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            self._sftp_remove(path, recursive)\n        else:\n            self._ftp_remove(path, recursive)\n\n        self._close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Provides the capability to remove a specified file or directory, optionally recursively, from the remote file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_remove", "line_number": 170, "body": "def _sftp_remove(self, path, recursive):\n        if self.conn.isfile(path):\n            self.conn.unlink(path)\n        else:\n            if not recursive:\n                raise RuntimeError(\"Path is not a regular file, and recursive option is not set\")\n            directories = []\n            # walk the tree, and execute call backs when files,\n            # directories and unknown types are encountered\n            # files must be removed first.  then directories can be removed\n            # after the files are gone.\n            self.conn.walktree(path, self.conn.unlink, directories.append, self.conn.unlink)\n            for directory in reversed(directories):\n                self.conn.rmdir(directory)\n            self.conn.rmdir(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This method deletes a specified file or directory on the remote SFTP server. It supports recursive removal of directories and their contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_remove", "line_number": 186, "body": "def _ftp_remove(self, path, recursive):\n        if recursive:\n            self._rm_recursive(self.conn, path)\n        else:\n            try:\n                # try delete file\n                self.conn.delete(path)\n            except ftplib.all_errors:\n                # it is a folder, delete it\n                self.conn.rmd(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Provides the capability to remove files or directories on a remote FTP server, supporting both single item and recursive deletion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_rm_recursive", "line_number": 197, "body": "def _rm_recursive(self, ftp, path):\n        \"\"\"\n        Recursively delete a directory tree on a remote server.\n\n        Source: https://gist.github.com/artlogic/2632647\n        \"\"\"\n        wd = ftp.pwd()\n\n        # check if it is a file first, because some FTP servers don't return\n        # correctly on ftp.nlst(file)\n        try:\n            ftp.cwd(path)\n        except ftplib.all_errors:\n            # this is a file, we will just delete the file\n            ftp.delete(path)\n            return\n\n        try:\n            names = ftp.nlst()\n        except ftplib.all_errors:\n            # some FTP servers complain when you try and list non-existent paths\n            return\n\n        for name in names:\n            if os.path.split(name)[1] in ('.', '..'):\n                continue\n\n            try:\n                ftp.cwd(name)   # if we can cwd to it, it's a folder\n                ftp.cwd(wd)   # don't try a nuke a folder we're in\n                ftp.cwd(path)  # then go back to where we were\n                self._rm_recursive(ftp, name)\n            except ftplib.all_errors:\n                ftp.delete(name)\n\n        try:\n            ftp.cwd(wd)  # do not delete the folder that we are in\n            ftp.rmd(path)\n        except ftplib.all_errors as e:\n            print('_rm_recursive: Could not remove {0}: {1}'.format(path, e))", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "This method recursively removes a directory and all its contents on a remote FTP server. It provides a robust way to clean up remote file system paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "put", "line_number": 238, "body": "def put(self, local_path, path, atomic=True):\n        \"\"\"\n        Put file from local filesystem to (s)FTP.\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            self._sftp_put(local_path, path, atomic)\n        else:\n            self._ftp_put(local_path, path, atomic)\n\n        self._close()", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Transfers a file from the local filesystem to the remote (S)FTP server. This method provides a core utility for uploading files to a remote system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_put", "line_number": 251, "body": "def _sftp_put(self, local_path, path, atomic):\n        normpath = os.path.normpath(path)\n        directory = os.path.dirname(normpath)\n        self.conn.makedirs(directory)\n\n        if atomic:\n            tmp_path = os.path.join(directory, 'luigi-tmp-{:09d}'.format(random.randrange(0, 1e10)))\n        else:\n            tmp_path = normpath\n\n        self.conn.put(local_path, tmp_path)\n\n        if atomic:\n            self.conn.rename(tmp_path, normpath)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "The `_sftp_put` method uploads a local file to a specified remote SFTP path. It supports an atomic write operation, ensuring the file appears completely only after a successful transfer and rename."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_put", "line_number": 266, "body": "def _ftp_put(self, local_path, path, atomic):\n        normpath = os.path.normpath(path)\n        folder = os.path.dirname(normpath)\n\n        # create paths if do not exists\n        for subfolder in folder.split(os.sep):\n            if subfolder and subfolder not in self.conn.nlst():\n                self.conn.mkd(subfolder)\n\n            self.conn.cwd(subfolder)\n\n        # go back to ftp root folder\n        self.conn.cwd(\"/\")\n\n        # random file name\n        if atomic:\n            tmp_path = folder + os.sep + 'luigi-tmp-%09d' % random.randrange(0, 1e10)\n        else:\n            tmp_path = normpath\n\n        self.conn.storbinary('STOR %s' % tmp_path, open(local_path, 'rb'))\n\n        if atomic:\n            self.conn.rename(tmp_path, normpath)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Uploads a local file to a specified path on a remote FTP server. It ensures necessary directories exist and can perform atomic uploads for data integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "get", "line_number": 291, "body": "def get(self, path, local_path):\n        \"\"\"\n        Download file from (s)FTP to local filesystem.\n        \"\"\"\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder and not os.path.exists(folder):\n            os.makedirs(folder)\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n\n        # download file\n        self._connect()\n\n        if self.sftp:\n            self._sftp_get(path, tmp_local_path)\n        else:\n            self._ftp_get(path, tmp_local_path)\n\n        self._close()\n\n        os.rename(tmp_local_path, local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Downloads a file from a remote (S)FTP server to the local filesystem. It handles directory creation and ensures atomic file transfer for reliability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_get", "line_number": 314, "body": "def _sftp_get(self, path, tmp_local_path):\n        self.conn.get(path, tmp_local_path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Downloads a file from the remote system at the specified path to a temporary local file. This method provides the core SFTP 'get' functionality."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_get", "line_number": 317, "body": "def _ftp_get(self, path, tmp_local_path):\n        self.conn.retrbinary('RETR %s' % path, open(tmp_local_path, 'wb').write)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Downloads a file from the remote FTP server to a specified local temporary path. It's a core utility for retrieving remote files within the RemoteFileSystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "listdir", "line_number": 320, "body": "def listdir(self, path='.'):\n        \"\"\"\n        Gets an list of the contents of path in (s)FTP\n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            contents = self._sftp_listdir(path)\n        else:\n            contents = self._ftp_listdir(path)\n\n        self._close()\n\n        return contents", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Provides a service to list the contents of a specified directory on the remote file system, supporting both FTP and SFTP."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_sftp_listdir", "line_number": 335, "body": "def _sftp_listdir(self, path):\n        return self.conn.listdir(remotepath=path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Lists the contents of a specified directory on the remote SFTP server."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "_ftp_listdir", "line_number": 338, "body": "def _ftp_listdir(self, path):\n        return self.conn.nlst(path)", "is_method": true, "class_name": "RemoteFileSystem", "function_description": "Lists the file and directory names within a specified path on the remote FTP server. It provides a way to inspect the contents of remote directories.\nLists the file and directory names within a specified path on the remote FTP server. It provides a way to inspect the contents of remote directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "move_to_final_destination", "line_number": 359, "body": "def move_to_final_destination(self):\n        self._fs.put(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicFtpFile", "function_description": "Moves a temporarily stored FTP file to its final, permanent destination. This action finalizes the file transfer, making it accessible at its intended location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "fs", "line_number": 363, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "AtomicFtpFile", "function_description": "Provides access to the underlying file system object managed by this `AtomicFtpFile` instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "fs", "line_number": 392, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "RemoteTarget", "function_description": "Provides access to the underlying file system interface of the remote target. This enables interaction with its file system for data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "open", "line_number": 395, "body": "def open(self, mode):\n        \"\"\"\n        Open the FileSystem target.\n\n        This method returns a file-like object which can either be read from or written to depending\n        on the specified mode.\n\n        :param mode: the mode `r` opens the FileSystemTarget in read-only mode, whereas `w` will\n                     open the FileSystemTarget in write mode. Subclasses can implement\n                     additional options.\n        :type mode: str\n        \"\"\"\n        if mode == 'w':\n            return self.format.pipe_writer(AtomicFtpFile(self._fs, self.path))\n\n        elif mode == 'r':\n            temppath = '{}-luigi-tmp-{:09d}'.format(\n                self.path.lstrip('/'), random.randrange(0, 1e10)\n            )\n            try:\n                # store reference to the TemporaryDirectory because it will be removed on GC\n                self.__temp_dir = tempfile.TemporaryDirectory(\n                    prefix=\"luigi-contrib-ftp_\"\n                )\n            except AttributeError:\n                # TemporaryDirectory only available in Python3, use old behaviour in Python2\n                # this file will not be cleaned up automatically\n                self.__tmp_path = os.path.join(\n                    tempfile.gettempdir(), 'luigi-contrib-ftp', temppath\n                )\n            else:\n                self.__tmp_path = os.path.join(self.__temp_dir.name, temppath)\n\n            # download file to local\n            self._fs.get(self.path, self.__tmp_path)\n\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(io.FileIO(self.__tmp_path, 'r')))\n            )\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "is_method": true, "class_name": "RemoteTarget", "function_description": "This method opens the `RemoteTarget`, providing a file-like object for reading or writing remote data. It simplifies interaction by handling remote access details."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "exists", "line_number": 437, "body": "def exists(self):\n        return self.fs.exists(self.path, self.mtime)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Checks if the remote target exists on its associated filesystem, verifying its presence and potentially its modification time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "put", "line_number": 440, "body": "def put(self, local_path, atomic=True):\n        self.fs.put(local_path, self.path, atomic)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Transfers data from a local path to the remote target's configured destination. It offers an atomic upload option for data integrity."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ftp.py", "function": "get", "line_number": 443, "body": "def get(self, local_path):\n        self.fs.get(self.path, local_path)", "is_method": true, "class_name": "RemoteTarget", "function_description": "Downloads the remote resource represented by this `RemoteTarget` object to a specified local path on the local filesystem."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_avro_uri", "line_number": 33, "body": "def _avro_uri(self, target):\n        path_or_uri = target.uri if hasattr(target, 'uri') else target.path\n        return path_or_uri if path_or_uri.endswith('.avro') else path_or_uri.rstrip('/') + '/*.avro'", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "This helper method generates a BigQuery-compatible URI for Avro files, ensuring it targets a specific `.avro` file or all `.avro` files within a directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "source_uris", "line_number": 37, "body": "def source_uris(self):\n        return [self._avro_uri(x) for x in flatten(self.input())]", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Provides the complete list of Avro file URIs from various inputs, which are then used by the `BigQueryLoadAvro` class for data ingestion into BigQuery."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_get_input_schema", "line_number": 40, "body": "def _get_input_schema(self):\n        \"\"\"Arbitrarily picks an object in input and reads the Avro schema from it.\"\"\"\n        assert avro, 'avro module required'\n\n        input_target = flatten(self.input())[0]\n        input_fs = input_target.fs if hasattr(input_target, 'fs') else GCSClient()\n        input_uri = self.source_uris()[0]\n        if '*' in input_uri:\n            file_uris = list(input_fs.list_wildcard(input_uri))\n            if file_uris:\n                input_uri = file_uris[0]\n            else:\n                raise RuntimeError('No match for ' + input_uri)\n\n        schema = []\n        exception_reading_schema = []\n\n        def read_schema(fp):\n            # fp contains the file part downloaded thus far. We rely on that the DataFileReader\n            # initializes itself fine as soon as the file header with schema is downloaded, without\n            # requiring the remainder of the file...\n            try:\n                reader = avro.datafile.DataFileReader(fp, avro.io.DatumReader())\n                schema[:] = [BigQueryLoadAvro._get_writer_schema(reader.datum_reader)]\n            except Exception as e:\n                # Save but assume benign unless schema reading ultimately fails. The benign\n                # exception in case of insufficiently big downloaded file part seems to be:\n                # TypeError('ord() expected a character, but string of length 0 found',).\n                exception_reading_schema[:] = [e]\n                return False\n            return True\n\n        input_fs.download(input_uri, 64 * 1024, read_schema).close()\n        if not schema:\n            raise exception_reading_schema[0]\n        return schema[0]", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Determines the Avro schema of the input data by sampling an input file. This schema is crucial for defining the target data structure or validating records."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_get_writer_schema", "line_number": 78, "body": "def _get_writer_schema(datum_reader):\n        \"\"\"Python-version agnostic getter for datum_reader writer(s)_schema attribute\n\n        Parameters:\n        datum_reader (avro.io.DatumReader): DatumReader\n\n        Returns:\n        Returning correct attribute name depending on Python version.\n        \"\"\"\n        return datum_reader.writer_schema", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Retrieves the writer's schema from an Avro DatumReader. It provides access to the schema used when the Avro data was written, facilitating data interpretation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "_set_output_doc", "line_number": 89, "body": "def _set_output_doc(self, avro_schema):\n        bq_client = self.output().client.client\n        table = self.output().table\n\n        patch = {\n            'description': avro_schema.doc,\n        }\n\n        bq_client.tables().patch(projectId=table.project_id,\n                                 datasetId=table.dataset_id,\n                                 tableId=table.table_id,\n                                 body=patch).execute()", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "Updates the description of the BigQuery output table with documentation extracted from the provided Avro schema. This ensures the table metadata is synchronized with its source schema."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "run", "line_number": 102, "body": "def run(self):\n        super(BigQueryLoadAvro, self).run()\n\n        # We propagate documentation in one fire-and-forget attempt; the output table is\n        # left to exist without documentation if this step raises an exception.\n        try:\n            self._set_output_doc(self._get_input_schema())\n        except Exception as e:\n            logger.warning('Could not propagate Avro doc to BigQuery table description: %r', e)", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "This method attempts to propagate documentation from the input Avro schema to the BigQuery table description. It enhances data discoverability and governance for the loaded table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/bigquery_avro.py", "function": "read_schema", "line_number": 57, "body": "def read_schema(fp):\n            # fp contains the file part downloaded thus far. We rely on that the DataFileReader\n            # initializes itself fine as soon as the file header with schema is downloaded, without\n            # requiring the remainder of the file...\n            try:\n                reader = avro.datafile.DataFileReader(fp, avro.io.DatumReader())\n                schema[:] = [BigQueryLoadAvro._get_writer_schema(reader.datum_reader)]\n            except Exception as e:\n                # Save but assume benign unless schema reading ultimately fails. The benign\n                # exception in case of insufficiently big downloaded file part seems to be:\n                # TypeError('ord() expected a character, but string of length 0 found',).\n                exception_reading_schema[:] = [e]\n                return False\n            return True", "is_method": true, "class_name": "BigQueryLoadAvro", "function_description": "This method reads and extracts the Avro schema from a file-like object. It provides the essential schema definition needed to process Avro data for BigQuery loading."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_home", "line_number": 42, "body": "def pig_home(self):\n        return configuration.get_config().get('pig', 'home', '/usr/share/pig')", "is_method": true, "class_name": "PigJobTask", "function_description": "Provides the configured home directory path for Pig. It returns a default path if no specific directory is set in the configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_command_path", "line_number": 45, "body": "def pig_command_path(self):\n        return os.path.join(self.pig_home(), \"bin/pig\")", "is_method": true, "class_name": "PigJobTask", "function_description": "Returns the absolute path to the `pig` executable within the configured Pig home directory. This provides the necessary command for executing Pig scripts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_env_vars", "line_number": 48, "body": "def pig_env_vars(self):\n        \"\"\"\n        Dictionary of environment variables that should be set when running Pig.\n\n        Ex::\n            return { 'PIG_CLASSPATH': '/your/path' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Provides a mechanism for defining environment variables necessary for a Pig job's execution, such as custom classpaths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_properties", "line_number": 57, "body": "def pig_properties(self):\n        \"\"\"\n        Dictionary of properties that should be set when running Pig.\n\n        Example::\n\n            return { 'pig.additional.jars':'/path/to/your/jar' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Provides a dictionary of custom configuration properties to be set when executing a Pig job. This allows tailoring the Pig runtime environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_parameters", "line_number": 67, "body": "def pig_parameters(self):\n        \"\"\"\n        Dictionary of parameters that should be set for the Pig job.\n\n        Example::\n\n            return { 'YOUR_PARAM_NAME':'Your param value' }\n        \"\"\"\n        return {}", "is_method": true, "class_name": "PigJobTask", "function_description": "Provides a dictionary of configuration parameters to be used when executing a Pig job. Subclasses override this method to supply specific job settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "pig_options", "line_number": 77, "body": "def pig_options(self):\n        \"\"\"\n        List of options that will be appended to the Pig command.\n\n        Example::\n\n            return ['-x', 'local']\n        \"\"\"\n        return []", "is_method": true, "class_name": "PigJobTask", "function_description": "Provides a list of command-line options to configure and customize the execution of a Pig job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "_build_pig_cmd", "line_number": 97, "body": "def _build_pig_cmd(self):\n        opts = self.pig_options()\n\n        def line(k, v):\n            return ('%s=%s%s' % (k, v, os.linesep)).encode('utf-8')\n\n        with tempfile.NamedTemporaryFile() as param_file, tempfile.NamedTemporaryFile() as prop_file:\n            if self.pig_parameters():\n                items = self.pig_parameters().items()\n                param_file.writelines(line(k, v) for (k, v) in items)\n                param_file.flush()\n                opts.append('-param_file')\n                opts.append(param_file.name)\n\n            if self.pig_properties():\n                items = self.pig_properties().items()\n                prop_file.writelines(line(k, v) for k, v in items)\n                prop_file.flush()\n                opts.append('-propertyFile')\n                opts.append(prop_file.name)\n\n            cmd = [self.pig_command_path()] + opts + [\"-f\", self.pig_script_path()]\n\n            logger.info(subprocess.list2cmdline(cmd))\n            yield cmd", "is_method": true, "class_name": "PigJobTask", "function_description": "Generates the full command-line instruction for running a Pig script. It dynamically incorporates parameters and properties by writing them to temporary files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "run", "line_number": 123, "body": "def run(self):\n        with self._build_pig_cmd() as cmd:\n            self.track_and_progress(cmd)", "is_method": true, "class_name": "PigJobTask", "function_description": "Executes the Pig job associated with this task. It builds the necessary Pig command and runs it, while also tracking its progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "track_and_progress", "line_number": 127, "body": "def track_and_progress(self, cmd):\n        temp_stdout = tempfile.TemporaryFile('wb')\n        env = os.environ.copy()\n        env['PIG_HOME'] = self.pig_home()\n        for k, v in self.pig_env_vars().items():\n            env[k] = v\n\n        proc = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        reads = [proc.stderr.fileno(), proc.stdout.fileno()]\n        # tracking the possible problems with this job\n        err_lines = []\n        with PigRunContext():\n            while proc.poll() is None:\n                ret = select.select(reads, [], [])\n                for fd in ret[0]:\n                    if fd == proc.stderr.fileno():\n                        line = proc.stderr.readline().decode('utf8')\n                        err_lines.append(line)\n                    if fd == proc.stdout.fileno():\n                        line_bytes = proc.stdout.readline()\n                        temp_stdout.write(line_bytes)\n                        line = line_bytes.decode('utf8')\n\n                err_line = line.lower()\n                if err_line.find('More information at:') != -1:\n                    logger.info(err_line.split('more information at: ')[-1].strip())\n                if err_line.find(' - '):\n                    t = err_line.split(' - ')[-1].strip()\n                    if t != \"\":\n                        logger.info(t)\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line.decode('utf8') for an_err_line in proc.stderr])\n        if proc.returncode == 0:\n            logger.info(\"Job completed successfully!\")\n        else:\n            logger.error(\"Error when running script:\\n%s\", self.pig_script_path())\n            logger.error(err)\n            raise PigJobError(\"Pig script failed with return value: %s\" % (proc.returncode,), err=err)", "is_method": true, "class_name": "PigJobTask", "function_description": "Executes an external Pig job command, capturing its standard and error output in real-time. It monitors progress, logs important information, and raises an error if the job fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__enter__", "line_number": 172, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "PigRunContext", "function_description": "Upon entering, this context manager intercepts the SIGTERM signal and assigns `self.kill_job` as its handler. This ensures controlled termination behavior for the associated Pig job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "kill_job", "line_number": 177, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        if self.job_id:\n            logger.info('Job interrupted, killing job %s', self.job_id)\n            subprocess.call(['pig', '-e', '\"kill %s\"' % self.job_id])\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "PigRunContext", "function_description": "Aborts the currently managed Pig job by issuing a kill command. It provides a mechanism to cleanly exit the Python script if interrupted by a signal."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__exit__", "line_number": 185, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "PigRunContext", "function_description": "The `__exit__` method of `PigRunContext` ensures graceful resource cleanup. It terminates the managed job if a KeyboardInterrupt occurs and restores the original signal handler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "__str__", "line_number": 198, "body": "def __str__(self):\n        info = self.message\n        if self.out:\n            info += \"\\nSTDOUT: \" + str(self.out)\n        if self.err:\n            info += \"\\nSTDERR: \" + str(self.err)\n        return info", "is_method": true, "class_name": "PigJobError", "function_description": "This method provides a comprehensive, human-readable string representation of a Pig job error. It includes the main error message along with captured standard output and error streams for debugging."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pig.py", "function": "line", "line_number": 100, "body": "def line(k, v):\n            return ('%s=%s%s' % (k, v, os.linesep)).encode('utf-8')", "is_method": true, "class_name": "PigJobTask", "function_description": "Formats a key-value pair into a newline-terminated, UTF-8 encoded byte string, suitable for line-by-line output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "get_path", "line_number": 82, "body": "def get_path(self):\n        \"\"\"\n        Returns a temporary file path based on a MD5 hash generated with the task's name and its arguments\n        \"\"\"\n        md5_hash = hashlib.md5(self.task_id.encode()).hexdigest()\n        logger.debug('Hash %s corresponds to task %s', md5_hash, self.task_id)\n\n        return os.path.join(self.temp_dir, str(self.unique.value), md5_hash)", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Provides a unique, temporary file path for a task instance. The path is derived from the task's identifier, ensuring consistent temporary storage for run data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "exists", "line_number": 91, "body": "def exists(self):\n        \"\"\"\n        Checks if the file exists\n        \"\"\"\n        return os.path.isfile(self.get_path())", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Determines if the file associated with this target object exists on the filesystem. Useful for checking file presence before operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/simulate.py", "function": "done", "line_number": 97, "body": "def done(self):\n        \"\"\"\n        Creates temporary file to mark the task as `done`\n        \"\"\"\n        logger.info('Marking %s as done', self)\n\n        fn = self.get_path()\n        try:\n            os.makedirs(os.path.dirname(fn))\n        except OSError:\n            pass\n        open(fn, 'w').close()", "is_method": true, "class_name": "RunAnywayTarget", "function_description": "Flags the associated task as completed by creating a specific file. This enables external systems or subsequent processes to recognize its finished state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "open", "line_number": 47, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            return self.format.pipe_reader(\n                ReadableWebHdfsFile(path=self.path, client=self.fs)\n            )\n\n        return self.format.pipe_writer(\n            AtomicWebHdfsFile(path=self.path, client=self.fs)\n        )", "is_method": true, "class_name": "WebHdfsTarget", "function_description": "This method opens the WebHDFS target path for reading or writing, returning a file-like object for streaming data. It leverages format-specific readers/writers for efficient and atomic operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "read", "line_number": 68, "body": "def read(self):\n        self.generator = self.client.read(self.path)\n        res = list(self.generator)[0]\n        return res", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Retrieves the initial chunk of data from the WebHDFS file associated with this object. It provides the very first portion of the file's content for immediate use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "readlines", "line_number": 73, "body": "def readlines(self, char='\\n'):\n        self.generator = self.client.read(self.path, buffer_char=char)\n        return self.generator", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Returns a generator for efficiently reading a WebHDFS file line by line. It yields content delimited by the specified character, defaulting to newlines."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__enter__", "line_number": 77, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Prepares the `ReadableWebHdfsFile` object for use as a context manager, enabling safe and automatic resource management with Python's `with` statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__exit__", "line_number": 80, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Ensures the WebHDFS file is properly closed when exiting a `with` statement, releasing associated resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "__iter__", "line_number": 83, "body": "def __iter__(self):\n        self.generator = self.readlines('\\n')\n        yield from self.generator\n        self.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Enables direct iteration over the lines of the WebHDFS file, providing its content sequentially and ensuring proper closure after full iteration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "close", "line_number": 88, "body": "def close(self):\n        self.generator.close()", "is_method": true, "class_name": "ReadableWebHdfsFile", "function_description": "Closes the connection to the WebHDFS file. This releases the underlying resources and ensures proper cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/webhdfs.py", "function": "move_to_final_destination", "line_number": 101, "body": "def move_to_final_destination(self):\n        if not self.client.exists(self.path):\n            self.client.upload(self.path, self.tmp_path)", "is_method": true, "class_name": "AtomicWebHdfsFile", "function_description": "Ensures a temporary HDFS file is moved to its final destination. It uploads the file only if the destination path does not yet exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "image", "line_number": 69, "body": "def image(self):\n        return 'alpine'", "is_method": true, "class_name": "DockerTask", "function_description": "Provides the default Docker image name ('alpine') for the task, useful for defining the execution environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "command", "line_number": 73, "body": "def command(self):\n        return \"echo hello world\"", "is_method": true, "class_name": "DockerTask", "function_description": "Defines the default shell command for a Docker task. It provides the hardcoded string \"echo hello world\" for container execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "container_options", "line_number": 81, "body": "def container_options(self):\n        return {}", "is_method": true, "class_name": "DockerTask", "function_description": "Returns a dictionary of default empty configuration options for the Docker container. Subclasses can override this method to define custom container settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "container_tmp_dir", "line_number": 89, "body": "def container_tmp_dir(self):\n        return '/tmp/luigi'", "is_method": true, "class_name": "DockerTask", "function_description": "Provides the standard temporary directory path used within a Docker container for Luigi tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "binds", "line_number": 93, "body": "def binds(self):\n        '''\n        Override this to mount local volumes, in addition to the /tmp/luigi\n        which gets defined by default. This should return a list of strings.\n        e.g. ['/hostpath1:/containerpath1', '/hostpath2:/containerpath2']\n        '''\n        return None", "is_method": true, "class_name": "DockerTask", "function_description": "This method allows a Docker task to specify additional local host paths to be mounted into its container. It defines extra volume binds beyond the default, useful for accessing data or configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "auto_remove", "line_number": 110, "body": "def auto_remove(self):\n        return True", "is_method": true, "class_name": "DockerTask", "function_description": "Configures the Docker task to automatically remove its container upon completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/docker_runner.py", "function": "run", "line_number": 169, "body": "def run(self):\n\n        # get image if missing\n        if self.force_pull or len(self._client.images(name=self._image)) == 0:\n            logger.info('Pulling docker image ' + self._image)\n            try:\n                for logline in self._client.pull(self._image, stream=True):\n                    logger.debug(logline.decode('utf-8'))\n            except APIError as e:\n                self.__logger.warning(\"Error in Docker API: \" + e.explanation)\n                raise\n\n        # remove clashing container if a container with the same name exists\n        if self.auto_remove and self.name:\n            try:\n                self._client.remove_container(self.name,\n                                              force=True)\n            except APIError as e:\n                self.__logger.warning(\"Ignored error in Docker API: \" + e.explanation)\n\n        # run the container\n        try:\n            logger.debug('Creating image: %s command: %s volumes: %s'\n                         % (self._image, self.command, self._binds))\n\n            host_config = self._client.create_host_config(binds=self._binds,\n                                                          network_mode=self.network_mode)\n\n            container = self._client.create_container(self._image,\n                                                      command=self.command,\n                                                      name=self.name,\n                                                      environment=self.environment,\n                                                      volumes=self._volumes,\n                                                      host_config=host_config,\n                                                      **self.container_options)\n            self._client.start(container['Id'])\n\n            exit_status = self._client.wait(container['Id'])\n            # docker-py>=3.0.0 returns a dict instead of the status code directly\n            if type(exit_status) is dict:\n                exit_status = exit_status['StatusCode']\n\n            if exit_status != 0:\n                stdout = False\n                stderr = True\n                error = self._client.logs(container['Id'],\n                                          stdout=stdout,\n                                          stderr=stderr)\n            if self.auto_remove:\n                try:\n                    self._client.remove_container(container['Id'])\n                except docker.errors.APIError:\n                    self.__logger.warning(\"Container \" + container['Id'] +\n                                          \" could not be removed\")\n            if exit_status != 0:\n                raise ContainerError(container, exit_status, self.command, self._image, error)\n\n        except ContainerError as e:\n            # catch non zero exti status and return it\n            container_name = ''\n            if self.name:\n                container_name = self.name\n            try:\n                message = e.message\n            except AttributeError:\n                message = str(e)\n            self.__logger.error(\"Container \" + container_name +\n                                \" exited with non zero code: \" + message)\n            raise\n        except ImageNotFound:\n            self.__logger.error(\"Image \" + self._image + \" not found\")\n            raise\n        except APIError as e:\n            self.__logger.error(\"Error in Docker API: \"+e.explanation)\n            raise\n\n        # delete temp dir\n        filesys = LocalFileSystem()\n        if self.mount_tmp and filesys.exists(self._host_tmp_dir):\n            filesys.remove(self._host_tmp_dir, recursive=True)", "is_method": true, "class_name": "DockerTask", "function_description": "Executes a Docker container by pulling its image if needed, then creating and running it with specified configurations. It manages container lifecycle, including error handling and cleanup, to perform a task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "attach", "line_number": 75, "body": "def attach(*packages):\n    \"\"\"\n    Attach a python package to hadoop map reduce tarballs to make those packages available\n    on the hadoop cluster.\n    \"\"\"\n    _attached_packages.extend(packages)", "is_method": false, "function_description": "Registers Python packages for inclusion in Hadoop MapReduce tarballs, ensuring their availability on the cluster for distributed jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "dereference", "line_number": 83, "body": "def dereference(f):\n    if os.path.islink(f):\n        # by joining with the dirname we are certain to get the absolute path\n        return dereference(os.path.join(os.path.dirname(f), os.readlink(f)))\n    else:\n        return f", "is_method": false, "function_description": "Recursively resolves a file path by following all symbolic links until it reaches the final, non-link target. It returns the true underlying file system path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "get_extra_files", "line_number": 91, "body": "def get_extra_files(extra_files):\n    result = []\n    for f in extra_files:\n        if isinstance(f, str):\n            src, dst = f, os.path.basename(f)\n        elif isinstance(f, tuple):\n            src, dst = f\n        else:\n            raise Exception()\n\n        if os.path.isdir(src):\n            src_prefix = os.path.join(src, '')\n            for base, dirs, files in os.walk(src):\n                for f in files:\n                    f_src = os.path.join(base, f)\n                    f_src_stripped = f_src[len(src_prefix):]\n                    f_dst = os.path.join(dst, f_src_stripped)\n                    result.append((f_src, f_dst))\n        else:\n            result.append((src, dst))\n\n    return result", "is_method": false, "function_description": "Transforms a list of file and directory paths into a standardized list of source-destination pairs. It recursively expands directories to include all contained files, preparing them for transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "create_packages_archive", "line_number": 115, "body": "def create_packages_archive(packages, filename):\n    \"\"\"\n    Create a tar archive which will contain the files for the packages listed in packages.\n    \"\"\"\n    import tarfile\n    tar = tarfile.open(filename, \"w\")\n\n    def add(src, dst):\n        logger.debug('adding to tar: %s -> %s', src, dst)\n        tar.add(src, dst)\n\n    def add_files_for_package(sub_package_path, root_package_path, root_package_name):\n        for root, dirs, files in os.walk(sub_package_path):\n            if '.svn' in dirs:\n                dirs.remove('.svn')\n            for f in files:\n                if not f.endswith(\".pyc\") and not f.startswith(\".\"):\n                    add(dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f)\n\n    for package in packages:\n        # Put a submodule's entire package in the archive. This is the\n        # magic that usually packages everything you need without\n        # having to attach packages/modules explicitly\n        if not getattr(package, \"__path__\", None) and '.' in package.__name__:\n            package = __import__(package.__name__.rpartition('.')[0], None, None, 'non_empty')\n\n        n = package.__name__.replace(\".\", \"/\")\n\n        if getattr(package, \"__path__\", None):\n            # TODO: (BUG) picking only the first path does not\n            # properly deal with namespaced packages in different\n            # directories\n            p = package.__path__[0]\n\n            if p.endswith('.egg') and os.path.isfile(p):\n                raise 'egg files not supported!!!'\n                # Add the entire egg file\n                # p = p[:p.find('.egg') + 4]\n                # add(dereference(p), os.path.basename(p))\n\n            else:\n                # include __init__ files from parent projects\n                root = []\n                for parent in package.__name__.split('.')[0:-1]:\n                    root.append(parent)\n                    module_name = '.'.join(root)\n                    directory = '/'.join(root)\n\n                    add(dereference(__import__(module_name, None, None, 'non_empty').__path__[0] + \"/__init__.py\"),\n                        directory + \"/__init__.py\")\n\n                add_files_for_package(p, p, n)\n\n                # include egg-info directories that are parallel:\n                for egg_info_path in glob.glob(p + '*.egg-info'):\n                    logger.debug(\n                        'Adding package metadata to archive for \"%s\" found at \"%s\"',\n                        package.__name__,\n                        egg_info_path\n                    )\n                    add_files_for_package(egg_info_path, p, n)\n\n        else:\n            f = package.__file__\n            if f.endswith(\"pyc\"):\n                f = f[:-3] + \"py\"\n            if n.find(\".\") == -1:\n                add(dereference(f), os.path.basename(f))\n            else:\n                add(dereference(f), n + \".py\")\n    tar.close()", "is_method": false, "function_description": "Creates a tar archive containing the source files and metadata for specified Python packages. It enables bundling Python code for distribution or deployment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "flatten", "line_number": 188, "body": "def flatten(sequence):\n    \"\"\"\n    A simple generator which flattens a sequence.\n\n    Only one level is flattened.\n\n    .. code-block:: python\n\n        (1, (2, 3), 4) -> (1, 2, 3, 4)\n\n    \"\"\"\n    for item in sequence:\n        if hasattr(item, \"__iter__\") and not isinstance(item, str) and not isinstance(item, bytes):\n            for i in item:\n                yield i\n        else:\n            yield item", "is_method": false, "function_description": "Flattens a sequence by unpacking one level of nested, non-string, non-byte iterables into a single, linear sequence. This is useful for processing all elements sequentially."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_and_track_hadoop_job", "line_number": 247, "body": "def run_and_track_hadoop_job(arglist, tracking_url_callback=None, env=None):\n    \"\"\"\n    Runs the job by invoking the command from the given arglist.\n    Finds tracking urls from the output and attempts to fetch errors using those urls if the job fails.\n    Throws HadoopJobError with information about the error\n    (including stdout and stderr from the process)\n    on failure and returns normally otherwise.\n\n    :param arglist:\n    :param tracking_url_callback:\n    :param env:\n    :return:\n    \"\"\"\n    logger.info('%s', subprocess.list2cmdline(arglist))\n\n    def write_luigi_history(arglist, history):\n        \"\"\"\n        Writes history to a file in the job's output directory in JSON format.\n        Currently just for tracking the job ID in a configuration where\n        no history is stored in the output directory by Hadoop.\n        \"\"\"\n        history_filename = configuration.get_config().get('core', 'history-filename', '')\n        if history_filename and '-output' in arglist:\n            output_dir = arglist[arglist.index('-output') + 1]\n            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')\n            f.write(json.dumps(history))\n            f.close()\n\n    def track_process(arglist, tracking_url_callback, env=None):\n        # Dump stdout to a temp file, poll stderr and log it\n        temp_stdout = tempfile.TemporaryFile('w+t')\n        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)\n\n        # We parse the output to try to find the tracking URL.\n        # This URL is useful for fetching the logs of the job.\n        tracking_url = None\n        job_id = None\n        application_id = None\n        err_lines = []\n\n        with HadoopRunContext() as hadoop_context:\n            while proc.poll() is None:\n                err_line = proc.stderr.readline()\n                err_lines.append(err_line)\n                err_line = err_line.strip()\n                if err_line:\n                    logger.info('%s', err_line)\n                err_line = err_line.lower()\n                tracking_url_match = TRACKING_RE.search(err_line)\n                if tracking_url_match:\n                    tracking_url = tracking_url_match.group('url')\n                    try:\n                        tracking_url_callback(tracking_url)\n                    except Exception as e:\n                        logger.error(\"Error in tracking_url_callback, disabling! %s\", e)\n\n                        def tracking_url_callback(x):\n                            return None\n                if err_line.find('running job') != -1:\n                    # hadoop jar output\n                    job_id = err_line.split('running job: ')[-1]\n                if err_line.find('submitted hadoop job:') != -1:\n                    # scalding output\n                    job_id = err_line.split('submitted hadoop job: ')[-1]\n                if err_line.find('submitted application ') != -1:\n                    application_id = err_line.split('submitted application ')[-1]\n                hadoop_context.job_id = job_id\n                hadoop_context.application_id = application_id\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])\n        temp_stdout.seek(0)\n        out = ''.join(temp_stdout.readlines())\n\n        if proc.returncode == 0:\n            write_luigi_history(arglist, {'job_id': job_id})\n            return (out, err)\n\n        # Try to fetch error logs if possible\n        message = 'Streaming job failed with exit code %d. ' % proc.returncode\n        if not tracking_url:\n            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)\n\n        try:\n            task_failures = fetch_task_failures(tracking_url)\n        except Exception as e:\n            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %\n                                 (tracking_url, e), out, err)\n\n        if not task_failures:\n            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)\n        else:\n            raise HadoopJobError(message + 'Output from tasks below:\\n%s' % task_failures, out, err)\n\n    if tracking_url_callback is None:\n        def tracking_url_callback(x): return None\n\n    return track_process(arglist, tracking_url_callback, env)", "is_method": false, "function_description": "Executes a Hadoop job, capturing its output and tracking URLs for monitoring. On failure, it attempts to fetch detailed error logs and raises an informative `HadoopJobError`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "fetch_task_failures", "line_number": 347, "body": "def fetch_task_failures(tracking_url):\n    \"\"\"\n    Uses mechanize to fetch the actual task logs from the task tracker.\n\n    This is highly opportunistic, and we might not succeed.\n    So we set a low timeout and hope it works.\n    If it does not, it's not the end of the world.\n\n    TODO: Yarn has a REST API that we should probably use instead:\n    http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html\n    \"\"\"\n    import mechanize\n    timeout = 3.0\n    failures_url = tracking_url.replace('jobdetails.jsp', 'jobfailures.jsp') + '&cause=failed'\n    logger.debug('Fetching data from %s', failures_url)\n    b = mechanize.Browser()\n    b.open(failures_url, timeout=timeout)\n    links = list(b.links(text_regex='Last 4KB'))  # For some reason text_regex='All' doesn't work... no idea why\n    links = random.sample(links, min(10, len(links)))  # Fetch a random subset of all failed tasks, so not to be biased towards the early fails\n    error_text = []\n    for link in links:\n        task_url = link.url.replace('&start=-4097', '&start=-100000')  # Increase the offset\n        logger.debug('Fetching data from %s', task_url)\n        b2 = mechanize.Browser()\n        try:\n            r = b2.open(task_url, timeout=timeout)\n            data = r.read()\n        except Exception as e:\n            logger.debug('Error fetching data from %s: %s', task_url, e)\n            continue\n        # Try to get the hex-encoded traceback back from the output\n        for exc in re.findall(r'luigi-exc-hex=[0-9a-f]+', data):\n            error_text.append('---------- %s:' % task_url)\n            error_text.append(exc.split('=')[-1].decode('hex'))\n\n    return '\\n'.join(error_text)", "is_method": false, "function_description": "Retrieves error tracebacks from a task tracking web interface by parsing specific web pages. This provides failure details for debugging and automated analysis of failed tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add", "line_number": 122, "body": "def add(src, dst):\n        logger.debug('adding to tar: %s -> %s', src, dst)\n        tar.add(src, dst)", "is_method": false, "function_description": "Adds a specified source file or directory to a tar archive. It maps the source path to a given destination path within the archive."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add_files_for_package", "line_number": 126, "body": "def add_files_for_package(sub_package_path, root_package_path, root_package_name):\n        for root, dirs, files in os.walk(sub_package_path):\n            if '.svn' in dirs:\n                dirs.remove('.svn')\n            for f in files:\n                if not f.endswith(\".pyc\") and not f.startswith(\".\"):\n                    add(dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f)", "is_method": false, "function_description": "Collects Python source files from a sub-package path, excluding compiled and hidden files. It maps their physical paths to logical package paths for inclusion in a root package."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__enter__", "line_number": 213, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Sets up the Hadoop job's execution context. It registers a `SIGTERM` handler to enable graceful termination of the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "kill_job", "line_number": 218, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        if self.application_id:\n            logger.info('Job interrupted, killing application %s' % self.application_id)\n            subprocess.call(['yarn', 'application', '-kill', self.application_id])\n        elif self.job_id:\n            logger.info('Job interrupted, killing job %s', self.job_id)\n            subprocess.call(['mapred', 'job', '-kill', self.job_id])\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Provides a mechanism to forcefully terminate a running Hadoop YARN application or MapReduce job associated with the context, typically in response to an interruption."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__exit__", "line_number": 229, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "HadoopRunContext", "function_description": "Cleans up the Hadoop job context on exit. It kills the job if a KeyboardInterrupt occurs and restores the original SIGTERM signal handler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__str__", "line_number": 243, "body": "def __str__(self):\n        return self.message", "is_method": true, "class_name": "HadoopJobError", "function_description": "Provides a human-readable string representation of a Hadoop job error, returning its stored message."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "write_luigi_history", "line_number": 262, "body": "def write_luigi_history(arglist, history):\n        \"\"\"\n        Writes history to a file in the job's output directory in JSON format.\n        Currently just for tracking the job ID in a configuration where\n        no history is stored in the output directory by Hadoop.\n        \"\"\"\n        history_filename = configuration.get_config().get('core', 'history-filename', '')\n        if history_filename and '-output' in arglist:\n            output_dir = arglist[arglist.index('-output') + 1]\n            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')\n            f.write(json.dumps(history))\n            f.close()", "is_method": false, "function_description": "This function writes Luigi job history, typically job IDs, to a JSON file within the job's specified output directory for tracking purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "track_process", "line_number": 275, "body": "def track_process(arglist, tracking_url_callback, env=None):\n        # Dump stdout to a temp file, poll stderr and log it\n        temp_stdout = tempfile.TemporaryFile('w+t')\n        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)\n\n        # We parse the output to try to find the tracking URL.\n        # This URL is useful for fetching the logs of the job.\n        tracking_url = None\n        job_id = None\n        application_id = None\n        err_lines = []\n\n        with HadoopRunContext() as hadoop_context:\n            while proc.poll() is None:\n                err_line = proc.stderr.readline()\n                err_lines.append(err_line)\n                err_line = err_line.strip()\n                if err_line:\n                    logger.info('%s', err_line)\n                err_line = err_line.lower()\n                tracking_url_match = TRACKING_RE.search(err_line)\n                if tracking_url_match:\n                    tracking_url = tracking_url_match.group('url')\n                    try:\n                        tracking_url_callback(tracking_url)\n                    except Exception as e:\n                        logger.error(\"Error in tracking_url_callback, disabling! %s\", e)\n\n                        def tracking_url_callback(x):\n                            return None\n                if err_line.find('running job') != -1:\n                    # hadoop jar output\n                    job_id = err_line.split('running job: ')[-1]\n                if err_line.find('submitted hadoop job:') != -1:\n                    # scalding output\n                    job_id = err_line.split('submitted hadoop job: ')[-1]\n                if err_line.find('submitted application ') != -1:\n                    application_id = err_line.split('submitted application ')[-1]\n                hadoop_context.job_id = job_id\n                hadoop_context.application_id = application_id\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])\n        temp_stdout.seek(0)\n        out = ''.join(temp_stdout.readlines())\n\n        if proc.returncode == 0:\n            write_luigi_history(arglist, {'job_id': job_id})\n            return (out, err)\n\n        # Try to fetch error logs if possible\n        message = 'Streaming job failed with exit code %d. ' % proc.returncode\n        if not tracking_url:\n            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)\n\n        try:\n            task_failures = fetch_task_failures(tracking_url)\n        except Exception as e:\n            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %\n                                 (tracking_url, e), out, err)\n\n        if not task_failures:\n            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)\n        else:\n            raise HadoopJobError(message + 'Output from tasks below:\\n%s' % task_failures, out, err)", "is_method": false, "function_description": "Monitors and tracks an external process's execution, extracting tracking URLs and job IDs from its output. It logs real-time stderr and retrieves detailed error logs upon job failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_job", "line_number": 414, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        packages = [luigi] + self.modules + job.extra_modules() + list(_attached_packages)\n\n        # find the module containing the job\n        packages.append(__import__(job.__module__, None, None, 'dummy'))\n\n        # find the path to out runner.py\n        runner_path = mrrunner.__file__\n        # assume source is next to compiled\n        if runner_path.endswith(\"pyc\"):\n            runner_path = runner_path[:-3] + \"py\"\n\n        base_tmp_dir = configuration.get_config().get('core', 'tmp-dir', None)\n        if base_tmp_dir:\n            warnings.warn(\"The core.tmp-dir configuration item is\"\n                          \" deprecated, please use the TMPDIR\"\n                          \" environment variable if you wish\"\n                          \" to control where luigi.contrib.hadoop may\"\n                          \" create temporary files and directories.\")\n            self.tmp_dir = os.path.join(base_tmp_dir, 'hadoop_job_%016x' % random.getrandbits(64))\n            os.makedirs(self.tmp_dir)\n        else:\n            self.tmp_dir = tempfile.mkdtemp()\n\n        logger.debug(\"Tmp dir: %s\", self.tmp_dir)\n\n        # build arguments\n        config = configuration.get_config()\n        python_executable = config.get('hadoop', 'python-executable', 'python')\n        runner_arg = 'mrrunner.pex' if job.package_binary is not None else 'mrrunner.py'\n        command = '{0} {1} {{step}}'.format(python_executable, runner_arg)\n        map_cmd = command.format(step='map')\n        cmb_cmd = command.format(step='combiner')\n        red_cmd = command.format(step='reduce')\n\n        output_final = job.output().path\n        # atomic output: replace output with a temporary work directory\n        if self.end_job_with_atomic_move_dir:\n            illegal_targets = (\n                luigi.contrib.s3.S3FlagTarget, luigi.contrib.gcs.GCSFlagTarget)\n            if isinstance(job.output(), illegal_targets):\n                raise TypeError(\"end_job_with_atomic_move_dir is not supported\"\n                                \" for {}\".format(illegal_targets))\n            output_hadoop = '{output}-temp-{time}'.format(\n                output=output_final,\n                time=datetime.datetime.now().isoformat().replace(':', '-'))\n        else:\n            output_hadoop = output_final\n\n        arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', self.streaming_jar]\n\n        # 'libjars' is a generic option, so place it first\n        libjars = [libjar for libjar in self.libjars]\n\n        for libjar in self.libjars_in_hdfs:\n            run_cmd = luigi.contrib.hdfs.load_hadoop_cmd() + ['fs', '-get', libjar, self.tmp_dir]\n            logger.debug(subprocess.list2cmdline(run_cmd))\n            subprocess.call(run_cmd)\n            libjars.append(os.path.join(self.tmp_dir, os.path.basename(libjar)))\n\n        if libjars:\n            arglist += ['-libjars', ','.join(libjars)]\n\n        # 'archives' is also a generic option\n        archives = []\n        extra_archives = job.extra_archives()\n\n        if self.archives:\n            archives = self.archives\n\n        if extra_archives:\n            archives += extra_archives\n\n        if archives:\n            arglist += ['-archives', ','.join(archives)]\n\n        # Add static files and directories\n        extra_files = get_extra_files(job.extra_files())\n\n        files = []\n        for src, dst in extra_files:\n            dst_tmp = '%s_%09d' % (dst.replace('/', '_'), random.randint(0, 999999999))\n            files += ['%s#%s' % (src, dst_tmp)]\n            # -files doesn't support subdirectories, so we need to create the dst_tmp -> dst manually\n            job.add_link(dst_tmp, dst)\n\n        if files:\n            arglist += ['-files', ','.join(files)]\n\n        jobconfs = job.jobconfs()\n\n        for k, v in self.jobconfs.items():\n            jobconfs.append('%s=%s' % (k, v))\n\n        for conf in jobconfs:\n            arglist += ['-D', conf]\n\n        arglist += self.streaming_args\n\n        # Add additional non-generic  per-job streaming args\n        extra_streaming_args = job.extra_streaming_arguments()\n        for (arg, value) in extra_streaming_args:\n            if not arg.startswith('-'):  # safety first\n                arg = '-' + arg\n            arglist += [arg, value]\n\n        arglist += ['-mapper', map_cmd]\n\n        if job.combiner != NotImplemented:\n            arglist += ['-combiner', cmb_cmd]\n        if job.reducer != NotImplemented:\n            arglist += ['-reducer', red_cmd]\n        packages_fn = 'mrrunner.pex' if job.package_binary is not None else 'packages.tar'\n        files = [\n            runner_path if job.package_binary is None else None,\n            os.path.join(self.tmp_dir, packages_fn),\n            os.path.join(self.tmp_dir, 'job-instance.pickle'),\n        ]\n\n        for f in filter(None, files):\n            arglist += ['-file', f]\n\n        if self.output_format:\n            arglist += ['-outputformat', self.output_format]\n        if self.input_format:\n            arglist += ['-inputformat', self.input_format]\n\n        allowed_input_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3Target,\n            luigi.contrib.gcs.GCSTarget)\n        for target in luigi.task.flatten(job.input_hadoop()):\n            if not isinstance(target, allowed_input_targets):\n                raise TypeError('target must one of: {}'.format(\n                    allowed_input_targets))\n            arglist += ['-input', target.path]\n\n        allowed_output_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3FlagTarget,\n            luigi.contrib.gcs.GCSFlagTarget)\n        if not isinstance(job.output(), allowed_output_targets):\n            raise TypeError('output must be one of: {}'.format(\n                allowed_output_targets))\n        arglist += ['-output', output_hadoop]\n\n        # submit job\n        if job.package_binary is not None:\n            shutil.copy(job.package_binary, os.path.join(self.tmp_dir, 'mrrunner.pex'))\n        else:\n            create_packages_archive(packages, os.path.join(self.tmp_dir, 'packages.tar'))\n\n        job.dump(self.tmp_dir)\n\n        run_and_track_hadoop_job(arglist, tracking_url_callback=job.set_tracking_url)\n\n        if self.end_job_with_atomic_move_dir:\n            luigi.contrib.hdfs.HdfsTarget(output_hadoop).move_dir(output_final)\n        self.finish()", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Runs a Luigi-defined Hadoop streaming job. It prepares all necessary files, configurations, and arguments, then submits the job for execution on the Hadoop cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "finish", "line_number": 578, "body": "def finish(self):\n        # FIXME: check for isdir?\n        if self.tmp_dir and os.path.exists(self.tmp_dir):\n            logger.debug('Removing directory %s', self.tmp_dir)\n            shutil.rmtree(self.tmp_dir)", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "This method performs cleanup for the Hadoop job runner. It removes the temporary directory associated with the job, ensuring resources are properly freed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "__del__", "line_number": 584, "body": "def __del__(self):\n        self.finish()", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "Ensures proper finalization and resource release for the Hadoop job when the `HadoopJobRunner` object is destroyed, typically upon garbage collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "sample", "line_number": 612, "body": "def sample(self, input_stream, n, output):\n        for i, line in enumerate(input_stream):\n            if n is not None and i >= n:\n                break\n            output.write(line)", "is_method": true, "class_name": "LocalJobRunner", "function_description": "Samples a specified number of lines from an input stream and writes them to an output. Useful for previewing or processing a subset of data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "group", "line_number": 618, "body": "def group(self, input_stream):\n        output = StringIO()\n        lines = []\n        for i, line in enumerate(input_stream):\n            parts = line.rstrip('\\n').split('\\t')\n            blob = md5(str(i).encode('ascii')).hexdigest()  # pseudo-random blob to make sure the input isn't sorted\n            lines.append((parts[:-1], blob, line))\n        for _, _, line in sorted(lines):\n            output.write(line)\n        output.seek(0)\n        return output", "is_method": true, "class_name": "LocalJobRunner", "function_description": "Groups lines from an input stream by their common leading tab-separated fields. Within each resulting group, lines are then deterministically ordered based on a hash of their original line number."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_job", "line_number": 630, "body": "def run_job(self, job):\n        map_input = StringIO()\n\n        for i in luigi.task.flatten(job.input_hadoop()):\n            self.sample(i.open('r'), self.samplelines, map_input)\n\n        map_input.seek(0)\n\n        if job.reducer == NotImplemented:\n            # Map only job; no combiner, no reducer\n            map_output = job.output().open('w')\n            job.run_mapper(map_input, map_output)\n            map_output.close()\n            return\n\n        # run job now...\n        map_output = StringIO()\n        job.run_mapper(map_input, map_output)\n        map_output.seek(0)\n\n        if job.combiner == NotImplemented:\n            reduce_input = self.group(map_output)\n        else:\n            combine_input = self.group(map_output)\n            combine_output = StringIO()\n            job.run_combiner(combine_input, combine_output)\n            combine_output.seek(0)\n            reduce_input = self.group(combine_output)\n\n        reduce_output = job.output().open('w')\n        job.run_reducer(reduce_input, reduce_output)\n        reduce_output.close()", "is_method": true, "class_name": "LocalJobRunner", "function_description": "This method executes a Luigi-defined job locally, orchestrating its mapper, optional combiner, and reducer stages. It provides an in-memory MapReduce-like execution environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_get_pool", "line_number": 679, "body": "def _get_pool(self):\n        \"\"\" Protected method \"\"\"\n        if self.pool:\n            return self.pool\n        if hadoop().pool:\n            return hadoop().pool", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "This method retrieves an execution pool for Hadoop jobs, prioritizing an instance-specific pool. It falls back to a global Hadoop default if no instance pool is configured."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "jobconfs", "line_number": 690, "body": "def jobconfs(self):\n        jcs = []\n        jcs.append('mapred.job.name=%s' % self)\n        if self.mr_priority != NotImplemented:\n            jcs.append('mapred.job.priority=%s' % self.mr_priority())\n        pool = self._get_pool()\n        if pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs.append('mapred.fairscheduler.pool=%s' % pool)\n            elif scheduler_type == 'capacity':\n                jcs.append('mapred.job.queue.name=%s' % pool)\n        return jcs", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Constructs a list of Hadoop job configuration parameters. It sets the job name, priority, and assigns the job to the appropriate resource pool or queue based on the configured scheduler type."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run", "line_number": 721, "body": "def run(self):\n        # The best solution is to store them as lazy `cached_property`, but it\n        # has extraneous dependency. And `property` is slow (need to be\n        # calculated every time when called), so we save them as attributes\n        # directly.\n        self.serialize = DataInterchange[self.data_interchange_format]['serialize']\n        self.internal_serialize = DataInterchange[self.data_interchange_format]['internal_serialize']\n        self.deserialize = DataInterchange[self.data_interchange_format]['deserialize']\n\n        self.init_local()\n        self.job_runner().run_job(self)", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "This method initializes data handling mechanisms for the `BaseHadoopJobTask` and then orchestrates the execution of the Hadoop job. It serves as the primary entry point for running the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "requires_local", "line_number": 733, "body": "def requires_local(self):\n        \"\"\"\n        Default impl - override this method if you need any local input to be accessible in init().\n        \"\"\"\n        return []", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "This method indicates if the task needs specific local files or inputs for its initialization. Subclasses override it to declare such requirements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "requires_hadoop", "line_number": 739, "body": "def requires_hadoop(self):\n        return self.requires()", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Determines if this Hadoop job task requires specific Hadoop-related dependencies or resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "input_local", "line_number": 742, "body": "def input_local(self):\n        return luigi.task.getpaths(self.requires_local())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Provides the file paths for local (non-HDFS) inputs required by this Hadoop job task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "input_hadoop", "line_number": 745, "body": "def input_hadoop(self):\n        return luigi.task.getpaths(self.requires_hadoop())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "It provides the concrete input paths required by the Hadoop job associated with this Luigi task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "deps", "line_number": 748, "body": "def deps(self):\n        # Overrides the default implementation\n        return luigi.task.flatten(self.requires_hadoop()) + luigi.task.flatten(self.requires_local())", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Specifies all direct upstream dependencies for this Hadoop job task, combining both Hadoop-specific and local task requirements. This enables Luigi's scheduler to build the task dependency graph."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "on_failure", "line_number": 752, "body": "def on_failure(self, exception):\n        if isinstance(exception, HadoopJobError):\n            return \"\"\"Hadoop job failed with message: {message}\n\n    stdout:\n    {stdout}\n\n\n    stderr:\n    {stderr}\n      \"\"\".format(message=exception.message, stdout=exception.out, stderr=exception.err)\n        else:\n            return super(BaseHadoopJobTask, self).on_failure(exception)", "is_method": true, "class_name": "BaseHadoopJobTask", "function_description": "Handles failures for Hadoop job tasks. It formats a detailed error message including stdout and stderr for `HadoopJobError` exceptions, otherwise delegating to the superclass."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "jobconfs", "line_number": 782, "body": "def jobconfs(self):\n        jcs = super(JobTask, self).jobconfs()\n        if self.reducer == NotImplemented:\n            jcs.append('mapred.reduce.tasks=0')\n        else:\n            jcs.append('mapred.reduce.tasks=%s' % self.n_reduce_tasks)\n        if self.jobconf_truncate >= 0:\n            jcs.append('stream.jobconf.truncate.limit=%i' % self.jobconf_truncate)\n        return jcs", "is_method": true, "class_name": "JobTask", "function_description": "Provides a customized list of job configurations for the task. It sets the reduce task count based on reducer presence and optionally adds a stream truncation limit."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_setup_remote", "line_number": 801, "body": "def _setup_remote(self):\n        self._setup_links()", "is_method": true, "class_name": "JobTask", "function_description": "Not sure\nChain of Thought:\nThe function `_setup_remote` is a method of the `JobTask` class.\nIts entire implementation is a single call to another private method, `self._setup_links()`.\nThis means `_setup_remote` itself does not perform any distinct operation or provide any direct service beyond simply invoking `_setup_links`.\nThe actual \"remote setup\" logic, if any, is entirely encapsulated within `_setup_links`.\nWithout knowing what `_setup_links` does, the *purpose* or *service* provided by `_setup_remote` is completely opaque. It's a very thin wrapper.\nTherefore, its purpose is unclear based solely on the provided code.\nNot sure"}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "job_runner", "line_number": 804, "body": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        \"\"\"\n        Get the MapReduce runner for this job.\n\n        If all outputs are HdfsTargets, the DefaultHadoopJobRunner will be used.\n        Otherwise, the LocalJobRunner which streams all data through the local machine\n        will be used (great for testing).\n        \"\"\"\n        outputs = luigi.task.flatten(self.output())\n        for output in outputs:\n            if not isinstance(output, luigi.contrib.hdfs.HdfsTarget):\n                warnings.warn(\"Job is using one or more non-HdfsTarget outputs\" +\n                              \" so it will be run in local mode\")\n                return LocalJobRunner()\n        else:\n            return DefaultHadoopJobRunner()", "is_method": true, "class_name": "JobTask", "function_description": "Determines and returns the appropriate MapReduce runner for the job. It selects between local or Hadoop execution based on whether all task outputs are HDFS targets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "reader", "line_number": 822, "body": "def reader(self, input_stream):\n        \"\"\"\n        Reader is a method which iterates over input lines and outputs records.\n\n        The default implementation yields one argument containing the line for each line in the input.\"\"\"\n        for line in input_stream:\n            yield line,", "is_method": true, "class_name": "JobTask", "function_description": "Provides a generic reader for a JobTask, iterating over an input stream. It yields each line as a single-element tuple, preparing raw data for processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "writer", "line_number": 830, "body": "def writer(self, outputs, stdout, stderr=sys.stderr):\n        \"\"\"\n        Writer format is a method which iterates over the output records\n        from the reducer and formats them for output.\n\n        The default implementation outputs tab separated items.\n        \"\"\"\n        for output in outputs:\n            try:\n                output = flatten(output)\n                if self.data_interchange_format == \"json\":\n                    # Only dump one json string, and skip another one, maybe key or value.\n                    output = filter(lambda x: x, output)\n                else:\n                    # JSON is already serialized, so we put `self.serialize` in a else statement.\n                    output = map(self.serialize, output)\n                print(\"\\t\".join(output), file=stdout)\n            except BaseException:\n                print(output, file=stderr)\n                raise", "is_method": true, "class_name": "JobTask", "function_description": "This method of the JobTask class formats output records from a reducer. It converts them to tab-separated or JSON strings and writes them to the standard output stream."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "mapper", "line_number": 851, "body": "def mapper(self, item):\n        \"\"\"\n        Re-define to process an input item (usually a line of input data).\n\n        Defaults to identity mapper that sends all lines to the same reducer.\n        \"\"\"\n        yield None, item", "is_method": true, "class_name": "JobTask", "function_description": "This method provides a default identity mapping for an input item within a job task. It is designed to be overridden by subclasses to implement custom data processing and key generation for reducer stages."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "incr_counter", "line_number": 861, "body": "def incr_counter(self, *args, **kwargs):\n        \"\"\"\n        Increments a Hadoop counter.\n\n        Since counters can be a bit slow to update, this batches the updates.\n        \"\"\"\n        threshold = kwargs.get(\"threshold\", self.batch_counter_default)\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            key = (group_name,)\n        else:\n            group, name, count = args\n            key = (group, name)\n\n        ct = self._counter_dict.get(key, 0)\n        ct += count\n        if ct >= threshold:\n            new_arg = list(key) + [ct]\n            self._incr_counter(*new_arg)\n            ct = 0\n        self._counter_dict[key] = ct", "is_method": true, "class_name": "JobTask", "function_description": "This method efficiently manages the incrementing of Hadoop counters by batching updates. It prevents slow updates from individual increments, crucial for distributed job monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_flush_batch_incr_counter", "line_number": 884, "body": "def _flush_batch_incr_counter(self):\n        \"\"\"\n        Increments any unflushed counter values.\n        \"\"\"\n        for key, count in self._counter_dict.items():\n            if count == 0:\n                continue\n            args = list(key) + [count]\n            self._incr_counter(*args)\n            self._counter_dict[key] = 0", "is_method": true, "class_name": "JobTask", "function_description": "The method flushes accumulated counter increments by applying them to an underlying counter system. It ensures all pending counts are processed and resets the internal batch storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_incr_counter", "line_number": 895, "body": "def _incr_counter(self, *args):\n        \"\"\"\n        Increments a Hadoop counter.\n\n        Note that this seems to be a bit slow, ~1 ms\n\n        Don't overuse this function by updating very frequently.\n        \"\"\"\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            print('reporter:counter:%s,%s' % (group_name, count), file=sys.stderr)\n        else:\n            group, name, count = args\n            print('reporter:counter:%s,%s,%s' % (group, name, count), file=sys.stderr)", "is_method": true, "class_name": "JobTask", "function_description": "This method increments a Hadoop counter, allowing a job task to report custom metrics or progress. It provides a mechanism for tracking specific events during distributed job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_modules", "line_number": 911, "body": "def extra_modules(self):\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "Provides a list of extra Python modules required by the job task. The default implementation specifies no additional modules."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "extra_files", "line_number": 914, "body": "def extra_files(self):\n        \"\"\"\n        Can be overriden in subclass.\n\n        Each element is either a string, or a pair of two strings (src, dst).\n\n        * `src` can be a directory (in which case everything will be copied recursively).\n        * `dst` can include subdirectories (foo/bar/baz.txt etc)\n\n        Uses Hadoop's -files option so that the same file is reused across tasks.\n        \"\"\"\n        return []", "is_method": true, "class_name": "JobTask", "function_description": "Provides an extensible way for subclasses to declare external files or directories required by job tasks. These resources are distributed to compute nodes for use during task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "add_link", "line_number": 938, "body": "def add_link(self, src, dst):\n        if not hasattr(self, '_links'):\n            self._links = []\n        self._links.append((src, dst))", "is_method": true, "class_name": "JobTask", "function_description": "Establishes a directed link between two entities (`src`, `dst`) within the JobTask. This defines dependencies or connections for workflow or process orchestration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_setup_links", "line_number": 943, "body": "def _setup_links(self):\n        if hasattr(self, '_links'):\n            missing = []\n            for src, dst in self._links:\n                d = os.path.dirname(dst)\n                if d:\n                    try:\n                        os.makedirs(d)\n                    except OSError:\n                        pass\n                if not os.path.exists(src):\n                    missing.append(src)\n                    continue\n                if not os.path.exists(dst):\n                    # If the combiner runs, the file might already exist,\n                    # so no reason to create the link again\n                    os.link(src, dst)\n            if missing:\n                raise HadoopJobError(\n                    'Missing files for distributed cache: ' +\n                    ', '.join(missing))", "is_method": true, "class_name": "JobTask", "function_description": "Prepares a distributed computing job's environment by creating hard links for required files. It ensures all specified source files exist and are linked to their destinations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "dump", "line_number": 965, "body": "def dump(self, directory=''):\n        \"\"\"\n        Dump instance to file.\n        \"\"\"\n        with self.no_unpicklable_properties():\n            file_name = os.path.join(directory, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'(c__main__', \"(c\" + module_name)\n                open(file_name, \"wb\").write(d)\n\n            else:\n                pickle.dump(self, open(file_name, \"wb\"))", "is_method": true, "class_name": "JobTask", "function_description": "Dumps the JobTask instance's state to a pickle file in the specified directory. This enables persistence and later reloading of the task's configuration and progress."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_map_input", "line_number": 980, "body": "def _map_input(self, input_stream):\n        \"\"\"\n        Iterate over input and call the mapper for each item.\n        If the job has a parser defined, the return values from the parser will\n        be passed as arguments to the mapper.\n\n        If the input is coded output from a previous run,\n        the arguments will be splitted in key and value.\n        \"\"\"\n        for record in self.reader(input_stream):\n            for output in self.mapper(*record):\n                yield output\n        if self.final_mapper != NotImplemented:\n            for output in self.final_mapper():\n                yield output\n        self._flush_batch_incr_counter()", "is_method": true, "class_name": "JobTask", "function_description": "This method orchestrates the \"map\" phase of a job, processing an input stream by applying a mapper function to each item and yielding the transformed results. It provides the core data transformation for a `JobTask` instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "_reduce_input", "line_number": 997, "body": "def _reduce_input(self, inputs, reducer, final=NotImplemented):\n        \"\"\"\n        Iterate over input, collect values with the same key, and call the reducer for each unique key.\n        \"\"\"\n        for key, values in groupby(inputs, key=lambda x: self.internal_serialize(x[0])):\n            for output in reducer(self.deserialize(key), (v[1] for v in values)):\n                yield output\n        if final != NotImplemented:\n            for output in final():\n                yield output\n        self._flush_batch_incr_counter()", "is_method": true, "class_name": "JobTask", "function_description": "Groups input data by key and applies a `reducer` function to process values for each unique key. This method performs the 'reduce' step in a data processing job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_mapper", "line_number": 1009, "body": "def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        Run the mapper on the hadoop node.\n        \"\"\"\n        self.init_hadoop()\n        self.init_mapper()\n        outputs = self._map_input((line[:-1] for line in stdin))\n        if self.reducer == NotImplemented:\n            self.writer(outputs, stdout)\n        else:\n            self.internal_writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "As part of a `JobTask`, this method executes the mapper component of a distributed processing job, transforming input data and directing the output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_reducer", "line_number": 1021, "body": "def run_reducer(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        Run the reducer on the hadoop node.\n        \"\"\"\n        self.init_hadoop()\n        self.init_reducer()\n        outputs = self._reduce_input(self.internal_reader((line[:-1] for line in stdin)), self.reducer, self.final_reducer)\n        self.writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Executes the reducer logic for a Hadoop MapReduce job. It processes streaming input, applies the defined reducer functions, and writes the final output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "run_combiner", "line_number": 1030, "body": "def run_combiner(self, stdin=sys.stdin, stdout=sys.stdout):\n        self.init_hadoop()\n        self.init_combiner()\n        outputs = self._reduce_input(self.internal_reader((line[:-1] for line in stdin)), self.combiner, self.final_combiner)\n        self.internal_writer(outputs, stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Orchestrates the local aggregation step of a job task. It processes input data using a defined combiner and writes the aggregated results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "internal_reader", "line_number": 1036, "body": "def internal_reader(self, input_stream):\n        \"\"\"\n        Reader which uses python eval on each part of a tab separated string.\n        Yields a tuple of python objects.\n        \"\"\"\n        for input_line in input_stream:\n            yield list(map(self.deserialize, input_line.split(\"\\t\")))", "is_method": true, "class_name": "JobTask", "function_description": "Parses tab-separated lines from an input stream, deserializing each string part into a Python object. It provides lists of these objects, facilitating structured data ingestion for JobTask processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "internal_writer", "line_number": 1044, "body": "def internal_writer(self, outputs, stdout):\n        \"\"\"\n        Writer which outputs the python repr for each item.\n        \"\"\"\n        for output in outputs:\n            print(\"\\t\".join(map(self.internal_serialize, output)), file=stdout)", "is_method": true, "class_name": "JobTask", "function_description": "Formats and writes the serialized representation of job outputs to a specified stream. Useful for standardizing result display."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop.py", "function": "get", "line_number": 400, "body": "def get(x, default):\n            return x is not None and x or default", "is_method": true, "class_name": "HadoopJobRunner", "function_description": "A utility method that returns `x` if `x` is not `None` and truthy. Otherwise, it returns the provided `default` value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "_replacer", "line_number": 78, "body": "def _replacer(self, match_object):\n        # this method is used as the replace function in the re.sub below\n        return self._replace_dict[match_object.group()]", "is_method": true, "class_name": "MultiReplacer", "function_description": "A callback method for `re.sub` that provides the replacement string for a regular expression match by looking it up in an internal dictionary."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "__call__", "line_number": 82, "body": "def __call__(self, search_string):\n        # using function replacing for a per-result replace\n        return self._search_re.sub(self._replacer, search_string)", "is_method": true, "class_name": "MultiReplacer", "function_description": "As the callable interface for `MultiReplacer`, this method applies multiple, pre-configured find-and-replace operations to an input string. It efficiently transforms text based on defined patterns."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "touch", "line_number": 137, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        Important: If the marker table doesn't exist, the connection transaction will be aborted\n        and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            # TODO: test this\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        if self.use_db_timestamps:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n                   VALUES (%s, %s)\n                \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table))\n        else:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table, inserted)\n                         VALUES (%s, %s, %s);\n                    \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table,\n                 datetime.datetime.now()))", "is_method": true, "class_name": "PostgresTarget", "function_description": "This PostgresTarget method marks a database update as complete. It inserts an entry into a marker table to log successful operations for a specific target table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "exists", "line_number": 166, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\"\"\"SELECT 1 FROM {marker_table}\n                WHERE update_id = %s\n                LIMIT 1\"\"\".format(marker_table=self.marker_table),\n                           (self.update_id,)\n                           )\n            row = cursor.fetchone()\n        except psycopg2.ProgrammingError as e:\n            if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE:\n                row = None\n            else:\n                raise\n        return row is not None", "is_method": true, "class_name": "PostgresTarget", "function_description": "Determines if a specific update ID exists in the PostgreSQL marker table. This indicates if a particular update or task has already been recorded."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "connect", "line_number": 185, "body": "def connect(self):\n        \"\"\"\n        Get a psycopg2 connection object to the database where the table is.\n        \"\"\"\n        connection = psycopg2.connect(\n            host=self.host,\n            port=self.port,\n            database=self.database,\n            user=self.user,\n            password=self.password)\n        connection.set_client_encoding('utf-8')\n        return connection", "is_method": true, "class_name": "PostgresTarget", "function_description": "Provides a psycopg2 database connection object to the configured PostgreSQL database, enabling further interaction and data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "create_marker_table", "line_number": 198, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        connection.autocommit = True\n        cursor = connection.cursor()\n        if self.use_db_timestamps:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP DEFAULT NOW())\n                  \"\"\".format(marker_table=self.marker_table)\n        else:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP);\n                  \"\"\".format(marker_table=self.marker_table)\n\n        try:\n            cursor.execute(sql)\n        except psycopg2.ProgrammingError as e:\n            if e.pgcode == psycopg2.errorcodes.DUPLICATE_TABLE:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "PostgresTarget", "function_description": "This method creates a marker table in the PostgreSQL database to track updates or states for other target tables, ensuring idempotency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "rows", "line_number": 245, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')", "is_method": true, "class_name": "CopyToTable", "function_description": "Yields parsed rows from the input source, where each row is a list of tab-separated values. This prepares data for table insertion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "map_column", "line_number": 253, "body": "def map_column(self, value):\n        \"\"\"\n        Applied to each column of every row returned by `rows`.\n\n        Default behaviour is to escape special characters and identify any self.null_values.\n        \"\"\"\n        if value in self.null_values:\n            return r'\\\\N'\n        else:\n            return default_escape(str(value))", "is_method": true, "class_name": "CopyToTable", "function_description": "Prepares a single column's value for table insertion by converting null values to a standard representation and escaping special characters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "output", "line_number": 266, "body": "def output(self):\n        \"\"\"\n        Returns a PostgresTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )", "is_method": true, "class_name": "CopyToTable", "function_description": "This method of `CopyToTable` generates a `PostgresTarget` object. It defines the specific PostgreSQL table and connection details where data will be copied."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "copy", "line_number": 282, "body": "def copy(self, cursor, file):\n        if isinstance(self.columns[0], str):\n            column_names = self.columns\n        elif len(self.columns[0]) == 2:\n            column_names = [c[0] for c in self.columns]\n        else:\n            raise Exception('columns must consist of column strings or (column string, type string) tuples (was %r ...)' % (self.columns[0],))\n        cursor.copy_from(file, self.table, null=r'\\\\N', sep=self.column_separator, columns=column_names)", "is_method": true, "class_name": "CopyToTable", "function_description": "The `CopyToTable` class uses a database cursor to efficiently copy data from a file into its designated table. It prepares columns and handles separators during transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "run", "line_number": 291, "body": "def run(self):\n        \"\"\"\n        Inserts data generated by rows() into target table.\n\n        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n\n        Normally you don't want to override this.\n        \"\"\"\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n        # transform all data generated by rows() using map_column and write data\n        # to a temporary file for import using postgres COPY\n        tmp_dir = luigi.configuration.get_config().get('postgres', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(dir=tmp_dir)\n        n = 0\n        for row in self.rows():\n            n += 1\n            if n % 100000 == 0:\n                logger.info(\"Wrote %d lines\", n)\n            rowstr = self.column_separator.join(self.map_column(val) for val in row)\n            rowstr += \"\\n\"\n            tmp_file.write(rowstr.encode('utf-8'))\n\n        logger.info(\"Done writing, importing at %s\", datetime.datetime.now())\n        tmp_file.seek(0)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, tmp_file)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except psycopg2.ProgrammingError as e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n\n        # commit and clean up\n        connection.commit()\n        connection.close()\n        tmp_file.close()", "is_method": true, "class_name": "CopyToTable", "function_description": "Inserts data generated by the task's `rows()` method into a target PostgreSQL table. It handles table creation if needed and utilizes efficient bulk data transfer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "run", "line_number": 364, "body": "def run(self):\n        connection = self.output().connect()\n        connection.autocommit = self.autocommit\n        cursor = connection.cursor()\n        sql = self.query\n\n        logger.info('Executing query from task: {name}'.format(name=self.__class__))\n        cursor.execute(sql)\n\n        # Update marker table\n        self.output().touch(connection)\n\n        # commit and close connection\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "PostgresQuery", "function_description": "Executes a pre-defined SQL query against a PostgreSQL database. This method manages the database connection, commits the transaction, and marks the operation's completion within a task workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/postgres.py", "function": "output", "line_number": 380, "body": "def output(self):\n        \"\"\"\n        Returns a PostgresTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )", "is_method": true, "class_name": "PostgresQuery", "function_description": "Returns a `PostgresTarget` object that encapsulates PostgreSQL connection details and the target table. This object represents the database resource associated with the executed query."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "touch", "line_number": 71, "body": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        connection.cursor().execute(\n            \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n               VALUES (%s, %s)\n               ON DUPLICATE KEY UPDATE\n               update_id = VALUES(update_id)\n            \"\"\".format(marker_table=self.marker_table),\n            (self.update_id, self.table)\n        )\n        # make sure update is properly marked\n        assert self.exists(connection)", "is_method": true, "class_name": "MySqlTarget", "function_description": "Marks a data update to the MySQL target as complete by recording its ID in a dedicated marker table. This tracks the successful execution of operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "exists", "line_number": 96, "body": "def exists(self, connection=None):\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\"\"\"SELECT 1 FROM {marker_table}\n                WHERE update_id = %s\n                LIMIT 1\"\"\".format(marker_table=self.marker_table),\n                           (self.update_id,)\n                           )\n            row = cursor.fetchone()\n        except mysql.connector.Error as e:\n            if e.errno == errorcode.ER_NO_SUCH_TABLE:\n                row = None\n            else:\n                raise\n        return row is not None", "is_method": true, "class_name": "MySqlTarget", "function_description": "Determines if a specific update ID is already recorded in the MySQL target's marker table. This indicates whether a particular operation or state has been processed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "connect", "line_number": 115, "body": "def connect(self, autocommit=False):\n        connection = mysql.connector.connect(user=self.user,\n                                             password=self.password,\n                                             host=self.host,\n                                             port=self.port,\n                                             database=self.database,\n                                             autocommit=autocommit,\n                                             **self.cnx_kwargs)\n        return connection", "is_method": true, "class_name": "MySqlTarget", "function_description": "Establishes and returns an active connection to a MySQL database, enabling subsequent data operations. It uses the class's configured database credentials and properties."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "create_marker_table", "line_number": 125, "body": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect(autocommit=True)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT(20)    NOT NULL AUTO_INCREMENT,\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      TIMESTAMP DEFAULT NOW(),\n                        PRIMARY KEY (update_id),\n                        KEY id (id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except mysql.connector.Error as e:\n            if e.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n                pass\n            else:\n                raise\n        connection.close()", "is_method": true, "class_name": "MySqlTarget", "function_description": "It ensures a dedicated marker table exists within the MySQL target database. This table is used to record update metadata for tracking data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "rows", "line_number": 166, "body": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides an iterable stream of structured rows by reading and parsing tab-separated data from its input. This prepares data for insertion into a table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "output", "line_number": 176, "body": "def output(self):\n        \"\"\"\n        Returns a MySqlTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return MySqlTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n\n        )", "is_method": true, "class_name": "CopyToTable", "function_description": "This method returns a MySqlTarget object. It defines the specific MySQL table and connection details where data will be inserted or managed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "copy", "line_number": 192, "body": "def copy(self, cursor, file=None):\n        values = '({})'.format(','.join(['%s' for i in range(len(self.columns))]))\n        columns = '({})'.format(','.join([c[0] for c in self.columns]))\n        query = 'INSERT INTO {} {} VALUES {}'.format(self.table, columns, values)\n        rows = []\n\n        for idx, row in enumerate(self.rows()):\n            rows.append(row)\n\n            if (idx + 1) % self.bulk_size == 0:\n                cursor.executemany(query, rows)\n                rows = []\n\n        cursor.executemany(query, rows)", "is_method": true, "class_name": "CopyToTable", "function_description": "This method efficiently inserts data from the `CopyToTable` object into a specified database table. It uses bulk insertion with prepared statements for optimal performance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "run", "line_number": 207, "body": "def run(self):\n        \"\"\"\n        Inserts data generated by rows() into target table.\n\n        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n\n        Normally you don't want to override this.\n        \"\"\"\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n\n        # attempt to copy the data into mysql\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                print(\"caling init copy...\")\n                self.init_copy(connection)\n                self.copy(cursor)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except Error as err:\n                if err.errno == errorcode.ER_NO_SUCH_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    # logger.info(\"Creating table %s\", self.table)\n                    connection.reconnect()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n        connection.commit()\n        connection.close()", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides the primary functionality for the `CopyToTable` class, inserting generated data into a target database table. It ensures the table exists by creating it if necessary."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mysqldb.py", "function": "bulk_size", "line_number": 249, "body": "def bulk_size(self):\n        return 10000", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides the default batch size for bulk operations, optimizing data transfers when copying data to a table."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "_random_id", "line_number": 84, "body": "def _random_id():\n    return 'batch-job-' + ''.join(random.sample(string.ascii_lowercase, 8))", "is_method": false, "function_description": "Generates a unique, random ID string prefixed with 'batch-job-'. This is useful for creating distinct identifiers for temporary processes or tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_active_queue", "line_number": 96, "body": "def get_active_queue(self):\n        \"\"\"Get name of first active job queue\"\"\"\n\n        # Get dict of active queues keyed by name\n        queues = {q['jobQueueName']: q for q in self._client.describe_job_queues()['jobQueues']\n                  if q['state'] == 'ENABLED' and q['status'] == 'VALID'}\n        if not queues:\n            raise Exception('No job queues with state=ENABLED and status=VALID')\n\n        # Pick the first queue as default\n        return list(queues.keys())[0]", "is_method": true, "class_name": "BatchClient", "function_description": "Retrieves the name of the first available and active job queue from the batch processing system. It identifies a suitable queue for submitting new batch jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_job_id_from_name", "line_number": 108, "body": "def get_job_id_from_name(self, job_name):\n        \"\"\"Retrieve the first job ID matching the given name\"\"\"\n        jobs = self._client.list_jobs(jobQueue=self._queue, jobStatus='RUNNING')['jobSummaryList']\n        matching_jobs = [job for job in jobs if job['jobName'] == job_name]\n        if matching_jobs:\n            return matching_jobs[0]['jobId']", "is_method": true, "class_name": "BatchClient", "function_description": "Retrieves the ID of the first running batch job that matches a given name, useful for identifying or managing specific active jobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_job_status", "line_number": 115, "body": "def get_job_status(self, job_id):\n        \"\"\"Retrieve task statuses from ECS API\n\n        :param job_id (str): AWS Batch job uuid\n\n        Returns one of {SUBMITTED|PENDING|RUNNABLE|STARTING|RUNNING|SUCCEEDED|FAILED}\n        \"\"\"\n        response = self._client.describe_jobs(jobs=[job_id])\n\n        # Error checking\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Job status request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n\n        return response['jobs'][0]['status']", "is_method": true, "class_name": "BatchClient", "function_description": "Provides the current execution status of an AWS Batch job, enabling real-time monitoring and automation of dependent workflows."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "get_logs", "line_number": 132, "body": "def get_logs(self, log_stream_name, get_last=50):\n        \"\"\"Retrieve log stream from CloudWatch\"\"\"\n        response = self._log_client.get_log_events(\n            logGroupName='/aws/batch/job',\n            logStreamName=log_stream_name,\n            startFromHead=False)\n        events = response['events']\n        return '\\n'.join(e['message'] for e in events[-get_last:])", "is_method": true, "class_name": "BatchClient", "function_description": "Retrieves the most recent log messages for a given AWS Batch job log stream from CloudWatch. It provides a quick way to inspect job output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "submit_job", "line_number": 141, "body": "def submit_job(self, job_definition, parameters, job_name=None, queue=None):\n        \"\"\"Wrap submit_job with useful defaults\"\"\"\n        if job_name is None:\n            job_name = _random_id()\n        response = self._client.submit_job(\n            jobName=job_name,\n            jobQueue=queue or self.get_active_queue(),\n            jobDefinition=job_definition,\n            parameters=parameters\n        )\n        return response['jobId']", "is_method": true, "class_name": "BatchClient", "function_description": "Provides a convenient way to submit jobs to a batch processing service. It automatically assigns a job name and selects the execution queue."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "wait_on_job", "line_number": 153, "body": "def wait_on_job(self, job_id):\n        \"\"\"Poll task status until STOPPED\"\"\"\n\n        while True:\n            status = self.get_job_status(job_id)\n            if status == 'SUCCEEDED':\n                logger.info('Batch job {} SUCCEEDED'.format(job_id))\n                return True\n            elif status == 'FAILED':\n                # Raise and notify if job failed\n                jobs = self._client.describe_jobs(jobs=[job_id])['jobs']\n                job_str = json.dumps(jobs, indent=4)\n                logger.debug('Job details:\\n' + job_str)\n\n                log_stream_name = jobs[0]['attempts'][0]['container']['logStreamName']\n                logs = self.get_logs(log_stream_name)\n                raise BatchJobException('Job {} failed: {}'.format(\n                    job_id, logs))\n\n            time.sleep(self.poll_time)\n            logger.debug('Batch job status for job {0}: {1}'.format(\n                job_id, status))", "is_method": true, "class_name": "BatchClient", "function_description": "This method of the BatchClient class waits for a specified batch job to complete. It polls the job's status, returning success on completion or raising an exception with logs if the job fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "register_job_definition", "line_number": 176, "body": "def register_job_definition(self, json_fpath):\n        \"\"\"Register a job definition with AWS Batch, using a JSON\"\"\"\n        with open(json_fpath) as f:\n            job_def = json.load(f)\n        response = self._client.register_job_definition(**job_def)\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Register job definition request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n        return response", "is_method": true, "class_name": "BatchClient", "function_description": "Registers a new AWS Batch job definition by loading its configuration from a JSON file and submitting it to the AWS service. This enables the setup of new batch processing tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/batch.py", "function": "run", "line_number": 207, "body": "def run(self):\n        bc = BatchClient(self.poll_time)\n        job_id = bc.submit_job(\n            self.job_definition,\n            self.parameters,\n            job_name=self.job_name,\n            queue=self.job_queue)\n        bc.wait_on_job(job_id)", "is_method": true, "class_name": "BatchTask", "function_description": "Executes the defined batch task. It submits the job to a batch service and waits for its completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "_get_task_statuses", "line_number": 68, "body": "def _get_task_statuses(task_ids, cluster):\n    \"\"\"\n    Retrieve task statuses from ECS API\n\n    Returns list of {RUNNING|PENDING|STOPPED} for each id in task_ids\n    \"\"\"\n    response = client.describe_tasks(tasks=task_ids, cluster=cluster)\n\n    # Error checking\n    if response['failures'] != []:\n        raise Exception('There were some failures:\\n{0}'.format(\n            response['failures']))\n    status_code = response['ResponseMetadata']['HTTPStatusCode']\n    if status_code != 200:\n        msg = 'Task status request received status code {0}:\\n{1}'\n        raise Exception(msg.format(status_code, response))\n\n    return [t['lastStatus'] for t in response['tasks']]", "is_method": false, "function_description": "This function retrieves the current operational status (e.g., RUNNING, PENDING, STOPPED) for a list of specified tasks. It queries an external API to provide real-time task status updates for monitoring or management."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "_track_tasks", "line_number": 88, "body": "def _track_tasks(task_ids, cluster):\n    \"\"\"Poll task status until STOPPED\"\"\"\n    while True:\n        statuses = _get_task_statuses(task_ids, cluster)\n        if all([status == 'STOPPED' for status in statuses]):\n            logger.info('ECS tasks {0} STOPPED'.format(','.join(task_ids)))\n            break\n        time.sleep(POLL_TIME)\n        logger.debug('ECS task status for tasks {0}: {1}'.format(task_ids, statuses))", "is_method": false, "function_description": "Monitors the status of a list of tasks on a given cluster. It continuously polls their status until all specified tasks have stopped."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "ecs_task_ids", "line_number": 141, "body": "def ecs_task_ids(self):\n        \"\"\"Expose the ECS task ID\"\"\"\n        if hasattr(self, '_task_ids'):\n            return self._task_ids", "is_method": true, "class_name": "ECSTask", "function_description": "Provides the ECS task IDs associated with this ECSTask instance, making them accessible for external use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/ecs.py", "function": "run", "line_number": 166, "body": "def run(self):\n        if (not self.task_def and not self.task_def_arn) or \\\n                (self.task_def and self.task_def_arn):\n            raise ValueError(('Either (but not both) a task_def (dict) or'\n                              'task_def_arn (string) must be assigned'))\n        if not self.task_def_arn:\n            # Register the task and get assigned taskDefinition ID (arn)\n            response = client.register_task_definition(**self.task_def)\n            self.task_def_arn = response['taskDefinition']['taskDefinitionArn']\n\n        # Submit the task to AWS ECS and get assigned task ID\n        # (list containing 1 string)\n        if self.command:\n            overrides = {'containerOverrides': self.command}\n        else:\n            overrides = {}\n        response = client.run_task(taskDefinition=self.task_def_arn,\n                                   overrides=overrides,\n                                   cluster=self.cluster)\n\n        if response['failures']:\n            raise Exception(\", \".join([\"fail to run task {0} reason: {1}\".format(failure['arn'], failure['reason'])\n                                       for failure in response['failures']]))\n\n        self._task_ids = [task['taskArn'] for task in response['tasks']]\n\n        # Wait on task completion\n        _track_tasks(self._task_ids, self.cluster)", "is_method": true, "class_name": "ECSTask", "function_description": "This method registers an Amazon ECS task definition if necessary, then runs the task on the specified cluster, and waits for its completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "metadata_columns", "line_number": 87, "body": "def metadata_columns(self):\n        \"\"\"Returns the default metadata columns.\n\n        Those columns are columns that we want each tables to have by default.\n        \"\"\"\n        return []", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Defines the standard, default metadata columns that tables are expected to have. This method specifies common metadata fields for data tables."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "enable_metadata_columns", "line_number": 99, "body": "def enable_metadata_columns(self):\n        return False", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Reports that metadata columns are not currently supported or enabled for this object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_add_metadata_columns", "line_number": 102, "body": "def _add_metadata_columns(self, connection):\n        cursor = connection.cursor()\n\n        for column in self.metadata_columns:\n            if len(column) == 0:\n                raise ValueError(\"_add_metadata_columns is unable to infer column information from column {column} for {table}\".format(column=column,\n                                                                                                                                       table=self.table))\n\n            column_name = column[0]\n            if not self._column_exists(cursor, column_name):\n                logger.info('Adding missing metadata column {column} to {table}'.format(column=column, table=self.table))\n                self._add_column_to_table(cursor, column)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Ensures the database table includes all specified metadata columns, adding any that are missing. This provides a mechanism for automatic schema updates or initialization."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_column_exists", "line_number": 115, "body": "def _column_exists(self, cursor, column_name):\n        if '.' in self.table:\n            schema, table = self.table.split('.')\n            query = \"SELECT 1 AS column_exists \" \\\n                    \"FROM information_schema.columns \" \\\n                    \"WHERE table_schema = LOWER('{0}') AND table_name = LOWER('{1}') AND column_name = LOWER('{2}') LIMIT 1;\".format(schema, table, column_name)\n        else:\n            query = \"SELECT 1 AS column_exists \" \\\n                    \"FROM information_schema.columns \" \\\n                    \"WHERE table_name = LOWER('{0}') AND column_name = LOWER('{1}') LIMIT 1;\".format(self.table, column_name)\n\n        cursor.execute(query)\n        result = cursor.fetchone()\n        return bool(result)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "This method verifies if a specified column exists within the database table associated with the instance. It's used to check for column presence before performing database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "_add_column_to_table", "line_number": 130, "body": "def _add_column_to_table(self, cursor, column):\n        if len(column) == 1:\n            raise ValueError(\"_add_column_to_table() column type not specified for {column}\".format(column=column[0]))\n        elif len(column) == 2:\n            query = \"ALTER TABLE {table} ADD COLUMN {column};\".format(table=self.table, column=' '.join(column))\n        elif len(column) == 3:\n            query = \"ALTER TABLE {table} ADD COLUMN {column} ENCODE {encoding};\".format(table=self.table, column=' '.join(column[0:2]), encoding=column[2])\n        else:\n            raise ValueError(\"_add_column_to_table() found no matching behavior for {column}\".format(column=column))\n\n        cursor.execute(query)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Adds a new column with specified name, type, and optional encoding to the database table managed by this mixin. It constructs and executes the appropriate SQL ALTER TABLE command."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "post_copy_metacolumns", "line_number": 142, "body": "def post_copy_metacolumns(self, cursor):\n        logger.info('Executing post copy metadata queries')\n        for query in self.metadata_queries:\n            cursor.execute(query)", "is_method": true, "class_name": "_MetadataColumnsMixin", "function_description": "Executes a set of predefined SQL queries to finalize or update metadata, typically after a data copying operation, ensuring data integrity or consistency."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "create_table", "line_number": 206, "body": "def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally) specified in columns.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(name=name, type=type) for name, type in self.columns\n            )\n            query = \"CREATE TABLE {table} ({coldefs})\".format(table=self.table, coldefs=coldefs)\n            connection.cursor().execute(query)", "is_method": true, "class_name": "CopyToTable", "function_description": "A method of `CopyToTable` that creates the target database table. It dynamically generates and executes a `CREATE TABLE` SQL query using specified column names and types."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "update_id", "line_number": 227, "body": "def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier for this insert on this table.\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "CopyToTable", "function_description": "Retrieves the unique identifier associated with a specific data insertion or update operation on the table. This ID serves to uniquely identify the record."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "init_copy", "line_number": 237, "body": "def init_copy(self, connection):\n        \"\"\"\n        Override to perform custom queries.\n\n        Any code here will be formed in the same transaction as the main copy, just prior to copying data.\n        Example use cases include truncating the table or removing all data older than X in the database\n        to keep a rolling window of data available in the table.\n        \"\"\"\n\n        # TODO: remove this after sufficient time so most people using the\n        # clear_table attribtue will have noticed it doesn't work anymore\n        if hasattr(self, \"clear_table\"):\n            raise Exception(\"The clear_table attribute has been removed. Override init_copy instead!\")\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection.cursor())", "is_method": true, "class_name": "CopyToTable", "function_description": "This method enables custom database operations to prepare the target table, such as truncating or cleaning data, just before new data is copied into it."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "post_copy", "line_number": 254, "body": "def post_copy(self, connection):\n        \"\"\"\n        Override to perform custom queries.\n\n        Any code here will be formed in the same transaction as the main copy, just after copying data.\n        Example use cases include cleansing data in temp table prior to insertion into real table.\n        \"\"\"\n        pass", "is_method": true, "class_name": "CopyToTable", "function_description": "Provides a customizable hook for executing database operations, such as data cleansing, right after data is copied within the same transaction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "autocommit", "line_number": 337, "body": "def autocommit(self):\n        return False", "is_method": true, "class_name": "Query", "function_description": "This method within the Query class signals that automatic transaction commitment is disabled for related database operations, requiring explicit commit actions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/rdbms.py", "function": "update_id", "line_number": 341, "body": "def update_id(self):\n        \"\"\"\n        Override to create a custom marker table 'update_id' signature for Query subclass task instances\n        \"\"\"\n        return self.task_id", "is_method": true, "class_name": "Query", "function_description": "Provides the unique identifier (`task_id`) for a Query subclass task instance, intended for use as a custom marker table 'update_id' signature."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_started", "line_number": 34, "body": "def handle_task_started(self, task):\n        title = \"Luigi: A task has been started!\"\n        text = \"A task has been started in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.started', tags=tags)\n\n        event_tags = tags + [\"task_state:STARTED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='info', priority='low')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Reports the start of a task to the monitoring system. It sends a metric increment and an informational event with task details for tracking and alerts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_failed", "line_number": 44, "body": "def handle_task_failed(self, task):\n        title = \"Luigi: A task has failed!\"\n        text = \"A task has failed in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.failed', tags=tags)\n\n        event_tags = tags + [\"task_state:FAILED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='error', priority='normal')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Reports a failed Luigi task to Datadog. It increments a 'task.failed' metric and dispatches an error event for monitoring and alerts.\nReports a failed Luigi task to Datadog. It increments a 'task.failed' metric and dispatches an error event for monitoring and alerts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_disabled", "line_number": 54, "body": "def handle_task_disabled(self, task, config):\n        title = \"Luigi: A task has been disabled!\"\n        lines = ['A task has been disabled in the pipeline named: {name}.']\n        lines.append('The task has failed {failures} times in the last {window}')\n        lines.append('seconds, so it is being disabled for {persist} seconds.')\n\n        preformated_text = ' '.join(lines)\n\n        text = preformated_text.format(name=task.family,\n                                       persist=config.disable_persist,\n                                       failures=config.retry_count,\n                                       window=config.disable_window)\n\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        self._send_increment('task.disabled', tags=tags)\n\n        event_tags = tags + [\"task_state:DISABLED\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='error', priority='normal')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Reports a disabled Luigi task to Datadog. It sends an increment metric and a detailed error event for monitoring and alerting on task failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "handle_task_done", "line_number": 74, "body": "def handle_task_done(self, task):\n        # The task is already done -- Let's not re-create an event\n        if task.time_running is None:\n            return\n\n        title = \"Luigi: A task has been completed!\"\n        text = \"A task has completed in the pipeline named: {name}\".format(name=task.family)\n        tags = [\"task_name:{name}\".format(name=task.family)] + self._format_task_params_to_tags(task)\n\n        time_elapse = task.updated - task.time_running\n\n        self._send_increment('task.done', tags=tags)\n        self._send_gauge('task.execution_time', time_elapse, tags=tags)\n\n        event_tags = tags + [\"task_state:DONE\"]\n        self._send_event(title=title, text=text, tags=event_tags, alert_type='info', priority='low')", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "For completed tasks, this method sends metrics like execution time and a completion event to Datadog, enabling pipeline monitoring and performance tracking."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_event", "line_number": 91, "body": "def _send_event(self, **params):\n        params['tags'] += self.default_tags\n\n        api.Event.create(**params)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Sends a formatted event to the Datadog API, automatically appending configured default tags. This provides a way to log important occurrences with consistent metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_gauge", "line_number": 96, "body": "def _send_gauge(self, metric_name, value, tags=[]):\n        all_tags = tags + self.default_tags\n\n        namespaced_metric = \"{namespace}.{metric_name}\".format(namespace=self._config.metric_namespace,\n                                                               metric_name=metric_name)\n        statsd.gauge(namespaced_metric, value, tags=all_tags)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Provides a utility to send namespaced gauge metrics to Datadog. It automatically applies relevant tags to report a current, fluctuating value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_send_increment", "line_number": 103, "body": "def _send_increment(self, metric_name, value=1, tags=[]):\n        all_tags = tags + self.default_tags\n\n        namespaced_metric = \"{namespace}.{metric_name}\".format(namespace=self._config.metric_namespace,\n                                                               metric_name=metric_name)\n        statsd.increment(namespaced_metric, value, tags=all_tags)", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Sends an increment metric to Datadog, applying a namespace and additional tags. It's used to track occurrences or count events for monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "_format_task_params_to_tags", "line_number": 110, "body": "def _format_task_params_to_tags(self, task):\n        params = []\n        for key, value in task.params.items():\n            params.append(\"{key}:{value}\".format(key=key, value=value))\n\n        return params", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Formats a task's parameters into a list of `key:value` strings. This prepares them for use as tags when reporting metrics to Datadog."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/datadog_metric.py", "function": "default_tags", "line_number": 118, "body": "def default_tags(self):\n        default_tags = []\n\n        env_tag = \"environment:{environment}\".format(environment=self._config.environment)\n        default_tags.append(env_tag)\n\n        if self._config.default_tags:\n            default_tags = default_tags + str.split(self._config.default_tags, ',')\n\n        return default_tags", "is_method": true, "class_name": "DatadogMetricsCollector", "function_description": "Generates a list of default tags for Datadog metrics, including the environment and any user-configured static tags. This ensures consistent tagging across all reported metrics."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_collection", "line_number": 38, "body": "def get_collection(self):\n        \"\"\"\n        Return targeted mongo collection to query on\n        \"\"\"\n        db_mongo = self._mongo_client[self._index]\n        return db_mongo[self._collection]", "is_method": true, "class_name": "MongoTarget", "function_description": "This method returns the specific MongoDB collection configured for the `MongoTarget` instance. It provides the necessary object for performing database operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_index", "line_number": 45, "body": "def get_index(self):\n        \"\"\"\n        Return targeted mongo index to query on\n        \"\"\"\n        return self._mongo_client[self._index]", "is_method": true, "class_name": "MongoTarget", "function_description": "Provides the designated MongoDB index (collection/database) object, enabling direct interaction for queries and data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 68, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted field exists\n        \"\"\"\n        return self.read() is not None", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Determines if the specified MongoDB target exists, signifying that a particular process or operation has been completed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 75, "body": "def read(self):\n        \"\"\"\n        Read the target value\n        Use $project aggregate operator in order to support nested objects\n        \"\"\"\n        result = self.get_collection().aggregate([\n            {'$match': {'_id': self._document_id}},\n            {'$project': {'_value': '$' + self._path, '_id': False}}\n        ])\n\n        for doc in result:\n            if '_value' not in doc:\n                break\n\n            return doc['_value']", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Retrieves a specific, potentially nested, data value from a MongoDB document. This method acts as a precise getter for a \"cell\" of data within a document."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "write", "line_number": 91, "body": "def write(self, value):\n        \"\"\"\n        Write value to the target\n        \"\"\"\n        self.get_collection().update_one(\n            {'_id': self._document_id},\n            {'$set': {self._path: value}},\n            upsert=True\n        )", "is_method": true, "class_name": "MongoCellTarget", "function_description": "Writes a value to a targeted field (cell) within a MongoDB document. It creates the document if it does not already exist."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 118, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted field exists in ALL documents\n        \"\"\"\n        return not self.get_empty_ids()", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Determines if the target has been processed by checking if the designated field exists in every document within the specified Mongo range."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 125, "body": "def read(self):\n        \"\"\"\n        Read the targets value\n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {self._field: True}\n        )\n\n        return {doc['_id']: doc[self._field] for doc in cursor}", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Reads a specific field's values for a given list of document IDs from a MongoDB collection. It returns a dictionary mapping each document ID to its corresponding field value."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "write", "line_number": 139, "body": "def write(self, values):\n        \"\"\"\n        Write values to the targeted documents\n        Values need to be a dict as : {document_id: value}\n        \"\"\"\n        # Insert only for docs targeted by the target\n        filtered = {_id: value for _id, value in values.items() if _id in self._document_ids}\n\n        if not filtered:\n            return\n\n        bulk = self.get_collection().initialize_ordered_bulk_op()\n        for _id, value in filtered.items():\n            bulk.find({'_id': _id}).upsert() \\\n                    .update_one({'$set': {self._field: value}})\n\n        bulk.execute()", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Updates a specific field for a filtered set of MongoDB documents managed by the target. It efficiently writes new values using bulk upsert operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "get_empty_ids", "line_number": 157, "body": "def get_empty_ids(self):\n        \"\"\"\n        Get documents id with missing targeted field\n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {'_id': True}\n        )\n\n        return set(self._document_ids) - {doc['_id'] for doc in cursor}", "is_method": true, "class_name": "MongoRangeTarget", "function_description": "Returns the IDs of documents from a predefined set that are missing a specific target field in the MongoDB collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 179, "body": "def exists(self):\n        \"\"\"\n        Test if target has been run\n        Target is considered run if the targeted collection exists in the database\n        \"\"\"\n        return self.read()", "is_method": true, "class_name": "MongoCollectionTarget", "function_description": "Determines if the target MongoDB collection exists in the database. This indicates whether the associated data processing task or pipeline step has been completed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 186, "body": "def read(self):\n        \"\"\"\n        Return if the target collection exists in the database\n        \"\"\"\n        return self._collection in self.get_index().collection_names()", "is_method": true, "class_name": "MongoCollectionTarget", "function_description": "This method verifies if the designated MongoDB collection exists within the connected database. It serves as a simple existence check for the target collection."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "exists", "line_number": 206, "body": "def exists(self):\n        \"\"\"\n        Test if the target has been run\n        Target is considered run if the number of items in the target matches value of self._target_count\n        \"\"\"\n        return self.read() == self._target_count", "is_method": true, "class_name": "MongoCountTarget", "function_description": "Checks if the counting target is met, indicating the completion of an operation. It compares the current item count with the expected target count."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/mongodb.py", "function": "read", "line_number": 213, "body": "def read(self):\n        \"\"\"\n        Using the aggregate method to avoid inaccurate count if using a sharded cluster\n        https://docs.mongodb.com/manual/reference/method/db.collection.count/#behavior\n        \"\"\"\n        for res in self.get_collection().aggregate([{'$group': {'_id': None, 'count': {'$sum': 1}}}]):\n            return res.get('count', None)\n        return None", "is_method": true, "class_name": "MongoCountTarget", "function_description": "Retrieves the total number of documents in a MongoDB collection. It uses an aggregation pipeline to ensure accurate counts, particularly for sharded clusters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "program_environment", "line_number": 99, "body": "def program_environment(self):\n        \"\"\"\n        Override this method to control environment variables for the program\n\n        :return: dict mapping environment variable names to values\n        \"\"\"\n        env = os.environ.copy()\n        return env", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Provides a customizable mechanism within ExternalProgramTask to define environment variables. Subclasses can override it to control an external program's execution environment."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "always_log_stderr", "line_number": 109, "body": "def always_log_stderr(self):\n        \"\"\"\n        When True, stderr will be logged even if program execution succeeded\n\n        Override to False to log stderr only when program execution fails.\n        \"\"\"\n        return True", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Provides a configuration setting to ensure standard error output from an external program is always logged, even upon successful execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_clean_output_file", "line_number": 117, "body": "def _clean_output_file(self, file_object):\n        file_object.seek(0)\n        return ''.join(map(lambda s: s.decode('utf-8'), file_object.readlines()))", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Processes raw file-like output, reading its entire content and decoding it into a single UTF-8 string. This cleans external program output for further use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "build_tracking_url", "line_number": 121, "body": "def build_tracking_url(self, logs_output):\n        \"\"\"\n        This method is intended for transforming pattern match in logs to an URL\n        :param logs_output: Found match of `self.tracking_url_pattern`\n        :return: a tracking URL for the task\n        \"\"\"\n        return logs_output", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Provides a tracking URL for an external program task. It returns the previously extracted URL pattern found in logs for direct access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "run", "line_number": 129, "body": "def run(self):\n        args = list(map(str, self.program_args()))\n\n        logger.info('Running command: %s', ' '.join(args))\n        env = self.program_environment()\n        kwargs = {'env': env}\n        tmp_stdout, tmp_stderr = None, None\n        if self.capture_output:\n            tmp_stdout, tmp_stderr = tempfile.TemporaryFile(), tempfile.TemporaryFile()\n            kwargs.update({'stdout': tmp_stdout, 'stderr': tmp_stderr})\n\n        try:\n            if self.stream_for_searching_tracking_url != 'none' and self.tracking_url_pattern is not None:\n                with self._proc_with_tracking_url_context(proc_args=args, proc_kwargs=kwargs) as proc:\n                    proc.wait()\n            else:\n                proc = subprocess.Popen(args, **kwargs)\n                with ExternalProgramRunContext(proc):\n                    proc.wait()\n            success = proc.returncode == 0\n\n            if self.capture_output:\n                stdout = self._clean_output_file(tmp_stdout)\n                stderr = self._clean_output_file(tmp_stderr)\n\n                if stdout:\n                    logger.info('Program stdout:\\n{}'.format(stdout))\n                if stderr:\n                    if self.always_log_stderr or not success:\n                        logger.info('Program stderr:\\n{}'.format(stderr))\n            else:\n                stdout, stderr = None, None\n\n            if not success:\n                raise ExternalProgramRunError(\n                    'Program failed with return code={}:'.format(proc.returncode),\n                    args, env=env, stdout=stdout, stderr=stderr)\n        finally:\n            if self.capture_output:\n                tmp_stderr.close()\n                tmp_stdout.close()", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Executes the configured external command-line program. It manages the program's environment, captures output, and reports success or failure."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_proc_with_tracking_url_context", "line_number": 172, "body": "def _proc_with_tracking_url_context(self, proc_args, proc_kwargs):\n        time_to_sleep = 0.5\n        file_to_write = proc_kwargs.get(self.stream_for_searching_tracking_url)\n        proc_kwargs.update({self.stream_for_searching_tracking_url: subprocess.PIPE})\n        main_proc = subprocess.Popen(proc_args, **proc_kwargs)\n        pipe_to_read = main_proc.stderr if self.stream_for_searching_tracking_url == 'stderr' else main_proc.stdout\n\n        def _track_url_by_pattern():\n            \"\"\"\n            Scans the pipe looking for a passed pattern, if the pattern is found, `set_tracking_url` callback is sent.\n            If tmp_stdout is passed, also appends lines to this file.\n            \"\"\"\n            pattern = re.compile(self.tracking_url_pattern)\n            for new_line in iter(pipe_to_read.readline, ''):\n                if new_line:\n                    if file_to_write:\n                        file_to_write.write(new_line)\n                    match = re.search(pattern, new_line.decode('utf-8'))\n                    if match:\n                        self.set_tracking_url(\n                            self.build_tracking_url(match.group(1))\n                        )\n                else:\n                    file_to_write.flush()\n                    sleep(time_to_sleep)\n\n        track_proc = Process(target=_track_url_by_pattern)\n        try:\n            track_proc.start()\n            with ExternalProgramRunContext(main_proc):\n                yield main_proc\n        finally:\n            # need to wait a bit to let the subprocess read the last lines\n            track_proc.join(time_to_sleep * 2)\n            if track_proc.is_alive():\n                track_proc.terminate()\n            pipe_to_read.close()", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Executes an external program, concurrently monitoring its output for a specific URL pattern. If found, it extracts and sets this tracking URL, enabling live progress monitoring."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__enter__", "line_number": 215, "body": "def __enter__(self):\n        self.__old_signal = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.kill_job)\n        return self", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "This context manager method intercepts the SIGTERM signal upon entry. It configures the signal to gracefully terminate external programs managed within this context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__exit__", "line_number": 220, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is KeyboardInterrupt:\n            self.kill_job()\n        signal.signal(signal.SIGTERM, self.__old_signal)", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "Manages the exit of an external program's execution context. It ensures cleanup by killing the job upon KeyboardInterrupt and restoring the original signal handler."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "kill_job", "line_number": 225, "body": "def kill_job(self, captured_signal=None, stack_frame=None):\n        self.proc.kill()\n        if captured_signal is not None:\n            # adding 128 gives the exit code corresponding to a signal\n            sys.exit(128 + captured_signal)", "is_method": true, "class_name": "ExternalProgramRunContext", "function_description": "Provides the capability to forcefully terminate the external program associated with this context. It ensures the managed process is stopped, optionally exiting the current program with a signal-specific exit code."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "__str__", "line_number": 241, "body": "def __str__(self):\n        info = self.message\n        info += '\\nCOMMAND: {}'.format(' '.join(self.args))\n        info += '\\nSTDOUT: {}'.format(self.out or '[empty]')\n        info += '\\nSTDERR: {}'.format(self.err or '[empty]')\n        env_string = None\n        if self.env:\n            env_string = ' '.join(['='.join([k, '\\'{}\\''.format(v)]) for k, v in self.env.items()])\n        info += '\\nENVIRONMENT: {}'.format(env_string or '[empty]')\n        # reset terminal color in case the ENVIRONMENT changes colors\n        info += '\\033[m'\n        return info", "is_method": true, "class_name": "ExternalProgramRunError", "function_description": "Generates a detailed, human-readable string representation of an external program execution error. It includes the command, stdout, stderr, and environment for debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "program_environment", "line_number": 275, "body": "def program_environment(self):\n        env = super(ExternalPythonProgramTask, self).program_environment()\n\n        if self.extra_pythonpath:\n            pythonpath = ':'.join([self.extra_pythonpath, env.get('PYTHONPATH', '')])\n            env.update({'PYTHONPATH': pythonpath})\n\n        if self.virtualenv:\n            # Make the same changes to the env that a normal venv/bin/activate script would\n            path = ':'.join(['{}/bin'.format(self.virtualenv), env.get('PATH', '')])\n            env.update({\n                'PATH': path,\n                'VIRTUAL_ENV': self.virtualenv\n            })\n            # remove PYTHONHOME env variable, if it exists\n            env.pop('PYTHONHOME', None)\n\n        return env", "is_method": true, "class_name": "ExternalPythonProgramTask", "function_description": "Configures the execution environment for an external Python program, incorporating specified virtual environments and additional Python paths for correct operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/external_program.py", "function": "_track_url_by_pattern", "line_number": 179, "body": "def _track_url_by_pattern():\n            \"\"\"\n            Scans the pipe looking for a passed pattern, if the pattern is found, `set_tracking_url` callback is sent.\n            If tmp_stdout is passed, also appends lines to this file.\n            \"\"\"\n            pattern = re.compile(self.tracking_url_pattern)\n            for new_line in iter(pipe_to_read.readline, ''):\n                if new_line:\n                    if file_to_write:\n                        file_to_write.write(new_line)\n                    match = re.search(pattern, new_line.decode('utf-8'))\n                    if match:\n                        self.set_tracking_url(\n                            self.build_tracking_url(match.group(1))\n                        )\n                else:\n                    file_to_write.flush()\n                    sleep(time_to_sleep)", "is_method": true, "class_name": "ExternalProgramTask", "function_description": "Scans an external program's output stream for a specified pattern. If the pattern is found, it extracts and sets a tracking URL, optionally logging the output to a file."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "s3", "line_number": 91, "body": "def s3(self):\n        # only import boto3 when needed to allow top-lvl s3 module import\n        import boto3\n\n        options = dict(self._options)\n\n        if self._s3:\n            return self._s3\n\n        aws_access_key_id = options.get('aws_access_key_id')\n        aws_secret_access_key = options.get('aws_secret_access_key')\n\n        # Removing key args would break backwards compatibility\n        role_arn = options.get('aws_role_arn')\n        role_session_name = options.get('aws_role_session_name')\n\n        # In case the aws_session_token is provided use it\n        aws_session_token = options.get('aws_session_token')\n\n        if role_arn and role_session_name:\n            sts_client = boto3.client('sts')\n            assumed_role = sts_client.assume_role(RoleArn=role_arn,\n                                                  RoleSessionName=role_session_name)\n            aws_secret_access_key = assumed_role['Credentials'].get(\n                'SecretAccessKey')\n            aws_access_key_id = assumed_role['Credentials'].get('AccessKeyId')\n            aws_session_token = assumed_role['Credentials'].get('SessionToken')\n            logger.debug('using aws credentials via assumed role {} as defined in luigi config'\n                         .format(role_session_name))\n\n        for key in ['aws_access_key_id', 'aws_secret_access_key',\n                    'aws_role_session_name', 'aws_role_arn', 'aws_session_token']:\n            if key in options:\n                options.pop(key)\n\n        # At this stage, if no credentials provided, boto3 would handle their resolution for us\n        # For finding out about the order in which it tries to find these credentials\n        # please see here details\n        # http://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials\n\n        if not (aws_access_key_id and aws_secret_access_key):\n            logger.debug('no credentials provided, delegating credentials resolution to boto3')\n\n        try:\n            self._s3 = boto3.resource('s3',\n                                      aws_access_key_id=aws_access_key_id,\n                                      aws_secret_access_key=aws_secret_access_key,\n                                      aws_session_token=aws_session_token,\n                                      **options)\n        except TypeError as e:\n            logger.error(e.args[0])\n            if 'got an unexpected keyword argument' in e.args[0]:\n                raise DeprecatedBotoClientException(\n                    \"Now using boto3. Check that you're passing the correct arguments\")\n            raise\n\n        return self._s3", "is_method": true, "class_name": "S3Client", "function_description": "Provides a configured boto3 S3 client for Amazon S3 interactions. It manages AWS credentials, including role assumption, and caches the client for efficient reuse."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "s3", "line_number": 150, "body": "def s3(self, value):\n        self._s3 = value", "is_method": true, "class_name": "S3Client", "function_description": "Sets the internal S3 client object, enabling the S3Client instance to interact with AWS S3 services."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "exists", "line_number": 153, "body": "def exists(self, path):\n        \"\"\"\n        Does provided path exist on S3?\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # root always exists\n        if self._is_root(key):\n            return True\n\n        # file\n        if self._exists(bucket, key):\n            return True\n\n        if self.isdir(path):\n            return True\n\n        logger.debug('Path %s does not exist', path)\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "This method checks if a given path, representing either a file or a directory, exists on S3. It provides a simple way to verify the presence of objects or prefixes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "remove", "line_number": 173, "body": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove a file or directory from S3.\n        :param path: File or directory to remove\n        :param recursive: Boolean indicator to remove object and children\n        :return: Boolean indicator denoting success of the removal of 1 or more files\n        \"\"\"\n        if not self.exists(path):\n            logger.debug('Could not delete %s; path does not exist', path)\n            return False\n\n        (bucket, key) = self._path_to_bucket_and_key(path)\n        s3_bucket = self.s3.Bucket(bucket)\n        # root\n        if self._is_root(key):\n            raise InvalidDeleteException('Cannot delete root of bucket at path %s' % path)\n\n        # file\n        if self._exists(bucket, key):\n            self.s3.meta.client.delete_object(Bucket=bucket, Key=key)\n            logger.debug('Deleting %s from bucket %s', key, bucket)\n            return True\n\n        if self.isdir(path) and not recursive:\n            raise InvalidDeleteException('Path %s is a directory. Must use recursive delete' % path)\n\n        delete_key_list = [{'Key': obj.key} for obj in s3_bucket.objects.filter(Prefix=self._add_path_delimiter(key))]\n\n        # delete the directory marker file if it exists\n        if self._exists(bucket, '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)):\n            delete_key_list.append({'Key': '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)})\n\n        if len(delete_key_list) > 0:\n            n = 1000\n            for i in range(0, len(delete_key_list), n):\n                self.s3.meta.client.delete_objects(Bucket=bucket, Delete={'Objects': delete_key_list[i: i + n]})\n            return True\n\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "Deletes a specified file or directory (prefix) from an S3 bucket. It supports recursive removal of all objects nested under the provided path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "move", "line_number": 213, "body": "def move(self, source_path, destination_path, **kwargs):\n        \"\"\"\n        Rename/move an object from one S3 location to another.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param kwargs: Keyword arguments are passed to the boto3 function `copy`\n        \"\"\"\n        self.copy(source_path, destination_path, **kwargs)\n        self.remove(source_path)", "is_method": true, "class_name": "S3Client", "function_description": "Relocates or renames an S3 object or directory from a given source path to a specified destination path. This enables reorganization of data within S3.\nRelocates or renames an S3 object or directory from a given source path to a specified destination path. This enables reorganization of data within S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_key", "line_number": 223, "body": "def get_key(self, path):\n        \"\"\"\n        Returns the object summary at the path\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        if self._exists(bucket, key):\n            return self.s3.ObjectSummary(bucket, key)", "is_method": true, "class_name": "S3Client", "function_description": "Provides an S3 object summary (metadata) for a given path, allowing callers to check object existence and retrieve its attributes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put", "line_number": 232, "body": "def put(self, local_path, destination_s3_path, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path.\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        # put the file\n        self.put_multipart(local_path, destination_s3_path, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Uploads a local file to a specified Amazon S3 path. It provides the core capability for storing data in S3 from a local source."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put_string", "line_number": 244, "body": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "Uploads string content directly to a specified S3 path. This provides a convenient way to store text data in S3 buckets."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "put_multipart", "line_number": 258, "body": "def put_multipart(self, local_path, destination_s3_path, part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path\n        using S3 multi-part upload (for files > 8Mb).\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param part_size: Part size in bytes. Default: 8388608 (8MB)\n        :param kwargs: Keyword arguments are passed to the boto function `upload_fileobj` as ExtraArgs\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        from boto3.s3.transfer import TransferConfig\n        # default part size for boto3 is 8Mb, changing it to fit part_size\n        # provided as a parameter\n        transfer_config = TransferConfig(multipart_chunksize=part_size)\n\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        self.s3.meta.client.upload_fileobj(\n            Fileobj=open(local_path, 'rb'), Bucket=bucket, Key=key, Config=transfer_config, ExtraArgs=kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "This method provides a service to upload local files to a specified S3 path. It utilizes multi-part transfer, which is optimized for robustly handling large objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "copy", "line_number": 279, "body": "def copy(self, source_path, destination_path, threads=DEFAULT_THREADS, start_time=None, end_time=None,\n             part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Copy object(s) from one S3 location to another. Works for individual keys or entire directories.\n        When files are larger than `part_size`, multipart uploading will be used.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param threads: Optional argument to define the number of threads to use when copying (min: 3 threads)\n        :param start_time: Optional argument to copy files with modified dates after start_time\n        :param end_time: Optional argument to copy files with modified dates before end_time\n        :param part_size: Part size in bytes\n        :param kwargs: Keyword arguments are passed to the boto function `copy` as ExtraArgs\n        :returns tuple (number_of_files_copied, total_size_copied_in_bytes)\n        \"\"\"\n\n        # don't allow threads to be less than 3\n        threads = 3 if threads < 3 else threads\n\n        if self.isdir(source_path):\n            return self._copy_dir(source_path, destination_path, threads=threads,\n                                  start_time=start_time, end_time=end_time, part_size=part_size, **kwargs)\n\n        # If the file isn't a directory just perform a simple copy\n        else:\n            return self._copy_file(source_path, destination_path, threads=threads, part_size=part_size, **kwargs)", "is_method": true, "class_name": "S3Client", "function_description": "The `S3Client.copy` method transfers single objects or entire directories between S3 locations. It supports multipart uploads for large files and optional time-based filtering."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_copy_file", "line_number": 305, "body": "def _copy_file(self, source_path, destination_path, threads=DEFAULT_THREADS, part_size=DEFAULT_PART_SIZE, **kwargs):\n        src_bucket, src_key = self._path_to_bucket_and_key(source_path)\n        dst_bucket, dst_key = self._path_to_bucket_and_key(destination_path)\n        transfer_config = TransferConfig(max_concurrency=threads, multipart_chunksize=part_size)\n        item = self.get_key(source_path)\n        copy_source = {\n            'Bucket': src_bucket,\n            'Key': src_key\n        }\n\n        self.s3.meta.client.copy(copy_source, dst_bucket, dst_key, Config=transfer_config, ExtraArgs=kwargs)\n\n        return 1, item.size", "is_method": true, "class_name": "S3Client", "function_description": "Provides the capability to copy an object between specified locations within Amazon S3. It configures the transfer for efficiency, returning status and size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_copy_dir", "line_number": 319, "body": "def _copy_dir(self, source_path, destination_path, threads=DEFAULT_THREADS,\n                  start_time=None, end_time=None, part_size=DEFAULT_PART_SIZE, **kwargs):\n        start = datetime.datetime.now()\n        copy_jobs = []\n        management_pool = ThreadPool(processes=threads)\n        transfer_config = TransferConfig(max_concurrency=threads, multipart_chunksize=part_size)\n        src_bucket, src_key = self._path_to_bucket_and_key(source_path)\n        dst_bucket, dst_key = self._path_to_bucket_and_key(destination_path)\n        src_prefix = self._add_path_delimiter(src_key)\n        dst_prefix = self._add_path_delimiter(dst_key)\n        key_path_len = len(src_prefix)\n        total_size_bytes = 0\n        total_keys = 0\n        for item in self.list(source_path, start_time=start_time, end_time=end_time, return_key=True):\n            path = item.key[key_path_len:]\n            # prevents copy attempt of empty key in folder\n            if path != '' and path != '/':\n                total_keys += 1\n                total_size_bytes += item.size\n                copy_source = {\n                    'Bucket': src_bucket,\n                    'Key': src_prefix + path\n                }\n                the_kwargs = {'Config': transfer_config, 'ExtraArgs': kwargs}\n                job = management_pool.apply_async(self.s3.meta.client.copy,\n                                                  args=(copy_source, dst_bucket, dst_prefix + path),\n                                                  kwds=the_kwargs)\n                copy_jobs.append(job)\n        # Wait for the pools to finish scheduling all the copies\n        management_pool.close()\n        management_pool.join()\n        # Raise any errors encountered in any of the copy processes\n        for result in copy_jobs:\n            result.get()\n        end = datetime.datetime.now()\n        duration = end - start\n        logger.info('%s : Complete : %s total keys copied in %s' %\n                    (datetime.datetime.now(), total_keys, duration))\n        return total_keys, total_size_bytes", "is_method": true, "class_name": "S3Client", "function_description": "This internal method of S3Client copies all objects from a source S3 directory (prefix) to a destination S3 directory, supporting concurrent transfers."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get", "line_number": 359, "body": "def get(self, s3_path, destination_local_path):\n        \"\"\"\n        Get an object stored in S3 and write it to a local path.\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        # download the file\n        self.s3.meta.client.download_file(bucket, key, destination_local_path)", "is_method": true, "class_name": "S3Client", "function_description": "This method downloads an object from a specified S3 path and saves it to a given local file path. It provides a service for retrieving data stored in S3 for local use."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_as_bytes", "line_number": 367, "body": "def get_as_bytes(self, s3_path):\n        \"\"\"\n        Get the contents of an object stored in S3 as bytes\n\n        :param s3_path: URL for target S3 location\n        :return: File contents as pure bytes\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        obj = self.s3.Object(bucket, key)\n        contents = obj.get()['Body'].read()\n        return contents", "is_method": true, "class_name": "S3Client", "function_description": "Retrieves the full contents of an object from Amazon S3. It returns the file's data as a raw byte stream, useful for binary or unparsed data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "get_as_string", "line_number": 379, "body": "def get_as_string(self, s3_path, encoding='utf-8'):\n        \"\"\"\n        Get the contents of an object stored in S3 as string.\n\n        :param s3_path: URL for target S3 location\n        :param encoding: Encoding to decode bytes to string\n        :return: File contents as a string\n        \"\"\"\n        content = self.get_as_bytes(s3_path)\n        return content.decode(encoding)", "is_method": true, "class_name": "S3Client", "function_description": "This `S3Client` method retrieves the content of an S3 object specified by its path and returns it as a decoded string. It is useful for directly reading text files stored in S3."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "isdir", "line_number": 390, "body": "def isdir(self, path):\n        \"\"\"\n        Is the parameter S3 path a directory?\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        s3_bucket = self.s3.Bucket(bucket)\n\n        # root is a directory\n        if self._is_root(key):\n            return True\n\n        for suffix in (S3_DIRECTORY_MARKER_SUFFIX_0,\n                       S3_DIRECTORY_MARKER_SUFFIX_1):\n            try:\n                self.s3.meta.client.get_object(\n                    Bucket=bucket, Key=key + suffix)\n            except botocore.exceptions.ClientError as e:\n                if not e.response['Error']['Code'] in ['NoSuchKey', '404']:\n                    raise\n            else:\n                return True\n\n        # files with this prefix\n        key_path = self._add_path_delimiter(key)\n        s3_bucket_list_result = list(itertools.islice(\n            s3_bucket.objects.filter(Prefix=key_path), 1))\n        if s3_bucket_list_result:\n            return True\n\n        return False", "is_method": true, "class_name": "S3Client", "function_description": "Provides a utility to determine if a specified S3 path represents a directory. It enables applications to treat S3 object prefixes as hierarchical folders."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "mkdir", "line_number": 424, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if raise_if_exists and self.isdir(path):\n            raise FileAlreadyExists()\n\n        bucket, key = self._path_to_bucket_and_key(path)\n        if self._is_root(key):\n            # isdir raises if the bucket doesn't exist; nothing to do here.\n            return\n\n        path = self._add_path_delimiter(path)\n\n        if not parents and not self.isdir(os.path.dirname(path)):\n            raise MissingParentDirectory()\n\n        return self.put_string(\"\", path)", "is_method": true, "class_name": "S3Client", "function_description": "Creates a directory (folder) at a given S3 path, optionally creating parent directories. It can prevent creation if the path already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "listdir", "line_number": 440, "body": "def listdir(self, path, start_time=None, end_time=None, return_key=False):\n        \"\"\"\n        Get an iterable with S3 folder contents.\n        Iterable contains paths relative to queried path.\n        :param path: URL for target S3 location\n        :param start_time: Optional argument to list files with modified (offset aware) datetime after start_time\n        :param end_time: Optional argument to list files with modified (offset aware) datetime before end_time\n        :param return_key: Optional argument, when set to True will return boto3's ObjectSummary (instead of the filename)\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # grab and validate the bucket\n        s3_bucket = self.s3.Bucket(bucket)\n\n        key_path = self._add_path_delimiter(key)\n        key_path_len = len(key_path)\n        for item in s3_bucket.objects.filter(Prefix=key_path):\n            last_modified_date = item.last_modified\n            if (\n                # neither are defined, list all\n                (not start_time and not end_time) or\n                # start defined, after start\n                (start_time and not end_time and start_time < last_modified_date) or\n                # end defined, prior to end\n                (not start_time and end_time and last_modified_date < end_time) or\n                (start_time and end_time and start_time <\n                 last_modified_date < end_time)  # both defined, between\n            ):\n                if return_key:\n                    yield item\n                else:\n                    yield self._add_path_delimiter(path) + item.key[key_path_len:]", "is_method": true, "class_name": "S3Client", "function_description": "Lists the contents of an S3 path, returning an iterable of object keys. It supports filtering by last modification time and returning full object metadata."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "list", "line_number": 473, "body": "def list(self, path, start_time=None, end_time=None, return_key=False):  # backwards compat\n        key_path_len = len(self._add_path_delimiter(path))\n        for item in self.listdir(path, start_time=start_time, end_time=end_time, return_key=return_key):\n            if return_key:\n                yield item\n            else:\n                yield item[key_path_len:]", "is_method": true, "class_name": "S3Client", "function_description": "This S3Client method lists objects within a specified S3 path, optionally filtering by time range. It yields either full object keys or their paths relative to the given prefix."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_get_s3_config", "line_number": 482, "body": "def _get_s3_config(key=None):\n        defaults = dict(configuration.get_config().defaults())\n        try:\n            config = dict(configuration.get_config().items('s3'))\n        except (NoSectionError, KeyError):\n            return {}\n        # So what ports etc can be read without us having to specify all dtypes\n        for k, v in config.items():\n            try:\n                config[k] = int(v)\n            except ValueError:\n                pass\n        if key:\n            return config.get(key)\n        section_only = {k: v for k, v in config.items() if k not in defaults or v != defaults[k]}\n\n        return section_only", "is_method": true, "class_name": "S3Client", "function_description": "Retrieves S3 configuration parameters from the system's global settings. It provides either a specific S3 configuration value or a filtered dictionary of custom S3 parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_path_to_bucket_and_key", "line_number": 501, "body": "def _path_to_bucket_and_key(path):\n        (scheme, netloc, path, query, fragment) = urlsplit(path,\n                                                           allow_fragments=False)\n        question_mark_plus_query = '?' + query if query else ''\n        path_without_initial_slash = path[1:] + question_mark_plus_query\n        return netloc, path_without_initial_slash", "is_method": true, "class_name": "S3Client", "function_description": "Converts an S3-style path into its constituent bucket name and object key. This is a utility for identifying specific S3 resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_is_root", "line_number": 509, "body": "def _is_root(key):\n        return (len(key) == 0) or (key == '/')", "is_method": true, "class_name": "S3Client", "function_description": "This function checks if an S3 key represents the root of a bucket or directory. It helps validate paths or manage operations at the S3 bucket's top level."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_add_path_delimiter", "line_number": 513, "body": "def _add_path_delimiter(key):\n        return key if key[-1:] == '/' or key == '' else key + '/'", "is_method": true, "class_name": "S3Client", "function_description": "Ensures an S3 object key or prefix string consistently ends with a forward slash. This normalizes paths for correct S3 operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_check_deprecated_argument", "line_number": 517, "body": "def _check_deprecated_argument(**kwargs):\n        \"\"\"\n        If `encrypt_key` or `host` is part of the arguments raise an exception\n        :return: None\n        \"\"\"\n        if 'encrypt_key' in kwargs:\n            raise DeprecatedBotoClientException(\n                'encrypt_key deprecated in boto3. Please refer to boto3 documentation for encryption details.')\n        if 'host' in kwargs:\n            raise DeprecatedBotoClientException(\n                'host keyword deprecated and is replaced by region_name in boto3.\\n'\n                'example: region_name=us-west-1\\n'\n                'For region names, refer to the amazon S3 region documentation\\n'\n                'https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region')", "is_method": true, "class_name": "S3Client", "function_description": "This internal helper method validates S3 client arguments. It raises an exception if deprecated parameters like 'encrypt_key' or 'host' are used, guiding users to correct boto3 API usage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_exists", "line_number": 532, "body": "def _exists(self, bucket, key):\n        try:\n            self.s3.Object(bucket, key).load()\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] in ['NoSuchKey', '404']:\n                return False\n            else:\n                raise\n\n        return True", "is_method": true, "class_name": "S3Client", "function_description": "Verifies if an object identified by its key exists within a specified S3 bucket. This method helps confirm the presence of S3 resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "move_to_final_destination", "line_number": 556, "body": "def move_to_final_destination(self):\n        self.s3_client.put_multipart(\n            self.tmp_path, self.path, **self.s3_options)", "is_method": true, "class_name": "AtomicS3File", "function_description": "This method completes an atomic S3 file write operation. It moves the staged temporary object to its final, permanent location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "read", "line_number": 568, "body": "def read(self, size=None):\n        f = self.s3_key.read(size)\n        return f", "is_method": true, "class_name": "ReadableS3File", "function_description": "Provides a file-like `read` interface for the associated S3 object. It retrieves a specified number of bytes or the entire file content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "close", "line_number": 572, "body": "def close(self):\n        self.s3_key.close()\n        self.closed = True", "is_method": true, "class_name": "ReadableS3File", "function_description": "Closes the S3 file by closing the underlying S3 key, releasing associated resources and marking the file as closed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__del__", "line_number": 576, "body": "def __del__(self):\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Ensures the S3 file connection and associated resources are closed when the `ReadableS3File` object is garbage collected. This prevents resource leaks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__exit__", "line_number": 579, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Ensures the S3 file connection is properly closed upon exiting a 'with' statement, preventing resource leaks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__enter__", "line_number": 582, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableS3File", "function_description": "Enables the `ReadableS3File` object to be used as a context manager. It returns the instance itself for use within a `with` statement."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_add_to_buffer", "line_number": 585, "body": "def _add_to_buffer(self, line):\n        self.buffer.append(line)", "is_method": true, "class_name": "ReadableS3File", "function_description": "This internal method adds a line of text to the object's buffer. It serves as a private helper for `ReadableS3File` to accumulate data for further processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "_flush_buffer", "line_number": 588, "body": "def _flush_buffer(self):\n        output = b''.join(self.buffer)\n        self.buffer = []\n        return output", "is_method": true, "class_name": "ReadableS3File", "function_description": "This private helper consolidates all buffered byte data into a single object. It then clears the buffer, making it ready for new incoming data chunks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "readable", "line_number": 593, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableS3File", "function_description": "Indicates that this S3 file object is always readable. This method confirms the object's capability to be read."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "writable", "line_number": 596, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableS3File", "function_description": "Provides a clear indication that a `ReadableS3File` object is not writable, reinforcing its read-only nature and preventing erroneous write attempts."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "seekable", "line_number": 599, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableS3File", "function_description": "Indicates that this S3 file stream does not support random access operations like seeking. This is common for network-based or streaming file interfaces."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "__iter__", "line_number": 602, "body": "def __iter__(self):\n        key_iter = self.s3_key.__iter__()\n\n        has_next = True\n        while has_next:\n            try:\n                # grab the next chunk\n                chunk = next(key_iter)\n\n                # split on newlines, preserving the newline\n                for line in chunk.splitlines(True):\n\n                    if not line.endswith(os.linesep):\n                        # no newline, so store in buffer\n                        self._add_to_buffer(line)\n                    else:\n                        # newline found, send it out\n                        if self.buffer:\n                            self._add_to_buffer(line)\n                            yield self._flush_buffer()\n                        else:\n                            yield line\n            except StopIteration:\n                # send out anything we have left in the buffer\n                output = self._flush_buffer()\n                if output:\n                    yield output\n                has_next = False\n        self.close()", "is_method": true, "class_name": "ReadableS3File", "function_description": "Enables line-by-line iteration over an S3 file, buffering partial lines across data chunks. This provides a robust and efficient way to read large S3 files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "open", "line_number": 652, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            s3_key = self.fs.get_key(self.path)\n            if not s3_key:\n                raise FileNotFoundException(\n                    \"Could not find file at %s\" % self.path)\n\n            fileobj = ReadableS3File(s3_key)\n            return self.format.pipe_reader(fileobj)\n        else:\n            return self.format.pipe_writer(AtomicS3File(self.path, self.fs, **self.s3_options))", "is_method": true, "class_name": "S3Target", "function_description": "Provides a file-like interface to open an S3 object, allowing data to be read from or written to it. This enables seamless interaction with S3-hosted files for various data operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "exists", "line_number": 710, "body": "def exists(self):\n        hadoopSemaphore = self.path + self.flag\n        return self.fs.exists(hadoopSemaphore)", "is_method": true, "class_name": "S3FlagTarget", "function_description": "This method checks for the existence of a specific flag file within the S3 target path. It indicates whether a data processing operation has completed or if the target data is present."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 731, "body": "def output(self):\n        return S3Target(self.path)", "is_method": true, "class_name": "S3PathTask", "function_description": "Specifies the S3 location where the task's output will be stored. This provides a target for downstream tasks that depend on its completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 741, "body": "def output(self):\n        return S3EmrTarget(self.path)", "is_method": true, "class_name": "S3EmrTask", "function_description": "Defines the S3 output location for the EMR task by returning an S3EmrTarget object. This specifies where the task's results will be stored."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/s3.py", "function": "output", "line_number": 752, "body": "def output(self):\n        return S3FlagTarget(self.path, flag=self.flag)", "is_method": true, "class_name": "S3FlagTask", "function_description": "Defines the output target for this S3 task, specifying the S3 path and a flag object to mark task completion or presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "fix_paths", "line_number": 33, "body": "def fix_paths(job):\n    \"\"\"\n    Coerce input arguments to use temporary files when used for output.\n\n    Return a list of temporary file pairs (tmpfile, destination path) and\n    a list of arguments.\n\n    Converts each HdfsTarget to a string for the path.\n    \"\"\"\n    tmp_files = []\n    args = []\n    for x in job.args():\n        if isinstance(x, luigi.contrib.hdfs.HdfsTarget):  # input/output\n            if x.exists() or not job.atomic_output():  # input\n                args.append(x.path)\n            else:  # output\n                x_path_no_slash = x.path[:-1] if x.path[-1] == '/' else x.path\n                y = luigi.contrib.hdfs.HdfsTarget(x_path_no_slash + '-luigi-tmp-%09d' % random.randrange(0, 1e10))\n                tmp_files.append((y, x_path_no_slash))\n                logger.info('Using temp path: %s for path %s', y.path, x.path)\n                args.append(y.path)\n        else:\n            try:\n                # hopefully the target has a path to use\n                args.append(x.path)\n            except AttributeError:\n                # if there's no path then hope converting it to a string will work\n                args.append(str(x))\n\n    return (tmp_files, args)", "is_method": false, "function_description": "Prepares job arguments by substituting HDFS output paths with temporary ones. This facilitates atomic writes to HDFS, returning temporary file mappings and the processed arguments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "run_job", "line_number": 77, "body": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        # TODO(jcrobak): libjars, files, etc. Can refactor out of\n        # hadoop.HadoopJobRunner\n        if not job.jar():\n            raise HadoopJarJobError(\"Jar not defined\")\n\n        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', job.jar()]\n        if job.main():\n            hadoop_arglist.append(job.main())\n\n        jobconfs = job.jobconfs()\n\n        for jc in jobconfs:\n            hadoop_arglist += ['-D' + jc]\n\n        (tmp_files, job_args) = fix_paths(job)\n\n        hadoop_arglist += job_args\n\n        ssh_config = job.ssh()\n        if ssh_config:\n            host = ssh_config.get(\"host\", None)\n            key_file = ssh_config.get(\"key_file\", None)\n            username = ssh_config.get(\"username\", None)\n            if not host or not key_file or not username:\n                raise HadoopJarJobError(\"missing some config for HadoopRemoteJarJobRunner\")\n            arglist = ['ssh', '-i', key_file,\n                       '-o', 'BatchMode=yes']  # no password prompts etc\n            if ssh_config.get(\"no_host_key_check\", False):\n                arglist += ['-o', 'UserKnownHostsFile=/dev/null',\n                            '-o', 'StrictHostKeyChecking=no']\n            arglist.append('{}@{}'.format(username, host))\n            hadoop_arglist = [pipes.quote(arg) for arg in hadoop_arglist]\n            arglist.append(' '.join(hadoop_arglist))\n        else:\n            if not os.path.exists(job.jar()):\n                logger.error(\"Can't find jar: %s, full path %s\", job.jar(),\n                             os.path.abspath(job.jar()))\n                raise HadoopJarJobError(\"job jar does not exist\")\n            arglist = hadoop_arglist\n\n        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist, job.set_tracking_url)\n\n        for a, b in tmp_files:\n            a.move(b)", "is_method": true, "class_name": "HadoopJarJobRunner", "function_description": "This method executes a configured Hadoop Jar job, handling argument assembly for both local and remote (SSH) execution. It provides the capability to run and track Hadoop-based tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "job_runner", "line_number": 145, "body": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        return HadoopJarJobRunner()", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Not Implemented."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hadoop_jar.py", "function": "atomic_output", "line_number": 149, "body": "def atomic_output(self):\n        \"\"\"\n        If True, then rewrite output arguments to be temp locations and\n        atomically move them into place after the job finishes.\n        \"\"\"\n        return True", "is_method": true, "class_name": "HadoopJarJobTask", "function_description": "Configures the Hadoop job to use atomic output. This ensures job outputs are written to temporary locations and moved into place only after successful completion, preventing partial data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "run", "line_number": 137, "body": "def run(cmd, task=None):\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            close_fds=True\n        )\n        output_lines = []\n        while True:\n            line = process.stdout.readline()\n            if not line:\n                break\n            line = line.decode(\"utf-8\")\n            output_lines += [line]\n            logger.info(line.rstrip(\"\\n\"))\n        process.stdout.close()\n        exit_code = process.wait()\n        if exit_code:\n            output = \"\".join(output_lines)\n            raise subprocess.CalledProcessError(exit_code, cmd, output=output)", "is_method": true, "class_name": "_CmdLineRunner", "function_description": "Executes a given command-line command, capturing and logging its output in real-time. It monitors the command's execution and raises an exception if it fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "args", "line_number": 248, "body": "def args(self):\n        \"\"\"\n        Extra String arguments that will be passed to your Dataflow job.\n        For example:\n\n        return ['--setup_file=setup.py']\n        \"\"\"\n        return []", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Provides extra string arguments to be passed directly to the Apache Beam Dataflow job. This allows for custom configuration or setup when launching the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "validate_output", "line_number": 271, "body": "def validate_output(self):\n        \"\"\"\n        Callback that can be used to validate your output before it is moved to\n        its final location. Returning false here will cause the job to fail, and\n        output to be removed instead of published.\n        \"\"\"\n        return True", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Not Implemented.\nCore utility method of the `BeamDataflowJobTask` class that provides a hook for validating job output before final publication."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "file_pattern", "line_number": 279, "body": "def file_pattern(self):\n        \"\"\"\n        If one/some of the input target files are not in the pattern of part-*,\n        we can add the key of the required target and the correct file pattern\n        that should be appended in the command line here. If the input target key is not found\n        in this dict, the file pattern will be assumed to be part-* for that target.\n\n        :return A dictionary of overridden file pattern that is not part-* for the inputs\n        \"\"\"\n        return {}", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Provides a mapping for custom file patterns to override default 'part-*' patterns for Beam Dataflow job input targets. This method currently returns an empty dictionary, indicating no custom overrides."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "run", "line_number": 304, "body": "def run(self):\n        cmd_line = self._mk_cmd_line()\n        logger.info(' '.join(cmd_line))\n\n        self.before_run()\n\n        try:\n            self.cmd_line_runner.run(cmd_line, self)\n        except subprocess.CalledProcessError as e:\n            logger.error(e, exc_info=True)\n            self.cleanup_on_error(e)\n            os._exit(e.returncode)\n\n        self.on_successful_run()\n\n        if self.validate_output():\n            self.on_successful_output_validation()\n        else:\n            error = ValueError(\"Output validation failed\")\n            self.cleanup_on_error(error)\n            raise error", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Executes a Beam Dataflow job by running a command-line process. It handles the job's lifecycle, including setup, execution, error management, and output validation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_mk_cmd_line", "line_number": 326, "body": "def _mk_cmd_line(self):\n        cmd_line = self.dataflow_executable()\n\n        cmd_line.extend(self._get_dataflow_args())\n        cmd_line.extend(self.args())\n        cmd_line.extend(self._format_input_args())\n        cmd_line.extend(self._format_output_args())\n        return cmd_line", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Assembles the complete command-line argument list required to execute a Beam Dataflow job. It combines the executable path with all necessary input, output, and Dataflow-specific parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_get_runner", "line_number": 335, "body": "def _get_runner(self):\n        if not self.runner:\n            logger.warning(\"Runner not supplied to BeamDataflowJobTask. \" +\n                           \"Defaulting to DirectRunner.\")\n            return \"DirectRunner\"\n        elif self.runner in [\n            \"DataflowRunner\",\n            \"DirectRunner\"\n        ]:\n            return self.runner\n        else:\n            raise ValueError(\"Runner %s is unsupported.\" % self.runner)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Determines the appropriate Apache Beam runner for the dataflow job. It validates the specified runner or defaults to DirectRunner if none is provided."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_get_dataflow_args", "line_number": 348, "body": "def _get_dataflow_args(self):\n        def f(key, value):\n            return '--{}={}'.format(key, value)\n\n        output = []\n\n        output.append(f(self.dataflow_params.runner, self._get_runner()))\n\n        if self.project:\n            output.append(f(self.dataflow_params.project, self.project))\n        if self.zone:\n            output.append(f(self.dataflow_params.zone, self.zone))\n        if self.region:\n            output.append(f(self.dataflow_params.region, self.region))\n        if self.staging_location:\n            output.append(f(self.dataflow_params.staging_location, self.staging_location))\n        if self.temp_location:\n            output.append(f(self.dataflow_params.temp_location, self.temp_location))\n        if self.gcp_temp_location:\n            output.append(f(self.dataflow_params.gcp_temp_location, self.gcp_temp_location))\n        if self.num_workers:\n            output.append(f(self.dataflow_params.num_workers, self.num_workers))\n        if self.autoscaling_algorithm:\n            output.append(f(self.dataflow_params.autoscaling_algorithm, self.autoscaling_algorithm))\n        if self.max_num_workers:\n            output.append(f(self.dataflow_params.max_num_workers, self.max_num_workers))\n        if self.disk_size_gb:\n            output.append(f(self.dataflow_params.disk_size_gb, self.disk_size_gb))\n        if self.worker_machine_type:\n            output.append(f(self.dataflow_params.worker_machine_type, self.worker_machine_type))\n        if self.worker_disk_type:\n            output.append(f(self.dataflow_params.worker_disk_type, self.worker_disk_type))\n        if self.network:\n            output.append(f(self.dataflow_params.network, self.network))\n        if self.subnetwork:\n            output.append(f(self.dataflow_params.subnetwork, self.subnetwork))\n        if self.job_name:\n            output.append(f(self.dataflow_params.job_name, self.job_name))\n        if self.service_account:\n            output.append(f(self.dataflow_params.service_account, self.service_account))\n        if self.labels:\n            output.append(f(self.dataflow_params.labels, json.dumps(self.labels)))\n\n        return output", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Assembles a list of command-line arguments from the task's configuration attributes. This prepares the necessary parameters for launching a Google Cloud Dataflow job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_format_input_args", "line_number": 393, "body": "def _format_input_args(self):\n        \"\"\"\n            Parses the result(s) of self.input() into a string-serialized\n            key-value list passed to the Dataflow job. Valid inputs include:\n\n            return FooTarget()\n\n            return {\"input1\": FooTarget(), \"input2\": FooTarget2())\n\n            return (\"input\", FooTarget())\n\n            return [(\"input1\", FooTarget()), (\"input2\": FooTarget2())]\n\n            return [FooTarget(), FooTarget2()]\n\n            Unlabeled input are passed in with under the default key \"input\".\n        \"\"\"\n        job_input = self.input()\n\n        if isinstance(job_input, luigi.Target):\n            job_input = {\"input\": job_input}\n\n        elif isinstance(job_input, tuple):\n            job_input = {job_input[0]: job_input[1]}\n\n        elif isinstance(job_input, list):\n            if all(isinstance(item, tuple) for item in job_input):\n                job_input = dict(job_input)\n            else:\n                job_input = {\"input\": job_input}\n\n        elif not isinstance(job_input, dict):\n            raise ValueError(\"Invalid job input requires(). Supported types: [\"\n                             \"Target, tuple of (name, Target), \"\n                             \"dict of (name: Target), list of Targets]\")\n\n        if not isinstance(self.file_pattern(), dict):\n            raise ValueError('file_pattern() must return a dict type')\n\n        input_args = []\n\n        for (name, targets) in job_input.items():\n            uris = [\n              self.get_target_path(uri_target) for uri_target in luigi.task.flatten(targets)\n            ]\n            if isinstance(targets, dict):\n                \"\"\"\n                If targets is a dict that means it had multiple outputs.\n                Make the input args in that case \"<input key>-<task output key>\"\n                \"\"\"\n                names = [\"%s-%s\" % (name, key) for key in targets.keys()]\n\n            else:\n                names = [name] * len(uris)\n\n            input_dict = {}\n\n            for (arg_name, uri) in zip(names, uris):\n                pattern = self.file_pattern().get(name, 'part-*')\n                input_value = input_dict.get(arg_name, [])\n                input_value.append(uri.rstrip('/') + '/' + pattern)\n                input_dict[arg_name] = input_value\n\n            for (key, paths) in input_dict.items():\n                input_args.append(\"--%s=%s\" % (key, ','.join(paths)))\n\n        return input_args", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Prepares a task's Luigi Target inputs as string-serialized command-line arguments for a Dataflow job. It converts various input formats into standardized URI paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "_format_output_args", "line_number": 461, "body": "def _format_output_args(self):\n        \"\"\"\n            Parses the result(s) of self.output() into a string-serialized\n            key-value list passed to the Dataflow job. Valid outputs include:\n\n            return FooTarget()\n\n            return {\"output1\": FooTarget(), \"output2\": FooTarget2()}\n\n            Unlabeled outputs are passed in with under the default key \"output\".\n        \"\"\"\n        job_output = self.output()\n        if isinstance(job_output, luigi.Target):\n            job_output = {\"output\": job_output}\n        elif not isinstance(job_output, dict):\n            raise ValueError(\n                \"Task output must be a Target or a dict from String to Target\")\n\n        output_args = []\n\n        for (name, target) in job_output.items():\n            uri = self.get_target_path(target)\n            output_args.append(\"--%s=%s\" % (name, uri))\n\n        return output_args", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Converts the task's output targets into command-line argument strings. This prepares the output paths for a Beam Dataflow job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "get_target_path", "line_number": 488, "body": "def get_target_path(target):\n        \"\"\"\n            Given a luigi Target, determine a stringly typed path to pass as a\n            Dataflow job argument.\n        \"\"\"\n        if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n            return target.path\n        elif isinstance(target, bigquery.BigQueryTarget):\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n        else:\n            raise ValueError(\"Target %s not supported\" % target)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Determines a standardized string path for various Luigi Target objects (local, GCS, BigQuery) to be used as Beam Dataflow job arguments. It enables seamless integration of Luigi targets with Dataflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/beam_dataflow.py", "function": "f", "line_number": 349, "body": "def f(key, value):\n            return '--{}={}'.format(key, value)", "is_method": true, "class_name": "BeamDataflowJobTask", "function_description": "Formats a key-value pair into a standard command-line argument string (`--key=value`). This helper likely constructs job parameters for Beam Dataflow tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sparkey.py", "function": "run", "line_number": 41, "body": "def run(self):\n        self._write_sparkey_file()", "is_method": true, "class_name": "SparkeyExportTask", "function_description": "Executes the core Sparkey file export operation. This method serves as the primary entry point for the `SparkeyExportTask`."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/sparkey.py", "function": "_write_sparkey_file", "line_number": 44, "body": "def _write_sparkey_file(self):\n        import sparkey\n\n        infile = self.input()\n        outfile = self.output()\n        if not isinstance(outfile, luigi.LocalTarget):\n            raise TypeError(\"output must be a LocalTarget\")\n\n        # write job output to temporary sparkey file\n        temp_output = luigi.LocalTarget(is_tmp=True)\n        w = sparkey.LogWriter(temp_output.path)\n        for line in infile.open('r'):\n            k, v = line.strip().split(self.separator, 1)\n            w[k] = v\n        w.close()\n\n        # move finished sparkey file to final destination\n        temp_output.move(outfile.path)", "is_method": true, "class_name": "SparkeyExportTask", "function_description": "As part of the SparkeyExportTask, this method converts key-value data from an input file into a Sparkey database file, enabling efficient and fast data lookups."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "slot_to_dict", "line_number": 51, "body": "def slot_to_dict(o):\n    o_dict = {}\n    for key in o.__slots__:\n        if not key.startswith('__'):\n            value = getattr(o, key, None)\n            if value is not None:\n                o_dict[key] = value\n    return o_dict", "is_method": false, "function_description": "This function converts an object's attributes defined in `__slots__` into a dictionary. It provides a standard dictionary representation of a slotted object's state."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "name", "line_number": 186, "body": "def name(self):\n        \"\"\"Name for the job, need to be unique, required\"\"\"\n        return 'SklearnExample'", "is_method": true, "class_name": "PaiTask", "function_description": "This method provides a unique, hardcoded name 'SklearnExample' for the PAI job. It serves as a default identifier for the task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "image", "line_number": 192, "body": "def image(self):\n        \"\"\"URL pointing to the Docker image for all tasks in the job, required\"\"\"\n        return 'openpai/pai.example.sklearn'", "is_method": true, "class_name": "PaiTask", "function_description": "This method defines the Docker image URL required for all tasks within the PAI job. It provides the default container environment for task execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "tasks", "line_number": 198, "body": "def tasks(self):\n        \"\"\"List of taskRole, one task role at least, required\"\"\"\n        return []", "is_method": true, "class_name": "PaiTask", "function_description": "Not Implemented\nChain of Thought:\nThe user wants a concise description for the `tasks` method of the `PaiTask` class.\n\n1.  **Understand the function:**\n    *   It's a method `tasks(self)`.\n    *   It has a docstring: `\"List of taskRole, one task role at least, required\"`.\n    *   The implementation is `return []`.\n\n2.  **Analyze the primary purpose/service:**\n    *   The docstring explicitly states that it should return a \"List of taskRole\" and that \"one task role at least\" is \"required\".\n    *   However, the actual implementation simply returns an empty list `[]`.\n\n3.  **Evaluate against description requirements:**\n    *   The function's implementation (`return []`) directly contradicts its own docstring, which implies it should provide a non-empty list of task roles.\n    *   This discrepancy strongly indicates that the function is a placeholder or is not yet fully implemented according to its stated purpose. It's not providing the service it claims to provide.\n\n4.  **Decide on the special return value:**\n    *   Since it's a placeholder or not implemented function (returning an empty list despite requirements), the correct output is \"Not Implemented\".\nNot Implemented"}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "output_dir", "line_number": 218, "body": "def output_dir(self):\n        \"\"\"Output directory on HDFS, $PAI_DEFAULT_FS_URI/$jobName/output will be used if not specified, optional\"\"\"\n        return '$PAI_DEFAULT_FS_URI/{0}/output'.format(self.name)", "is_method": true, "class_name": "PaiTask", "function_description": "Returns the default HDFS output directory path for the PAI task, typically used for storing job results."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "virtual_cluster", "line_number": 223, "body": "def virtual_cluster(self):\n        \"\"\"The virtual cluster job runs on. If omitted, the job will run on default virtual cluster, optional\"\"\"\n        return 'default'", "is_method": true, "class_name": "PaiTask", "function_description": "This method provides the default virtual cluster name for a PAI task. It is used if no specific cluster is configured for the job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "__init_token", "line_number": 237, "body": "def __init_token(self):\n        self.__openpai = OpenPai()\n\n        request_json = json.dumps({'username': self.__openpai.username, 'password': self.__openpai.password,\n                                   'expiration': self.__openpai.expiration})\n        logger.debug('Get token request {0}'.format(request_json))\n        response = rs.post(urljoin(self.__openpai.pai_url, '/api/v1/token'),\n                           headers={'Content-Type': 'application/json'}, data=request_json)\n        logger.debug('Get token response {0}'.format(response.text))\n        if response.status_code != 200:\n            msg = 'Get token request failed, response is {}'.format(response.text)\n            logger.error(msg)\n            raise Exception(msg)\n        else:\n            self.__token = response.json()['token']", "is_method": true, "class_name": "PaiTask", "function_description": "Authenticates with the OpenPai service to acquire and store an access token. This token enables subsequent authorized API requests."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "__check_job_status", "line_number": 261, "body": "def __check_job_status(self):\n        response = rs.get(urljoin(self.__openpai.pai_url, '/api/v1/jobs/{0}'.format(self.name)))\n        logger.debug('Check job response {0}'.format(response.text))\n        if response.status_code == 404:\n            msg = 'Job {0} is not found'.format(self.name)\n            logger.debug(msg)\n            raise HTTPError(msg, response=response)\n        elif response.status_code != 200:\n            msg = 'Get job request failed, response is {}'.format(response.text)\n            logger.error(msg)\n            raise HTTPError(msg, response=response)\n        job_state = response.json()['jobStatus']['state']\n        if job_state in ['UNKNOWN', 'WAITING', 'RUNNING']:\n            logger.debug('Job {0} is running in state {1}'.format(self.name, job_state))\n            return False\n        else:\n            msg = 'Job {0} finished in state {1}'.format(self.name, job_state)\n            logger.info(msg)\n            if job_state == 'SUCCEED':\n                return True\n            else:\n                raise RuntimeError(msg)", "is_method": true, "class_name": "PaiTask", "function_description": "Queries the OpenPAI service to determine the current state of the task's associated job. It indicates if the job is still active, completed successfully, or failed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "run", "line_number": 284, "body": "def run(self):\n        job = PaiJob(self.name, self.image, self.tasks)\n        job.virtualCluster = self.virtual_cluster\n        job.authFile = self.auth_file_path\n        job.codeDir = self.code_dir\n        job.dataDir = self.data_dir\n        job.outputDir = self.output_dir\n        job.retryCount = self.retry_count\n        job.gpuType = self.gpu_type\n        request_json = json.dumps(job,  default=slot_to_dict)\n        logger.debug('Submit job request {0}'.format(request_json))\n        response = rs.post(urljoin(self.__openpai.pai_url, '/api/v1/jobs'),\n                           headers={'Content-Type': 'application/json',\n                                    'Authorization': 'Bearer {}'.format(self.__token)}, data=request_json)\n        logger.debug('Submit job response {0}'.format(response.text))\n        # 202 is success for job submission, see https://github.com/Microsoft/pai/blob/master/docs/rest-server/API.md\n        if response.status_code != 202:\n            msg = 'Submit job failed, response code is {0}, body is {1}'.format(response.status_code, response.text)\n            logger.error(msg)\n            raise HTTPError(msg, response=response)\n        while not self.__check_job_status():\n            time.sleep(self.__POLL_TIME)", "is_method": true, "class_name": "PaiTask", "function_description": "Submits the defined AI job to an OpenPAI cluster for execution. It then continuously monitors the job status until completion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pai.py", "function": "complete", "line_number": 310, "body": "def complete(self):\n        try:\n            return self.__check_job_status()\n        except HTTPError:\n            return False\n        except RuntimeError:\n            return False", "is_method": true, "class_name": "PaiTask", "function_description": "Determines if the associated Pai task has completed its execution. It returns True on success or False if status retrieval fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "accept_trailing_slash_in_existing_dirpaths", "line_number": 41, "body": "def accept_trailing_slash_in_existing_dirpaths(func):\n    @wraps(func)\n    def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            logger.warning(\"Dropbox paths should NOT have trailing slashes. This causes additional API calls\")\n            logger.warning(\"Consider modifying your calls to {}, so that they don't use paths than end with '/'\".format(func.__name__))\n\n            if self._exists_and_is_dir(path[:-1]):\n                path = path[:-1]\n\n        return func(self, path, *args, **kwargs)\n\n    return wrapped", "is_method": false, "function_description": "This decorator transparently removes trailing slashes from existing directory paths before passing them to the decorated function, while also issuing warnings. It ensures consistent path handling and prevents redundant API calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "accept_trailing_slash", "line_number": 56, "body": "def accept_trailing_slash(func):\n    @wraps(func)\n    def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            path = path[:-1]\n        return func(self, path, *args, **kwargs)\n\n    return wrapped", "is_method": false, "function_description": "A decorator that normalizes path arguments for a function by removing a trailing slash, unless the path is the root '/'. It ensures consistent path handling for decorated functions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "wrapped", "line_number": 43, "body": "def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            logger.warning(\"Dropbox paths should NOT have trailing slashes. This causes additional API calls\")\n            logger.warning(\"Consider modifying your calls to {}, so that they don't use paths than end with '/'\".format(func.__name__))\n\n            if self._exists_and_is_dir(path[:-1]):\n                path = path[:-1]\n\n        return func(self, path, *args, **kwargs)", "is_method": false, "function_description": "This wrapper function standardizes Dropbox paths by removing trailing slashes. It optimizes API calls and issues warnings about non-compliant path formats."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "wrapped", "line_number": 58, "body": "def wrapped(self, path, *args, **kwargs):\n        if path != '/' and path.endswith('/'):\n            path = path[:-1]\n        return func(self, path, *args, **kwargs)", "is_method": false, "function_description": "Provides path normalization by stripping trailing slashes from non-root paths before invoking the wrapped function. This ensures consistent path handling for routing or resource access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "exists", "line_number": 87, "body": "def exists(self, path):\n        if path == '/':\n            return True\n        if path.endswith('/'):\n            path = path[:-1]\n            return self._exists_and_is_dir(path)\n\n        try:\n            self.conn.files_get_metadata(path)\n            return True\n        except dropbox.exceptions.ApiError as e:\n            if isinstance(e.error.get_path(), dropbox.files.LookupError):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "DropboxClient", "function_description": "Checks if a file or folder exists at the specified path on the Dropbox service. It returns true if found, false otherwise."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "remove", "line_number": 104, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        if not self.exists(path):\n            return False\n        self.conn.files_delete_v2(path)\n        return True", "is_method": true, "class_name": "DropboxClient", "function_description": "Removes a specified file or folder from Dropbox. It ensures the path exists before performing a permanent deletion, not moving to trash."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "mkdir", "line_number": 111, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if self.exists(path):\n            if not self.isdir(path):\n                raise luigi.target.NotADirectory()\n            elif raise_if_exists:\n                raise luigi.target.FileAlreadyExists()\n            else:\n                return\n\n        self.conn.files_create_folder_v2(path)", "is_method": true, "class_name": "DropboxClient", "function_description": "Creates a new folder at the specified path on the connected Dropbox account. It handles cases where a folder or file already exists at that location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "isdir", "line_number": 123, "body": "def isdir(self, path):\n        if path == '/':\n            return True\n        try:\n            md = self.conn.files_get_metadata(path)\n            return isinstance(md, dropbox.files.FolderMetadata)\n        except dropbox.exceptions.ApiError as e:\n            if isinstance(e.error.get_path(), dropbox.files.LookupError):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "DropboxClient", "function_description": "Checks if a specified path on Dropbox points to a directory. It accurately identifies existing folders, including the root, returning false for files or non-existent paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "listdir", "line_number": 136, "body": "def listdir(self, path, **kwargs):\n        dirs = []\n        lister = self.conn.files_list_folder(path, recursive=True, **kwargs)\n        dirs.extend(lister.entries)\n        while lister.has_more:\n            lister = self.conn.files_list_folder_continue(lister.cursor)\n            dirs.extend(lister.entries)\n        return [d.path_display for d in dirs]", "is_method": true, "class_name": "DropboxClient", "function_description": "This method lists all files and folders, including subdirectories, from a specified path on Dropbox. It retrieves the complete directory contents, handling pagination automatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "move", "line_number": 146, "body": "def move(self, path, dest):\n        self.conn.files_move_v2(from_path=path, to_path=dest)", "is_method": true, "class_name": "DropboxClient", "function_description": "The `move` method of `DropboxClient` relocates a file or folder from a specified path to a new destination within a user's Dropbox account. It provides a convenient way to organize or rename items."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "copy", "line_number": 150, "body": "def copy(self, path, dest):\n        self.conn.files_copy_v2(from_path=path, to_path=dest)", "is_method": true, "class_name": "DropboxClient", "function_description": "This method copies a file or folder from a specified path to a destination path within the user's Dropbox account."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "download_as_bytes", "line_number": 153, "body": "def download_as_bytes(self, path):\n        metadata, response = self.conn.files_download(path)\n        return response.content", "is_method": true, "class_name": "DropboxClient", "function_description": "This method downloads a file from Dropbox at the specified path. It provides the capability to retrieve the file's raw binary content."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "upload", "line_number": 157, "body": "def upload(self, tmp_path, dest_path):\n        with open(tmp_path, 'rb') as f:\n            file_size = os.path.getsize(tmp_path)\n\n            CHUNK_SIZE = 4 * 1000 * 1000\n            upload_session_start_result = self.conn.files_upload_session_start(f.read(CHUNK_SIZE))\n            commit = dropbox.files.CommitInfo(path=dest_path)\n            cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id,\n                                                       offset=f.tell())\n\n            if f.tell() >= file_size:\n                self.conn.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)\n                return\n\n            while f.tell() < file_size:\n                if (file_size - f.tell()) <= CHUNK_SIZE:\n                    self.conn.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)\n                else:\n                    self.conn.files_upload_session_append_v2(f.read(CHUNK_SIZE), cursor)\n                    cursor.offset = f.tell()", "is_method": true, "class_name": "DropboxClient", "function_description": "Enables the secure and efficient transfer of a local file to a specified destination path within the Dropbox cloud storage. It manages chunked uploads for large files automatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "_exists_and_is_dir", "line_number": 178, "body": "def _exists_and_is_dir(self, path):\n        \"\"\"\n        Auxiliary method, used by the 'accept_trailing_slash' and 'accept_trailing_slash_in_existing_dirpaths' decorators\n        :param path: a Dropbox path that does NOT ends with a '/' (even if it is a directory)\n        \"\"\"\n        if path == '/':\n            return True\n        try:\n            md = self.conn.files_get_metadata(path)\n            is_dir = isinstance(md, dropbox.files.FolderMetadata)\n            return is_dir\n        except dropbox.exceptions.ApiError:\n            return False", "is_method": true, "class_name": "DropboxClient", "function_description": "Determines if a given Dropbox path corresponds to an existing directory within the Dropbox file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "read", "line_number": 209, "body": "def read(self):\n        return self.client.download_as_bytes(self.path)", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "This method of `ReadableDropboxFile` retrieves the entire content of the associated Dropbox file. It returns the file's data as raw bytes, ready for processing."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__enter__", "line_number": 212, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Prepares the `ReadableDropboxFile` instance for use within a `with` statement, making it available as a context manager."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__exit__", "line_number": 215, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "This method ensures the Dropbox file resource is automatically closed when exiting a `with` statement, preventing resource leaks and ensuring proper cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "__del__", "line_number": 218, "body": "def __del__(self):\n        self.close()\n        if os.path.exists(self.download_file_location):\n            os.remove(self.download_file_location)", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "This method ensures proper resource cleanup for `ReadableDropboxFile` objects. It closes the file and deletes the temporary local download when the object is destroyed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "close", "line_number": 223, "body": "def close(self):\n        self.closed = True", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "This method marks the Dropbox file as closed. It signals that the file is no longer active for operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "readable", "line_number": 226, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Indicates that this Dropbox file object is inherently readable, serving as a capability flag for operations requiring read access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "writable", "line_number": 229, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "Returns `False` to indicate that this `ReadableDropboxFile` instance is not writable. It serves as a property to check if write operations are supported."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "seekable", "line_number": 232, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableDropboxFile", "function_description": "It signals that the Dropbox file stream does not support arbitrary seek operations, meaning its read position cannot be changed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "move_to_final_destination", "line_number": 248, "body": "def move_to_final_destination(self):\n        \"\"\"\n        After editing the file locally, this function uploads it to the Dropbox cloud\n        \"\"\"\n        self.client.upload(self.tmp_path, self.path)", "is_method": true, "class_name": "AtomicWritableDropboxFile", "function_description": "Uploads the locally modified file to its final destination on Dropbox. This action finalizes the atomic write operation, committing changes to the cloud."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "fs", "line_number": 302, "body": "def fs(self):\n        return self.client", "is_method": true, "class_name": "DropboxTarget", "function_description": "Provides direct access to the underlying Dropbox client object. This enables other parts of the system to interact with the Dropbox file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "temporary_path", "line_number": 306, "body": "def temporary_path(self):\n        tmp_dir = tempfile.mkdtemp()\n        num = random.randrange(0, 1e10)\n        temp_path = '{}{}luigi-tmp-{:010}{}'.format(\n            tmp_dir, os.sep,\n            num, ntpath.basename(self.path))\n\n        yield temp_path\n        # We won't reach here if there was an user exception.\n        self.fs.upload(temp_path, self.path)", "is_method": true, "class_name": "DropboxTarget", "function_description": "This method provides a temporary file path for writing data. After processing, the data is automatically uploaded to the final Dropbox target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/dropbox.py", "function": "open", "line_number": 317, "body": "def open(self, mode):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableDropboxFile(self.path, self.client))\n        else:\n            return self.format.pipe_writer(AtomicWritableDropboxFile(self.path, self.client))", "is_method": true, "class_name": "DropboxTarget", "function_description": "Provides file-like access to a Dropbox path for reading or writing. It returns a reader or writer object tailored to the configured data format, enabling stream-based operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "tracking_url_pattern", "line_number": 60, "body": "def tracking_url_pattern(self):\n        if self.deploy_mode == \"cluster\":\n            # in cluster mode client only receives application status once a period of time\n            return r\"tracking URL: (https?://.*)\\s\"\n        else:\n            return r\"Bound (?:.*) to (?:.*), and started at (https?://.*)\\s\"", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Provides a regular expression pattern to extract the Spark application's tracking URL. It adapts the pattern based on the Spark deployment mode."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "spark_submit", "line_number": 91, "body": "def spark_submit(self):\n        return configuration.get_config().get(self.spark_version, 'spark-submit', 'spark-submit')", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method retrieves the `spark-submit` command or path configured for the specified Spark version. It enables the execution of Spark applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "master", "line_number": 95, "body": "def master(self):\n        return configuration.get_config().get(self.spark_version, \"master\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Provides the configured Spark master URL specific to the task's Spark version. This enables the SparkSubmitTask to connect to the correct Spark cluster."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "deploy_mode", "line_number": 99, "body": "def deploy_mode(self):\n        return configuration.get_config().get(self.spark_version, \"deploy-mode\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the Spark deployment mode for the task's specified Spark version from the application configuration. This setting determines how the Spark application will be launched."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "jars", "line_number": 103, "body": "def jars(self):\n        return self._list_config(configuration.get_config().get(self.spark_version, \"jars\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Provides the list of external JAR file paths required for a Spark job submission. It fetches these paths from the task's specific Spark version configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "packages", "line_number": 107, "body": "def packages(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"packages\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method retrieves the list of Spark packages configured for the task's specific Spark version. It ensures that required dependencies are available for Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "py_files", "line_number": 112, "body": "def py_files(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"py-files\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the list of Python files configured for the task's specific Spark version. This information is crucial for including necessary Python dependencies in a Spark submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "files", "line_number": 117, "body": "def files(self):\n        return self._list_config(configuration.get_config().get(self.spark_version, \"files\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the list of files configured for a specific Spark version, essential for preparing the Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_conf", "line_number": 121, "body": "def _conf(self):\n        conf = collections.OrderedDict(self.conf or {})\n        if self.pyspark_python:\n            conf['spark.pyspark.python'] = self.pyspark_python\n        if self.pyspark_driver_python:\n            conf['spark.pyspark.driver.python'] = self.pyspark_driver_python\n        return conf", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This internal helper method of SparkSubmitTask prepares a Spark configuration dictionary. It consolidates and adds PySpark-specific Python interpreter settings to the configuration for job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "conf", "line_number": 130, "body": "def conf(self):\n        return self._dict_config(configuration.get_config().get(self.spark_version, \"conf\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the Spark configuration dictionary relevant to the task's specified Spark version. It provides the necessary parameters for Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "properties_file", "line_number": 134, "body": "def properties_file(self):\n        return configuration.get_config().get(self.spark_version, \"properties-file\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method retrieves the path to the Spark properties file associated with the task's configured Spark version. It provides essential configuration for Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_memory", "line_number": 138, "body": "def driver_memory(self):\n        return configuration.get_config().get(self.spark_version, \"driver-memory\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured driver memory setting for a Spark job based on its Spark version. This provides a crucial parameter for submitting Spark applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_java_options", "line_number": 142, "body": "def driver_java_options(self):\n        return configuration.get_config().get(self.spark_version, \"driver-java-options\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method retrieves the specific Java options configured for the Spark driver component. It allows customizing the JVM behavior of the Spark driver based on the Spark version."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_library_path", "line_number": 146, "body": "def driver_library_path(self):\n        return configuration.get_config().get(self.spark_version, \"driver-library-path\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method retrieves the configured driver library path specific to the associated Spark version. It provides critical path information for Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_class_path", "line_number": 150, "body": "def driver_class_path(self):\n        return configuration.get_config().get(self.spark_version, \"driver-class-path\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured driver class path for a Spark application. This value helps the Spark runtime locate necessary Java classes for the driver."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "executor_memory", "line_number": 154, "body": "def executor_memory(self):\n        return configuration.get_config().get(self.spark_version, \"executor-memory\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured memory allocation for Spark executors based on the task's Spark version. This value is used for configuring Spark job resource requirements."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "driver_cores", "line_number": 158, "body": "def driver_cores(self):\n        return configuration.get_config().get(self.spark_version, \"driver-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured number of CPU cores for the Spark driver based on the specified Spark version. This setting is used to properly configure Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "supervise", "line_number": 162, "body": "def supervise(self):\n        return bool(configuration.get_config().get(self.spark_version, \"supervise\", False))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the supervision status for the task's configured Spark version from global settings. It returns true if supervision is enabled, false otherwise."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "total_executor_cores", "line_number": 166, "body": "def total_executor_cores(self):\n        return configuration.get_config().get(self.spark_version, \"total-executor-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the total number of executor cores configured for the task's specific Spark version. This provides a key resource allocation detail for Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "executor_cores", "line_number": 170, "body": "def executor_cores(self):\n        return configuration.get_config().get(self.spark_version, \"executor-cores\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured number of executor cores for the Spark application's specified version. This provides a key resource setting for Spark job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "queue", "line_number": 174, "body": "def queue(self):\n        return configuration.get_config().get(self.spark_version, \"queue\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured Spark queue name for the specific Spark version this task targets. This is essential for proper job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "num_executors", "line_number": 178, "body": "def num_executors(self):\n        return configuration.get_config().get(self.spark_version, \"num-executors\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured number of Spark executors for the task's specific Spark version. Essential for defining resource allocation during job submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "archives", "line_number": 182, "body": "def archives(self):\n        return self._list_config(configuration.get_config().get(\n            self.spark_version, \"archives\", None))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the configured list of archive paths for the specific Spark version associated with this submission task."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "hadoop_conf_dir", "line_number": 187, "body": "def hadoop_conf_dir(self):\n        return configuration.get_config().get(self.spark_version, \"hadoop-conf-dir\", None)", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Retrieves the Hadoop configuration directory path for the current Spark version. This configuration is crucial for Spark tasks interacting with Hadoop services."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "get_environment", "line_number": 190, "body": "def get_environment(self):\n        env = os.environ.copy()\n        for prop in ('HADOOP_CONF_DIR', 'HADOOP_USER_NAME'):\n            var = getattr(self, prop.lower(), None)\n            if var:\n                env[prop] = var\n        return env", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method prepares an environment dictionary for a Spark submission. It incorporates task-specific Hadoop configuration paths and user names, ensuring the Spark job runs with the necessary context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "program_environment", "line_number": 198, "body": "def program_environment(self):\n        return self.get_environment()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method provides the execution environment configuration for the Spark program managed by the task. It allows other components to access the settings relevant to the job's runtime."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "program_args", "line_number": 201, "body": "def program_args(self):\n        return self.spark_command() + self.app_command()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method of SparkSubmitTask provides the combined set of command-line arguments required for submitting a Spark application, including both Spark-specific and application-specific parameters."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "spark_command", "line_number": 204, "body": "def spark_command(self):\n        command = [self.spark_submit]\n        command += self._text_arg('--master', self.master)\n        command += self._text_arg('--deploy-mode', self.deploy_mode)\n        command += self._text_arg('--name', self.name)\n        command += self._text_arg('--class', self.entry_class)\n        command += self._list_arg('--jars', self.jars)\n        command += self._list_arg('--packages', self.packages)\n        command += self._list_arg('--py-files', self.py_files)\n        command += self._list_arg('--files', self.files)\n        command += self._list_arg('--archives', self.archives)\n        command += self._dict_arg('--conf', self._conf)\n        command += self._text_arg('--properties-file', self.properties_file)\n        command += self._text_arg('--driver-memory', self.driver_memory)\n        command += self._text_arg('--driver-java-options', self.driver_java_options)\n        command += self._text_arg('--driver-library-path', self.driver_library_path)\n        command += self._text_arg('--driver-class-path', self.driver_class_path)\n        command += self._text_arg('--executor-memory', self.executor_memory)\n        command += self._text_arg('--driver-cores', self.driver_cores)\n        command += self._flag_arg('--supervise', self.supervise)\n        command += self._text_arg('--total-executor-cores', self.total_executor_cores)\n        command += self._text_arg('--executor-cores', self.executor_cores)\n        command += self._text_arg('--queue', self.queue)\n        command += self._text_arg('--num-executors', self.num_executors)\n        return command", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Constructs the full `spark-submit` command-line arguments array, incorporating all configured parameters for a Spark job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "app_command", "line_number": 230, "body": "def app_command(self):\n        if not self.app:\n            raise NotImplementedError(\"subclass should define an app (.jar or .py file)\")\n        return [self.app] + self.app_options()", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Assembles the command-line argument list for a Spark application. It combines the application file path with any specified options, ready for submission."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_list_config", "line_number": 235, "body": "def _list_config(self, config):\n        if config and isinstance(config, str):\n            return list(map(lambda x: x.strip(), config.split(',')))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "A utility method that parses a comma-separated string configuration into a list of cleaned string values. It enables easy consumption of multi-value string settings."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dict_config", "line_number": 239, "body": "def _dict_config(self, config):\n        if config and isinstance(config, str):\n            return dict(map(lambda i: i.split('=', 1), config.split('|')))", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Converts a pipe-separated string of key-value pairs into a dictionary. This internal utility processes configuration strings for Spark job submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_text_arg", "line_number": 243, "body": "def _text_arg(self, name, value):\n        if value:\n            return [name, value]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "It formats a command-line argument and its value into a list if the value is present. This is used to conditionally include arguments for Spark submissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_list_arg", "line_number": 248, "body": "def _list_arg(self, name, value):\n        if value and isinstance(value, (list, tuple)):\n            return [name, ','.join(value)]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This private helper method formats a list or tuple of values into a comma-separated string, paired with its argument name. It prepares arguments for submission to Spark, where multiple values for an option are needed."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dict_arg", "line_number": 253, "body": "def _dict_arg(self, name, value):\n        command = []\n        if value and isinstance(value, dict):\n            for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n        return command", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "Converts a dictionary into a list of formatted command-line arguments, suitable for passing configuration properties to Spark submit commands."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_flag_arg", "line_number": 260, "body": "def _flag_arg(self, name, value):\n        if value:\n            return [name]\n        return []", "is_method": true, "class_name": "SparkSubmitTask", "function_description": "This method prepares a command-line flag argument. It includes the flag name in a list only if the provided value is true, aiding `spark-submit` command construction."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "name", "line_number": 281, "body": "def name(self):\n        return self.__class__.__name__", "is_method": true, "class_name": "PySparkTask", "function_description": "Provides the class name of the PySpark task. This can be used for identification or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "py_packages", "line_number": 285, "body": "def py_packages(self):\n        packages = configuration.get_config().get('spark', 'py-packages', None)\n        if packages:\n            return map(lambda s: s.strip(), packages.split(','))", "is_method": true, "class_name": "PySparkTask", "function_description": "Retrieves configured Python package names required for PySpark task execution. It parses a comma-separated string from the Spark configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "files", "line_number": 291, "body": "def files(self):\n        if self.deploy_mode == \"cluster\":\n            return [self.run_pickle]", "is_method": true, "class_name": "PySparkTask", "function_description": "This method of `PySparkTask` identifies and returns a list of files required for the task to run specifically when deployed in \"cluster\" mode. It specifies resources needed for distribution to Spark worker nodes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "setup", "line_number": 295, "body": "def setup(self, conf):\n        \"\"\"\n        Called by the pyspark_runner with a SparkConf instance that will be used to instantiate the SparkContext\n\n        :param conf: SparkConf\n        \"\"\"", "is_method": true, "class_name": "PySparkTask", "function_description": "Initializes the PySpark task by receiving the SparkConf instance. This prepares the task's environment before the SparkContext is instantiated.\nInitializes the PySpark task by receiving the SparkConf instance. This prepares the task's environment before the SparkContext is instantiated."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "setup_remote", "line_number": 302, "body": "def setup_remote(self, sc):\n        self._setup_packages(sc)", "is_method": true, "class_name": "PySparkTask", "function_description": "Prepares the remote Spark environment to ensure all necessary Python packages are available for the task's execution on worker nodes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "app_command", "line_number": 314, "body": "def app_command(self):\n        if self.deploy_mode == \"cluster\":\n            pickle_loc = os.path.basename(self.run_pickle)\n        else:\n            pickle_loc = self.run_pickle\n        return [self.app, pickle_loc] + self.app_options()", "is_method": true, "class_name": "PySparkTask", "function_description": "This method prepares the core command components required to execute a PySpark application. It dynamically determines the path to the application's pickled entry point based on the deployment mode."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "run", "line_number": 321, "body": "def run(self):\n        path_name_fragment = re.sub(r'[^\\w]', '_', self.name)\n        self.run_path = tempfile.mkdtemp(prefix=path_name_fragment)\n        self.run_pickle = os.path.join(self.run_path, '.'.join([path_name_fragment, 'pickle']))\n        with open(self.run_pickle, 'wb') as fd:\n            # Copy module file to run path.\n            module_path = os.path.abspath(inspect.getfile(self.__class__))\n            shutil.copy(module_path, os.path.join(self.run_path, '.'))\n            self._dump(fd)\n        try:\n            super(PySparkTask, self).run()\n        finally:\n            shutil.rmtree(self.run_path)", "is_method": true, "class_name": "PySparkTask", "function_description": "Sets up an isolated, temporary environment and marshals necessary files for a PySpark task. It executes the task and ensures the temporary directory is properly cleaned up afterward."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_dump", "line_number": 335, "body": "def _dump(self, fd):\n        with self.no_unpicklable_properties():\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'c__main__', b'c' + module_name.encode('ascii'))\n                fd.write(d)\n            else:\n                pickle.dump(self, fd)", "is_method": true, "class_name": "PySparkTask", "function_description": "Serializes the `PySparkTask` instance to a file, enabling its transmission across a cluster. It addresses specific serialization issues for objects defined in the main script."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/spark.py", "function": "_setup_packages", "line_number": 345, "body": "def _setup_packages(self, sc):\n        \"\"\"\n        This method compresses and uploads packages to the cluster\n\n        \"\"\"\n        packages = self.py_packages\n        if not packages:\n            return\n        for package in packages:\n            mod = importlib.import_module(package)\n            try:\n                mod_path = mod.__path__[0]\n            except AttributeError:\n                mod_path = mod.__file__\n            tar_path = os.path.join(self.run_path, package + '.tar.gz')\n            tar = tarfile.open(tar_path, \"w:gz\")\n            tar.add(mod_path, os.path.basename(mod_path))\n            tar.close()\n            sc.addPyFile(tar_path)", "is_method": true, "class_name": "PySparkTask", "function_description": "Distributes specified Python packages to a PySpark cluster's worker nodes. It compresses each package and uploads it via the SparkContext, ensuring dependencies are available for tasks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_opener", "line_number": 89, "body": "def get_opener(self, name):\n        \"\"\"Retrieve an opener for the given protocol\n\n        :param name: name of the opener to open\n        :type name: string\n        :raises NoOpenerError: if no opener has been registered of that name\n\n        \"\"\"\n        if name not in self.registry:\n            raise NoOpenerError(\"No opener for %s\" % name)\n        index = self.registry[name]\n        return self.openers[index]", "is_method": true, "class_name": "OpenerRegistry", "function_description": "Retrieves a registered opener object based on its protocol name. It provides the correct handler for specific protocol operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "add", "line_number": 102, "body": "def add(self, opener):\n        \"\"\"Adds an opener to the registry\n\n        :param opener: Opener object\n        :type opener: Opener inherited object\n\n        \"\"\"\n\n        index = len(self.openers)\n        self.openers[index] = opener\n        for name in opener.names:\n            self.registry[name] = index", "is_method": true, "class_name": "OpenerRegistry", "function_description": "Registers a new opener object within the registry, mapping its associated names to the opener for easy lookup by other components. This makes the opener discoverable and usable."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "open", "line_number": 115, "body": "def open(self, target_uri, **kwargs):\n        \"\"\"Open target uri.\n\n        :param target_uri: Uri to open\n        :type target_uri: string\n\n        :returns: Target object\n\n        \"\"\"\n        target = urlsplit(target_uri, scheme=self.default_opener)\n\n        opener = self.get_opener(target.scheme)\n        query = opener.conform_query(target.query)\n\n        target = opener.get_target(\n            target.scheme,\n            target.path,\n            target.fragment,\n            target.username,\n            target.password,\n            target.hostname,\n            target.port,\n            query,\n            **kwargs\n        )\n        target.opener_path = target_uri\n\n        return target", "is_method": true, "class_name": "OpenerRegistry", "function_description": "The `OpenerRegistry`'s `open` method resolves a given URI to the appropriate handler. It processes the URI to return a target object representing the opened resource."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "conform_query", "line_number": 157, "body": "def conform_query(cls, query):\n        \"\"\"Converts the query string from a target uri, uses\n        cls.allowed_kwargs, and cls.filter_kwargs to drive logic.\n\n        :param query: Unparsed query string\n        :type query: urllib.parse.unsplit(uri).query\n        :returns: Dictionary of parsed values, everything in cls.allowed_kwargs\n            with values set to True will be parsed as json strings.\n\n        \"\"\"\n        query = parse_qs(query, keep_blank_values=True)\n\n        # Remove any unexpected keywords from the query string.\n        if cls.filter_kwargs:\n            query = {x: y for x, y in query.items() if x in cls.allowed_kwargs}\n\n        for key, vals in query.items():\n            # Multiple values of the same name could be passed use first\n            # Also params without strings will be treated as true values\n            if cls.allowed_kwargs.get(key, False):\n                val = json.loads(vals[0] or 'true')\n            else:\n                val = vals[0] or 'true'\n\n            query[key] = val\n\n        return query", "is_method": true, "class_name": "Opener", "function_description": "Converts a raw URI query string into a validated, structured dictionary. It filters parameters and parses values, optionally as JSON, based on class-defined rules."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 212, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        full_path = (hostname or '') + path\n        query.update(kwargs)\n        return MockTarget(full_path, **query)", "is_method": true, "class_name": "MockOpener", "function_description": "This class method creates a simulated target resource from provided URL components and additional parameters. It is useful for testing URL parsing and resource access logic without actual network or file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 237, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        full_path = (hostname or '') + path\n        query.update(kwargs)\n        return LocalTarget(full_path, **query)", "is_method": true, "class_name": "LocalOpener", "function_description": "This `LocalOpener` method constructs a `LocalTarget` object. It consolidates various URL components and additional parameters to define a specific local file system target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/opener.py", "function": "get_target", "line_number": 261, "body": "def get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        query.update(kwargs)\n        return S3Target('{scheme}://{hostname}{path}'.format(\n            scheme=scheme, hostname=hostname, path=path), **query)", "is_method": true, "class_name": "S3Opener", "function_description": "Constructs an `S3Target` object from parsed URL components, providing a standardized way to reference an S3 resource. This aids in preparing S3 paths for file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "connection", "line_number": 69, "body": "def connection(self):\n        return blockblobservice.BlockBlobService(account_name=self.options.get(\"account_name\"),\n                                                 account_key=self.options.get(\"account_key\"),\n                                                 sas_token=self.options.get(\"sas_token\"),\n                                                 protocol=self.kwargs.get(\"protocol\"),\n                                                 connection_string=self.kwargs.get(\"connection_string\"),\n                                                 endpoint_suffix=self.kwargs.get(\"endpoint_suffix\"),\n                                                 custom_domain=self.kwargs.get(\"custom_domain\"),\n                                                 is_emulated=self.kwargs.get(\"is_emulated\") or False)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides a configured client object for interacting with Azure Blob Storage. It returns a `BlockBlobService` instance, enabling operations on blobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "upload", "line_number": 79, "body": "def upload(self, tmp_path, container, blob, **kwargs):\n        logging.debug(\"Uploading file '{tmp_path}' to container '{container}' and blob '{blob}'\".format(\n            tmp_path=tmp_path, container=container, blob=blob))\n        self.create_container(container)\n        lease_id = self.connection.acquire_blob_lease(container, blob)\\\n            if self.exists(\"{container}/{blob}\".format(container=container, blob=blob)) else None\n        try:\n            self.connection.create_blob_from_path(container, blob, tmp_path, lease_id=lease_id, progress_callback=kwargs.get(\"progress_callback\"))\n        finally:\n            if lease_id is not None:\n                self.connection.release_blob_lease(container, blob, lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Uploads a local file to a specified blob within an Azure storage container. It manages container creation and ensures safe concurrent writes using blob leases."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "download_as_bytes", "line_number": 91, "body": "def download_as_bytes(self, container, blob, bytes_to_read=None):\n        start_range, end_range = (0, bytes_to_read-1) if bytes_to_read is not None else (None, None)\n        logging.debug(\"Downloading from container '{container}' and blob '{blob}' as bytes\".format(\n            container=container, blob=blob))\n        return self.connection.get_blob_to_bytes(container, blob, start_range=start_range, end_range=end_range).content", "is_method": true, "class_name": "AzureBlobClient", "function_description": "This method downloads the content of a specified Azure blob as raw bytes. It can retrieve the entire blob or a partial range of bytes."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "download_as_file", "line_number": 97, "body": "def download_as_file(self, container, blob, location):\n        logging.debug(\"Downloading from container '{container}' and blob '{blob}' to {location}\".format(\n            container=container, blob=blob, location=location))\n        return self.connection.get_blob_to_path(container, blob, location)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "This method downloads a specific blob from an Azure Blob Storage container and saves it to a local file, serving as a core utility for data retrieval."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "create_container", "line_number": 102, "body": "def create_container(self, container_name):\n        return self.connection.create_container(container_name)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Creates a new storage container in Azure Blob Storage. This enables organizing blobs and managing access permissions for data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "delete_container", "line_number": 105, "body": "def delete_container(self, container_name):\n        lease_id = self.connection.acquire_container_lease(container_name)\n        self.connection.delete_container(container_name, lease_id=lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides the AzureBlobClient with the capability to permanently delete a specified container from Azure Blob Storage."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "exists", "line_number": 109, "body": "def exists(self, path):\n        container, blob = self.splitfilepath(path)\n        return self.connection.exists(container, blob)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Checks if a specific file (blob) exists at the given path within the Azure Blob Storage container. It provides a quick way to verify resource availability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "remove", "line_number": 113, "body": "def remove(self, path, recursive=True, skip_trash=True):\n        container, blob = self.splitfilepath(path)\n        if not self.exists(path):\n            return False\n        lease_id = self.connection.acquire_blob_lease(container, blob)\n        self.connection.delete_blob(container, blob, lease_id=lease_id)\n        return True", "is_method": true, "class_name": "AzureBlobClient", "function_description": "This method removes a specified blob from Azure Blob Storage. It safely deletes the blob by first acquiring a lease and verifying the blob's existence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "isdir", "line_number": 127, "body": "def isdir(self, path):\n        \"\"\"\n        Azure Blob Storage has no concept of directories. It always returns False\n        :param str path: Path of the Azure blob storage\n        :return: False\n        \"\"\"\n        return False", "is_method": true, "class_name": "AzureBlobClient", "function_description": "This method informs callers that traditional directories do not exist in Azure Blob Storage, always returning False for any given path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "move", "line_number": 135, "body": "def move(self, path, dest):\n        try:\n            return self.copy(path, dest) and self.remove(path)\n        except IOError:\n            self.remove(dest)\n            return False", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Moves a blob within Azure Blob Storage by copying it to a new destination and then deleting the original, handling potential I/O errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "copy", "line_number": 142, "body": "def copy(self, path, dest):\n        source_container, source_blob = self.splitfilepath(path)\n        dest_container, dest_blob = self.splitfilepath(dest)\n        if source_container != dest_container:\n            raise Exception(\n                \"Can't copy blob from '{source_container}' to '{dest_container}'. File can be moved within container\".format(\n                    source_container=source_container, dest_container=dest_container\n                ))\n\n        source_lease_id = self.connection.acquire_blob_lease(source_container, source_blob)\n        destination_lease_id = self.connection.acquire_blob_lease(dest_container, dest_blob) if self.exists(dest) else None\n        try:\n            return self.connection.copy_blob(source_container, dest_blob, self.connection.make_blob_url(\n                source_container, source_blob),\n                destination_lease_id=destination_lease_id, source_lease_id=source_lease_id)\n        finally:\n            self.connection.release_blob_lease(source_container, source_blob, source_lease_id)\n            if destination_lease_id is not None:\n                self.connection.release_blob_lease(dest_container, dest_blob, destination_lease_id)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides the service to copy an Azure Blob Storage blob from a source path to a destination path. It strictly supports copying only within the same container and ensures data integrity by managing blob leases."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "rename_dont_move", "line_number": 162, "body": "def rename_dont_move(self, path, dest):\n        self.move(path, dest)", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Provides a mechanism to rename an Azure Blob without altering its storage location, by leveraging the client's internal move operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "splitfilepath", "line_number": 166, "body": "def splitfilepath(filepath):\n        splitpath = filepath.split(\"/\")\n        container = splitpath[0]\n        blobsplit = splitpath[1:]\n        blob = None if not blobsplit else \"/\".join(blobsplit)\n        return container, blob", "is_method": true, "class_name": "AzureBlobClient", "function_description": "Decomposes a given file path string into its primary container and the remaining blob path. This is useful for structured storage access, such as Azure Blobs."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "read", "line_number": 185, "body": "def read(self, n=None):\n        return self.client.download_as_bytes(self.container, self.blob, n)", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Provides the capability to read content from the Azure Blob file. It downloads a specified number of bytes or the entire file as raw byte data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__enter__", "line_number": 188, "body": "def __enter__(self):\n        if self.download_when_reading:\n            self.client.download_as_file(self.container, self.blob, self.download_file_location)\n            self.fid = open(self.download_file_location)\n            return self.fid\n        else:\n            return self", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Prepares the Azure Blob for reading within a `with` statement. It either downloads the blob to a local file or returns itself for direct content access."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__exit__", "line_number": 196, "body": "def __exit__(self, exc_type, exc, traceback):\n        self.close()", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Ensures the Azure blob file resource is automatically closed upon exiting a 'with' statement, providing reliable resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "__del__", "line_number": 199, "body": "def __del__(self):\n        self.close()\n        if os._exists(self.download_file_location):\n            os.remove(self.download_file_location)", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "This destructor method ensures proper resource cleanup for `ReadableAzureBlobFile` objects. It closes the file and removes any temporary downloaded local files, preventing resource leaks."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "close", "line_number": 204, "body": "def close(self):\n        if self.download_when_reading:\n            if self.fid is not None and not self.fid.closed:\n                self.fid.close()\n                self.fid = None", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Closes the underlying local file handle associated with the Azure blob if its content was downloaded for reading, releasing system resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "readable", "line_number": 210, "body": "def readable(self):\n        return True", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Indicates that this Azure blob file stream is readable, confirming its capability for read operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "writable", "line_number": 213, "body": "def writable(self):\n        return False", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "This method indicates that an instance of `ReadableAzureBlobFile` is not writable. It confirms the read-only nature of the file object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "seekable", "line_number": 216, "body": "def seekable(self):\n        return False", "is_method": true, "class_name": "ReadableAzureBlobFile", "function_description": "Indicates that this Azure Blob file stream does not support random access. It signals that the file can only be read sequentially from its current position."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "move_to_final_destination", "line_number": 231, "body": "def move_to_final_destination(self):\n        self.client.upload(self.tmp_path, self.container, self.blob, **self.azure_blob_options)", "is_method": true, "class_name": "AtomicAzureBlobFile", "function_description": "Moves a temporary file to its intended, final location within Azure Blob Storage. This action effectively commits the file after an atomic write operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "fs", "line_number": 269, "body": "def fs(self):\n        \"\"\"\n        The :py:class:`FileSystem` associated with :class:`.AzureBlobTarget`\n        \"\"\"\n        return self.client", "is_method": true, "class_name": "AzureBlobTarget", "function_description": "Provides access to the underlying file system object associated with the Azure Blob storage target. This enables file operations on the target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/azureblob.py", "function": "open", "line_number": 275, "body": "def open(self, mode):\n        \"\"\"\n        Open the target for reading or writing\n\n        :param char mode:\n            'r' for reading and 'w' for writing.\n\n            'b' is not supported and will be stripped if used. For binary mode, use `format`\n        :return:\n            * :class:`.ReadableAzureBlobFile` if 'r'\n            * :class:`.AtomicAzureBlobFile` if 'w'\n        \"\"\"\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableAzureBlobFile(self.container, self.blob, self.client, self.download_when_reading, **self.azure_blob_options))\n        else:\n            return self.format.pipe_writer(AtomicAzureBlobFile(self.container, self.blob, self.client, **self.azure_blob_options))", "is_method": true, "class_name": "AzureBlobTarget", "function_description": "Provides a file-like interface to an Azure Blob Storage object. It allows opening the blob for reading ('r') or writing ('w'), returning appropriate file handler objects."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_pyspark_runner_with", "line_number": 115, "body": "def _pyspark_runner_with(name, entry_point_class):\n    return type(name, (AbstractPySparkRunner,), {'_entry_point_class': entry_point_class})", "is_method": false, "function_description": "Dynamically generates a PySpark runner class that inherits from `AbstractPySparkRunner` and is configured with a specified entry point for job execution."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_use_spark_session", "line_number": 123, "body": "def _use_spark_session():\n    return bool(configuration.get_config().get('pyspark_runner', \"use_spark_session\", False))", "is_method": false, "function_description": "Checks the system configuration to determine if a PySpark session should be utilized. This function helps other modules decide whether to engage Spark functionalities."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_get_runner_class", "line_number": 127, "body": "def _get_runner_class():\n    if _use_spark_session():\n        return PySparkSessionRunner\n    return PySparkRunner", "is_method": false, "function_description": "Determines and returns the appropriate PySpark runner class. It selects between a session-based or direct runner based on the current Spark configuration."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__enter__", "line_number": 57, "body": "def __enter__(self):\n        from pyspark import SparkContext\n        self.sc = SparkContext(conf=self.conf)\n        return self.sc, self.sc", "is_method": true, "class_name": "SparkContextEntryPoint", "function_description": "As the entry point for a context manager, this method creates and returns a configured SparkContext, facilitating Spark operations within a `with` block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__exit__", "line_number": 62, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.sc.stop()", "is_method": true, "class_name": "SparkContextEntryPoint", "function_description": "This method automatically stops the SparkContext when exiting a `with` statement, ensuring proper resource cleanup for Spark applications."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "_check_major_spark_version", "line_number": 69, "body": "def _check_major_spark_version(self):\n        from pyspark import __version__ as spark_version\n        major_version = int(spark_version.split('.')[0])\n        if major_version < 2:\n            raise RuntimeError(\n                '''\n                Apache Spark {} does not support SparkSession entrypoint.\n                Try to set 'pyspark_runner.use_spark_session' to 'False' and switch to old-style syntax\n                '''.format(spark_version)\n            )", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "This method verifies if the installed Apache Spark version supports SparkSession functionality. It raises an error for incompatible versions, advising users on alternative configurations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__enter__", "line_number": 80, "body": "def __enter__(self):\n        self._check_major_spark_version()\n        from pyspark.sql import SparkSession\n        self.spark = SparkSession \\\n            .builder \\\n            .config(conf=self.conf) \\\n            .enableHiveSupport() \\\n            .getOrCreate()\n\n        return self.spark, self.spark.sparkContext", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "Initializes and provides a configured SparkSession and SparkContext, ready for Spark operations including Hive support, when used as a context manager."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "__exit__", "line_number": 91, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.spark.stop()", "is_method": true, "class_name": "SparkSessionEntryPoint", "function_description": "As a context manager exit, this method gracefully stops the Spark session, ensuring proper resource release upon exiting the 'with' block."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/pyspark_runner.py", "function": "run", "line_number": 106, "body": "def run(self):\n        from pyspark import SparkConf\n        conf = SparkConf()\n        self.job.setup(conf)\n        with self._entry_point_class(conf=conf) as (entry_point, sc):\n            self.job.setup_remote(sc)\n            self.job.main(entry_point, *self.args)", "is_method": true, "class_name": "AbstractPySparkRunner", "function_description": "This method orchestrates the setup and execution of a PySpark application, handling Spark configuration and resource management. It provides the core capability to run a defined Spark job."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "create_hadoopcli_client", "line_number": 38, "body": "def create_hadoopcli_client():\n    \"\"\"\n    Given that we want one of the hadoop cli clients,\n    this one will return the right one.\n    \"\"\"\n    version = hdfs_config.get_configured_hadoop_version()\n    if version == \"cdh4\":\n        return HdfsClient()\n    elif version == \"cdh3\":\n        return HdfsClientCdh3()\n    elif version == \"apache1\":\n        return HdfsClientApache1()\n    else:\n        raise ValueError(\"Error: Unknown version specified in Hadoop version\"\n                         \"configuration parameter\")", "is_method": false, "function_description": "Provides the correct Hadoop CLI client instance, automatically selected based on the configured Hadoop version. This ensures compatibility with different Hadoop distributions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "call_check", "line_number": 63, "body": "def call_check(command):\n        p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode != 0:\n            raise hdfs_error.HDFSCliError(command, p.returncode, stdout, stderr)\n        return stdout", "is_method": true, "class_name": "HdfsClient", "function_description": "Executes an HDFS CLI command, returning its standard output upon success. It raises a specific error if the command fails, ensuring robust command execution and error handling.\nCore utility for the HdfsClient that executes a given command, returning its standard output. It ensures command success by raising a specific error if the execution fails."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "exists", "line_number": 70, "body": "def exists(self, path):\n        \"\"\"\n        Use ``hadoop fs -stat`` to check file existence.\n        \"\"\"\n\n        cmd = load_hadoop_cmd() + ['fs', '-stat', path]\n        logger.debug('Running file existence check: %s', subprocess.list2cmdline(cmd))\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        else:\n            not_found_pattern = \"^.*No such file or directory$\"\n            not_found_re = re.compile(not_found_pattern)\n            for line in stderr.split('\\n'):\n                if not_found_re.match(line):\n                    return False\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "is_method": true, "class_name": "HdfsClient", "function_description": "This HdfsClient method checks if a specified file or directory path exists on HDFS. It serves as a fundamental utility for verifying resource presence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "move", "line_number": 89, "body": "def move(self, path, dest):\n        parent_dir = os.path.dirname(dest)\n        if parent_dir != '' and not self.exists(parent_dir):\n            self.mkdir(parent_dir)\n        if not isinstance(path, (list, tuple)):\n            path = [path]\n        else:\n            warnings.warn(\"Renaming multiple files at once is not atomic.\", stacklevel=2)\n        self.call_check(load_hadoop_cmd() + ['fs', '-mv'] + path + [dest])", "is_method": true, "class_name": "HdfsClient", "function_description": "Moves or renames files and directories on HDFS, creating the destination directory if necessary. It provides a convenient way to manage HDFS file paths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "remove", "line_number": 99, "body": "def remove(self, path, recursive=True, skip_trash=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-rm', '-r']\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-rm']\n\n        if skip_trash:\n            cmd = cmd + ['-skipTrash']\n\n        cmd = cmd + [path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Removes specified files or directories from the HDFS filesystem. It supports recursive deletion and can bypass the HDFS trash bin."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "chmod", "line_number": 111, "body": "def chmod(self, path, permissions, recursive=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-chmod', '-R', permissions, path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-chmod', permissions, path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Modifies the permissions of a specified file or directory within HDFS. It can optionally apply these permission changes recursively to its contents."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "chown", "line_number": 118, "body": "def chown(self, path, owner, group, recursive=False):\n        if owner is None:\n            owner = ''\n        if group is None:\n            group = ''\n        ownership = \"%s:%s\" % (owner, group)\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-chown', '-R', ownership, path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-chown', ownership, path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Changes the owner and group of a specified path within the HDFS file system. It supports recursive application for directories."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "count", "line_number": 130, "body": "def count(self, path):\n        cmd = load_hadoop_cmd() + ['fs', '-count', path]\n        stdout = self.call_check(cmd)\n        lines = stdout.split('\\n')\n        for line in stdout.split('\\n'):\n            if line.startswith(\"OpenJDK 64-Bit Server VM warning\") or line.startswith(\"It's highly recommended\") or not line:\n                lines.pop(lines.index(line))\n            else:\n                (dir_count, file_count, content_size, ppath) = stdout.split()\n        results = {'content_size': content_size, 'dir_count': dir_count, 'file_count': file_count}\n        return results", "is_method": true, "class_name": "HdfsClient", "function_description": "This method retrieves comprehensive statistics for a given HDFS path. It returns the total count of directories and files, along with their combined content size."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "copy", "line_number": 142, "body": "def copy(self, path, destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-cp', path, destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "This method copies a file or directory from one location to another within HDFS. It provides a core HDFS file system management capability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "put", "line_number": 145, "body": "def put(self, local_path, destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-put', local_path, destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "As a method of `HdfsClient`, it copies a file from the local filesystem to a specified destination path in HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "get", "line_number": 148, "body": "def get(self, path, local_destination):\n        self.call_check(load_hadoop_cmd() + ['fs', '-get', path, local_destination])", "is_method": true, "class_name": "HdfsClient", "function_description": "Retrieves a file from HDFS and copies it to a specified local destination on the client machine. This enables downloading data from the distributed file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "getmerge", "line_number": 151, "body": "def getmerge(self, path, local_destination, new_line=False):\n        if new_line:\n            cmd = load_hadoop_cmd() + ['fs', '-getmerge', '-nl', path, local_destination]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-getmerge', path, local_destination]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClient", "function_description": "Merges all files from a specified HDFS directory into a single file on the local filesystem. Optionally adds a newline character between merged files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "mkdir", "line_number": 158, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        if parents and raise_if_exists:\n            raise NotImplementedError(\"HdfsClient.mkdir can't raise with -p\")\n        try:\n            cmd = (load_hadoop_cmd() + ['fs', '-mkdir'] +\n                   (['-p'] if parents else []) +\n                   [path])\n            self.call_check(cmd)\n        except hdfs_error.HDFSCliError as ex:\n            if \"File exists\" in ex.stderr:\n                if raise_if_exists:\n                    raise FileAlreadyExists(ex.stderr)\n            else:\n                raise", "is_method": true, "class_name": "HdfsClient", "function_description": "Creates a directory on the HDFS file system, optionally creating parent directories. It can also raise an error if the directory already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "listdir", "line_number": 173, "body": "def listdir(self, path, ignore_directories=False, ignore_files=False,\n                include_size=False, include_type=False, include_time=False, recursive=False):\n        if not path:\n            path = \".\"  # default to current/home catalog\n\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs'] + self.recursive_listdir_cmd + [path]\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-ls', path]\n        lines = self.call_check(cmd).split('\\n')\n\n        for line in lines:\n            if not line:\n                continue\n            elif line.startswith('OpenJDK 64-Bit Server VM warning') or line.startswith('It\\'s highly recommended') or line.startswith('Found'):\n                continue  # \"hadoop fs -ls\" outputs \"Found %d items\" as its first line\n            elif ignore_directories and line[0] == 'd':\n                continue\n            elif ignore_files and line[0] == '-':\n                continue\n            data = line.split(' ')\n\n            file = data[-1]\n            size = int(data[-4])\n            line_type = line[0]\n            extra_data = ()\n\n            if include_size:\n                extra_data += (size,)\n            if include_type:\n                extra_data += (line_type,)\n            if include_time:\n                time_str = '%sT%s' % (data[-3], data[-2])\n                modification_time = datetime.datetime.strptime(time_str,\n                                                               '%Y-%m-%dT%H:%M')\n                extra_data += (modification_time,)\n\n            if len(extra_data) > 0:\n                yield (file,) + extra_data\n            else:\n                yield file", "is_method": true, "class_name": "HdfsClient", "function_description": "Provides a service to list contents of an HDFS directory. It can recursively retrieve file and directory names, optionally including metadata like size, type, and modification time."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "touchz", "line_number": 215, "body": "def touchz(self, path):\n        self.call_check(load_hadoop_cmd() + ['fs', '-touchz', path])", "is_method": true, "class_name": "HdfsClient", "function_description": "The `HdfsClient` method creates a new, zero-byte file at the specified path in HDFS. This is useful for establishing placeholders or markers within the distributed file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "mkdir", "line_number": 224, "body": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        \"\"\"\n        No explicit -p switch, this version of Hadoop always creates parent directories.\n        \"\"\"\n        try:\n            self.call_check(load_hadoop_cmd() + ['fs', '-mkdir', path])\n        except hdfs_error.HDFSCliError as ex:\n            if \"File exists\" in ex.stderr:\n                if raise_if_exists:\n                    raise FileAlreadyExists(ex.stderr)\n            else:\n                raise", "is_method": true, "class_name": "HdfsClientCdh3", "function_description": "Creates a directory on the HDFS file system. It optionally raises an error if the directory already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "remove", "line_number": 237, "body": "def remove(self, path, recursive=True, skip_trash=False):\n        if recursive:\n            cmd = load_hadoop_cmd() + ['fs', '-rmr']\n        else:\n            cmd = load_hadoop_cmd() + ['fs', '-rm']\n\n        if skip_trash:\n            cmd = cmd + ['-skipTrash']\n\n        cmd = cmd + [path]\n        self.call_check(cmd)", "is_method": true, "class_name": "HdfsClientCdh3", "function_description": "Deletes specified files or directories from HDFS, providing options for recursive removal. It can also bypass the HDFS trash for permanent deletion."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/hadoopcli_clients.py", "function": "exists", "line_number": 258, "body": "def exists(self, path):\n        cmd = load_hadoop_cmd() + ['fs', '-test', '-e', path]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        elif p.returncode == 1:\n            return False\n        else:\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "is_method": true, "class_name": "HdfsClientApache1", "function_description": "Checks if a specified file or directory path exists within the Hadoop Distributed File System (HDFS). This method provides a reliable way to verify path existence."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "url", "line_number": 62, "body": "def url(self):\n        # the hdfs package allows it to specify multiple namenodes by passing a string containing\n        # multiple namenodes separated by ';'\n        hosts = self.host.split(\";\")\n        urls = ['http://' + host + ':' + str(self.port) for host in hosts]\n        return \";\".join(urls)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Generates the complete HTTP URL string(s) for WebHDFS communication. It supports multiple NameNodes by formatting them into a single semicolon-separated string."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "client", "line_number": 70, "body": "def client(self):\n        # A naive benchmark showed that 1000 existence checks took 2.5 secs\n        # when not recreating the client, and 4.0 secs when recreating it. So\n        # not urgent to memoize it. Note that it *might* be issues with process\n        # forking and whatnot (as the one in the snakebite client) if we\n        # memoize it too trivially.\n        if self.client_type == 'kerberos':\n            from hdfs.ext.kerberos import KerberosClient\n            return KerberosClient(url=self.url)\n        else:\n            import hdfs\n            return hdfs.InsecureClient(url=self.url, user=self.user)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Creates and returns an HDFS client instance, configured for either Kerberos or insecure connections, to enable interaction with the WebHDFS service."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "walk", "line_number": 83, "body": "def walk(self, path, depth=1):\n        return self.client.walk(path, depth=depth)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides the capability to traverse the HDFS file system, listing files and directories from a specified path. It supports controlling the traversal depth."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "exists", "line_number": 86, "body": "def exists(self, path):\n        \"\"\"\n        Returns true if the path exists and false otherwise.\n        \"\"\"\n        import hdfs\n        try:\n            self.client.status(path)\n            return True\n        except hdfs.util.HdfsError as e:\n            if str(e).startswith('File does not exist: '):\n                return False\n            else:\n                raise e", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Verifies if a specified path exists on the HDFS file system. It returns true if found, false if not, handling HDFS-specific existence errors."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "upload", "line_number": 100, "body": "def upload(self, hdfs_path, local_path, overwrite=False):\n        return self.client.upload(hdfs_path, local_path, overwrite=overwrite)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "As a core method of `WebHdfsClient`, this function uploads a specified local file to a given path on HDFS. It provides an option to overwrite the destination file if it already exists."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "download", "line_number": 103, "body": "def download(self, hdfs_path, local_path, overwrite=False, n_threads=-1):\n        return self.client.download(hdfs_path, local_path, overwrite=overwrite,\n                                    n_threads=n_threads)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides the capability to download a file from HDFS to a local filesystem path, offering overwrite and multithreading options."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "remove", "line_number": 107, "body": "def remove(self, hdfs_path, recursive=True, skip_trash=False):\n        assert skip_trash  # Yes, you need to explicitly say skip_trash=True\n        return self.client.delete(hdfs_path, recursive=recursive)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides the capability to permanently delete a file or directory from HDFS. This method supports recursive removal for directories, bypassing the trash."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "read", "line_number": 111, "body": "def read(self, hdfs_path, offset=0, length=None, buffer_size=None,\n             chunk_size=1024, buffer_char=None):\n        return self.client.read(hdfs_path, offset=offset, length=length,\n                                buffer_size=buffer_size, chunk_size=chunk_size,\n                                buffer_char=buffer_char)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Provides a service to read data from a specified HDFS path. It supports reading partial content with configurable offsets and lengths."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "move", "line_number": 117, "body": "def move(self, path, dest):\n        parts = dest.rstrip('/').split('/')\n        if len(parts) > 1:\n            dir_path = '/'.join(parts[0:-1])\n            if not self.exists(dir_path):\n                self.mkdir(dir_path, parents=True)\n        self.client.rename(path, dest)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "This method moves a file or directory within WebHDFS. It automatically creates any necessary parent directories at the destination path."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "mkdir", "line_number": 125, "body": "def mkdir(self, path, parents=True, mode=0o755, raise_if_exists=False):\n        \"\"\"\n        Has no returnvalue (just like WebHDFS)\n        \"\"\"\n        if not parents or raise_if_exists:\n            warnings.warn('webhdfs mkdir: parents/raise_if_exists not implemented')\n        permission = int(oct(mode)[2:])  # Convert from int(decimal) to int(octal)\n        self.client.makedirs(path, permission=permission)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Creates a new directory on the WebHDFS file system at the specified path. This method allows setting permissions for the new directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "put", "line_number": 158, "body": "def put(self, local_path, destination):\n        \"\"\"\n        Restricted version of upload\n        \"\"\"\n        self.upload(local_path, destination)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "This method uploads a local file to a specified destination on WebHDFS. It serves as a direct interface for transferring data from the local system to HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "get", "line_number": 164, "body": "def get(self, path, local_destination):\n        \"\"\"\n        Restricted version of download\n        \"\"\"\n        self.download(path, local_destination)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Facilitates downloading a file from HDFS to a local destination. It serves as a specific or restricted interface for the underlying download operation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "listdir", "line_number": 170, "body": "def listdir(self, path, ignore_directories=False, ignore_files=False,\n                include_size=False, include_type=False, include_time=False,\n                recursive=False):\n        assert not recursive\n        return self.client.list(path, status=False)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "Lists the non-recursive contents of a specified directory on the WebHDFS file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/webhdfs_client.py", "function": "touchz", "line_number": 176, "body": "def touchz(self, path):\n        \"\"\"\n        To touchz using the web hdfs \"write\" cmd.\n        \"\"\"\n        self.client.write(path, data='', overwrite=False)", "is_method": true, "class_name": "WebHdfsClient", "function_description": "This method creates an empty file at the specified WebHDFS path. It is useful for creating zero-byte marker files or initializing new files in the distributed file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "__del__", "line_number": 93, "body": "def __del__(self):\n        # TODO: not sure is_tmp belongs in Targets construction arguments\n        if self.is_tmp and self.exists():\n            self.remove(skip_trash=True)", "is_method": true, "class_name": "HdfsTarget", "function_description": "This destructor method automatically removes temporary HDFS files or directories associated with the target when the object is destroyed. It ensures transient data is cleaned up."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "fs", "line_number": 99, "body": "def fs(self):\n        return self._fs", "is_method": true, "class_name": "HdfsTarget", "function_description": "Provides access to the underlying HDFS file system object. This allows direct interaction with the HDFS instance for various file operations related to the target."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "glob_exists", "line_number": 102, "body": "def glob_exists(self, expected_files):\n        ls = list(self.fs.listdir(self.path))\n        if len(ls) == expected_files:\n            return True\n        return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Checks if an HDFS directory contains exactly the specified number of files. This verifies the presence and count of expected files in the target location."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "open", "line_number": 108, "body": "def open(self, mode='r'):\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n\n        if mode == 'r':\n            return self.format.pipe_reader(self.path)\n        else:\n            return self.format.pipe_writer(self.path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Opens the HDFS target specified by its path for reading or writing data. It returns an appropriate pipe for streaming operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "remove", "line_number": 117, "body": "def remove(self, skip_trash=False):\n        self.fs.remove(self.path, skip_trash=skip_trash)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Removes the HDFS file or directory associated with this target. It can optionally bypass the trash for immediate deletion, providing a cleanup capability."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "rename", "line_number": 120, "body": "def rename(self, path, raise_if_exists=False):\n        \"\"\"\n        Does not change self.path.\n\n        Unlike ``move_dir()``, ``rename()`` might cause nested directories.\n        See spotify/luigi#522\n        \"\"\"\n        if isinstance(path, HdfsTarget):\n            path = path.path\n        if raise_if_exists and self.fs.exists(path):\n            raise RuntimeError('Destination exists: %s' % path)\n        self.fs.rename(self.path, path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Renames the HDFS path managed by this target to a new specified path. It performs an atomic move operation and can prevent overwriting existing destinations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "move", "line_number": 133, "body": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        Alias for ``rename()``\n        \"\"\"\n        self.rename(path, raise_if_exists=raise_if_exists)", "is_method": true, "class_name": "HdfsTarget", "function_description": "This method serves as an alias for `rename()`, allowing the HDFS target to be moved or renamed. It provides a convenient way to manage file paths within HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "move_dir", "line_number": 139, "body": "def move_dir(self, path):\n        \"\"\"\n        Move using :py:class:`~luigi.contrib.hdfs.abstract_client.HdfsFileSystem.rename_dont_move`\n\n        New since after luigi v2.1: Does not change self.path\n\n        One could argue that the implementation should use the\n        mkdir+raise_if_exists approach, but we at Spotify have had more trouble\n        with that over just using plain mv.  See spotify/luigi#557\n        \"\"\"\n        self.fs.rename_dont_move(self.path, path)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Moves the HDFS path associated with this target to a new destination. Useful for renaming or reorganizing HDFS data programmatically."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "copy", "line_number": 151, "body": "def copy(self, dst_dir):\n        \"\"\"\n        Copy to destination directory.\n        \"\"\"\n        self.fs.copy(self.path, dst_dir)", "is_method": true, "class_name": "HdfsTarget", "function_description": "Copies the HDFS target (file or directory) represented by this object to a specified destination directory within the HDFS file system."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "is_writable", "line_number": 157, "body": "def is_writable(self):\n        \"\"\"\n        Currently only works with hadoopcli\n        \"\"\"\n        if \"/\" in self.path:\n            # example path: /log/ap/2013-01-17/00\n            parts = self.path.split(\"/\")\n            # start with the full path and then up the tree until we can check\n            length = len(parts)\n            for part in range(length):\n                path = \"/\".join(parts[0:length - part]) + \"/\"\n                if self.fs.exists(path):\n                    # if the path exists and we can write there, great!\n                    if self._is_writable(path):\n                        return True\n                    # if it exists and we can't =( sad panda\n                    else:\n                        return False\n            # We went through all parts of the path and we still couldn't find\n            # one that exists.\n            return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Determines if the HDFS target path is writable. It checks the path itself or its closest existing parent directory for write permissions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "_is_writable", "line_number": 179, "body": "def _is_writable(self, path):\n        test_path = path + '.test_write_access-%09d' % random.randrange(1e10)\n        try:\n            self.fs.touchz(test_path)\n            self.fs.remove(test_path, recursive=False)\n            return True\n        except hdfs_clients.HDFSCliError:\n            return False", "is_method": true, "class_name": "HdfsTarget", "function_description": "Determines if the HDFS target has write permissions for the specified path. It enables pre-checking access before file operations."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/target.py", "function": "exists", "line_number": 219, "body": "def exists(self):\n        hadoopSemaphore = self.path + self.flag\n        return self.fs.exists(hadoopSemaphore)", "is_method": true, "class_name": "HdfsFlagTarget", "function_description": "Checks for the existence of the HDFS flag file. This indicates a specific state or completion status within an HDFS-based workflow."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/clients.py", "function": "get_autoconfig_client", "line_number": 33, "body": "def get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):\n    \"\"\"\n    Creates the client as specified in the `luigi.cfg` configuration.\n    \"\"\"\n    try:\n        return client_cache.client\n    except AttributeError:\n        configured_client = hdfs_config.get_configured_hdfs_client()\n        if configured_client == \"webhdfs\":\n            client_cache.client = hdfs_webhdfs_client.WebHdfsClient()\n        elif configured_client == \"hadoopcli\":\n            client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()\n        else:\n            raise Exception(\"Unknown hdfs client \" + configured_client)\n        return client_cache.client", "is_method": false, "function_description": "Provides a configured HDFS client instance based on `luigi.cfg` settings. It caches the client for efficient reuse in subsequent calls."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/clients.py", "function": "_with_ac", "line_number": 50, "body": "def _with_ac(method_name):\n    def result(*args, **kwargs):\n        return getattr(get_autoconfig_client(), method_name)(*args, **kwargs)\n    return result", "is_method": false, "function_description": "It generates a callable that dynamically invokes a specified method on the global autoconfig client. This provides a convenient proxy for accessing autoconfiguration functionalities."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/clients.py", "function": "result", "line_number": 51, "body": "def result(*args, **kwargs):\n        return getattr(get_autoconfig_client(), method_name)(*args, **kwargs)", "is_method": false, "function_description": "Proxies calls to a specific method of an auto-configured client object. It dynamically dispatches all arguments to the underlying client method."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "load_hadoop_cmd", "line_number": 53, "body": "def load_hadoop_cmd():\n    return hadoopcli().command.split()", "is_method": false, "function_description": "Provides the main Hadoop command broken into a list of arguments, ready for execution. It dynamically retrieves the command string from a Hadoop CLI utility."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "get_configured_hadoop_version", "line_number": 57, "body": "def get_configured_hadoop_version():\n    \"\"\"\n    CDH4 (hadoop 2+) has a slightly different syntax for interacting with hdfs\n    via the command line.\n\n    The default version is CDH4, but one can override\n    this setting with \"cdh3\" or \"apache1\" in the hadoop section of the config\n    in order to use the old syntax.\n    \"\"\"\n    return hadoopcli().version.lower()", "is_method": false, "function_description": "Retrieves the configured Hadoop distribution version. This informs other system components how to correctly interact with HDFS via command-line tools."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "get_configured_hdfs_client", "line_number": 69, "body": "def get_configured_hdfs_client():\n    \"\"\"\n    This is a helper that fetches the configuration value for 'client' in\n    the [hdfs] section. It will return the client that retains backwards\n    compatibility when 'client' isn't configured.\n    \"\"\"\n    return hdfs().client", "is_method": false, "function_description": "Provides access to the configured HDFS client, handling backward compatibility for older settings. It offers a consistent interface for HDFS interactions."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/config.py", "function": "tmppath", "line_number": 78, "body": "def tmppath(path=None, include_unix_username=True):\n    \"\"\"\n    @param path: target path for which it is needed to generate temporary location\n    @type path: str\n    @type include_unix_username: bool\n    @rtype: str\n\n    Note that include_unix_username might work on windows too.\n    \"\"\"\n    addon = \"luigitemp-%08d\" % random.randrange(1e9)\n    temp_dir = '/tmp'  # default tmp dir if none is specified in config\n\n    # 1. Figure out to which temporary directory to place\n    configured_hdfs_tmp_dir = hdfs().tmp_dir\n    if configured_hdfs_tmp_dir is not None:\n        # config is superior\n        base_dir = configured_hdfs_tmp_dir\n    elif path is not None:\n        # need to copy correct schema and network location\n        parsed = urlparse(path)\n        base_dir = urlunparse((parsed.scheme, parsed.netloc, temp_dir, '', '', ''))\n    else:\n        # just system temporary directory\n        base_dir = temp_dir\n\n    # 2. Figure out what to place\n    if path is not None:\n        if path.startswith(temp_dir + '/'):\n            # Not 100%, but some protection from directories like /tmp/tmp/file\n            subdir = path[len(temp_dir):]\n        else:\n            # Protection from /tmp/hdfs:/dir/file\n            parsed = urlparse(path)\n            subdir = parsed.path\n        subdir = subdir.lstrip('/') + '-'\n    else:\n        # just return any random temporary location\n        subdir = ''\n\n    if include_unix_username:\n        subdir = os.path.join(getpass.getuser(), subdir)\n\n    return os.path.join(base_dir, subdir + addon)", "is_method": false, "function_description": "Generates a unique temporary file path for diverse scenarios. It determines the base directory from configuration or an input path, optionally including the current user's name for isolation."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "abort", "line_number": 43, "body": "def abort(self):\n        logger.info(\"Aborting %s('%s'). Removing temporary file '%s'\",\n                    self.__class__.__name__, self.path, self.tmppath)\n        super(HdfsAtomicWritePipe, self).abort()\n        remove(self.tmppath, skip_trash=True)", "is_method": true, "class_name": "HdfsAtomicWritePipe", "function_description": "This method aborts an ongoing HDFS atomic write operation. It ensures data consistency by deleting the temporary file and cleaning up associated resources."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "close", "line_number": 49, "body": "def close(self):\n        super(HdfsAtomicWritePipe, self).close()\n        try:\n            if exists(self.path):\n                remove(self.path)\n        except Exception as ex:\n            if isinstance(ex, HDFSCliError) or ex.args[0].contains(\"FileNotFoundException\"):\n                pass\n            else:\n                raise ex\n        if not all(result['result'] for result in rename(self.tmppath, self.path) or []):\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))", "is_method": true, "class_name": "HdfsAtomicWritePipe", "function_description": "Finalizes an atomic HDFS write operation by renaming the temporary file to its permanent path. It ensures data integrity and handles commit failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "abort", "line_number": 74, "body": "def abort(self):\n        logger.info(\"Aborting %s('%s'). Removing temporary dir '%s'\",\n                    self.__class__.__name__, self.path, self.tmppath)\n        super(HdfsAtomicWriteDirPipe, self).abort()\n        remove(self.tmppath, skip_trash=True)", "is_method": true, "class_name": "HdfsAtomicWriteDirPipe", "function_description": "This method aborts an ongoing atomic HDFS directory write operation. It ensures data consistency and a clean state by removing the temporary staging directory."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "close", "line_number": 80, "body": "def close(self):\n        super(HdfsAtomicWriteDirPipe, self).close()\n        try:\n            if exists(self.path):\n                remove(self.path)\n        except Exception as ex:\n            if isinstance(ex, HDFSCliError) or ex.args[0].contains(\"FileNotFoundException\"):\n                pass\n            else:\n                raise ex\n\n        # it's unlikely to fail in this way but better safe than sorry\n        if not all(result['result'] for result in rename(self.tmppath, self.path) or []):\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))\n\n        if os.path.basename(self.tmppath) in map(os.path.basename, listdir(self.path)):\n            remove(self.path)\n            raise HdfsAtomicWriteError('Atomic write to {} failed'.format(self.path))", "is_method": true, "class_name": "HdfsAtomicWriteDirPipe", "function_description": "This method finalizes the atomic HDFS directory write operation. It renames the temporary path to the target, performing cleanup and robust checks to ensure atomicity and handle failures."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 105, "body": "def hdfs_writer(self, path):\n        return self.pipe_writer(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "Returns a writer object for a specified HDFS path. This method facilitates writing plain formatted data to HDFS."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 108, "body": "def hdfs_reader(self, path):\n        return self.pipe_reader(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "Enables reading data from a given HDFS path by leveraging the class's internal pipe reading mechanism."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 111, "body": "def pipe_reader(self, path):\n        return HdfsReadPipe(path)", "is_method": true, "class_name": "PlainFormat", "function_description": "Creates and returns an HdfsReadPipe object, enabling plain format data to be read from a specified HDFS path. This method provides a dedicated reader for processing data within the PlainFormat context."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 114, "body": "def pipe_writer(self, output_pipe):\n        return HdfsAtomicWritePipe(output_pipe)", "is_method": true, "class_name": "PlainFormat", "function_description": "Provides an HDFS atomic write pipe for the plain data format, enabling robust and safe data output."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 123, "body": "def hdfs_writer(self, path):\n        return self.pipe_writer(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Provides an HDFS-specific writer object for the given path. This method enables writing data to Hadoop Distributed File System."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 126, "body": "def hdfs_reader(self, path):\n        return self.pipe_reader(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Provides a dedicated reader for HDFS paths, leveraging the class's general pipe reading mechanism to access data."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 129, "body": "def pipe_reader(self, path):\n        # exclude underscore-prefixedfiles/folders (created by MapReduce)\n        return HdfsReadPipe(\"%s/[^_]*\" % path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Provides an HDFS pipe reader configured to access data within a PlainDirFormat directory. It automatically filters out MapReduce-generated temporary files."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 133, "body": "def pipe_writer(self, path):\n        return HdfsAtomicWriteDirPipe(path)", "is_method": true, "class_name": "PlainDirFormat", "function_description": "Provides an atomic directory writer object for a given path. This enables safe and reliable write operations, often in HDFS-like environments."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_writer", "line_number": 152, "body": "def pipe_writer(self, output):\n        return self.writer(output)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Provides a `pipe_writer` interface for data output within the `CompatibleHdfsFormat` class. It delegates the actual writing operation directly to the class's internal writer."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "pipe_reader", "line_number": 155, "body": "def pipe_reader(self, input):\n        return self.reader(input)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Provides a standardized `pipe_reader` interface for HDFS formats, delegating the actual data reading to an internal reader object."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_writer", "line_number": 158, "body": "def hdfs_writer(self, output):\n        return self.writer(output)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Provides a HDFS writing mechanism, delegating the output operation to an internal writer instance."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "hdfs_reader", "line_number": 161, "body": "def hdfs_reader(self, input):\n        return self.reader(input)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "This method provides a specific interface for reading data from HDFS, delegating the actual read operation to the format's configured internal reader."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "__getstate__", "line_number": 168, "body": "def __getstate__(self):\n        d = self.__dict__.copy()\n        for attr in ('reader', 'writer'):\n            method = getattr(self, attr)\n            try:\n                # if instance method, pickle instance and method name\n                d[attr] = method.__self__, method.__func__.__name__\n            except AttributeError:\n                pass  # not an instance method\n        return d", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "Customizes object serialization for `CompatibleHdfsFormat` instances. It ensures instance methods like 'reader' and 'writer' are correctly handled during pickling, enabling proper reconstruction upon unpickling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/format.py", "function": "__setstate__", "line_number": 179, "body": "def __setstate__(self, d):\n        self.__dict__ = d\n        for attr in ('reader', 'writer'):\n            try:\n                method_self, method_name = d[attr]\n            except ValueError:\n                continue\n            method = getattr(method_self, method_name)\n            setattr(self, attr, method)", "is_method": true, "class_name": "CompatibleHdfsFormat", "function_description": "This method customizes object deserialization, ensuring `reader` and `writer` attributes, which are bound methods, are correctly re-attached to their respective objects during unpickling."}, {"file": "./dataset/RepoExec/test-apps/luigi/luigi/contrib/hdfs/abstract_client.py", "function": "rename", "line_number": 31, "body": "def rename(self, path, dest):\n        \"\"\"\n        Rename or move a file.\n\n        In hdfs land, \"mv\" is often called rename. So we add an alias for\n        ``move()`` called ``rename()``. This is also to keep backward\n        compatibility since ``move()`` became standardized in luigi's\n        filesystem interface.\n        \"\"\"\n        return self.move(path, dest)", "is_method": true, "class_name": "HdfsFileSystem", "function_description": "Renames or moves a file within the HDFS file system. This method provides an alias for the `move()` operation for backward compatibility and HDFS terminology."}]