[{"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/cfg.py", "function": "config", "line_number": 44, "body": "def config(metadata: dict = None, *,\n           # TODO: these can be typed more precisely\n           # Specifically, a Callable[A, B], where `B` is bound as a JSON type\n           encoder: Callable = None,\n           decoder: Callable = None,\n           mm_field: MarshmallowField = None,\n           letter_case: Callable[[str], str] = None,\n           undefined: Optional[Union[str, Undefined]] = None,\n           field_name: str = None,\n           exclude: Optional[Callable[[str, T], bool]] = None,\n           ) -> Dict[str, dict]:\n    if metadata is None:\n        metadata = {}\n\n    lib_metadata = metadata.setdefault('dataclasses_json', {})\n\n    if encoder is not None:\n        lib_metadata['encoder'] = encoder\n\n    if decoder is not None:\n        lib_metadata['decoder'] = decoder\n\n    if mm_field is not None:\n        lib_metadata['mm_field'] = mm_field\n\n    if field_name is not None:\n        if letter_case is not None:\n            @functools.wraps(letter_case)\n            def override(_, _letter_case=letter_case, _field_name=field_name):\n                return _letter_case(_field_name)\n        else:\n            def override(_, _field_name=field_name):\n                return _field_name\n        letter_case = override\n\n    if letter_case is not None:\n        lib_metadata['letter_case'] = letter_case\n\n    if undefined is not None:\n        # Get the corresponding action for undefined parameters\n        if isinstance(undefined, str):\n            if not hasattr(Undefined, undefined.upper()):\n                valid_actions = list(action.name for action in Undefined)\n                raise UndefinedParameterError(\n                    f\"Invalid undefined parameter action, \"\n                    f\"must be one of {valid_actions}\")\n            undefined = Undefined[undefined.upper()]\n\n        lib_metadata['undefined'] = undefined\n\n    if exclude is not None:\n        lib_metadata['exclude'] = exclude\n\n    return metadata", "is_method": false, "function_description": "Function providing a flexible way to build or update metadata dictionaries with customizable JSON serialization options, including encoding, decoding, field naming, letter casing, exclusion rules, and handling of undefined values."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/cfg.py", "function": "override", "line_number": 72, "body": "def override(_, _letter_case=letter_case, _field_name=field_name):\n                return _letter_case(_field_name)", "is_method": false, "function_description": "This function applies a case transformation to a predefined field name using a specified letter case function, enabling flexible case formatting of field identifiers."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_get_type_cons", "line_number": 7, "body": "def _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons", "is_method": false, "function_description": "Internal utility function that extracts the underlying constructor or base type from a given typing object, accounting for differences between Python 3.6 and 3.7+ type system implementations."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_get_type_origin", "line_number": 29, "body": "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        origin = type_.__origin__\n    except AttributeError:\n        if sys.version_info.minor == 6:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = type_\n            else:\n                origin = type_ if origin is None else origin\n        else:\n            origin = type_\n    return origin", "is_method": false, "function_description": "Utility function that determines the original type behind a typing construct, handling compatibility differences between Python 3.6 and 3.7 typing APIs."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_hasargs", "line_number": 47, "body": "def _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res", "is_method": false, "function_description": "Utility function that checks if a given generic type includes all specified type arguments, supporting type introspection and validation in type hinting scenarios."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_isinstance_safe", "line_number": 56, "body": "def _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result", "is_method": false, "function_description": "Utility function that safely checks whether an object is an instance of a given type without raising exceptions, ensuring robust type checking in uncertain or error-prone contexts."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_issubclass_safe", "line_number": 65, "body": "def _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)", "is_method": false, "function_description": "Utility function that safely determines if one class is a subclass of another, handling exceptions and supporting special type checks for new type constructs."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_new_type_subclass_safe", "line_number": 74, "body": "def _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False", "is_method": false, "function_description": "Utility function that safely checks if a class or its __supertype__ chain is a subclass of another, preventing errors during subclass checks by handling exceptions gracefully."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_new_type", "line_number": 86, "body": "def _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")", "is_method": false, "function_description": "This function checks if a given type is a new-style type, specifically a function that includes a \"__supertype__\" attribute. It helps identify specialized type annotations or custom type constructs."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_optional", "line_number": 90, "body": "def _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)", "is_method": false, "function_description": "Helper function that determines if a given type hint represents an optional type, including direct Optional, Union with None, or Any for flexible typing contexts."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_mapping", "line_number": 96, "body": "def _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)", "is_method": false, "function_description": "Utility function that checks if a given type is a subclass or instance of the Mapping abstract base class, helping to identify mapping-like types safely."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_collection", "line_number": 100, "body": "def _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)", "is_method": false, "function_description": "Utility function that checks if a given type is a subclass or instance of a collection type, supporting safe type origin extraction."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_is_nonstr_collection", "line_number": 104, "body": "def _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))", "is_method": false, "function_description": "Utility function that checks if a type is a non-string collection, distinguishing iterable container types from string types for type-checking or validation purposes."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_timestamp_to_dt_aware", "line_number": 109, "body": "def _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt", "is_method": false, "function_description": "Converts a UNIX timestamp to a timezone-aware datetime object reflecting the local timezone, facilitating accurate date and time handling across different regions."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_undefined_parameter_action_safe", "line_number": 115, "body": "def _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum", "is_method": false, "function_description": "Utility method that safely retrieves the configured action for undefined parameters from a class's JSON dataclass configuration, ensuring robustness against missing or incomplete configuration attributes."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/utils.py", "function": "_handle_undefined_parameters_safe", "line_number": 129, "body": "def _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")", "is_method": false, "function_description": "Utility method that manages actions for undefined parameters based on usage context ('to', 'from', 'dump', or 'init'), enabling controlled handling of missing parameters during object serialization or initialization processes."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_to_dict", "line_number": 27, "body": "def handle_to_dict(obj, kvs: Dict[Any, Any]) -> Dict[Any, Any]:\n        \"\"\"\n        Return the parameters that will be written to the output dict\n        \"\"\"\n        return kvs", "is_method": true, "class_name": "_UndefinedParameterAction", "function_description": "Utility method that returns the given dictionary of parameters as-is for serialization or output purposes."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_dump", "line_number": 34, "body": "def handle_dump(obj) -> Dict[Any, Any]:\n        \"\"\"\n        Return the parameters that will be added to the schema dump.\n        \"\"\"\n        return {}", "is_method": true, "class_name": "_UndefinedParameterAction", "function_description": "Returns an empty dictionary representing no parameters for schema dumping. This placeholder function indicates no parameters are added in the schema context."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "create_init", "line_number": 41, "body": "def create_init(obj) -> Callable:\n        return obj.__init__", "is_method": true, "class_name": "_UndefinedParameterAction", "function_description": "Returns the __init__ method of a given object, enabling access to its constructor function."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "_separate_defined_undefined_kvs", "line_number": 45, "body": "def _separate_defined_undefined_kvs(cls, kvs: Dict) -> \\\n            Tuple[KnownParameters, UnknownParameters]:\n        \"\"\"\n        Returns a 2 dictionaries: defined and undefined parameters\n        \"\"\"\n        class_fields = fields(cls)\n        field_names = [field.name for field in class_fields]\n        unknown_given_parameters = {k: v for k, v in kvs.items() if\n                                    k not in field_names}\n        known_given_parameters = {k: v for k, v in kvs.items() if\n                                  k in field_names}\n        return known_given_parameters, unknown_given_parameters", "is_method": true, "class_name": "_UndefinedParameterAction", "function_description": "Utility method in _UndefinedParameterAction class that separates input parameters into two dictionaries: those defined in the class and those not recognized, facilitating parameter validation or handling of unexpected inputs."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_from_dict", "line_number": 66, "body": "def handle_from_dict(cls, kvs: Dict) -> Dict[str, Any]:\n        known, unknown = \\\n            _UndefinedParameterAction._separate_defined_undefined_kvs(\n                cls=cls, kvs=kvs)\n        if len(unknown) > 0:\n            raise UndefinedParameterError(\n                f\"Received undefined initialization arguments {unknown}\")\n        return known", "is_method": true, "class_name": "_RaiseUndefinedParameters", "function_description": "Utility method of the _RaiseUndefinedParameters class that filters initialization arguments, raising an error if any undefined parameters are detected, ensuring only known parameters are accepted."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_from_dict", "line_number": 87, "body": "def handle_from_dict(cls, kvs: Dict) -> Dict[str, Any]:\n        known_given_parameters, _ = \\\n            _UndefinedParameterAction._separate_defined_undefined_kvs(\n                cls=cls, kvs=kvs)\n        return known_given_parameters", "is_method": true, "class_name": "_IgnoreUndefinedParameters", "function_description": "Utility method of the _IgnoreUndefinedParameters class that filters input dictionary to include only parameters defined in the class, enabling safe instantiation by excluding unknown keys."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "create_init", "line_number": 94, "body": "def create_init(obj) -> Callable:\n        original_init = obj.__init__\n        init_signature = inspect.signature(original_init)\n\n        @functools.wraps(obj.__init__)\n        def _ignore_init(self, *args, **kwargs):\n            known_kwargs, _ = \\\n                _CatchAllUndefinedParameters._separate_defined_undefined_kvs(\n                    obj, kwargs)\n            num_params_takeable = len(\n                init_signature.parameters) - 1  # don't count self\n            num_args_takeable = num_params_takeable - len(known_kwargs)\n\n            args = args[:num_args_takeable]\n            bound_parameters = init_signature.bind_partial(self, *args,\n                                                           **known_kwargs)\n            bound_parameters.apply_defaults()\n\n            arguments = bound_parameters.arguments\n            arguments.pop(\"self\", None)\n            final_parameters = \\\n                _IgnoreUndefinedParameters.handle_from_dict(obj, arguments)\n            original_init(self, **final_parameters)\n\n        return _ignore_init", "is_method": true, "class_name": "_IgnoreUndefinedParameters", "function_description": "Utility method of the _IgnoreUndefinedParameters class that creates an __init__ wrapper ignoring unexpected keyword arguments during object initialization, allowing flexible instantiation without errors from unknown parameters."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_from_dict", "line_number": 134, "body": "def handle_from_dict(cls, kvs: Dict) -> Dict[str, Any]:\n        known, unknown = _UndefinedParameterAction \\\n            ._separate_defined_undefined_kvs(cls=cls, kvs=kvs)\n        catch_all_field = _CatchAllUndefinedParameters._get_catch_all_field(\n            cls=cls)\n\n        if catch_all_field.name in known:\n\n            already_parsed = isinstance(known[catch_all_field.name], dict)\n            default_value = _CatchAllUndefinedParameters._get_default(\n                catch_all_field=catch_all_field)\n            received_default = default_value == known[catch_all_field.name]\n\n            value_to_write: Any\n            if received_default and len(unknown) == 0:\n                value_to_write = default_value\n            elif received_default and len(unknown) > 0:\n                value_to_write = unknown\n            elif already_parsed:\n                # Did not receive default\n                value_to_write = known[catch_all_field.name]\n                if len(unknown) > 0:\n                    value_to_write.update(unknown)\n            else:\n                error_message = f\"Received input field with \" \\\n                                f\"same name as catch-all field: \" \\\n                                f\"'{catch_all_field.name}': \" \\\n                                f\"'{known[catch_all_field.name]}'\"\n                raise UndefinedParameterError(error_message)\n        else:\n            value_to_write = unknown\n\n        known[catch_all_field.name] = value_to_write\n        return known", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Utility method of the _CatchAllUndefinedParameters class that consolidates known and unknown dictionary key-value pairs, assigning all undefined parameters to a designated catch-all field for flexible input handling during object creation."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "_get_default", "line_number": 170, "body": "def _get_default(catch_all_field: Field) -> Any:\n        # access to the default factory currently causes\n        # a false-positive mypy error (16. Dec 2019):\n        # https://github.com/python/mypy/issues/6910\n\n        # noinspection PyProtectedMember\n        has_default = not isinstance(catch_all_field.default,\n                                     dataclasses._MISSING_TYPE)\n        # noinspection PyProtectedMember\n        has_default_factory = not isinstance(catch_all_field.default_factory,\n                                             # type: ignore\n                                             dataclasses._MISSING_TYPE)\n        default_value = _CatchAllUndefinedParameters._SentinelNoDefault\n        if has_default:\n            default_value = catch_all_field.default\n        elif has_default_factory:\n            # This might be unwanted if the default factory constructs\n            # something expensive,\n            # because we have to construct it again just for this test\n            default_value = catch_all_field.default_factory()  # type: ignore\n\n        return default_value", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Core utility method of _CatchAllUndefinedParameters that obtains the default value of a dataclass field, whether from a direct default or a default factory, aiding in handling undefined parameters with possible defaults."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_to_dict", "line_number": 194, "body": "def handle_to_dict(obj, kvs: Dict[Any, Any]) -> Dict[Any, Any]:\n        catch_all_field = \\\n            _CatchAllUndefinedParameters._get_catch_all_field(obj)\n        undefined_parameters = kvs.pop(catch_all_field.name)\n        if isinstance(undefined_parameters, dict):\n            kvs.update(\n                undefined_parameters)  # If desired handle letter case here\n        return kvs", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Method of _CatchAllUndefinedParameters that merges undefined parameters stored under a catch-all field from an object into a given dictionary, consolidating all parameters for easier access and manipulation."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "handle_dump", "line_number": 204, "body": "def handle_dump(obj) -> Dict[Any, Any]:\n        catch_all_field = _CatchAllUndefinedParameters._get_catch_all_field(\n            cls=obj)\n        return getattr(obj, catch_all_field.name)", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Utility method that retrieves the catch-all field from an object, providing access to all undefined or additional parameters stored within that object for flexible data handling."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "create_init", "line_number": 210, "body": "def create_init(obj) -> Callable:\n        original_init = obj.__init__\n        init_signature = inspect.signature(original_init)\n\n        @functools.wraps(obj.__init__)\n        def _catch_all_init(self, *args, **kwargs):\n            known_kwargs, unknown_kwargs = \\\n                _CatchAllUndefinedParameters._separate_defined_undefined_kvs(\n                    obj, kwargs)\n            num_params_takeable = len(\n                init_signature.parameters) - 1  # don't count self\n            if _CatchAllUndefinedParameters._get_catch_all_field(\n                    obj).name not in known_kwargs:\n                num_params_takeable -= 1\n            num_args_takeable = num_params_takeable - len(known_kwargs)\n\n            args, unknown_args = args[:num_args_takeable], args[\n                                                           num_args_takeable:]\n            bound_parameters = init_signature.bind_partial(self, *args,\n                                                           **known_kwargs)\n\n            unknown_args = {f\"_UNKNOWN{i}\": v for i, v in\n                            enumerate(unknown_args)}\n            arguments = bound_parameters.arguments\n            arguments.update(unknown_args)\n            arguments.update(unknown_kwargs)\n            arguments.pop(\"self\", None)\n            final_parameters = _CatchAllUndefinedParameters.handle_from_dict(\n                obj, arguments)\n            original_init(self, **final_parameters)\n\n        return _catch_all_init", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Provides a wrapped __init__ method that intercepts and categorizes known and unknown initialization parameters, enabling classes to gracefully accept extra undefined arguments during instantiation. Useful for flexible object creation with dynamic or evolving parameter sets."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/undefined.py", "function": "_get_catch_all_field", "line_number": 244, "body": "def _get_catch_all_field(cls) -> Field:\n        catch_all_fields = list(\n            filter(lambda f: f.type == Optional[CatchAllVar], fields(cls)))\n        number_of_catch_all_fields = len(catch_all_fields)\n        if number_of_catch_all_fields == 0:\n            raise UndefinedParameterError(\n                \"No field of type dataclasses_json.CatchAll defined\")\n        elif number_of_catch_all_fields > 1:\n            raise UndefinedParameterError(\n                f\"Multiple catch-all fields supplied: \"\n                f\"{number_of_catch_all_fields}.\")\n        else:\n            return catch_all_fields[0]", "is_method": true, "class_name": "_CatchAllUndefinedParameters", "function_description": "Retrieves the single catch-all field of a dataclass if defined, raising errors when none or multiple such fields exist. It ensures correct identification of a unique catch-all parameter in the class."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "dataclass_json", "line_number": 118, "body": "def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n        ...\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        return wrap\n    return wrap(_cls)", "is_method": false, "function_description": "Utility decorator supporting optional parentheses that enhances dataclasses with JSON serialization/deserialization, configurable with letter casing and undefined field handling options."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "_process_class", "line_number": 138, "body": "def _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls", "is_method": false, "function_description": "Utility function that enhances a class to support JSON serialization and deserialization with optional configuration for letter casing and handling of undefined fields. It registers the class as a virtual subclass of DataClassJsonMixin, enabling JSON-related methods."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "to_json", "line_number": 39, "body": "def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)", "is_method": true, "class_name": "DataClassJsonMixin", "function_description": "Provides a JSON string representation of a dataclass instance with customizable serialization options, facilitating easy and flexible conversion of data class objects to JSON format for storage or transmission."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "from_json", "line_number": 63, "body": "def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)", "is_method": true, "class_name": "DataClassJsonMixin", "function_description": "Utility method in DataClassJsonMixin that creates an instance from a JSON string by parsing it and converting the resulting data into a class instance, optionally handling missing fields and custom number parsing."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "from_dict", "line_number": 79, "body": "def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)", "is_method": true, "class_name": "DataClassJsonMixin", "function_description": "Utility method of DataClassJsonMixin that creates an instance of the data class from a dictionary, optionally inferring missing fields during the conversion."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "to_dict", "line_number": 85, "body": "def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)", "is_method": true, "class_name": "DataClassJsonMixin", "function_description": "Utility method in DataClassJsonMixin that converts the instance into a dictionary, optionally encoding the data as JSON-compatible types for seamless serialization or data interchange."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/api.py", "function": "schema", "line_number": 89, "body": "def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)", "is_method": true, "class_name": "DataClassJsonMixin", "function_description": "Provides a schema object for the data class enabling controlled serialization and deserialization with options for field inclusion, exclusion, partial loading, and handling unknown fields. This supports flexible JSON (de)serialization configurations tailored to various use cases."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_user_overrides_or_exts", "line_number": 53, "body": "def _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides", "is_method": false, "function_description": "Generates a dictionary of field-specific configuration overrides for a dataclass by combining global, class-level, and field-level encoder, decoder, and metadata settings. It enables customized serialization behavior tailored to each field type in JSON processing."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_encode_json_type", "line_number": 90, "body": "def _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)", "is_method": false, "function_description": "Utility function that encodes values into JSON-compatible types, returning the value directly if already JSON-serializable; otherwise, it applies a default encoding method."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_encode_overrides", "line_number": 96, "body": "def _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs", "is_method": false, "function_description": "Utility function that applies specified key-based transformations, such as exclusion, letter casing, and custom encoding, to a dictionary's entries; optionally encodes values into JSON-compatible types."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_decode_letter_case_overrides", "line_number": 118, "body": "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names", "is_method": false, "function_description": "Utility function that applies letter case overrides to a list of field names, returning a mapping of overridden names to original names for encoding or decoding purposes."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_decode_dataclass", "line_number": 130, "body": "def _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)", "is_method": false, "function_description": "Core utility function that decodes input data into an instance of a specified dataclass, handling missing fields, type conversions, nested dataclasses, and user-defined decoding overrides. It simplifies transforming raw data into structured dataclass objects for flexible deserialization."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_support_extended_types", "line_number": 211, "body": "def _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res", "is_method": false, "function_description": "Utility function that converts a given value to its corresponding extended type (datetime, Decimal, or UUID) when applicable, ensuring proper type handling for data processing or serialization tasks."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_is_supported_generic", "line_number": 234, "body": "def _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum", "is_method": false, "function_description": "Utility function that determines if a type is a supported generic, including collections (excluding strings), optionals, unions, or Enums, facilitating type checking and validation in type-related operations."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_decode_generic", "line_number": 241, "body": "def _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res", "is_method": false, "function_description": "Function that recursively decodes a given value into the specified generic Python type, handling enums, collections, optionals, and dataclasses for flexible type transformation during deserialization or data parsing."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_decode_dict_keys", "line_number": 283, "body": "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))", "is_method": false, "function_description": "Utility function that converts dictionary keys from strings back to a specified Python type, supporting accurate deserialization of JSON-like objects with non-string keys."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_decode_items", "line_number": 295, "body": "def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items", "is_method": false, "function_description": "Utility function that decodes a sequence of items based on an annotated type or dataclass structure, supporting generics and type inference to convert raw values into typed Python objects."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "_asdict", "line_number": 315, "body": "def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)", "is_method": false, "function_description": "Utility function that converts dataclass instances and nested collections or mappings into dictionaries, optionally encoding them for JSON serialization. It enables consistent and flexible serialization of complex data structures beyond standard dataclasses."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/core.py", "function": "default", "line_number": 33, "body": "def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result", "is_method": true, "class_name": "_ExtendedEncoder", "function_description": "Core utility method of the _ExtendedEncoder class that converts various non-standard Python objects (collections, datetime, UUID, Enum, Decimal) into JSON-serializable formats for custom JSON encoding."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "build_type", "line_number": 227, "body": "def build_type(type_, options, mixin, field, cls):\n    def inner(type_, options):\n        while True:\n            if not _is_new_type(type_):\n                break\n\n            type_ = type_.__supertype__\n\n        if is_dataclass(type_):\n            if _issubclass_safe(type_, mixin):\n                options['field_many'] = bool(\n                    _is_supported_generic(field.type) and _is_collection(\n                        field.type))\n                return fields.Nested(type_.schema(), **options)\n            else:\n                warnings.warn(f\"Nested dataclass field {field.name} of type \"\n                              f\"{field.type} detected in \"\n                              f\"{cls.__name__} that is not an instance of \"\n                              f\"dataclass_json. Did you mean to recursively \"\n                              f\"serialize this field? If so, make sure to \"\n                              f\"augment {type_} with either the \"\n                              f\"`dataclass_json` decorator or mixin.\")\n                return fields.Field(**options)\n\n        origin = getattr(type_, '__origin__', type_)\n        args = [inner(a, {}) for a in getattr(type_, '__args__', []) if\n                a is not type(None)]\n\n        if _is_optional(type_):\n            options[\"allow_none\"] = True\n\n        if origin in TYPES:\n            return TYPES[origin](*args, **options)\n\n        if _issubclass_safe(origin, Enum):\n            return EnumField(enum=origin, by_value=True, *args, **options)\n\n        if is_union_type(type_):\n            union_types = [a for a in getattr(type_, '__args__', []) if\n                           a is not type(None)]\n            union_desc = dict(zip(union_types, args))\n            return _UnionField(union_desc, cls, field, **options)\n\n        warnings.warn(\n            f\"Unknown type {type_} at {cls.__name__}.{field.name}: {field.type} \"\n            f\"It's advised to pass the correct marshmallow type to `mm_field`.\")\n        return fields.Field(**options)\n\n    return inner(type_, options)", "is_method": false, "function_description": "Core function that generates appropriate marshmallow fields for type-annotated dataclass fields, enabling correct serialization and deserialization of complex, nested, optional, enum, and union types."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "schema", "line_number": 278, "body": "def schema(cls, mixin, infer_missing):\n    schema = {}\n    overrides = _user_overrides_or_exts(cls)\n    # TODO check the undefined parameters and add the proper schema action\n    #  https://marshmallow.readthedocs.io/en/stable/quickstart.html\n    for field in dc_fields(cls):\n        metadata = (field.metadata or {}).get('dataclasses_json', {})\n        metadata = overrides[field.name]\n        if metadata.mm_field is not None:\n            schema[field.name] = metadata.mm_field\n        else:\n            type_ = field.type\n            options = {}\n            missing_key = 'missing' if infer_missing else 'default'\n            if field.default is not MISSING:\n                options[missing_key] = field.default\n            elif field.default_factory is not MISSING:\n                options[missing_key] = field.default_factory\n\n            if options.get(missing_key, ...) is None:\n                options['allow_none'] = True\n\n            if _is_optional(type_):\n                options.setdefault(missing_key, None)\n                options['allow_none'] = True\n                if len(type_.__args__) == 2:\n                    # Union[str, int, None] is optional too, but it has more than 1 typed field.\n                    type_ = type_.__args__[0]\n\n            if metadata.letter_case is not None:\n                options['data_key'] = metadata.letter_case(field.name)\n\n            t = build_type(type_, options, mixin, field, cls)\n            # if type(t) is not fields.Field:  # If we use `isinstance` we would return nothing.\n            if field.type != typing.Optional[CatchAllVar]:\n                schema[field.name] = t\n\n    return schema", "is_method": false, "function_description": "Generates a Marshmallow serialization schema for a dataclass by inspecting its fields, types, defaults, and user overrides. It enables customized and flexible serialization/deserialization of dataclass instances based on their metadata and type annotations."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "build_schema", "line_number": 318, "body": "def build_schema(cls: typing.Type[A],\n                 mixin,\n                 infer_missing,\n                 partial) -> typing.Type[SchemaType]:\n    Meta = type('Meta',\n                (),\n                {'fields': tuple(field.name for field in dc_fields(cls)\n                                 if\n                                 field.name != 'dataclass_json_config' and field.type !=\n                                 typing.Optional[CatchAllVar]),\n                 # TODO #180\n                 # 'render_module': global_config.json_module\n                 })\n\n    @post_load\n    def make_instance(self, kvs, **kwargs):\n        return _decode_dataclass(cls, kvs, partial)\n\n    def dumps(self, *args, **kwargs):\n        if 'cls' not in kwargs:\n            kwargs['cls'] = _ExtendedEncoder\n\n        return Schema.dumps(self, *args, **kwargs)\n\n    def dump(self, obj, *, many=None):\n        dumped = Schema.dump(self, obj, many=many)\n        # TODO This is hacky, but the other option I can think of is to generate a different schema\n        #  depending on dump and load, which is even more hacky\n\n        # The only problem is the catch all field, we can't statically create a schema for it\n        # so we just update the dumped dict\n        if many:\n            for i, _obj in enumerate(obj):\n                dumped[i].update(\n                    _handle_undefined_parameters_safe(cls=_obj, kvs={},\n                                                      usage=\"dump\"))\n        else:\n            dumped.update(_handle_undefined_parameters_safe(cls=obj, kvs={},\n                                                            usage=\"dump\"))\n        return dumped\n\n    schema_ = schema(cls, mixin, infer_missing)\n    DataClassSchema: typing.Type[SchemaType] = type(\n        f'{cls.__name__.capitalize()}Schema',\n        (Schema,),\n        {'Meta': Meta,\n         f'make_{cls.__name__.lower()}': make_instance,\n         'dumps': dumps,\n         'dump': dump,\n         **schema_})\n\n    return DataClassSchema", "is_method": false, "function_description": "Creates and returns a customized Marshmallow schema class for a given dataclass, enabling serialization and deserialization with support for partial loading and handling of undefined fields. This function automates schema generation to facilitate JSON encoding/decoding of dataclass instances."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_serialize", "line_number": 30, "body": "def _serialize(self, value, attr, obj, **kwargs):\n        if value is not None:\n            return value.timestamp()\n        else:\n            if not self.required:\n                return None\n            else:\n                raise ValidationError(self.default_error_messages[\"required\"])", "is_method": true, "class_name": "_TimestampField", "function_description": "Converts a given timestamp value to its numeric timestamp representation or enforces presence of the value if required, raising an error if missing. Useful for serializing datetime-like fields to a standardized format."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_deserialize", "line_number": 39, "body": "def _deserialize(self, value, attr, data, **kwargs):\n        if value is not None:\n            return _timestamp_to_dt_aware(value)\n        else:\n            if not self.required:\n                return None\n            else:\n                raise ValidationError(self.default_error_messages[\"required\"])", "is_method": true, "class_name": "_TimestampField", "function_description": "Deserializes a timestamp value into an aware datetime object, enforcing presence if required. It ensures valid datetime conversion within data validation processes in the _TimestampField context."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_serialize", "line_number": 50, "body": "def _serialize(self, value, attr, obj, **kwargs):\n        if value is not None:\n            return value.isoformat()\n        else:\n            if not self.required:\n                return None\n            else:\n                raise ValidationError(self.default_error_messages[\"required\"])", "is_method": true, "class_name": "_IsoField", "function_description": "Utility method of the _IsoField class that serializes a value to ISO format if present, and handles missing required values by raising a validation error or returning None when not required."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_deserialize", "line_number": 59, "body": "def _deserialize(self, value, attr, data, **kwargs):\n        if value is not None:\n            return datetime.fromisoformat(value)\n        else:\n            if not self.required:\n                return None\n            else:\n                raise ValidationError(self.default_error_messages[\"required\"])", "is_method": true, "class_name": "_IsoField", "function_description": "Deserializes an ISO format datetime string into a datetime object, enforcing presence if required. It supports validation by raising an error when a required value is missing."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_serialize", "line_number": 76, "body": "def _serialize(self, value, attr, obj, **kwargs):\n        if self.allow_none and value is None:\n            return None\n        for type_, schema_ in self.desc.items():\n            if _issubclass_safe(type(value), type_):\n                if is_dataclass(value):\n                    res = schema_._serialize(value, attr, obj, **kwargs)\n                    res['__type'] = str(type_.__name__)\n                    return res\n                break\n            elif isinstance(value, _get_type_origin(type_)):\n                return schema_._serialize(value, attr, obj, **kwargs)\n        else:\n            warnings.warn(\n                f'The type \"{type(value).__name__}\" (value: \"{value}\") '\n                f'is not in the list of possible types of typing.Union '\n                f'(dataclass: {self.cls.__name__}, field: {self.field.name}). '\n                f'Value cannot be serialized properly.')\n        return super()._serialize(value, attr, obj, **kwargs)", "is_method": true, "class_name": "_UnionField", "function_description": "Provides serialization of union-typed fields by selecting and applying the appropriate schema based on the runtime type of the value, supporting dataclasses and type origin checks while handling None values and unsupported types gracefully."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "_deserialize", "line_number": 96, "body": "def _deserialize(self, value, attr, data, **kwargs):\n        tmp_value = deepcopy(value)\n        if isinstance(tmp_value, dict) and '__type' in tmp_value:\n            dc_name = tmp_value['__type']\n            for type_, schema_ in self.desc.items():\n                if is_dataclass(type_) and type_.__name__ == dc_name:\n                    del tmp_value['__type']\n                    return schema_._deserialize(tmp_value, attr, data, **kwargs)\n        for type_, schema_ in self.desc.items():\n            if isinstance(tmp_value, _get_type_origin(type_)):\n                return schema_._deserialize(tmp_value, attr, data, **kwargs)\n        else:\n            warnings.warn(\n                f'The type \"{type(tmp_value).__name__}\" (value: \"{tmp_value}\") '\n                f'is not in the list of possible types of typing.Union '\n                f'(dataclass: {self.cls.__name__}, field: {self.field.name}). '\n                f'Value cannot be deserialized properly.')\n        return super()._deserialize(tmp_value, attr, data, **kwargs)", "is_method": true, "class_name": "_UnionField", "function_description": "Method of the _UnionField class that deserializes a value based on its matching data type within a union, supporting discriminated dataclasses and type checking for accurate data reconstruction."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "dumps", "line_number": 336, "body": "def dumps(self, *args, **kwargs):\n        if 'cls' not in kwargs:\n            kwargs['cls'] = _ExtendedEncoder\n\n        return Schema.dumps(self, *args, **kwargs)", "is_method": false, "function_description": "Core utility method that serializes an object to a JSON-formatted string, using a custom encoder by default to handle extended or complex data types."}, {"file": "./dataset/RepoExec/test-apps/dataclasses-json/dataclasses_json/mm.py", "function": "dump", "line_number": 342, "body": "def dump(self, obj, *, many=None):\n        dumped = Schema.dump(self, obj, many=many)\n        # TODO This is hacky, but the other option I can think of is to generate a different schema\n        #  depending on dump and load, which is even more hacky\n\n        # The only problem is the catch all field, we can't statically create a schema for it\n        # so we just update the dumped dict\n        if many:\n            for i, _obj in enumerate(obj):\n                dumped[i].update(\n                    _handle_undefined_parameters_safe(cls=_obj, kvs={},\n                                                      usage=\"dump\"))\n        else:\n            dumped.update(_handle_undefined_parameters_safe(cls=obj, kvs={},\n                                                            usage=\"dump\"))\n        return dumped", "is_method": false, "function_description": "Method enhancing schema serialization by dumping an object or list of objects, while safely handling undefined parameters to ensure complete and robust data output."}]