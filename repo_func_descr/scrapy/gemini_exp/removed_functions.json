[{"file": "./dataset/RepoExec/test-apps/scrapy/extras/qpsclient.py", "function": "parse", "line_number": 54, "body": "def parse(self, response):\n        pass", "is_method": true, "class_name": "QPSSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_get_mwlist_from_settings", "line_number": 24, "body": "def _get_mwlist_from_settings(cls, settings):\n        raise NotImplementedError", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "from_settings", "line_number": 11, "body": "def from_settings(cls, settings):\n        return cls()", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_seen", "line_number": 14, "body": "def request_seen(self, request):\n        return False", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "open", "line_number": 17, "body": "def open(self):  # can return deferred\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "close", "line_number": 20, "body": "def close(self, reason):  # can return a deferred\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "log", "line_number": 23, "body": "def log(self, request, spider):  # log that a request has been filtered\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 39, "body": "def __new__(cls, *args, **kwargs):\n        if issubclass(cls, BaseItem) and not issubclass(cls, (Item, DictItem)):\n            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls, *args, **kwargs)", "is_method": true, "class_name": "BaseItem", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "open_spider", "line_number": 41, "body": "def open_spider(self, spider):\n        pass", "is_method": true, "class_name": "StatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "_persist_stats", "line_number": 50, "body": "def _persist_stats(self, stats, spider):\n        pass", "is_method": true, "class_name": "StatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_value", "line_number": 66, "body": "def get_value(self, key, default=None, spider=None):\n        return default", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_value", "line_number": 69, "body": "def set_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_stats", "line_number": 72, "body": "def set_stats(self, stats, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "inc_value", "line_number": 75, "body": "def inc_value(self, key, count=1, start=0, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "max_value", "line_number": 78, "body": "def max_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "min_value", "line_number": 81, "body": "def min_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 33, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n        pass", "is_method": true, "class_name": "RobotParser", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 46, "body": "def allowed(self, url, user_agent):\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: str\n\n        :param user_agent: User agent\n        :type user_agent: str\n        \"\"\"\n        pass", "is_method": true, "class_name": "RobotParser", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "from_crawler", "line_number": 146, "body": "def from_crawler(cls, crawler):\n        return cls()", "is_method": true, "class_name": "LogFormatter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "cancel", "line_number": 58, "body": "def cancel(self):\n        raise NotImplementedError()", "is_method": true, "class_name": "HostResolution", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 44, "body": "def export_item(self, item):\n        raise NotImplementedError", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 51, "body": "def start_exporting(self):\n        pass", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 54, "body": "def finish_exporting(self):\n        pass", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_spider_exception", "line_number": 122, "body": "def process_spider_exception(_failure):\n            return self._process_spider_exception(response, spider, _failure)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "pauseProducing", "line_number": 480, "body": "def pauseProducing(self):\n        pass", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "stopProducing", "line_number": 483, "body": "def stopProducing(self):\n        pass", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "normvalue", "line_number": 43, "body": "def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value", "is_method": true, "class_name": "CaselessDict", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__instancecheck__", "line_number": 80, "body": "def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})", "is_method": true, "class_name": "DeprecatedClass", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "syntax", "line_number": 32, "body": "def syntax(self):\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "short_desc", "line_number": 38, "body": "def short_desc(self):\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "run", "line_number": 104, "body": "def run(self, args, opts):\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "_start_requests", "line_number": 147, "body": "def _start_requests(spider):\n            yield self.prepare_request(spider, Request(url), opts)", "is_method": true, "class_name": "Command", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "update_vars", "line_number": 43, "body": "def update_vars(self, vars):\n        \"\"\"You can use this function to update the Scrapy objects that will be\n        available in the shell\n        \"\"\"\n        pass", "is_method": true, "class_name": "Command", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "acquire", "line_number": 110, "body": "def acquire(self):\n        pass", "is_method": true, "class_name": "_DummyLock", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "release", "line_number": 113, "body": "def release(self):\n        pass", "is_method": true, "class_name": "_DummyLock", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "info", "line_number": 192, "body": "def info(self):\n        return self", "is_method": true, "class_name": "WrappedResponse", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/common.py", "function": "newsetter", "line_number": 2, "body": "def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n        raise AttributeError(msg)", "is_method": false, "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "xpath", "line_number": 128, "body": "def xpath(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "parse_start_url", "line_number": 78, "body": "def parse_start_url(self, response, **kwargs):\n        return []", "is_method": true, "class_name": "CrawlSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "process_results", "line_number": 81, "body": "def process_results(self, response, results):\n        return results", "is_method": true, "class_name": "CrawlSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "adapt_response", "line_number": 38, "body": "def adapt_response(self, response):\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_node", "line_number": 45, "body": "def parse_node(self, response, selector):\n        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n        if hasattr(self, 'parse_item'):  # backward compatibility\n            return self.parse_item(response, selector)\n        raise NotImplementedError", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "process_results", "line_number": 107, "body": "def process_results(self, response, results):\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return results", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "adapt_response", "line_number": 111, "body": "def adapt_response(self, response):\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return response", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_row", "line_number": 115, "body": "def parse_row(self, response, row):\n        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n        raise NotImplementedError", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "make_requests_from_url", "line_number": 79, "body": "def make_requests_from_url(self, url):\n        \"\"\" This method is deprecated. \"\"\"\n        warnings.warn(\n            \"Spider.make_requests_from_url method is deprecated: \"\n            \"it will be removed and not be called by the default \"\n            \"Spider.start_requests method in future Scrapy releases. \"\n            \"Please override Spider.start_requests method instead.\"\n        )\n        return Request(url, dont_filter=True)", "is_method": true, "class_name": "Spider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "parse", "line_number": 92, "body": "def parse(self, response, **kwargs):\n        raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')", "is_method": true, "class_name": "Spider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 77, "body": "def _store_in_thread(self, file):\n        raise NotImplementedError", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 97, "body": "def store(self, file):\n        pass", "is_method": true, "class_name": "StdoutFeedStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 115, "body": "def store(self, file):\n        file.close()", "is_method": true, "class_name": "FileFeedStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_fresh", "line_number": 35, "body": "def is_cached_response_fresh(self, cachedresponse, request):\n        return True", "is_method": true, "class_name": "DummyPolicy", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "open_spider", "line_number": 282, "body": "def open_spider(self, spider):\n        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n                     extra={'spider': spider})", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "close_spider", "line_number": 286, "body": "def close_spider(self, spider):\n        pass", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 37, "body": "def referrer(self, response_url, request_url):\n        raise NotImplementedError()", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "adjust_request_args", "line_number": 69, "body": "def adjust_request_args(self, args):\n        return args", "is_method": true, "class_name": "Contract", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_to_download", "line_number": 220, "body": "def media_to_download(self, request, info, *, item=None):\n        \"\"\"Check request before starting download\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "get_media_requests", "line_number": 224, "body": "def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "file_path", "line_number": 249, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}]