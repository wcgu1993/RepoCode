[{"file": "./dataset/RepoExec/test-apps/scrapy/setup.py", "function": "has_environment_marker_platform_impl_support", "line_number": 10, "body": "def has_environment_marker_platform_impl_support():\n    \"\"\"Code extracted from 'pytest/setup.py'\n    https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31\n\n    The first known release to support environment marker with range operators\n    it is 18.5, see:\n    https://setuptools.readthedocs.io/en/latest/history.html#id235\n    \"\"\"\n    return parse_version(setuptools_version) >= parse_version('18.5')", "is_method": false, "function_description": "Determines if the installed `setuptools` version supports environment markers with range operators. It provides a compatibility check for advanced dependency specifications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "_py_files", "line_number": 10, "body": "def _py_files(folder):\n    return (str(p) for p in Path(folder).rglob('*.py'))", "is_method": false, "function_description": "Generates paths to all Python files found within a specified folder and its subdirectories. This is useful for tasks like code analysis or project-wide file operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "chdir", "line_number": 30, "body": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()", "is_method": false, "function_description": "Changes the current working directory to a pytest-provided temporary directory. This is useful for tests requiring an isolated file system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_collection_modifyitems", "line_number": 35, "body": "def pytest_collection_modifyitems(session, config, items):\n    # Avoid executing tests when executing `--flake8` flag (pytest-flake8)\n    try:\n        from pytest_flake8 import Flake8Item\n        if config.getoption('--flake8'):\n            items[:] = [item for item in items if isinstance(item, Flake8Item)]\n    except ImportError:\n        pass", "is_method": false, "function_description": "Modifies pytest's collected test items to run only Flake8 checks when the `--flake8` option is specified. This prevents execution of other tests, allowing focused linting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_addoption", "line_number": 45, "body": "def pytest_addoption(parser):\n    parser.addoption(\n        \"--reactor\",\n        default=\"default\",\n        choices=[\"default\", \"asyncio\"],\n    )", "is_method": false, "function_description": "This pytest hook adds a `--reactor` command-line option. It allows users to specify the reactor type (e.g., default or asyncio) for test execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "reactor_pytest", "line_number": 54, "body": "def reactor_pytest(request):\n    if not request.cls:\n        # doctests\n        return\n    request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n    return request.cls.reactor_pytest", "is_method": false, "function_description": "Injects the value of the `--reactor` pytest command-line option into test classes. This enables tests to configure behavior based on the specified reactor."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "only_asyncio", "line_number": 63, "body": "def only_asyncio(request, reactor_pytest):\n    if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n        pytest.skip('This test is only run with --reactor=asyncio')", "is_method": false, "function_description": "This Pytest utility function conditionally skips a test. It ensures tests marked for 'only_asyncio' are skipped if the `asyncio` reactor is not currently active."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_configure", "line_number": 68, "body": "def pytest_configure(config):\n    if config.getoption(\"--reactor\") == \"asyncio\":\n        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")", "is_method": false, "function_description": "Configures the Twisted reactor for tests. If the `--reactor=asyncio` option is provided, it installs the AsyncioSelectorReactor, enabling asyncio integration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conf.py", "function": "setup", "line_number": 315, "body": "def setup(app):\n    app.connect('autodoc-skip-member', maybe_skip_member)", "is_method": false, "function_description": "Sets up an application's autodoc extension to selectively skip members. It registers a callback for the 'autodoc-skip-member' event."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conf.py", "function": "maybe_skip_member", "line_number": 319, "body": "def maybe_skip_member(app, what, name, obj, skip, options):\n    if not skip:\n        # autodocs was generating a text \"alias of\" for the following members\n        # https://github.com/sphinx-doc/sphinx/issues/4422\n        return name in {'default_item_class', 'default_selector_class'}\n    return skip", "is_method": false, "function_description": "Determines if a documentation member should be skipped. It specifically forces skipping for 'default_item_class' and 'default_selector_class' to address an autodoc generation issue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conftest.py", "function": "load_response", "line_number": 11, "body": "def load_response(url, filename):\n    input_path = os.path.join(os.path.dirname(__file__), '_tests', filename)\n    with open(input_path, 'rb') as input_file:\n        return HtmlResponse(url, body=input_file.read())", "is_method": false, "function_description": "Loads content from a local file into an `HtmlResponse` object for a given URL. This service is primarily used for simulating web responses in tests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conftest.py", "function": "setup", "line_number": 17, "body": "def setup(namespace):\n    namespace['load_response'] = load_response", "is_method": false, "function_description": "Injects the `load_response` function into a given namespace, making it accessible for use within that context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/utils/linkfix.py", "function": "main", "line_number": 18, "body": "def main():\n\n    # Used for remembering the file (and its contents)\n    # so we don't have to open the same file again.\n    _filename = None\n    _contents = None\n\n    # A regex that matches standard linkcheck output lines\n    line_re = re.compile(r'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n\n    # Read lines from the linkcheck output file\n    try:\n        with open(\"build/linkcheck/output.txt\") as out:\n            output_lines = out.readlines()\n    except IOError:\n        print(\"linkcheck output not found; please run linkcheck first.\")\n        exit(1)\n\n    # For every line, fix the respective file\n    for line in output_lines:\n        match = re.match(line_re, line)\n\n        if match:\n            newfilename = match.group(1)\n            errortype = match.group(2)\n\n            # Broken links can't be fixed and\n            # I am not sure what do with the local ones.\n            if errortype.lower() in [\"broken\", \"local\"]:\n                print(\"Not Fixed: \" + line)\n            else:\n                # If this is a new file\n                if newfilename != _filename:\n\n                    # Update the previous file\n                    if _filename:\n                        with open(_filename, \"w\") as _file:\n                            _file.write(_contents)\n\n                    _filename = newfilename\n\n                    # Read the new file to memory\n                    with open(_filename) as _file:\n                        _contents = _file.read()\n\n                _contents = _contents.replace(match.group(3), match.group(4))\n        else:\n            # We don't understand what the current line means!\n            print(\"Not Understood: \" + line)", "is_method": false, "function_description": "Reads a linkcheck output file and attempts to automatically fix identified issues in source files. It parses each error line, updates the respective file content in memory, and writes changes back to disk."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "is_setting_index", "line_number": 17, "body": "def is_setting_index(node):\n    if node.tagname == 'index':\n        # index entries for setting directives look like:\n        # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n        entry_type, info, refid = node['entries'][0][:3]\n        return entry_type == 'pair' and info.endswith('; setting')\n    return False", "is_method": false, "function_description": "Determines if a given document node represents an index entry for a 'setting' directive. It specifically checks the tag name and entry format to identify such nodes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "get_setting_target", "line_number": 26, "body": "def get_setting_target(node):\n    # target nodes are placed next to the node in the doc tree\n    return node.parent[node.parent.index(node) + 1]", "is_method": false, "function_description": "This function retrieves the node immediately following a given node within a document tree structure, useful for processing sequentially related elements."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "get_setting_name_and_refid", "line_number": 31, "body": "def get_setting_name_and_refid(node):\n    \"\"\"Extract setting name from directive index node\"\"\"\n    entry_type, info, refid = node['entries'][0][:3]\n    return info.replace('; setting', ''), refid", "is_method": false, "function_description": "Extracts a clean setting name and its reference ID from a directive index node. This function is useful for processing documentation entries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "collect_scrapy_settings_refs", "line_number": 37, "body": "def collect_scrapy_settings_refs(app, doctree):\n    env = app.builder.env\n\n    if not hasattr(env, 'scrapy_all_settings'):\n        env.scrapy_all_settings = []\n\n    for node in doctree.traverse(is_setting_index):\n        targetnode = get_setting_target(node)\n        assert isinstance(targetnode, nodes.target), \"Next node is not a target\"\n\n        setting_name, refid = get_setting_name_and_refid(node)\n\n        env.scrapy_all_settings.append({\n            'docname': env.docname,\n            'setting_name': setting_name,\n            'refid': refid,\n        })", "is_method": false, "function_description": "This function collects references to Scrapy settings within a document's parse tree. It stores their names, document locations, and reference IDs in the Sphinx build environment for later indexing or cross-referencing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "make_setting_element", "line_number": 56, "body": "def make_setting_element(setting_data, app, fromdocname):\n    refnode = make_refnode(app.builder, fromdocname,\n                           todocname=setting_data['docname'],\n                           targetid=setting_data['refid'],\n                           child=nodes.Text(setting_data['setting_name']))\n    p = nodes.paragraph()\n    p += refnode\n\n    item = nodes.list_item()\n    item += p\n    return item", "is_method": false, "function_description": "Creates a document list item containing a clickable reference to a specific setting. This function is used to generate structured, navigable lists of settings within documentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "replace_settingslist_nodes", "line_number": 69, "body": "def replace_settingslist_nodes(app, doctree, fromdocname):\n    env = app.builder.env\n\n    for node in doctree.traverse(settingslist_node):\n        settings_list = nodes.bullet_list()\n        settings_list.extend([make_setting_element(d, app, fromdocname)\n                              for d in sorted(env.scrapy_all_settings,\n                                              key=itemgetter('setting_name'))\n                              if fromdocname != d['docname']])\n        node.replace_self(settings_list)", "is_method": false, "function_description": "This function replaces custom placeholder nodes in a Sphinx document tree with an automatically generated, sorted bulleted list of Scrapy settings from other documents. It dynamically populates documentation with configuration details."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "setup", "line_number": 81, "body": "def setup(app):\n    app.add_crossref_type(\n        directivename = \"setting\",\n        rolename      = \"setting\",\n        indextemplate = \"pair: %s; setting\",\n    )\n    app.add_crossref_type(\n        directivename = \"signal\",\n        rolename      = \"signal\",\n        indextemplate = \"pair: %s; signal\",\n    )\n    app.add_crossref_type(\n        directivename = \"command\",\n        rolename      = \"command\",\n        indextemplate = \"pair: %s; command\",\n    )\n    app.add_crossref_type(\n        directivename = \"reqmeta\",\n        rolename      = \"reqmeta\",\n        indextemplate = \"pair: %s; reqmeta\",\n    )\n    app.add_role('source', source_role)\n    app.add_role('commit', commit_role)\n    app.add_role('issue', issue_role)\n    app.add_role('rev', rev_role)\n\n    app.add_node(settingslist_node)\n    app.add_directive('settingslist', SettingsListDirective)\n\n    app.connect('doctree-read', collect_scrapy_settings_refs)\n    app.connect('doctree-resolved', replace_settingslist_nodes)", "is_method": false, "function_description": "Configures a Sphinx application by registering custom cross-reference types, roles, and directives. It extends documentation capabilities for specific project elements, including Scrapy settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "source_role", "line_number": 114, "body": "def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "This function defines a reStructuredText role that generates a hyperlink to a specific file within the Scrapy GitHub repository. It facilitates embedding direct source code references in documentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "issue_role", "line_number": 121, "body": "def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/issues/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'issue ' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Provides a utility to convert an issue number into a direct hyperlink to its corresponding GitHub issue page within the Scrapy project. Simplifies referencing specific Scrapy issues in text."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "commit_role", "line_number": 128, "body": "def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/commit/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'commit ' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Creates a reStructuredText reference node that links to a specific GitHub commit on the Scrapy repository. This allows documentation to easily reference and navigate to particular code changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "rev_role", "line_number": 135, "body": "def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'http://hg.scrapy.org/scrapy/changeset/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'r' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "This function creates a reStructuredText reference node, providing a direct link from a revision number to its corresponding changeset in the Scrapy Mercurial repository."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "run", "line_number": 13, "body": "def run(self):\n        return [settingslist_node('')]", "is_method": true, "class_name": "SettingsListDirective", "function_description": "This method serves as the SettingsListDirective's execution logic, creating and returning a list containing a single `settingslist_node` object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qpsclient.py", "function": "start_requests", "line_number": 37, "body": "def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += f'?latency={self.latency}'\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace('localhost', f'127.0.0.{x + 1}') for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1", "is_method": true, "class_name": "QPSSpider", "function_description": "Generates a continuous stream of initial HTTP requests for the spider. It can include latency and simulate multiple source IPs, likely for load testing or benchmarking purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "_reset_stats", "line_number": 17, "body": "def _reset_stats(self):\n        self.tail.clear()\n        self.start = self.lastmark = self.lasttime = time()", "is_method": true, "class_name": "Root", "function_description": "Resets internal statistical data and associated timers for the object. This prepares the object to begin a new measurement or tracking period."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "getChild", "line_number": 21, "body": "def getChild(self, request, name):\n        return self", "is_method": true, "class_name": "Root", "function_description": "This Root class method consistently returns the root instance itself, effectively treating itself as its only \"child\" regardless of the requested name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "render", "line_number": 24, "body": "def render(self, request):\n        now = time()\n        delta = now - self.lasttime\n\n        # reset stats on high iter-request times caused by client restarts\n        if delta > 3: # seconds\n            self._reset_stats()\n            return ''\n\n        self.tail.appendleft(delta)\n        self.lasttime = now\n        self.concurrent += 1\n\n        if now - self.lastmark >= 3:\n            self.lastmark = now\n            qps = len(self.tail) / sum(self.tail)\n            print(f'samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}')\n\n        if 'latency' in request.args:\n            latency = float(request.args['latency'][0])\n            reactor.callLater(latency, self._finish, request)\n            return NOT_DONE_YET\n\n        self.concurrent -= 1\n        return ''", "is_method": true, "class_name": "Root", "function_description": "This Root class method processes incoming requests, continuously tracking and reporting performance metrics like QPS and concurrent connections. It can also simulate response latency for testing or debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "_finish", "line_number": 50, "body": "def _finish(self, request):\n        self.concurrent -= 1\n        if not request.finished and not request._disconnected:\n            request.finish()", "is_method": true, "class_name": "Root", "function_description": "Decrements the concurrent request counter and safely finalizes an active request. It calls the request's finish method unless already completed or disconnected."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "crawl", "line_number": 80, "body": "def crawl(self, *args, **kwargs):\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        self.crawling = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield defer.maybeDeferred(self.engine.start)\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n            raise", "is_method": true, "class_name": "Crawler", "function_description": "Initiates and manages the entire web crawling operation. It sets up the spider and engine to begin collecting data from designated sources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_spider", "line_number": 97, "body": "def _create_spider(self, *args, **kwargs):\n        return self.spidercls.from_crawler(self, *args, **kwargs)", "is_method": true, "class_name": "Crawler", "function_description": "Creates an instance of the configured spider class using the crawler as context. This prepares a spider for its crawling operations within the framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_engine", "line_number": 100, "body": "def _create_engine(self):\n        return ExecutionEngine(self, lambda _: self.stop())", "is_method": true, "class_name": "Crawler", "function_description": "Creates and configures an `ExecutionEngine` specific to the crawler instance. This engine is responsible for managing the overall crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "stop", "line_number": 104, "body": "def stop(self):\n        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        if self.crawling:\n            self.crawling = False\n            yield defer.maybeDeferred(self.engine.stop)", "is_method": true, "class_name": "Crawler", "function_description": "Initiates a graceful shutdown of the crawler, returning an object that signals its completion. This method provides a clean way to halt the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_get_spider_loader", "line_number": 132, "body": "def _get_spider_loader(settings):\n        \"\"\" Get SpiderLoader instance from settings \"\"\"\n        cls_path = settings.get('SPIDER_LOADER_CLASS')\n        loader_cls = load_object(cls_path)\n        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n        try:\n            verifyClass(ISpiderLoader, loader_cls)\n        except excs:\n            warnings.warn(\n                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n                'Please add all missing methods to avoid unexpected runtime errors.',\n                category=ScrapyDeprecationWarning, stacklevel=2\n            )\n        return loader_cls.from_settings(settings.frozencopy())", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Retrieves and initializes a SpiderLoader instance based on provided settings. This enables the CrawlerRunner to locate and manage defined web crawling spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "spiders", "line_number": 159, "body": "def spiders(self):\n        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n                      \"CrawlerRunner.spider_loader.\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        return self.spider_loader", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Provides access to the spider loader attribute. This method is deprecated and issues a warning, serving as a compatibility alias."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "crawl", "line_number": 165, "body": "def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Initiates and runs a web crawling process. It can take a spider class to create a new crawler or use an existing crawler instance, tracking its lifecycle."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_crawl", "line_number": 194, "body": "def _crawl(self, crawler, *args, **kwargs):\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result\n\n        return d.addBoth(_done)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Manages the execution lifecycle of a web crawler. It initiates an asynchronous crawl, tracking its active status and ensuring cleanup upon completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "create_crawler", "line_number": 207, "body": "def create_crawler(self, crawler_or_spidercls):\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "This `CrawlerRunner` method provides a Scrapy `Crawler` object, creating it from a spider class or name, or returning an existing `Crawler` instance. It streamlines the setup for running web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_crawler", "line_number": 226, "body": "def _create_crawler(self, spidercls):\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Provides a Crawler instance for a given spider class, handling spider class loading by name. This internal method prepares a crawler for the CrawlerRunner's execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "stop", "line_number": 231, "body": "def stop(self):\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return defer.DeferredList([c.stop() for c in list(self.crawlers)])", "is_method": true, "class_name": "CrawlerRunner", "function_description": "This method stops all active crawling jobs managed by the `CrawlerRunner` instance. It returns a Deferred that fires when all crawlers have successfully terminated."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "join", "line_number": 240, "body": "def join(self):\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "This method allows waiting for all managed crawlers to complete their execution. It returns a Deferred that fires once all crawlers have finished, providing a synchronization point."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_handle_twisted_reactor", "line_number": 250, "body": "def _handle_twisted_reactor(self):\n        if self.settings.get(\"TWISTED_REACTOR\"):\n            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])", "is_method": true, "class_name": "CrawlerRunner", "function_description": "This internal method of `CrawlerRunner` verifies the configured Twisted reactor if specified in the settings. It ensures the correct event loop is prepared for crawling operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_signal_shutdown", "line_number": 285, "body": "def _signal_shutdown(self, signum, _):\n        from twisted.internet import reactor\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                    {'signame': signame})\n        reactor.callFromThread(self._graceful_stop_reactor)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Handles system shutdown signals to initiate a graceful termination of the crawler process, allowing it to complete ongoing tasks before shutting down the reactor."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_signal_kill", "line_number": 293, "body": "def _signal_kill(self, signum, _):\n        from twisted.internet import reactor\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n                    {'signame': signame})\n        reactor.callFromThread(self._stop_reactor)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "This internal method of `CrawlerProcess` handles a repeated termination signal by forcing an unclean shutdown. It ensures the Twisted reactor stops when standard shutdown procedures fail."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "start", "line_number": 301, "body": "def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n        \"\"\"\n        from twisted.internet import reactor\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(self._stop_reactor)\n\n        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n        resolver.install_on_reactor()\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n        reactor.run(installSignalHandlers=False)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Starts the core asynchronous event loop (Twisted reactor) for the web crawling process. It configures the reactor's thread pool and DNS resolution for efficient operation, optionally stopping it after all crawls complete."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_graceful_stop_reactor", "line_number": 329, "body": "def _graceful_stop_reactor(self):\n        d = self.stop()\n        d.addBoth(self._stop_reactor)\n        return d", "is_method": true, "class_name": "CrawlerProcess", "function_description": "This method provides a controlled, graceful shutdown for the entire crawling process. It ensures the underlying Twisted reactor is properly stopped after the crawler completes its stop sequence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_stop_reactor", "line_number": 334, "body": "def _stop_reactor(self, _=None):\n        from twisted.internet import reactor\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass", "is_method": true, "class_name": "CrawlerProcess", "function_description": "As a method of CrawlerProcess, it safely stops the Twisted reactor, halting the asynchronous event loop that drives the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_handle_twisted_reactor", "line_number": 341, "body": "def _handle_twisted_reactor(self):\n        if self.settings.get(\"TWISTED_REACTOR\"):\n            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n        super()._handle_twisted_reactor()", "is_method": true, "class_name": "CrawlerProcess", "function_description": "The `CrawlerProcess` uses this method to configure and install the Twisted reactor and asyncio event loop. This ensures the appropriate asynchronous I/O backend is ready for the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_done", "line_number": 199, "body": "def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Registers a crawler as completed by removing it from active sets within the runner. It also updates the runner's status if the crawler's bootstrap failed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "from_settings", "line_number": 28, "body": "def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                mw = create_instance(mwcls, settings, crawler)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': pprint.pformat(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Configures and initializes a MiddlewareManager instance by dynamically loading and instantiating middleware components based on application settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings, crawler)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Initializes a MiddlewareManager instance by extracting settings from a given crawler object. This method serves as a convenient constructor for crawler-specific middleware setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_add_middleware", "line_number": 55, "body": "def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].appendleft(mw.close_spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "This method registers specific lifecycle methods (`open_spider`, `close_spider`) from a middleware object. It enables the `MiddlewareManager` to execute these methods at corresponding points in a spider's operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_parallel", "line_number": 61, "body": "def _process_parallel(self, methodname, obj, *args):\n        return process_parallel(self.methods[methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "This method orchestrates the parallel execution of a specific registered method within the MiddlewareManager. It applies this method to a given object with provided arguments, enabling concurrent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_chain", "line_number": 64, "body": "def _process_chain(self, methodname, obj, *args):\n        return process_chain(self.methods[methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Chain of Thought:\nThe function `_process_chain` is a private method within `MiddlewareManager`. It takes a `methodname`, an `obj`, and `*args`. Its sole action is to call `process_chain`, passing `self.methods[methodname]`, `obj`, and `*args`. This indicates it's responsible for initiating the processing of a specific sequence of \"methods\" (likely middleware functions) associated with the given `methodname`, applying them to `obj` and its arguments. It acts as an internal dispatcher for different middleware pipelines.\nInitiates the processing of a specified middleware chain. It retrieves the chain by name and applies it to the given object and arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_chain_both", "line_number": 67, "body": "def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n        return process_chain_both(self.methods[cb_methodname],\n                                  self.methods[eb_methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "This internal method within MiddlewareManager facilitates the execution of two specified middleware chains. It dynamically retrieves and applies these named chains to a given object and its arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "open_spider", "line_number": 71, "body": "def open_spider(self, spider):\n        return self._process_parallel('open_spider', spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Coordinates the `open_spider` action across all registered middleware components. This allows them to perform essential setup for a new spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "close_spider", "line_number": 74, "body": "def close_spider(self, spider):\n        return self._process_parallel('close_spider', spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Notifies all registered middleware components that a spider is closing, allowing them to perform necessary cleanup or finalization actions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "connect", "line_number": 10, "body": "def connect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.connect(receiver, signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "This method connects a receiver function to a specified signal, enabling event-driven communication. It allows registering callbacks that execute automatically when the signal is emitted."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "disconnect", "line_number": 27, "body": "def disconnect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.disconnect(receiver, signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Disconnects a specified receiver function from a given signal. This method reverses the effect of previously connecting a receiver to a signal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "send_catch_log", "line_number": 36, "body": "def send_catch_log(self, signal, **kwargs):\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Dispatches a specified signal to all connected handlers. It catches and logs any exceptions raised during signal processing, ensuring robust communication within the system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "send_catch_log_deferred", "line_number": 46, "body": "def send_catch_log_deferred(self, signal, **kwargs):\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "This method sends a signal to its connected handlers and logs any exceptions. It supports asynchronous handlers by returning a Deferred that resolves after all their Deferred objects fire."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "disconnect_all", "line_number": 60, "body": "def disconnect_all(self, signal, **kwargs):\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.disconnect_all(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "This method disconnects all registered receivers from a specified signal. It provides a utility to clear all connections for a given signal within the SignalManager."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "from_settings", "line_number": 42, "body": "def from_settings(cls, settings):\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(job_dir(settings), debug)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Provides a factory method to instantiate an RFPDupeFilter, configuring it with a job directory and debug mode based on application settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_seen", "line_number": 46, "body": "def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + '\\n')", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Determines if a given request has been previously processed by checking its unique fingerprint. If new, it registers the request to prevent future duplicates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_fingerprint", "line_number": 54, "body": "def request_fingerprint(self, request):\n        return request_fingerprint(request)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "This method computes a unique fingerprint for a web request. This fingerprint is used by `RFPDupeFilter` to identify and filter duplicate requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "close", "line_number": 57, "body": "def close(self, reason):\n        if self.file:\n            self.file.close()", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Closes the file resource used by the duplicate filter. This ensures proper cleanup and prevents resource leaks when the filter is no longer needed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "log", "line_number": 61, "body": "def log(self, request, spider):\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {'request': request, 'referer': referer_str(request)}\n            self.logger.debug(msg, args, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request: %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False\n\n        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Logs details of a filtered duplicate web request. It also increments a statistic to track the number of duplicates encountered by the filter."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__instancecheck__", "line_number": 27, "body": "def __instancecheck__(cls, instance):\n        if cls is BaseItem:\n            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__instancecheck__(instance)", "is_method": true, "class_name": "_BaseItemMeta", "function_description": "This method customizes `isinstance()` behavior for its metaclass. It issues a deprecation warning when checking for `scrapy.item.BaseItem` instances, encouraging migration to `scrapy.item.Item`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 56, "body": "def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super().__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        return super().__new__(mcs, class_name, bases, new_attrs)", "is_method": true, "class_name": "ItemMeta", "function_description": "Defines and structures data item classes. It processes declared `Field` attributes into a `fields` dictionary and manages internal class representation and inheritance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 81, "body": "def __new__(cls, *args, **kwargs):\n        if issubclass(cls, DictItem) and not issubclass(cls, Item):\n            warn('scrapy.item.DictItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls, *args, **kwargs)", "is_method": true, "class_name": "DictItem", "function_description": "This constructor method issues a deprecation warning when `DictItem` is instantiated, advising users to transition to `scrapy.item.Item`. It serves as a migration helper for API changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__getitem__", "line_number": 93, "body": "def __getitem__(self, key):\n        return self._values[key]", "is_method": true, "class_name": "DictItem", "function_description": "Enables dictionary-style key-based access to retrieve specific values from the `DictItem` object. It allows instances to be treated like dictionaries for value lookup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__setitem__", "line_number": 96, "body": "def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")", "is_method": true, "class_name": "DictItem", "function_description": "Allows dictionary-style value assignment for a `DictItem` instance. It strictly permits setting values only for keys defined in its pre-configured fields."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__delitem__", "line_number": 102, "body": "def __delitem__(self, key):\n        del self._values[key]", "is_method": true, "class_name": "DictItem", "function_description": "Allows items to be deleted from the `DictItem` instance by key, similar to how elements are removed from a dictionary."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__getattr__", "line_number": 105, "body": "def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n        raise AttributeError(name)", "is_method": true, "class_name": "DictItem", "function_description": "Prevents direct attribute access for specific fields, redirecting users to dictionary-style item access. It ensures a consistent data access pattern for dictionary-like objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__setattr__", "line_number": 110, "body": "def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n        super().__setattr__(name, value)", "is_method": true, "class_name": "DictItem", "function_description": "This method controls how attributes are set on a `DictItem` instance. It prevents direct assignment for public fields, guiding users to use dictionary-like item assignment instead, while allowing private attributes to be set normally."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__len__", "line_number": 115, "body": "def __len__(self):\n        return len(self._values)", "is_method": true, "class_name": "DictItem", "function_description": "Enables the `len()` function to return the count of items contained within a `DictItem` object. This provides a standard way to determine its size."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__iter__", "line_number": 118, "body": "def __iter__(self):\n        return iter(self._values)", "is_method": true, "class_name": "DictItem", "function_description": "Enables iteration over the internal values stored within a `DictItem` instance. This allows it to be used directly in loops and other iterable contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "keys", "line_number": 123, "body": "def keys(self):\n        return self._values.keys()", "is_method": true, "class_name": "DictItem", "function_description": "Provides access to the keys of the dictionary-like data stored within the `DictItem` instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__repr__", "line_number": 126, "body": "def __repr__(self):\n        return pformat(dict(self))", "is_method": true, "class_name": "DictItem", "function_description": "Provides a human-readable string representation of the DictItem instance. It displays the object's dictionary-like contents in a pretty-printed format for debugging and introspection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "copy", "line_number": 129, "body": "def copy(self):\n        return self.__class__(self)", "is_method": true, "class_name": "DictItem", "function_description": "Creates and returns a new `DictItem` instance that is a shallow copy of the current object. This allows for duplicating `DictItem` objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "deepcopy", "line_number": 132, "body": "def deepcopy(self):\n        \"\"\"Return a :func:`~copy.deepcopy` of this item.\n        \"\"\"\n        return deepcopy(self)", "is_method": true, "class_name": "DictItem", "function_description": "Returns a completely independent, deep copy of the `DictItem` instance, including all nested mutable objects. This allows modifications to the copy without affecting the original."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extension.py", "function": "_get_mwlist_from_settings", "line_number": 15, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('EXTENSIONS'))", "is_method": true, "class_name": "ExtensionManager", "function_description": "Extracts extension configurations from application settings and builds a structured list of components. Used internally by the ExtensionManager to prepare a list of available extensions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_mimetype", "line_number": 41, "body": "def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = f\"{mimetype.split('/')[0]}/*\"\n            return self.classes.get(basetype, Response)", "is_method": true, "class_name": "ResponseTypes", "function_description": "Maps a given MIME type string to the most suitable `Response` class. It provides specific class mappings, handles broader base types, and defaults to a generic `Response` if no match is found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_content_type", "line_number": 51, "body": "def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_unicode(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)", "is_method": true, "class_name": "ResponseTypes", "function_description": "Provides the most appropriate `Response` class by interpreting an HTTP `Content-Type` header. It maps various content types to specific response handling logic."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_content_disposition", "line_number": 59, "body": "def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_unicode(\n                content_disposition, encoding='latin-1', errors='replace'\n            ).split(';')[1].split('=')[1].strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response", "is_method": true, "class_name": "ResponseTypes", "function_description": "Determines the appropriate response type by parsing the filename from a `Content-Disposition` header, providing a fallback if the filename cannot be extracted."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_headers", "line_number": 68, "body": "def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-Type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls", "is_method": true, "class_name": "ResponseTypes", "function_description": "This method identifies the most appropriate `Response` class from HTTP headers. It intelligently selects the specific `Response` subclass by examining `Content-Type` and `Content-Disposition` for proper handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_filename", "line_number": 81, "body": "def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response", "is_method": true, "class_name": "ResponseTypes", "function_description": "Infers and returns the most appropriate Response class for a file based on its filename's MIME type. It helps select the correct handler for different file formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_body", "line_number": 89, "body": "def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')", "is_method": true, "class_name": "ResponseTypes", "function_description": "This method of the ResponseTypes class infers the appropriate content type (MIME type) for a response by analyzing its raw body content. It serves as a fallback when standard type guessing is not possible."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_args", "line_number": 105, "body": "def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls", "is_method": true, "class_name": "ResponseTypes", "function_description": "Provides a service to dynamically determine the most suitable Response class. It infers the appropriate type based on provided headers, URL, filename, or body content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_value", "line_number": 16, "body": "def get_value(self, key, default=None, spider=None):\n        return self._stats.get(key, default)", "is_method": true, "class_name": "StatsCollector", "function_description": "Retrieves a specific statistic's value by its key from the collector's internal storage. It provides a default value if the requested key is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_stats", "line_number": 19, "body": "def get_stats(self, spider=None):\n        return self._stats", "is_method": true, "class_name": "StatsCollector", "function_description": "Provides access to the current statistics collected by the `StatsCollector`. Use this method to retrieve and inspect the ongoing metrics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_value", "line_number": 22, "body": "def set_value(self, key, value, spider=None):\n        self._stats[key] = value", "is_method": true, "class_name": "StatsCollector", "function_description": "Allows directly setting a specific statistical value identified by a key within the `StatsCollector` instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_stats", "line_number": 25, "body": "def set_stats(self, stats, spider=None):\n        self._stats = stats", "is_method": true, "class_name": "StatsCollector", "function_description": "This method establishes the statistics object for the `StatsCollector`, enabling it to gather and manage operational metrics throughout its lifecycle."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "inc_value", "line_number": 28, "body": "def inc_value(self, key, count=1, start=0, spider=None):\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count", "is_method": true, "class_name": "StatsCollector", "function_description": "This method increments a specified statistical counter by a given amount. It automatically initializes the counter if it doesn't exist."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "max_value", "line_number": 32, "body": "def max_value(self, key, value, spider=None):\n        self._stats[key] = max(self._stats.setdefault(key, value), value)", "is_method": true, "class_name": "StatsCollector", "function_description": "Updates a specific statistic `key` to store the highest `value` observed. This method tracks peak metrics within the collector."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "min_value", "line_number": 35, "body": "def min_value(self, key, value, spider=None):\n        self._stats[key] = min(self._stats.setdefault(key, value), value)", "is_method": true, "class_name": "StatsCollector", "function_description": "The `min_value` method of `StatsCollector` updates a specified statistic key to store the lowest value it has encountered so far. It provides a running minimum for various tracked metrics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "clear_stats", "line_number": 38, "body": "def clear_stats(self, spider=None):\n        self._stats.clear()", "is_method": true, "class_name": "StatsCollector", "function_description": "Resets all collected statistics managed by the `StatsCollector` instance, effectively clearing previous metrics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "close_spider", "line_number": 44, "body": "def close_spider(self, spider, reason):\n        if self._dump:\n            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                        extra={'spider': spider})\n        self._persist_stats(self._stats, spider)", "is_method": true, "class_name": "StatsCollector", "function_description": "Finalizes statistics collection for a spider upon its closure. It logs and persists the gathered statistics for future reference."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "_persist_stats", "line_number": 60, "body": "def _persist_stats(self, stats, spider):\n        self.spider_stats[spider.name] = stats", "is_method": true, "class_name": "MemoryStatsCollector", "function_description": "Stores collected statistics for a specific spider in memory. This internal method saves a spider's performance metrics for later retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_with_mkdir", "line_number": 14, "body": "def _with_mkdir(queue_class):\n\n    class DirectoriesCreated(queue_class):\n\n        def __init__(self, path, *args, **kwargs):\n            dirname = os.path.dirname(path)\n            if not os.path.exists(dirname):\n                os.makedirs(dirname, exist_ok=True)\n\n            super().__init__(path, *args, **kwargs)\n\n    return DirectoriesCreated", "is_method": false, "function_description": "This function is a class factory that enhances an input class by automatically creating the parent directory for a specified path during object initialization. It ensures that necessary directories exist before the wrapped class operates on a file."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_serializable_queue", "line_number": 28, "body": "def _serializable_queue(queue_class, serialize, deserialize):\n\n    class SerializableQueue(queue_class):\n\n        def push(self, obj):\n            s = serialize(obj)\n            super().push(s)\n\n        def pop(self):\n            s = super().pop()\n            if s:\n                return deserialize(s)\n\n    return SerializableQueue", "is_method": false, "function_description": "Creates a new queue class that automatically serializes objects when pushed and deserializes them when popped. This enables queues to store and retrieve complex or persistent data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_scrapy_serialization_queue", "line_number": 44, "body": "def _scrapy_serialization_queue(queue_class):\n\n    class ScrapyRequestQueue(queue_class):\n\n        def __init__(self, crawler, key):\n            self.spider = crawler.spider\n            super().__init__(key)\n\n        @classmethod\n        def from_crawler(cls, crawler, key, *args, **kwargs):\n            return cls(crawler, key)\n\n        def push(self, request):\n            request = request_to_dict(request, self.spider)\n            return super().push(request)\n\n        def pop(self):\n            request = super().pop()\n\n            if not request:\n                return None\n\n            request = request_from_dict(request, self.spider)\n            return request\n\n    return ScrapyRequestQueue", "is_method": false, "function_description": "Generates a Scrapy queue wrapper that automatically serializes and deserializes `Request` objects. This allows persistent storage of Scrapy requests in a standard queue backend."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_scrapy_non_serialization_queue", "line_number": 72, "body": "def _scrapy_non_serialization_queue(queue_class):\n\n    class ScrapyRequestQueue(queue_class):\n        @classmethod\n        def from_crawler(cls, crawler, *args, **kwargs):\n            return cls()\n\n    return ScrapyRequestQueue", "is_method": false, "function_description": "Creates a Scrapy-compatible queue class that wraps a given queue. It adds a `from_crawler` method, allowing it to integrate with Scrapy's initialization process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_pickle_serialize", "line_number": 82, "body": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=4)\n    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n    # TypeError is raised from parsel.Selector\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e)) from e", "is_method": false, "function_description": "Serializes a Python object into a byte stream using pickle protocol 4. This enables the object to be stored, transmitted, or otherwise persisted."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "push", "line_number": 32, "body": "def push(self, obj):\n            s = serialize(obj)\n            super().push(s)", "is_method": true, "class_name": "SerializableQueue", "function_description": "Adds an object to the queue by first serializing it. This prepares the object for reliable storage or transmission within the queue system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "pop", "line_number": 36, "body": "def pop(self):\n            s = super().pop()\n            if s:\n                return deserialize(s)", "is_method": true, "class_name": "SerializableQueue", "function_description": "Retrieves and deserializes the next item from the queue. This makes stored objects immediately usable upon removal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "from_crawler", "line_number": 53, "body": "def from_crawler(cls, crawler, key, *args, **kwargs):\n            return cls(crawler, key)", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Factory method for the ScrapyRequestQueue, enabling its instantiation from a Scrapy crawler object and a unique key."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "push", "line_number": 56, "body": "def push(self, request):\n            request = request_to_dict(request, self.spider)\n            return super().push(request)", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "This method adds a Scrapy request to the queue. It converts the request object into a dictionary format, then enqueues it via the base queue implementation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "pop", "line_number": 60, "body": "def pop(self):\n            request = super().pop()\n\n            if not request:\n                return None\n\n            request = request_from_dict(request, self.spider)\n            return request", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Pops a raw request from the queue and reconstructs it into a Scrapy Request object, making it ready for processing by the spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "from_crawler", "line_number": 76, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n            return cls()", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Creates an instance of ScrapyRequestQueue. This class method acts as a standard factory for the Scrapy framework to initialize queue objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "decode_robotstxt", "line_number": 11, "body": "def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n    try:\n        if to_native_str_type:\n            robotstxt_body = to_unicode(robotstxt_body)\n        else:\n            robotstxt_body = robotstxt_body.decode('utf-8')\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\n            \"Failure while parsing robots.txt. File either contains garbage or \"\n            \"is in an encoding other than UTF-8, treating it as an empty file.\",\n            exc_info=sys.exc_info(),\n            extra={'spider': spider},\n        )\n        robotstxt_body = ''\n    return robotstxt_body", "is_method": false, "function_description": "Decodes raw `robots.txt` content from bytes to a Unicode string. It gracefully handles decoding errors, treating malformed input as empty to ensure consistent parsing for web crawling policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 67, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "PythonRobotParser", "function_description": "This class method creates a PythonRobotParser instance, initializing it with a `robotstxt_body` and an optional `spider` derived from a `crawler` object. It serves as an alternative constructor for crawler integration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 72, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(user_agent, url)", "is_method": true, "class_name": "PythonRobotParser", "function_description": "This method determines if a specific user agent is permitted to fetch a given URL, based on the `robots.txt` rules parsed by the `PythonRobotParser`. It helps web crawlers respect website access policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 85, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "ReppyRobotParser", "function_description": "Creates a `ReppyRobotParser` instance, initializing it with `robots.txt` body and an optional spider extracted from a web crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 90, "body": "def allowed(self, url, user_agent):\n        return self.rp.allowed(url, user_agent)", "is_method": true, "class_name": "ReppyRobotParser", "function_description": "Provides a service to check if a user agent is permitted to access a given URL, adhering to robots.txt protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 103, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "RerpRobotParser", "function_description": "Constructs a `RerpRobotParser` instance. It uses the `robotstxt_body` and an optional spider from a crawler for initialization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 108, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)", "is_method": true, "class_name": "RerpRobotParser", "function_description": "Checks if a given user agent is allowed to access a specific URL based on the robot exclusion rules parsed by the `RerpRobotParser`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 122, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "ProtegoRobotParser", "function_description": "This class method serves as an alternative constructor for `ProtegoRobotParser`. It creates a parser instance from a crawler's context, using the `robots.txt` body and an optional spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 127, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)", "is_method": true, "class_name": "ProtegoRobotParser", "function_description": "Determines if a specified user agent is permitted to access a given URL, based on the parsed robots.txt rules for the site."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_to_bytes_or_none", "line_number": 29, "body": "def _to_bytes_or_none(text):\n    if text is None:\n        return None\n    return to_bytes(text)", "is_method": false, "function_description": "Converts text to bytes, returning None if the input text is None. It provides a null-safe way to transform strings into byte sequences."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "from_settings", "line_number": 50, "body": "def from_settings(cls, settings):\n        return cls(\n            smtphost=settings['MAIL_HOST'],\n            mailfrom=settings['MAIL_FROM'],\n            smtpuser=settings['MAIL_USER'],\n            smtppass=settings['MAIL_PASS'],\n            smtpport=settings.getint('MAIL_PORT'),\n            smtptls=settings.getbool('MAIL_TLS'),\n            smtpssl=settings.getbool('MAIL_SSL'),\n        )", "is_method": true, "class_name": "MailSender", "function_description": "This class method creates a `MailSender` instance by extracting all necessary email configuration details from a provided settings object, simplifying object instantiation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "send", "line_number": 61, "body": "def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n        from twisted.internet import reactor\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split('/', 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg['From'] = self.mailfrom\n        msg['To'] = COMMASPACE.join(to)\n        msg['Date'] = formatdate(localtime=True)\n        msg['Subject'] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg['Cc'] = COMMASPACE.join(cc)\n\n        if charset:\n            msg.set_charset(charset)\n\n        if attachs:\n            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split('/'))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header('Content-Disposition', 'attachment', filename=attach_name)\n                msg.attach(part)\n        else:\n            msg.set_payload(body)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                          'mailattachs': len(attachs)})\n            return\n\n        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n        dfd.addCallbacks(\n            callback=self._sent_ok,\n            errback=self._sent_failed,\n            callbackArgs=[to, cc, subject, len(attachs)],\n            errbackArgs=[to, cc, subject, len(attachs)],\n        )\n        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n        return dfd", "is_method": true, "class_name": "MailSender", "function_description": "Dispatches an email message, including specified recipients, subject, body content, and optional file attachments. It handles message formatting and asynchronous delivery."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sent_ok", "line_number": 114, "body": "def _sent_ok(self, result, to, cc, subject, nattachs):\n        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                    'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                     'mailattachs': nattachs})", "is_method": true, "class_name": "MailSender", "function_description": "This internal method logs the successful dispatch of an email, detailing its recipients, subject, and number of attachments for tracking and auditing purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sent_failed", "line_number": 120, "body": "def _sent_failed(self, failure, to, cc, subject, nattachs):\n        errstr = str(failure.value)\n        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n                     'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n                     '- %(mailerr)s',\n                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                      'mailattachs': nattachs, 'mailerr': errstr})", "is_method": true, "class_name": "MailSender", "function_description": "This method logs comprehensive error details when an email sending operation fails. It captures recipient, subject, attachment count, and the specific error for debugging and monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sendmail", "line_number": 128, "body": "def _sendmail(self, to_addrs, msg):\n        # Import twisted.mail here because it is not available in python3\n        from twisted.internet import reactor\n        from twisted.mail.smtp import ESMTPSenderFactory\n        msg = BytesIO(msg)\n        d = defer.Deferred()\n        factory = ESMTPSenderFactory(\n            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n        )\n        factory.noisy = False\n\n        if self.smtpssl:\n            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d", "is_method": true, "class_name": "MailSender", "function_description": "Provides the core capability to asynchronously send an email message via an SMTP server. It handles connection establishment, authentication, and secure transport."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "crawled", "line_number": 55, "body": "def crawled(self, request, response, spider):\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = f' {str(request.flags)}' if request.flags else ''\n        response_flags = f' {str(response.flags)}' if response.flags else ''\n        return {\n            'level': logging.DEBUG,\n            'msg': CRAWLEDMSG,\n            'args': {\n                'status': response.status,\n                'request': request,\n                'request_flags': request_flags,\n                'referer': referer_str(request),\n                'response_flags': response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                'flags': response_flags\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats and returns logging data for a successfully crawled webpage. It captures essential details like HTTP status and request/response flags, aiding in debugging crawling activity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "scraped", "line_number": 73, "body": "def scraped(self, item, response, spider):\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            'level': logging.DEBUG,\n            'msg': SCRAPEDMSG,\n            'args': {\n                'src': src,\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats a log message when a spider scrapes an item. It provides structured data for logging successful or failed scraping events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "dropped", "line_number": 88, "body": "def dropped(self, item, exception, response, spider):\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        return {\n            'level': logging.WARNING,\n            'msg': DROPPEDMSG,\n            'args': {\n                'exception': exception,\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats a warning log message for an item that was dropped during pipeline processing. This method provides structured data for logging such events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "item_error", "line_number": 99, "body": "def item_error(self, item, exception, response, spider):\n        \"\"\"Logs a message when an item causes an error while it is passing\n        through the item pipeline.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            'level': logging.ERROR,\n            'msg': ITEMERRORMSG,\n            'args': {\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats a structured error log entry for an item that failed processing within an item pipeline. This prepares detailed information about the problematic item for logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "spider_error", "line_number": 113, "body": "def spider_error(self, failure, request, response, spider):\n        \"\"\"Logs an error message from a spider.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            'level': logging.ERROR,\n            'msg': SPIDERERRORMSG,\n            'args': {\n                'request': request,\n                'referer': referer_str(request),\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats spider error details into a structured dictionary, providing data for an ERROR level log message concerning a spider's failed request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "download_error", "line_number": 127, "body": "def download_error(self, failure, request, spider, errmsg=None):\n        \"\"\"Logs a download error message from a spider (typically coming from\n        the engine).\n\n        .. versionadded:: 2.0\n        \"\"\"\n        args = {'request': request}\n        if errmsg:\n            msg = DOWNLOADERRORMSG_LONG\n            args['errmsg'] = errmsg\n        else:\n            msg = DOWNLOADERRORMSG_SHORT\n        return {\n            'level': logging.ERROR,\n            'msg': msg,\n            'args': args,\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Formats a structured log entry for a download error, including request details and an optional custom message, for use by a logging system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "from_crawler", "line_number": 25, "body": "def from_crawler(cls, crawler, reactor):\n        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "This class method configures and instantiates a CachingThreadedResolver. It initializes the DNS resolver with cache settings and timeout values derived from the provided crawler's configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "install_on_reactor", "line_number": 32, "body": "def install_on_reactor(self):\n        self.reactor.installResolver(self)", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "This method registers the CachingThreadedResolver instance with the reactor. It makes the resolver available for use by the reactor's event loop or processing mechanisms."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "getHostByName", "line_number": 35, "body": "def getHostByName(self, name, timeout=None):\n        if name in dnscache:\n            return defer.succeed(dnscache[name])\n        # in Twisted<=16.6, getHostByName() is always called with\n        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n        # so the input argument above is simply overridden\n        # to enforce Scrapy's DNS_TIMEOUT setting's value\n        timeout = (self.timeout,)\n        d = super().getHostByName(name, timeout)\n        if dnscache.limit:\n            d.addCallback(self._cache_result, name)\n        return d", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Resolves a hostname to an IP address, leveraging an internal cache for speed. It returns cached results or performs a new lookup with a configurable timeout, storing the outcome."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "_cache_result", "line_number": 48, "body": "def _cache_result(self, result, name):\n        dnscache[name] = result\n        return result", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Stores a given resolution result in the DNS cache, indexed by name, to enable efficient future lookups."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolutionBegan", "line_number": 69, "body": "def resolutionBegan(self, resolution):\n        self.resolutionReceiver.resolutionBegan(resolution)\n        self.resolution = resolution", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "Notifies an internal receiver that a resolution process has begun. It also stores the resolution object for potential internal caching or tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "addressResolved", "line_number": 73, "body": "def addressResolved(self, address):\n        self.resolutionReceiver.addressResolved(address)\n        self.addresses.append(address)", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "Forwards a resolved address to a primary receiver and stores it in an internal cache."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolutionComplete", "line_number": 77, "body": "def resolutionComplete(self):\n        self.resolutionReceiver.resolutionComplete()\n        if self.addresses:\n            dnscache[self.hostName] = self.addresses", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "Completes a hostname resolution, caching the resolved addresses in a DNS cache. It also forwards the completion signal to an underlying receiver."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "from_crawler", "line_number": 96, "body": "def from_crawler(cls, crawler, reactor):\n        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size)", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "This factory method creates a CachingHostnameResolver instance. It configures the DNS cache size based on the provided crawler's settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "install_on_reactor", "line_number": 103, "body": "def install_on_reactor(self):\n        self.reactor.installNameResolver(self)", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "This method installs the caching hostname resolver onto the reactor, enabling it to provide name resolution services to the system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolveHostName", "line_number": 106, "body": "def resolveHostName(\n        self, resolutionReceiver, hostName, portNumber=0, addressTypes=None, transportSemantics=\"TCP\"\n    ):\n        try:\n            addresses = dnscache[hostName]\n        except KeyError:\n            return self.original_resolver.resolveHostName(\n                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                hostName,\n                portNumber,\n                addressTypes,\n                transportSemantics,\n            )\n        else:\n            resolutionReceiver.resolutionBegan(HostResolution(hostName))\n            for addr in addresses:\n                resolutionReceiver.addressResolved(addr)\n            resolutionReceiver.resolutionComplete()\n            return resolutionReceiver", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "This method resolves a hostname to its IP addresses, leveraging a DNS cache for efficiency. It provides a faster hostname resolution service by returning cached results or delegating to an underlying resolver."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "from_settings", "line_number": 6, "body": "def from_settings(settings):\n        \"\"\"Return an instance of the class for the given settings\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "It creates and returns an instance of the spider loader class, initialized with the provided settings. This serves as a factory method for the loader."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "load", "line_number": 9, "body": "def load(spider_name):\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Provides the Spider class corresponding to a given name. It serves as a lookup mechanism, raising a KeyError if the name is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "list", "line_number": 13, "body": "def list():\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Provides a list of names for all available spiders in the project. This allows other components to discover and manage the defined web crawlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "find_by_request", "line_number": 17, "body": "def find_by_request(request):\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "This method identifies and lists the names of spiders capable of processing a given request. It serves as a lookup utility to route requests to the appropriate spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__eq__", "line_number": 38, "body": "def __eq__(self, other):\n        return (\n            self.url == other.url\n            and self.text == other.text\n            and self.fragment == other.fragment\n            and self.nofollow == other.nofollow\n        )", "is_method": true, "class_name": "Link", "function_description": "Defines equality for two Link objects. They are considered equal if their URL, text, fragment, and nofollow attributes all match."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__hash__", "line_number": 46, "body": "def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)", "is_method": true, "class_name": "Link", "function_description": "Enables `Link` objects to be hashed by combining the hashes of its core attributes. This makes `Link` instances suitable for use in sets or as dictionary keys."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__repr__", "line_number": 49, "body": "def __repr__(self):\n        return (\n            f'Link(url={self.url!r}, text={self.text!r}, '\n            f'fragment={self.fragment!r}, nofollow={self.nofollow!r})'\n        )", "is_method": true, "class_name": "Link", "function_description": "Provides a developer-friendly string representation of a Link object. It includes the URL, text, fragment, and nofollow status, useful for debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_check_name_duplicates", "line_number": 26, "body": "def _check_name_duplicates(self):\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend([\n                f\"  {cls} named {name!r} (in {mod})\"\n                for mod, cls in locations\n                if len(locations) > 1\n            ])\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )", "is_method": true, "class_name": "SpiderLoader", "function_description": "Checks for and warns about duplicate spider names detected across different modules. This prevents potential unexpected behavior caused by naming collisions during spider loading."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_load_spiders", "line_number": 43, "body": "def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls", "is_method": true, "class_name": "SpiderLoader", "function_description": "This method identifies and registers Scrapy spider classes found within a given module. It stores these spider classes internally for later retrieval by name, enabling the SpiderLoader to manage available spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_load_all_spiders", "line_number": 48, "body": "def _load_all_spiders(self):\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError:\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()", "is_method": true, "class_name": "SpiderLoader", "function_description": "Loads all spider classes from configured modules within the application. It handles import errors during module import and checks for duplicate spider names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "from_settings", "line_number": 66, "body": "def from_settings(cls, settings):\n        return cls(settings)", "is_method": true, "class_name": "SpiderLoader", "function_description": "This class method constructs a new `SpiderLoader` instance, initializing it directly from the provided settings. It serves as a factory for creating configurable loader objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "load", "line_number": 69, "body": "def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")", "is_method": true, "class_name": "SpiderLoader", "function_description": "Retrieves the Python class for a spider identified by its name. This allows other components to access and use registered spider definitions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "find_by_request", "line_number": 79, "body": "def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items()\n            if cls.handles_request(request)\n        ]", "is_method": true, "class_name": "SpiderLoader", "function_description": "Returns the names of spiders that are capable of handling a specific web request. This identifies which spiders can process a given URL or data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "list", "line_number": 88, "body": "def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())", "is_method": true, "class_name": "SpiderLoader", "function_description": "Provides a list of all spider names available in the project. This enables discovery and management of defined web scraping spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_iter_command_classes", "line_number": 17, "body": "def _iter_command_classes(module_name):\n    # TODO: add `name` attribute to commands and and merge this function with\n    # scrapy.utils.spider.iter_spider_classes\n    for module in walk_modules(module_name):\n        for obj in vars(module).values():\n            if (\n                inspect.isclass(obj)\n                and issubclass(obj, ScrapyCommand)\n                and obj.__module__ == module.__name__\n                and not obj == ScrapyCommand\n            ):\n                yield obj", "is_method": false, "function_description": "Discovers and yields all concrete ScrapyCommand subclasses within a specified module and its submodules. It enables dynamic enumeration of available Scrapy commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_from_module", "line_number": 31, "body": "def _get_commands_from_module(module, inproject):\n    d = {}\n    for cmd in _iter_command_classes(module):\n        if inproject or not cmd.requires_project:\n            cmdname = cmd.__module__.split('.')[-1]\n            d[cmdname] = cmd()\n    return d", "is_method": false, "function_description": "Retrieves and instantiates available command objects from a given module, filtering them based on whether a project context is active."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_from_entry_points", "line_number": 40, "body": "def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n    cmds = {}\n    for entry_point in pkg_resources.iter_entry_points(group):\n        obj = entry_point.load()\n        if inspect.isclass(obj):\n            cmds[entry_point.name] = obj()\n        else:\n            raise Exception(f\"Invalid entry point {entry_point.name}\")\n    return cmds", "is_method": false, "function_description": "Dynamically discovers and instantiates command classes registered as package entry points under a specified group. This provides a way to extend command-line tools by loading available commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_dict", "line_number": 51, "body": "def _get_commands_dict(settings, inproject):\n    cmds = _get_commands_from_module('scrapy.commands', inproject)\n    cmds.update(_get_commands_from_entry_points(inproject))\n    cmds_module = settings['COMMANDS_MODULE']\n    if cmds_module:\n        cmds.update(_get_commands_from_module(cmds_module, inproject))\n    return cmds", "is_method": false, "function_description": "Aggregates Scrapy commands from built-in modules, installed entry points, and custom settings. It provides a complete dictionary of available commands for the framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_pop_command_name", "line_number": 60, "body": "def _pop_command_name(argv):\n    i = 0\n    for arg in argv[1:]:\n        if not arg.startswith('-'):\n            del argv[i]\n            return arg\n        i += 1", "is_method": false, "function_description": "Extracts and removes the first non-option argument from a list of strings. This is useful for parsing the primary command or subcommand from command-line arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_header", "line_number": 69, "body": "def _print_header(settings, inproject):\n    version = scrapy.__version__\n    if inproject:\n        print(f\"Scrapy {version} - project: {settings['BOT_NAME']}\\n\")\n    else:\n        print(f\"Scrapy {version} - no active project\\n\")", "is_method": false, "function_description": "Displays a consistent header indicating the Scrapy version and whether a project is currently active. Provides immediate context for Scrapy command-line operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_commands", "line_number": 77, "body": "def _print_commands(settings, inproject):\n    _print_header(settings, inproject)\n    print(\"Usage:\")\n    print(\"  scrapy <command> [options] [args]\\n\")\n    print(\"Available commands:\")\n    cmds = _get_commands_dict(settings, inproject)\n    for cmdname, cmdclass in sorted(cmds.items()):\n        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n    if not inproject:\n        print()\n        print(\"  [ more ]      More commands available when run from project directory\")\n    print()\n    print('Use \"scrapy <command> -h\" to see more info about a command')", "is_method": false, "function_description": "Displays the available Scrapy commands, their brief descriptions, and general usage instructions for the command-line interface. It serves as a comprehensive help menu for users."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_unknown_command", "line_number": 92, "body": "def _print_unknown_command(settings, cmdname, inproject):\n    _print_header(settings, inproject)\n    print(f\"Unknown command: {cmdname}\\n\")\n    print('Use \"scrapy\" to see available commands')", "is_method": false, "function_description": "Informs the user that an entered command is unknown within the CLI, guiding them to discover available commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_print_help", "line_number": 98, "body": "def _run_print_help(parser, func, *a, **kw):\n    try:\n        func(*a, **kw)\n    except UsageError as e:\n        if str(e):\n            parser.error(str(e))\n        if e.print_help:\n            parser.print_help()\n        sys.exit(2)", "is_method": false, "function_description": "Executes a function, gracefully handling `UsageError` by displaying an error message or help using the provided parser, and then exits."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "execute", "line_number": 109, "body": "def execute(argv=None, settings=None):\n    if argv is None:\n        argv = sys.argv\n\n    if settings is None:\n        settings = get_project_settings()\n        # set EDITOR from environment if available\n        try:\n            editor = os.environ['EDITOR']\n        except KeyError:\n            pass\n        else:\n            settings['EDITOR'] = editor\n\n    inproject = inside_project()\n    cmds = _get_commands_dict(settings, inproject)\n    cmdname = _pop_command_name(argv)\n    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n                                   conflict_handler='resolve')\n    if not cmdname:\n        _print_commands(settings, inproject)\n        sys.exit(0)\n    elif cmdname not in cmds:\n        _print_unknown_command(settings, cmdname, inproject)\n        sys.exit(2)\n\n    cmd = cmds[cmdname]\n    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n    parser.description = cmd.long_desc()\n    settings.setdict(cmd.default_settings, priority='command')\n    cmd.settings = settings\n    cmd.add_options(parser)\n    opts, args = parser.parse_args(args=argv[1:])\n    _run_print_help(parser, cmd.process_options, args, opts)\n\n    cmd.crawler_process = CrawlerProcess(settings)\n    _run_print_help(parser, _run_command, cmd, args, opts)\n    sys.exit(cmd.exitcode)", "is_method": false, "function_description": "Serves as the main entry point for the Scrapy command-line interface. It parses arguments, dispatches to the specified Scrapy command, and manages its execution within a CrawlerProcess."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_command", "line_number": 149, "body": "def _run_command(cmd, args, opts):\n    if opts.profile:\n        _run_command_profiled(cmd, args, opts)\n    else:\n        cmd.run(args, opts)", "is_method": false, "function_description": "Provides a flexible utility to execute a command, with an optional mode for profiling its performance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_command_profiled", "line_number": 156, "body": "def _run_command_profiled(cmd, args, opts):\n    if opts.profile:\n        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n    loc = locals()\n    p = cProfile.Profile()\n    p.runctx('cmd.run(args, opts)', globals(), loc)\n    if opts.profile:\n        p.dump_stats(opts.profile)", "is_method": false, "function_description": "Executes a specified command. Optionally, it profiles the command's execution using `cProfile` and saves the performance statistics to a file."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "_path_safe", "line_number": 9, "body": "def _path_safe(text):\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n                             for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n    return '-'.join([pathable_slot, unique_slot])", "is_method": false, "function_description": "Converts any string into a filesystem-safe version by replacing invalid characters. It also appends a unique hash to prevent naming collisions, making it suitable for use as a filename or path component."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n        return cls(crawler, downstream_queue_cls, key, startprios)", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Creates a `ScrapyPriorityQueue` instance. This class method serves as an alternative constructor, typically used by a Scrapy crawler to initialize the queue with necessary context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "init_prios", "line_number": 63, "body": "def init_prios(self, startprios):\n        if not startprios:\n            return\n\n        for priority in startprios:\n            self.queues[priority] = self.qfactory(priority)\n\n        self.curprio = min(startprios)", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Initializes the priority queue by creating individual queues for specified starting priorities. It then sets the lowest priority as the current one for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "qfactory", "line_number": 72, "body": "def qfactory(self, key):\n        return create_instance(self.downstream_queue_cls,\n                               None,\n                               self.crawler,\n                               self.key + '/' + str(key))", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "This method serves as a factory to create new instances of internal 'downstream' queues. It enables the `ScrapyPriorityQueue` to manage separate queues, likely for different priority levels."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "priority", "line_number": 78, "body": "def priority(self, request):\n        return -request.priority", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Calculates the effective priority for a request within the Scrapy priority queue, ensuring requests with a higher configured priority are processed first."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "push", "line_number": 81, "body": "def push(self, request):\n        priority = self.priority(request)\n        if priority not in self.queues:\n            self.queues[priority] = self.qfactory(priority)\n        q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Adds a web request to the priority queue system. It places the request into a specific sub-queue based on its priority, dynamically managing these sub-queues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pop", "line_number": 90, "body": "def pop(self):\n        if self.curprio is None:\n            return\n        q = self.queues[self.curprio]\n        m = q.pop()\n        if not q:\n            del self.queues[self.curprio]\n            q.close()\n            prios = [p for p, q in self.queues.items() if q]\n            self.curprio = min(prios) if prios else None\n        return m", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Retrieves and removes the item with the highest priority from the queue. It automatically advances to the next priority level when the current one is depleted."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "close", "line_number": 102, "body": "def close(self):\n        active = []\n        for p, q in self.queues.items():\n            active.append(p)\n            q.close()\n        return active", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Closes all active, internal priority-based queues managed by this instance and returns the priorities of those queues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__len__", "line_number": 109, "body": "def __len__(self):\n        return sum(len(x) for x in self.queues.values()) if self.queues else 0", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Returns the total number of items currently stored across all internal queues within the Scrapy priority queue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "stats", "line_number": 118, "body": "def stats(self, possible_slots):\n        return [(self._active_downloads(slot), slot)\n                for slot in possible_slots]", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Reports the number of active downloads for each specified download slot, providing current status for download resource allocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "get_slot_key", "line_number": 122, "body": "def get_slot_key(self, request):\n        return self.downloader._get_slot_key(request, None)", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Provides a unique identifier for a download slot based on the given request, by delegating to the internal downloader. This key helps manage download concurrency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "_active_downloads", "line_number": 125, "body": "def _active_downloads(self, slot):\n        \"\"\" Return a number of requests in a Downloader for a given slot \"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Counts the number of currently active download requests associated with a specific slot within the downloader. This provides insight into ongoing download activity per slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "from_crawler", "line_number": 139, "body": "def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n        return cls(crawler, downstream_queue_cls, key, startprios)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "This class method serves as an alternative constructor for `DownloaderAwarePriorityQueue`, enabling its instantiation directly from a crawler object and other queue parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pqfactory", "line_number": 164, "body": "def pqfactory(self, slot, startprios=()):\n        return ScrapyPriorityQueue(self.crawler,\n                                   self.downstream_queue_cls,\n                                   self.key + '/' + _path_safe(slot),\n                                   startprios)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Provides a factory for creating and configuring new `ScrapyPriorityQueue` instances. Each queue is tailored for a specific slot, optionally initialized with starting priorities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pop", "line_number": 170, "body": "def pop(self):\n        stats = self._downloader_interface.stats(self.pqueues)\n\n        if not stats:\n            return\n\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Retrieves the next prioritized request by selecting from an internal queue based on current downloader statistics. It ensures optimal request distribution among available downloaders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "push", "line_number": 183, "body": "def push(self, request):\n        slot = self._downloader_interface.get_slot_key(request)\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory(slot)\n        queue = self.pqueues[slot]\n        queue.push(request)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "The `push` method adds a request to a dedicated priority queue, determined by its downloader slot. It ensures requests are organized and routed efficiently for specific download operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "close", "line_number": 190, "body": "def close(self):\n        active = {slot: queue.close()\n                  for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "The `close` method within `DownloaderAwarePriorityQueue` gracefully shuts down all managed priority queues. It clears the internal queue storage and returns the closing status for each slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__len__", "line_number": 196, "body": "def __len__(self):\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Provides the total count of all items stored across the various internal priority queues managed by this downloader-aware system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__contains__", "line_number": 199, "body": "def __contains__(self, slot):\n        return slot in self.pqueues", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Checks if a specific slot (or item) is currently present within the priority queue's managed elements. This enables quick membership testing using the `in` operator."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "inspect_response", "line_number": 159, "body": "def inspect_response(response, spider):\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    Shell(spider.crawler).start(response=response, spider=spider)", "is_method": false, "function_description": "Provides an interactive shell for debugging web scraping processes. It allows developers to inspect a given response object and the spider's current state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_request_deferred", "line_number": 164, "body": "def _request_deferred(request):\n    \"\"\"Wrap a request inside a Deferred.\n\n    This function is harmful, do not use it until you know what you are doing.\n\n    This returns a Deferred whose first pair of callbacks are the request\n    callback and errback. The Deferred also triggers when the request\n    callback/errback is executed (i.e. when the request is downloaded)\n\n    WARNING: Do not call request.replace() until after the deferred is called.\n    \"\"\"\n    request_callback = request.callback\n    request_errback = request.errback\n\n    def _restore_callbacks(result):\n        request.callback = request_callback\n        request.errback = request_errback\n        return result\n\n    d = defer.Deferred()\n    d.addBoth(_restore_callbacks)\n    if request.callback:\n        d.addCallbacks(request.callback, request.errback)\n\n    request.callback, request.errback = d.callback, d.errback\n    return d", "is_method": false, "function_description": "Wraps a request's asynchronous callbacks within a Twisted Deferred object. This integrates request completion with the Deferred pattern but is explicitly harmful for general use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "start", "line_number": 39, "body": "def start(self, url=None, request=None, response=None, spider=None, redirect=True):\n        # disable accidental Ctrl-C key press from shutting down the engine\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if url:\n            self.fetch(url, spider, redirect=redirect)\n        elif request:\n            self.fetch(request, spider)\n        elif response:\n            request = response.request\n            self.populate_vars(response, request, spider)\n        else:\n            self.populate_vars()\n        if self.code:\n            print(eval(self.code, globals(), self.vars))\n        else:\n            \"\"\"\n            Detect interactive shell setting in scrapy.cfg\n            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n            [settings]\n            # shell can be one of ipython, bpython or python;\n            # to be used as the interactive python console, if available.\n            # (default is ipython, fallbacks in the order listed above)\n            shell = python\n            \"\"\"\n            cfg = get_config()\n            section, option = 'settings', 'shell'\n            env = os.environ.get('SCRAPY_PYTHON_SHELL')\n            shells = []\n            if env:\n                shells += env.strip().lower().split(',')\n            elif cfg.has_option(section, option):\n                shells += [cfg.get(section, option).strip().lower()]\n            else:  # try all by default\n                shells += DEFAULT_PYTHON_SHELLS.keys()\n            # always add standard shell as fallback\n            shells += ['python']\n            start_python_console(self.vars, shells=shells,\n                                 banner=self.vars.pop('banner', ''))", "is_method": true, "class_name": "Shell", "function_description": "Initiates an interactive Python shell, optionally pre-loading a URL, request, or response for immediate inspection. It supports various console types like IPython, providing a powerful debugging environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_schedule", "line_number": 78, "body": "def _schedule(self, request, spider):\n        spider = self._open_spider(request, spider)\n        d = _request_deferred(request)\n        d.addCallback(lambda x: (x, spider))\n        self.crawler.engine.crawl(request, spider)\n        return d", "is_method": true, "class_name": "Shell", "function_description": "Schedules a web request to be processed by the crawler engine using a given spider. It returns a deferred object that resolves upon the request's completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_open_spider", "line_number": 85, "body": "def _open_spider(self, request, spider):\n        if self.spider:\n            return self.spider\n\n        if spider is None:\n            spider = self.crawler.spider or self.crawler._create_spider()\n\n        self.crawler.spider = spider\n        self.crawler.engine.open_spider(spider, close_if_idle=False)\n        self.spider = spider\n        return spider", "is_method": true, "class_name": "Shell", "function_description": "Provides the capability to initialize and activate a web crawling spider. It ensures a spider is opened within the crawler's engine, preparing it for subsequent operations within the shell."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "fetch", "line_number": 97, "body": "def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n        from twisted.internet import reactor\n        if isinstance(request_or_url, Request):\n            request = request_or_url\n        else:\n            url = any_to_uri(request_or_url)\n            request = Request(url, dont_filter=True, **kwargs)\n            if redirect:\n                request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n            else:\n                request.meta['handle_httpstatus_all'] = True\n        response = None\n        try:\n            response, spider = threads.blockingCallFromThread(\n                reactor, self._schedule, request, spider)\n        except IgnoreRequest:\n            pass\n        self.populate_vars(response, request, spider)", "is_method": true, "class_name": "Shell", "function_description": "Executes a web request and retrieves its response. It then populates the shell's environment for interactive inspection and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "populate_vars", "line_number": 116, "body": "def populate_vars(self, response=None, request=None, spider=None):\n        import scrapy\n\n        self.vars['scrapy'] = scrapy\n        self.vars['crawler'] = self.crawler\n        self.vars['item'] = self.item_class()\n        self.vars['settings'] = self.crawler.settings\n        self.vars['spider'] = spider\n        self.vars['request'] = request\n        self.vars['response'] = response\n        if self.inthread:\n            self.vars['fetch'] = self.fetch\n        self.vars['view'] = open_in_browser\n        self.vars['shelp'] = self.print_help\n        self.update_vars(self.vars)\n        if not self.code:\n            self.vars['banner'] = self.get_help()", "is_method": true, "class_name": "Shell", "function_description": "Prepares the interactive Scrapy shell environment by populating its variables with essential Scrapy objects and utility functions for user interaction and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "print_help", "line_number": 134, "body": "def print_help(self):\n        print(self.get_help())", "is_method": true, "class_name": "Shell", "function_description": "Displays the shell's help information to the console. This method provides users with guidance on how to use the shell."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "get_help", "line_number": 137, "body": "def get_help(self):\n        b = []\n        b.append(\"Available Scrapy objects:\")\n        b.append(\"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\")\n        for k, v in sorted(self.vars.items()):\n            if self._is_relevant(v):\n                b.append(f\"  {k:<10} {v}\")\n        b.append(\"Useful shortcuts:\")\n        if self.inthread:\n            b.append(\"  fetch(url[, redirect=True]) \"\n                     \"Fetch URL and update local objects (by default, redirects are followed)\")\n            b.append(\"  fetch(req)                  \"\n                     \"Fetch a scrapy.Request and update local objects \")\n        b.append(\"  shelp()           Shell help (print this help)\")\n        b.append(\"  view(response)    View response in a browser\")\n\n        return \"\\n\".join(f\"[s] {line}\" for line in b)", "is_method": true, "class_name": "Shell", "function_description": "Provides an interactive help message for the Scrapy shell. It lists available objects and useful shortcuts for common operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_is_relevant", "line_number": 155, "body": "def _is_relevant(self, value):\n        return isinstance(value, self.relevant_classes) or is_item(value)", "is_method": true, "class_name": "Shell", "function_description": "This method determines if a value is relevant to the Shell by checking its type against predefined classes or if it's considered an item. It helps in filtering and categorizing data for Shell operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_restore_callbacks", "line_number": 178, "body": "def _restore_callbacks(result):\n        request.callback = request_callback\n        request.errback = request_errback\n        return result", "is_method": false, "function_description": "Restores the original callback and error-handling functions on a 'request' object. This service ensures proper state management, typically after temporary modifications in asynchronous operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_configure", "line_number": 32, "body": "def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding = options.pop('encoding', None)\n        self.fields_to_export = options.pop('fields_to_export', None)\n        self.export_empty_fields = options.pop('export_empty_fields', False)\n        self.indent = options.pop('indent', None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Configures the exporter by extracting specific options from a dictionary. It ensures only recognized configuration parameters are applied, raising an error for unexpected ones unless suppressed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 47, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', lambda x: x)\n        return serializer(value)", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Applies a specified serialization function to a field's value, or returns it unchanged if no serializer is defined. Ensures consistent data formatting for export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_get_serialized_fields", "line_number": 57, "body": "def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if field_name in item:\n                field_meta = item.get_field_meta(field_name)\n                value = self.serialize_field(field_meta, field_name, item[field_name])\n            else:\n                value = default_value\n\n            yield field_name, value", "is_method": true, "class_name": "BaseItemExporter", "function_description": "This core helper method of BaseItemExporter serializes an item's selected fields. It yields tuples of field names and their serialized values, allowing control over field inclusion for export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 95, "body": "def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + '\\n'\n        self.file.write(to_bytes(data, self.encoding))", "is_method": true, "class_name": "JsonLinesItemExporter", "function_description": "Serializes an input item into a JSON string and writes it as a single line to the configured file, suitable for JSON Lines format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_newline", "line_number": 115, "body": "def _beautify_newline(self):\n        if self.indent is not None:\n            self.file.write(b'\\n')", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Adds a newline to the JSON output file when indentation is enabled, ensuring pretty-printed formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 119, "body": "def start_exporting(self):\n        self.file.write(b\"[\")\n        self._beautify_newline()", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Initializes the export process by writing the opening bracket of a JSON array to the output file, preparing it to receive serialized items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 123, "body": "def finish_exporting(self):\n        self._beautify_newline()\n        self.file.write(b\"]\")", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Completes the JSON export process by writing the closing array bracket. This ensures the output is a valid JSON array, ready for consumption."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 127, "body": "def export_item(self, item):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b',')\n            self._beautify_newline()\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict)\n        self.file.write(to_bytes(data, self.encoding))", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Exports a single data item by converting it to a JSON object and writing it to the output file. It handles proper JSON formatting, including comma separation for multiple items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_newline", "line_number": 148, "body": "def _beautify_newline(self, new_item=False):\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters('\\n')", "is_method": true, "class_name": "XmlItemExporter", "function_description": "This internal method of `XmlItemExporter` adds newlines to the XML output for improved readability. It ensures proper formatting and indentation when enabled or when starting a new item."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_indent", "line_number": 152, "body": "def _beautify_indent(self, depth=1):\n        if self.indent:\n            self.xg.characters(' ' * self.indent * depth)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Adds appropriate indentation spaces to the XML output. This internal helper method beautifies the generated XML for better readability, based on the current depth."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 156, "body": "def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})\n        self._beautify_newline(new_item=True)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Initializes the XML export by starting the document and its root element. This prepares the output for subsequent item serialization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 161, "body": "def export_item(self, item):\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, {})\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=''):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Serializes a given Python item into a properly formatted XML element. It processes all item fields and writes them as nested XML tags."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 171, "body": "def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Completes the XML export by closing the root element and finalizing the XML document."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_export_xml_field", "line_number": 175, "body": "def _export_xml_field(self, name, serialized_value, depth):\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, 'items'):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field('value', value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Recursively exports a Python field, including nested structures like dictionaries and lists, into a properly formatted XML element. It manages element creation and content, ensuring correct XML hierarchy and indentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 215, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._join_if_needed)\n        return serializer(value)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Serializes a given field's value using a field-specific serializer or a default method. This prepares the data for export into a CSV format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_join_if_needed", "line_number": 219, "body": "def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Converts list or tuple values into a single string, suitable for CSV export. This method ensures multi-valued fields are properly formatted for a single CSV cell."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 227, "body": "def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value='',\n                                             include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Converts a data item into a CSV row for export. It writes the CSV header only once, then appends the item's serialized values to the file."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_build_row", "line_number": 237, "body": "def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Prepares a sequence of values for CSV export by converting each to Unicode strings, handling potential type conversion errors. It yields each formatted value for a row."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_write_headers_and_set_fields_to_export", "line_number": 244, "body": "def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            row = list(self._build_row(self.fields_to_export))\n            self.csv_writer.writerow(row)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "This method of `CsvItemExporter` writes the header row to the CSV file if configured. It also dynamically determines the fields to be exported based on the provided item's structure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 260, "body": "def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)", "is_method": true, "class_name": "PickleItemExporter", "function_description": "Serializes an item's data and saves it to a file using the pickle protocol. This method provides persistence for Python objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 278, "body": "def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)", "is_method": true, "class_name": "MarshalItemExporter", "function_description": "Serializes and exports a Python item's specific fields to a file using the `marshal` module. This function facilitates efficient internal object storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 288, "body": "def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + '\\n'))", "is_method": true, "class_name": "PprintItemExporter", "function_description": "This method pretty-prints a given item as a dictionary and writes it to the configured file. It provides human-readable output for structured data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_configure", "line_number": 303, "body": "def _configure(self, options, dont_fail=False):\n        self.binary = options.pop('binary', True)\n        super()._configure(options, dont_fail)\n        if self.binary:\n            warnings.warn(\n                \"PythonItemExporter will drop support for binary export in the future\",\n                ScrapyDeprecationWarning)\n        if not self.encoding:\n            self.encoding = 'utf-8'", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Initializes the exporter by configuring options like binary export mode and ensuring a default 'utf-8' encoding. It extends the configuration of its parent class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 313, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._serialize_value)\n        return serializer(value)", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Serializes a field's value for export. It applies a field-specific serializer if available, otherwise it uses the default serializer for proper data formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_serialize_value", "line_number": 317, "body": "def _serialize_value(self, value):\n        if isinstance(value, _BaseItem):\n            return self.export_item(value)\n        elif is_item(value):\n            return dict(self._serialize_item(value))\n        elif is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (str, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Recursively serializes various Python data types, including items and nested lists, into a basic exportable format. It also handles encoding of string and byte values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_serialize_item", "line_number": 329, "body": "def _serialize_item(self, item):\n        for key, value in ItemAdapter(item).items():\n            key = to_bytes(key) if self.binary else key\n            yield key, self._serialize_value(value)", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Prepares an item for export by iterating through its fields, optionally converting keys to bytes, and delegating value serialization. It yields these processed key-value pairs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 334, "body": "def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        if self.binary:\n            result = dict(self._serialize_item(result))\n        return result", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Prepares an item for export by serializing its fields into a Python dictionary. It applies an additional binary serialization step if configured."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/loader/common.py", "function": "wrap_loader_context", "line_number": 10, "body": "def wrap_loader_context(function, context):\n    \"\"\"Wrap functions that receive loader_context to contain the context\n    \"pre-loaded\" and expose a interface that receives only one argument\n    \"\"\"\n    warnings.warn(\n        \"scrapy.loader.common.wrap_loader_context has moved to a new library.\"\n        \"Please update your reference to itemloaders.common.wrap_loader_context\",\n        ScrapyDeprecationWarning,\n        stacklevel=2\n    )\n\n    return common.wrap_loader_context(function, context)", "is_method": false, "function_description": "Wraps a function that expects a `loader_context` argument, pre-loading the context into it. This simplifies the wrapped function's interface to accept only one argument."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "add_request", "line_number": 35, "body": "def add_request(self, request):\n        self.inprogress.add(request)", "is_method": true, "class_name": "Slot", "function_description": "This method adds a given request to the Slot's collection of ongoing, in-progress operations. It helps the Slot track and manage active requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "remove_request", "line_number": 38, "body": "def remove_request(self, request):\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()", "is_method": true, "class_name": "Slot", "function_description": "Removes a specific request from the slot's active operations. If no requests remain after removal, it may trigger the slot's closing process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close", "line_number": 42, "body": "def close(self):\n        self.closing = defer.Deferred()\n        self._maybe_fire_closing()\n        return self.closing", "is_method": true, "class_name": "Slot", "function_description": "Initiates an asynchronous shutdown for the Slot object. It returns a future-like object that signals completion, allowing other components to react when the slot is fully closed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_maybe_fire_closing", "line_number": 47, "body": "def _maybe_fire_closing(self):\n        if self.closing and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)", "is_method": true, "class_name": "Slot", "function_description": "Finalizes the slot's closing sequence when no operations are in progress. It cancels pending calls, stops heartbeats, and signals the completion of the closure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "start", "line_number": 74, "body": "def start(self):\n        \"\"\"Start the execution engine\"\"\"\n        if self.running:\n            raise RuntimeError(\"Engine already running\")\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait = defer.Deferred()\n        yield self._closewait", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Initiates the execution engine's operation, setting its active state and signaling its start. It then keeps the engine running until explicitly signaled to stop."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "stop", "line_number": 84, "body": "def stop(self):\n        \"\"\"Stop the execution engine gracefully\"\"\"\n        if not self.running:\n            raise RuntimeError(\"Engine not running\")\n        self.running = False\n        dfd = self._close_all_spiders()\n        return dfd.addBoth(lambda _: self._finish_stopping_engine())", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Provides a graceful, asynchronous shutdown mechanism for the ExecutionEngine. It ensures all internal components are properly closed before the engine fully stops."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close", "line_number": 92, "body": "def close(self):\n        \"\"\"Close the execution engine gracefully.\n\n        If it has already been started, stop it. In all cases, close all spiders\n        and the downloader.\n        \"\"\"\n        if self.running:\n            # Will also close spiders and downloader\n            return self.stop()\n        elif self.open_spiders:\n            # Will also close downloader\n            return self._close_all_spiders()\n        else:\n            return defer.succeed(self.downloader.close())", "is_method": true, "class_name": "ExecutionEngine", "function_description": "This method gracefully shuts down the execution engine. It stops the engine if running, and ensures all active spiders and the downloader are closed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "pause", "line_number": 107, "body": "def pause(self):\n        \"\"\"Pause the execution engine\"\"\"\n        self.paused = True", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Pauses the execution engine, halting its current operation until explicitly resumed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "unpause", "line_number": 111, "body": "def unpause(self):\n        \"\"\"Resume the execution engine\"\"\"\n        self.paused = False", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Resumes the operation of the ExecutionEngine. It sets the engine's internal 'paused' state to false, allowing it to continue processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_next_request", "line_number": 115, "body": "def _next_request(self, spider):\n        slot = self.slot\n        if not slot:\n            return\n\n        if self.paused:\n            return\n\n        while not self._needs_backout(spider):\n            if not self._next_request_from_scheduler(spider):\n                break\n\n        if slot.start_requests and not self._needs_backout(spider):\n            try:\n                request = next(slot.start_requests)\n            except StopIteration:\n                slot.start_requests = None\n            except Exception:\n                slot.start_requests = None\n                logger.error('Error while obtaining start requests',\n                             exc_info=True, extra={'spider': spider})\n            else:\n                self.crawl(request, spider)\n\n        if self.spider_is_idle(spider) and slot.close_if_idle:\n            self._spider_idle(spider)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "This method manages the continuous retrieval and processing of the next request for a given spider. It fetches requests from the scheduler or start URLs, ensuring the spider remains active during its crawling operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_needs_backout", "line_number": 142, "body": "def _needs_backout(self, spider):\n        slot = self.slot\n        return (\n            not self.running\n            or slot.closing\n            or self.downloader.needs_backout()\n            or self.scraper.slot.needs_backout()\n        )", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Checks if the execution engine or its components are in a state requiring a backout, indicating an unstable or terminating condition. It prevents further processing under such circumstances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_next_request_from_scheduler", "line_number": 151, "body": "def _next_request_from_scheduler(self, spider):\n        slot = self.slot\n        request = slot.scheduler.next_request()\n        if not request:\n            return\n        d = self._download(request, spider)\n        d.addBoth(self._handle_downloader_output, request, spider)\n        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.remove_request(request))\n        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.nextcall.schedule())\n        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Retrieves and orchestrates the processing of a single web request for a spider. It handles the request's download, response processing, and ensures proper slot management for subsequent operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_handle_downloader_output", "line_number": 171, "body": "def _handle_downloader_output(self, response, request, spider):\n        if not isinstance(response, (Request, Response, Failure)):\n            raise TypeError(\n                \"Incorrect type: expected Request, Response or Failure, got \"\n                f\"{type(response)}: {response!r}\"\n            )\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(response, Request):\n            self.crawl(response, spider)\n            return\n        # response is a Response or Failure\n        d = self.scraper.enqueue_scrape(response, request, spider)\n        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Manages the output from the web downloader component. It either re-queues new requests for crawling or dispatches fetched responses and failures to the scraper for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "spider_is_idle", "line_number": 188, "body": "def spider_is_idle(self, spider):\n        if not self.scraper.slot.is_idle():\n            # scraper is not idle\n            return False\n\n        if self.downloader.active:\n            # downloader has pending requests\n            return False\n\n        if self.slot.start_requests is not None:\n            # not all start requests are handled\n            return False\n\n        if self.slot.scheduler.has_pending_requests():\n            # scheduler has pending requests\n            return False\n\n        return True", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Determines if a web crawling spider is fully idle by checking its scraper, downloader, and scheduler states. This service is crucial for managing the spider's lifecycle and resource allocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "open_spiders", "line_number": 208, "body": "def open_spiders(self):\n        return [self.spider] if self.spider else []", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Retrieves the active web spider(s) managed by the engine as a list. It returns an empty list if no spider is currently configured."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "has_capacity", "line_number": 211, "body": "def has_capacity(self):\n        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n        return not bool(self.slot)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Checks if the execution engine has available capacity to process additional spiders or tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "crawl", "line_number": 215, "body": "def crawl(self, request, spider):\n        if spider not in self.open_spiders:\n            raise RuntimeError(f\"Spider {spider.name!r} not opened when crawling: {request}\")\n        self.schedule(request, spider)\n        self.slot.nextcall.schedule()", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Initiates the crawling process by scheduling a given request for a specified spider. It ensures the spider is active within the engine before scheduling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "schedule", "line_number": 221, "body": "def schedule(self, request, spider):\n        self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)\n        if not self.slot.scheduler.enqueue_request(request):\n            self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Schedules a web request by attempting to enqueue it for processing by a spider. It signals if the request is successfully scheduled or dropped."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "download", "line_number": 226, "body": "def download(self, request, spider):\n        d = self._download(request, spider)\n        d.addBoth(self._downloaded, self.slot, request, spider)\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Orchestrates a web download request for a spider, ensuring its processing upon completion or failure. This method provides the core capability for fetching resources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_downloaded", "line_number": 231, "body": "def _downloaded(self, response, slot, request, spider):\n        slot.remove_request(request)\n        return self.download(response, spider) if isinstance(response, Request) else response", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Processes a download response, removing the original request from its active slot. It handles redirects by initiating new downloads or simply returns the final response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_download", "line_number": 235, "body": "def _download(self, request, spider):\n        slot = self.slot\n        slot.add_request(request)\n\n        def _on_success(response):\n            if not isinstance(response, (Response, Request)):\n                raise TypeError(\n                    \"Incorrect type: expected Response or Request, got \"\n                    f\"{type(response)}: {response!r}\"\n                )\n            if isinstance(response, Response):\n                if response.request is None:\n                    response.request = request\n                logkws = self.logformatter.crawled(response.request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=response,\n                    request=response.request,\n                    spider=spider,\n                )\n            return response\n\n        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _\n\n        dwld = self.downloader.fetch(request, spider)\n        dwld.addCallbacks(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Orchestrates the asynchronous download of a web request using the internal downloader. It processes successful responses, logs activity, dispatches signals, and manages the download slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "open_spider", "line_number": 269, "body": "def open_spider(self, spider, start_requests=(), close_if_idle=True):\n        if not self.has_capacity():\n            raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n        logger.info(\"Spider opened\", extra={'spider': spider})\n        nextcall = CallLaterOnce(self._next_request, spider)\n        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.slot = slot\n        self.spider = spider\n        yield scheduler.open(spider)\n        yield self.scraper.open_spider(spider)\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        slot.nextcall.schedule()\n        slot.heartbeat.start(5)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "This method prepares and integrates a web spider for execution within the `ExecutionEngine`. It configures its scheduler, processes initial requests, and sets up its operational environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_spider_idle", "line_number": 286, "body": "def _spider_idle(self, spider):\n        \"\"\"Called when a spider gets idle. This function is called when there\n        are no remaining pages to download or schedule. It can be called\n        multiple times. If some extension raises a DontCloseSpider exception\n        (in the spider_idle signal handler) the spider is not closed until the\n        next loop and this function is guaranteed to be called (at least) once\n        again for this spider.\n        \"\"\"\n        res = self.signals.send_catch_log(signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n            return\n\n        if self.spider_is_idle(spider):\n            self.close_spider(spider, reason='finished')", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Manages a spider's idle state by signaling when it has no more work. It initiates spider closure unless an extension prevents it, ensuring efficient resource management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close_spider", "line_number": 301, "body": "def close_spider(self, spider, reason='cancelled'):\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n        slot = self.slot\n        if slot.closing:\n            return slot.closing\n        logger.info(\"Closing spider (%(reason)s)\",\n                    {'reason': reason},\n                    extra={'spider': spider})\n\n        dfd = slot.close()\n\n        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure('Downloader close failure'))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure('Scraper close failure'))\n\n        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n        dfd.addErrback(log_failure('Scheduler close failure'))\n\n        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n            signal=signals.spider_closed, spider=spider, reason=reason))\n        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n        dfd.addErrback(log_failure('Stats close failure'))\n\n        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                          {'reason': reason},\n                                          extra={'spider': spider}))\n\n        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n        dfd.addErrback(log_failure('Error while unassigning slot'))\n\n        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n        dfd.addErrback(log_failure('Error while unassigning spider'))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Gracefully terminates a spider's operation by closing its associated downloader, scraper, scheduler, and other resources. It ensures a complete and orderly shutdown process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_close_all_spiders", "line_number": 352, "body": "def _close_all_spiders(self):\n        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n        dlist = defer.DeferredList(dfds)\n        return dlist", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Initiates the shutdown of all open spiders currently managed by the engine, returning a DeferredList to track their completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_finish_stopping_engine", "line_number": 358, "body": "def _finish_stopping_engine(self):\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n        self._closewait.callback(None)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Finalizes the engine's shutdown, signaling that it has stopped. It also resolves any pending operations waiting for the engine to close."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_on_success", "line_number": 239, "body": "def _on_success(response):\n            if not isinstance(response, (Response, Request)):\n                raise TypeError(\n                    \"Incorrect type: expected Response or Request, got \"\n                    f\"{type(response)}: {response!r}\"\n                )\n            if isinstance(response, Response):\n                if response.request is None:\n                    response.request = request\n                logkws = self.logformatter.crawled(response.request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=response,\n                    request=response.request,\n                    spider=spider,\n                )\n            return response", "is_method": true, "class_name": "ExecutionEngine", "function_description": "This method processes a successful response from an executed web request. It ensures type correctness, logs the crawl activity, and dispatches a 'response received' signal for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_on_complete", "line_number": 259, "body": "def _on_complete(_):\n            slot.nextcall.schedule()\n            return _", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Provides a callback mechanism to schedule the next operation within an execution pipeline once the current step completes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "log_failure", "line_number": 313, "body": "def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Provides a factory for creating error logging callbacks. The returned function logs a specific message along with detailed failure information and execution context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "errback", "line_number": 314, "body": "def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Logs detailed error information, including exception details and context, when a processing failure occurs within the execution engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "from_crawler", "line_number": 50, "body": "def from_crawler(cls, crawler):\n        settings = crawler.settings\n        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n        logunser = settings.getbool('SCHEDULER_DEBUG')\n        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n                   mqclass=mqclass, crawler=crawler)", "is_method": true, "class_name": "Scheduler", "function_description": "Initializes a `Scheduler` instance by configuring it with necessary components and settings loaded from a given `crawler`. It acts as a factory method for crawler-specific schedulers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "has_pending_requests", "line_number": 62, "body": "def has_pending_requests(self):\n        return len(self) > 0", "is_method": true, "class_name": "Scheduler", "function_description": "Determines if the scheduler currently holds any outstanding requests or tasks awaiting processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "open", "line_number": 65, "body": "def open(self, spider):\n        self.spider = spider\n        self.mqs = self._mq()\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()", "is_method": true, "class_name": "Scheduler", "function_description": "Prepares the Scheduler for operation by associating it with a spider. It initializes internal memory and disk queues and opens the deduplication filter, making it ready to manage requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "close", "line_number": 71, "body": "def close(self, reason):\n        if self.dqs:\n            state = self.dqs.close()\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)", "is_method": true, "class_name": "Scheduler", "function_description": "This method gracefully shuts down the scheduler, terminating its internal data queue and data flow components. It also persists the state of the data queue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "enqueue_request", "line_number": 77, "body": "def enqueue_request(self, request):\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        if dqok:\n            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Receives a request and adds it to the scheduler's queue for processing. It prevents duplicate requests and intelligently stores them in memory or on disk based on availability."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "next_request", "line_number": 90, "body": "def next_request(self):\n        request = self.mqs.pop()\n        if request:\n            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request:\n                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n        if request:\n            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n        return request", "is_method": true, "class_name": "Scheduler", "function_description": "The `Scheduler`'s `next_request` method retrieves the next available request, prioritizing an in-memory queue before falling back to disk storage. It provides the essential next item for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "__len__", "line_number": 102, "body": "def __len__(self):\n        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)", "is_method": true, "class_name": "Scheduler", "function_description": "Determines the total count of scheduled tasks. It sums the items in its primary (`mqs`) and secondary (`dqs`) queues, if the secondary queue contains items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqpush", "line_number": 105, "body": "def _dqpush(self, request):\n        if self.dqs is None:\n            return\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\"Unable to serialize request: %(request)s - reason:\"\n                       \" %(reason)s - no more unserializable requests will be\"\n                       \" logged (stats being collected)\")\n                logger.warning(msg, {'request': request, 'reason': e},\n                               exc_info=True, extra={'spider': self.spider})\n                self.logunser = False\n            self.stats.inc_value('scheduler/unserializable',\n                                 spider=self.spider)\n            return\n        else:\n            return True", "is_method": true, "class_name": "Scheduler", "function_description": "Submits a request to the scheduler's distributed queue. It handles and logs non-serializable requests while incrementing statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_mqpush", "line_number": 124, "body": "def _mqpush(self, request):\n        self.mqs.push(request)", "is_method": true, "class_name": "Scheduler", "function_description": "Adds a processing request to the Scheduler's internal message queue. This enables asynchronous handling of scheduled tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqpop", "line_number": 127, "body": "def _dqpop(self):\n        if self.dqs:\n            return self.dqs.pop()", "is_method": true, "class_name": "Scheduler", "function_description": "This helper method provides the capability to retrieve and remove the next available item from an internal collection. It supports the Scheduler by managing its queue or stack of elements."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_mq", "line_number": 131, "body": "def _mq(self):\n        \"\"\" Create a new priority queue instance, with in-memory storage \"\"\"\n        return create_instance(self.pqclass,\n                               settings=None,\n                               crawler=self.crawler,\n                               downstream_queue_cls=self.mqclass,\n                               key='')", "is_method": true, "class_name": "Scheduler", "function_description": "Creates a new in-memory priority queue instance configured for the scheduler. This queue manages tasks and integrates with downstream processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dq", "line_number": 139, "body": "def _dq(self):\n        \"\"\" Create a new priority queue instance, with disk storage \"\"\"\n        state = self._read_dqs_state(self.dqdir)\n        q = create_instance(self.pqclass,\n                            settings=None,\n                            crawler=self.crawler,\n                            downstream_queue_cls=self.dqclass,\n                            key=self.dqdir,\n                            startprios=state)\n        if q:\n            logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                        {'queuesize': len(q)}, extra={'spider': self.spider})\n        return q", "is_method": true, "class_name": "Scheduler", "function_description": "Initializes or resumes the Scheduler's disk-backed priority queue. This queue manages and persists crawl requests, enabling the spider to resume its operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqdir", "line_number": 153, "body": "def _dqdir(self, jobdir):\n        \"\"\" Return a folder name to keep disk queue state at \"\"\"\n        if jobdir:\n            dqdir = join(jobdir, 'requests.queue')\n            if not exists(dqdir):\n                os.makedirs(dqdir)\n            return dqdir", "is_method": true, "class_name": "Scheduler", "function_description": "Provides and ensures the existence of a designated directory within a job's workspace. This directory is where the scheduler stores its disk queue state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_read_dqs_state", "line_number": 161, "body": "def _read_dqs_state(self, dqdir):\n        path = join(dqdir, 'active.json')\n        if not exists(path):\n            return ()\n        with open(path) as f:\n            return json.load(f)", "is_method": true, "class_name": "Scheduler", "function_description": "Reads the 'active' state from a JSON file within a specified directory. This internal method retrieves configuration or current operational data for the scheduler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_write_dqs_state", "line_number": 168, "body": "def _write_dqs_state(self, dqdir, state):\n        with open(join(dqdir, 'active.json'), 'w') as f:\n            json.dump(state, f)", "is_method": true, "class_name": "Scheduler", "function_description": "The `_write_dqs_state` method persists a given state object to an 'active.json' file within a specified directory. It enables the Scheduler to save its current operational configuration or status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_isiterable", "line_number": 17, "body": "def _isiterable(possible_iterator):\n    return hasattr(possible_iterator, '__iter__')", "is_method": false, "function_description": "Determines if a given object is iterable. This utility helps validate inputs and safely process collection-like data structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_get_mwlist_from_settings", "line_number": 26, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Retrieves and prepares the list of spider middleware components defined in the application settings. It configures the SpiderMiddlewareManager for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_add_middleware", "line_number": 29, "body": "def _add_middleware(self, mw):\n        super()._add_middleware(mw)\n        if hasattr(mw, 'process_spider_input'):\n            self.methods['process_spider_input'].append(mw.process_spider_input)\n        if hasattr(mw, 'process_start_requests'):\n            self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n        process_spider_output = getattr(mw, 'process_spider_output', None)\n        self.methods['process_spider_output'].appendleft(process_spider_output)\n        process_spider_exception = getattr(mw, 'process_spider_exception', None)\n        self.methods['process_spider_exception'].appendleft(process_spider_exception)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Integrates a spider middleware by registering its specific processing methods (e.g., input, output, exception) into the manager's callable pipelines for later execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_input", "line_number": 40, "body": "def _process_spider_input(self, scrape_func, response, request, spider):\n        for method in self.methods['process_spider_input']:\n            try:\n                result = method(response=response, spider=spider)\n                if result is not None:\n                    msg = (f\"Middleware {method.__qualname__} must return None \"\n                           f\"or raise an exception, got {type(result)}\")\n                    raise _InvalidOutput(msg)\n            except _InvalidOutput:\n                raise\n            except Exception:\n                return scrape_func(Failure(), request, spider)\n        return scrape_func(response, request, spider)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Manages the sequential execution of registered spider input middleware methods, processing the response before it reaches the main scraping function. It ensures middleware compliance and handles exceptions, passing failures to the scraper."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_evaluate_iterable", "line_number": 54, "body": "def _evaluate_iterable(self, response, spider, iterable, exception_processor_index, recover_to):\n        try:\n            for r in iterable:\n                yield r\n        except Exception as ex:\n            exception_result = self._process_spider_exception(response, spider, Failure(ex),\n                                                              exception_processor_index)\n            if isinstance(exception_result, Failure):\n                raise\n            recover_to.extend(exception_result)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Manages the robust iteration over items or requests, catching and processing exceptions via a dedicated handler to allow for recovery or re-raising."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_exception", "line_number": 65, "body": "def _process_spider_exception(self, response, spider, _failure, start_index=0):\n        exception = _failure.value\n        # don't handle _InvalidOutput exception\n        if isinstance(exception, _InvalidOutput):\n            return _failure\n        method_list = islice(self.methods['process_spider_exception'], start_index, None)\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            result = method(response=response, exception=exception, spider=spider)\n            if _isiterable(result):\n                # stop exception handling by handing control over to the\n                # process_spider_output chain if an iterable has been returned\n                return self._process_spider_output(response, spider, result, method_index + 1)\n            elif result is None:\n                continue\n            else:\n                msg = (f\"Middleware {method.__qualname__} must return None \"\n                       f\"or an iterable, got {type(result)}\")\n                raise _InvalidOutput(msg)\n        return _failure", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Executes the exception handling methods of registered spider middleware. It allows middleware to process exceptions during spider execution, potentially generating new output items or propagating the error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_output", "line_number": 87, "body": "def _process_spider_output(self, response, spider, result, start_index=0):\n        # items in this iterable do not need to go through the process_spider_output\n        # chain, they went through it already from the process_spider_exception method\n        recovered = MutableChain()\n\n        method_list = islice(self.methods['process_spider_output'], start_index, None)\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            try:\n                # might fail directly if the output value is not a generator\n                result = method(response=response, result=result, spider=spider)\n            except Exception as ex:\n                exception_result = self._process_spider_exception(response, spider, Failure(ex), method_index + 1)\n                if isinstance(exception_result, Failure):\n                    raise\n                return exception_result\n            if _isiterable(result):\n                result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)\n            else:\n                msg = (f\"Middleware {method.__qualname__} must return an \"\n                       f\"iterable, got {type(result)}\")\n                raise _InvalidOutput(msg)\n\n        return MutableChain(result, recovered)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Applies a sequence of `process_spider_output` middleware methods to a spider's generated results. It orchestrates output transformation and error handling, ensuring an iterable result for downstream processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_callback_output", "line_number": 113, "body": "def _process_callback_output(self, response, spider, result):\n        recovered = MutableChain()\n        result = self._evaluate_iterable(response, spider, result, 0, recovered)\n        return MutableChain(self._process_spider_output(response, spider, result), recovered)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Orchestrates the processing of a spider's callback output. It evaluates yielded items/requests, potentially recovering some, and routes them through the spider output middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "scrape_response", "line_number": 118, "body": "def scrape_response(self, scrape_func, response, request, spider):\n        def process_callback_output(result):\n            return self._process_callback_output(response, spider, result)\n\n        def process_spider_exception(_failure):\n            return self._process_spider_exception(response, spider, _failure)\n\n        dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n        dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n        return dfd", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Orchestrates the asynchronous processing of a web response using spider middleware. It applies input middleware, then handles the spider's callback output and exceptions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_start_requests", "line_number": 129, "body": "def process_start_requests(self, start_requests, spider):\n        return self._process_chain('process_start_requests', start_requests, spider)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Processes a spider's initial start requests by passing them through the configured middleware chain."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_callback_output", "line_number": 119, "body": "def process_callback_output(result):\n            return self._process_callback_output(response, spider, result)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Manages and processes the output (items or requests) yielded by a spider's callback function. It integrates these results into the SpiderMiddlewareManager's internal pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "add_response_request", "line_number": 37, "body": "def add_response_request(self, response, request):\n        deferred = defer.Deferred()\n        self.queue.append((response, request, deferred))\n        if isinstance(response, Response):\n            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred", "is_method": true, "class_name": "Slot", "function_description": "Queues a response-request pair for asynchronous processing within the slot, updating its active size. It returns a Deferred object to enable tracking the operation's future completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "next_response_request_deferred", "line_number": 46, "body": "def next_response_request_deferred(self):\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred", "is_method": true, "class_name": "Slot", "function_description": "Retrieves the next pending request from a queue, along with its associated response and deferred data. It then marks the request as active for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "finish_response", "line_number": 51, "body": "def finish_response(self, response, request):\n        self.active.remove(request)\n        if isinstance(response, Response):\n            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE", "is_method": true, "class_name": "Slot", "function_description": "Manages the completion of a request-response cycle within a slot. It removes the request from active tracking and reduces the slot's consumed capacity based on the response size."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "is_idle", "line_number": 58, "body": "def is_idle(self):\n        return not (self.queue or self.active)", "is_method": true, "class_name": "Slot", "function_description": "Determines if the slot is currently idle, indicating it has no pending items in its queue and no active operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "needs_backout", "line_number": 61, "body": "def needs_backout(self):\n        return self.active_size > self.max_active_size", "is_method": true, "class_name": "Slot", "function_description": "Determines if the slot's current active usage exceeds its maximum allowed capacity. This indicates if a backout or resource adjustment is required."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "open_spider", "line_number": 78, "body": "def open_spider(self, spider):\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))\n        yield self.itemproc.open_spider(spider)", "is_method": true, "class_name": "Scraper", "function_description": "Initializes a spider for scraping by allocating necessary resources and preparing its item processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "close_spider", "line_number": 83, "body": "def close_spider(self, spider):\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        slot = self.slot\n        slot.closing = defer.Deferred()\n        slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider, slot)\n        return slot.closing", "is_method": true, "class_name": "Scraper", "function_description": "This Scraper method initiates the asynchronous shutdown of a spider. It ensures all associated resources are properly released upon completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "is_idle", "line_number": 91, "body": "def is_idle(self):\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot", "is_method": true, "class_name": "Scraper", "function_description": "Determines if the scraper has any pending spiders to process, indicating an idle state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_check_if_closing", "line_number": 95, "body": "def _check_if_closing(self, spider, slot):\n        if slot.closing and slot.is_idle():\n            slot.closing.callback(spider)", "is_method": true, "class_name": "Scraper", "function_description": "Triggers a shutdown callback for a scraping slot when it is marked for closing and becomes idle, enabling graceful resource release."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "enqueue_scrape", "line_number": 99, "body": "def enqueue_scrape(self, response, request, spider):\n        slot = self.slot\n        dfd = slot.add_response_request(response, request)\n\n        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _\n\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error('Scraper bug processing %(request)s',\n                                   {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n        self._scrape_next(spider, slot)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Manages the asynchronous processing lifecycle of a scraped web response and its associated request. It handles completion, error logging, and ensures the scraping pipeline continues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape_next", "line_number": 118, "body": "def _scrape_next(self, spider, slot):\n        while slot.queue:\n            response, request, deferred = slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)", "is_method": true, "class_name": "Scraper", "function_description": "Manages the asynchronous processing of queued scraping tasks. It continuously retrieves pending responses and requests from a slot, dispatching them to the core `_scrape` method for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape", "line_number": 123, "body": "def _scrape(self, result, request, spider):\n        \"\"\"\n        Handle the downloaded response or failure through the spider callback/errback\n        \"\"\"\n        if not isinstance(result, (Response, Failure)):\n            raise TypeError(f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\")\n        dfd = self._scrape2(result, request, spider)  # returns spider's processed output\n        dfd.addErrback(self.handle_spider_error, request, result, spider)\n        dfd.addCallback(self.handle_spider_output, request, result, spider)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Routes a downloaded web response or failure to the spider's processing callbacks and errbacks. This ensures comprehensive handling of scraping outcomes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape2", "line_number": 134, "body": "def _scrape2(self, result, request, spider):\n        \"\"\"\n        Handle the different cases of request's result been a Response or a Failure\n        \"\"\"\n        if isinstance(result, Response):\n            return self.spidermw.scrape_response(self.call_spider, result, request, spider)\n        else:  # result is a Failure\n            dfd = self.call_spider(result, request, spider)\n            return dfd.addErrback(self._log_download_errors, result, request, spider)", "is_method": true, "class_name": "Scraper", "function_description": "Handles the outcome of a web request during scraping. It processes a successful response or logs errors for a download failure, ensuring appropriate continuation of the scraping process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "call_spider", "line_number": 144, "body": "def call_spider(self, result, request, spider):\n        if isinstance(result, Response):\n            if getattr(result, \"request\", None) is None:\n                result.request = request\n            callback = result.request.callback or spider._parse\n            warn_on_generator_with_return_value(spider, callback)\n            dfd = defer_succeed(result)\n            dfd.addCallback(callback, **result.request.cb_kwargs)\n        else:  # result is a Failure\n            result.request = request\n            warn_on_generator_with_return_value(spider, request.errback)\n            dfd = defer_fail(result)\n            dfd.addErrback(request.errback)\n        return dfd.addCallback(iterate_spider_output)", "is_method": true, "class_name": "Scraper", "function_description": "Dispatches a web request's result (response or failure) to the appropriate spider callback or errback. It orchestrates the asynchronous processing of the result, including iterating over its output."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "handle_spider_error", "line_number": 159, "body": "def handle_spider_error(self, _failure, request, response, spider):\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n            return\n        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n        logger.log(\n            *logformatter_adapter(logkws),\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure, response=response,\n            spider=spider\n        )\n        self.crawler.stats.inc_value(\n            f\"spider_exceptions/{_failure.value.__class__.__name__}\",\n            spider=spider\n        )", "is_method": true, "class_name": "Scraper", "function_description": "Manages and processes exceptions encountered by a web spider during scraping. It logs errors, sends signals, updates statistics, and handles spider shutdown for specific exceptions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "handle_spider_output", "line_number": 180, "body": "def handle_spider_output(self, result, request, response, spider):\n        if not result:\n            return defer_succeed(None)\n        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n        dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,\n                       request, response, spider)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Manages and processes the results (items) yielded by a web spider. It orchestrates their concurrent processing through spider middleware for efficient data handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_process_spidermw_output", "line_number": 188, "body": "def _process_spidermw_output(self, output, request, response, spider):\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        if isinstance(output, Request):\n            self.crawler.engine.crawl(request=output, spider=spider)\n        elif is_item(output):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error(\n                'Spider must return request, item, or None, got %(typename)r in %(request)s',\n                {'request': request, 'typename': typename},\n                extra={'spider': spider},\n            )", "is_method": true, "class_name": "Scraper", "function_description": "This internal method of the Scraper class processes the output from a spider or middleware. It dispatches `Request` objects for further crawling and sends `Item` objects to the item processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_log_download_errors", "line_number": 209, "body": "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here)\n        \"\"\"\n        if isinstance(download_failure, Failure) and not download_failure.check(IgnoreRequest):\n            if download_failure.frames:\n                logkws = self.logformatter.download_error(download_failure, request, spider)\n                logger.log(\n                    *logformatter_adapter(logkws),\n                    extra={'spider': spider},\n                    exc_info=failure_to_exc_info(download_failure),\n                )\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logkws = self.logformatter.download_error(\n                        download_failure, request, spider, errmsg)\n                    logger.log(\n                        *logformatter_adapter(logkws),\n                        extra={'spider': spider},\n                    )\n\n        if spider_failure is not download_failure:\n            return spider_failure", "is_method": true, "class_name": "Scraper", "function_description": "This private method logs and optionally silences download-related errors encountered during web scraping. It provides structured error reporting using a dedicated log formatter for debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_itemproc_finished", "line_number": 234, "body": "def _itemproc_finished(self, output, item, response, spider):\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n        \"\"\"\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped, item=item, response=response,\n                    spider=spider, exception=output.value)\n            else:\n                logkws = self.logformatter.item_error(item, ex, response, spider)\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n                           exc_info=failure_to_exc_info(output))\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_error, item=item, response=response,\n                    spider=spider, failure=output)\n        else:\n            logkws = self.logformatter.scraped(output, response, spider)\n            if logkws is not None:\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_scraped, item=output, response=response,\n                spider=spider)", "is_method": true, "class_name": "Scraper", "function_description": "Handles the completion of item processing, logging its outcome and dispatching appropriate signals. This includes managing successful scrapes, items dropped by policy, or processing errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "finish_scraping", "line_number": 103, "body": "def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _", "is_method": true, "class_name": "Scraper", "function_description": "Handles the completion of a scraping response, initiates the next scrape, and checks the scraper's closing status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "_get_mwlist_from_settings", "line_number": 20, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(\n            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "This method retrieves and processes the configured list of downloader middlewares from application settings. It prepares these components for active use by the manager."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "_add_middleware", "line_number": 24, "body": "def _add_middleware(self, mw):\n        if hasattr(mw, 'process_request'):\n            self.methods['process_request'].append(mw.process_request)\n        if hasattr(mw, 'process_response'):\n            self.methods['process_response'].appendleft(mw.process_response)\n        if hasattr(mw, 'process_exception'):\n            self.methods['process_exception'].appendleft(mw.process_exception)", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Adds a middleware object to the manager, registering its `process_request`, `process_response`, and `process_exception` methods. This integrates the middleware into the downloader's processing pipeline.\nChain of Thought:\nThe user wants a concise description of the `_add_middleware` method within the `DownloaderMiddlewareManager` class.\n\n1.  **Understand the context:** The class `DownloaderMiddlewareManager` suggests it manages middleware for a \"downloader\" process, likely involving requests and responses. The `_` prefix indicates it's an internal helper method.\n\n2.  **Analyze the method's purpose (`_add_middleware(self, mw)`):** It takes a `mw` (middleware) object as input.\n\n3.  **Break down the core logic:**\n    *   It checks if the `mw` object has specific methods: `process_request`, `process_response`, `process_exception`.\n    *   If present, it appends these methods to corresponding lists/deques within `self.methods`. Note the `append` vs. `appendleft` for different method types, indicating specific ordering for execution.\n\n4.  **Identify the \"WHAT\" (primary service/capability):** The function's core purpose is to register the callback methods of a given middleware object with the manager. This registration enables the middleware to participate in the downloader's request/response/exception handling flow.\n\n5.  **Formulate a draft description (initial thoughts):**\n    *   \"This method registers a middleware object's request, response, and exception processing methods with the manager, allowing it to hook into the downloader's lifecycle.\" (Too long, slightly verbose)\n    *   \"It adds a middleware and registers its specific processing callbacks.\" (A bit too short, lacks detail on *which* callbacks).\n\n6.  **Refine for conciseness (under 40 words, max 2 sentences) and clarity, focusing on the \"service\":**\n    *   The service is integrating a middleware by registering its operational methods.\n    *   \"Integrates a middleware object by registering its `process_request`, `process_response`, and `process_exception` methods with the manager.\" (21 words). This is good. It clearly states what it does and which specific methods are registered.\n    *   Let's add the \"why\" or \"what for\": \"This enables the middleware to participate in the downloader's processing pipeline.\"\n\n7.  **Combine and finalize:** \"Integrates a middleware object by registering its `process_request`, `process_response`, and `process_exception` methods. This enables the middleware to participate in the downloader's processing pipeline.\" (29 words). This fulfills all requirements.\nIntegrates a middleware object by registering its `process_request`, `process_response`, and `process_exception` methods. This enables the middleware to participate in the downloader's processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "download", "line_number": 32, "body": "def download(self, download_func, request, spider):\n        @defer.inlineCallbacks\n        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield deferred_from_coro(method(request=request, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request=request, spider=spider))\n\n        @defer.inlineCallbacks\n        def process_response(response):\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods['process_response']:\n                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response\n\n        @defer.inlineCallbacks\n        def process_exception(failure):\n            exception = failure.value\n            for method in self.methods['process_exception']:\n                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure\n\n        deferred = mustbe_deferred(process_request, request)\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Orchestrates the entire web download process by applying a chain of downloader middleware. It processes requests, handles responses, and manages exceptions through registered middleware components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_request", "line_number": 34, "body": "def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield deferred_from_coro(method(request=request, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request=request, spider=spider))", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Processes a download request by passing it through a chain of configured middleware. Middleware can modify the request, return an immediate response, or allow it to proceed to the main download function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_response", "line_number": 47, "body": "def process_response(response):\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods['process_response']:\n                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Orchestrates the processing of a web response by chaining it through registered downloader middleware. Middleware can modify the response or transform it into a new request for redirection or retry."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_exception", "line_number": 65, "body": "def process_exception(failure):\n            exception = failure.value\n            for method in self.methods['process_exception']:\n                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Handles download exceptions by passing them through a chain of registered middleware. Middleware can process the exception, potentially returning a new response or request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_parsed_url_args", "line_number": 14, "body": "def _parsed_url_args(parsed):\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    path = to_bytes(path, encoding=\"ascii\")\n    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n    port = parsed.port\n    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n    if port is None:\n        port = 443 if scheme == b'https' else 80\n    return scheme, netloc, host, port, path", "is_method": false, "function_description": "It extracts and formats key URL components (scheme, netloc, host, port, path) from a parsed URL object. This function prepares URL parts as byte strings, ensuring default ports are set for network operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_parse", "line_number": 28, "body": "def _parse(url):\n    \"\"\" Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)", "is_method": false, "function_description": "This function parses an ASCII URL string, returning its scheme, network location, host, port, and path components as a structured tuple for internal use. It provides a standardized way to decompose URLs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "connectionMade", "line_number": 43, "body": "def connectionMade(self):\n        self.headers = Headers()  # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Called upon connection establishment, this method initializes and sends the full HTTP request. It transmits the request line, headers, and body to the server to begin fetching the page."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "lineReceived", "line_number": 57, "body": "def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "This method receives a raw line of data, removes trailing whitespace, and then forwards the cleaned line to the base HTTP client for further processing.\nThis method receives a raw line of data, removes trailing whitespace, and then forwards the cleaned line to the base HTTP client for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleHeader", "line_number": 60, "body": "def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Adds an HTTP header key-value pair to the page getter's internal headers collection, accommodating multiple values for a single header."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleStatus", "line_number": 63, "body": "def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Notifies the associated factory of the HTTP response's status line, including version, status code, and message. This allows the factory to handle the response outcome."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleEndHeaders", "line_number": 66, "body": "def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Signals to the managing factory that the HTTP response headers are fully available. This enables the factory to proceed with further processing of the response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "connectionLost", "line_number": 69, "body": "def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Handles a lost network connection during HTTP page retrieval. It records the reason and notifies the factory that the page could not be fetched, enabling error handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleResponse", "line_number": 74, "body": "def handleResponse(self, response):\n        if self.factory.method.upper() == b'HEAD':\n            self.factory.page(b'')\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "As a method of ScrapyHTTPPageGetter, it processes an incoming HTTP response. It signals whether a page was successfully received (handling HEAD requests) or if the connection was lost, then closes the connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "timeout", "line_number": 83, "body": "def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b'https'):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\n            defer.TimeoutError(f\"Getting {self.factory.url} took longer \"\n                               f\"than {self.factory.timeout} seconds.\"))", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "When an HTTP page retrieval exceeds its time limit, this method terminates the connection. It signals a `TimeoutError` to the requesting factory, indicating the page could not be fetched."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_build_response", "line_number": 108, "body": "def _build_response(self, body, request):\n        request.meta['download_latency'] = self.headers_time - self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url)\n        return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Constructs a Scrapy `Response` object from raw HTTP data. It includes download latency and determines the correct response class for use in Scrapy's processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_set_connection_attributes", "line_number": 115, "body": "def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n        proxy = request.meta.get('proxy')\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Configures the client's connection parameters based on a given request, adapting them for direct or proxy-based communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "__repr__", "line_number": 157, "body": "def __repr__(self):\n        return f\"<{self.__class__.__name__}: {self.url}>\"", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Provides a concise, developer-friendly string representation of the client factory instance. It includes the class name and the associated URL for debugging and logging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_cancelTimeout", "line_number": 160, "body": "def _cancelTimeout(self, result, timeoutCall):\n        if timeoutCall.active():\n            timeoutCall.cancel()\n        return result", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Cancels a pending network request timeout, preventing it from firing if the operation has already completed. This ensures proper resource cleanup within the HTTP client."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "buildProtocol", "line_number": 165, "body": "def buildProtocol(self, addr):\n        p = ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "This method creates and configures an HTTP client protocol. It applies factory settings like redirect handling and timeout behavior, preparing the protocol for web scraping requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "gotHeaders", "line_number": 174, "body": "def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "This method records the time of header reception and stores the HTTP response headers. It captures essential metadata upon receiving a response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "gotStatus", "line_number": 178, "body": "def gotStatus(self, version, status, message):\n        \"\"\"\n        Set the status of the request on us.\n        @param version: The HTTP version.\n        @type version: L{bytes}\n        @param status: The HTTP status code, an integer represented as a\n            bytestring.\n        @type status: L{bytes}\n        @param message: The HTTP status message.\n        @type message: L{bytes}\n        \"\"\"\n        self.version, self.status, self.message = version, status, message", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Records the HTTP response's version, status code, and message within the client factory. This makes critical response metadata available for subsequent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "page", "line_number": 191, "body": "def page(self, page):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.callback(page)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Delivers a received web page to a waiting asynchronous callback, fulfilling an outstanding request. It signals the completion of a page retrieval operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "noPage", "line_number": 196, "body": "def noPage(self, reason):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.errback(reason)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Handles the failure to retrieve a requested page by propagating the error reason to the pending asynchronous operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "clientConnectionFailed", "line_number": 201, "body": "def clientConnectionFailed(self, _, reason):\n        \"\"\"\n        When a connection attempt fails, the request cannot be issued.  If no\n        result has yet been provided to the result Deferred, provide the\n        connection failure reason as an error result.\n        \"\"\"\n        if self.waiting:\n            self.waiting = 0\n            # If the connection attempt failed, there is nothing more to\n            # disconnect, so just fire that Deferred now.\n            self._disconnectedDeferred.callback(None)\n            self.deferred.errback(reason)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Notifies the request's deferred that a client connection attempt failed, propagating the error reason to ensure proper error handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_concurrency_delay", "line_number": 58, "body": "def _get_concurrency_delay(concurrency, spider, settings):\n    delay = settings.getfloat('DOWNLOAD_DELAY')\n    if hasattr(spider, 'download_delay'):\n        delay = spider.download_delay\n\n    if hasattr(spider, 'max_concurrent_requests'):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay", "is_method": false, "function_description": "This function determines the effective concurrency and download delay for a web spider. It overrides global settings with spider-specific configurations if available, providing tailored request throttling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "free_transfer_slots", "line_number": 30, "body": "def free_transfer_slots(self):\n        return self.concurrency - len(self.transferring)", "is_method": true, "class_name": "Slot", "function_description": "Calculates the remaining capacity for concurrent transfers. It provides the number of available slots before reaching the defined concurrency limit."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "download_delay", "line_number": 33, "body": "def download_delay(self):\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n        return self.delay", "is_method": true, "class_name": "Slot", "function_description": "Calculates and returns a download delay duration. It can provide a fixed delay or a randomized delay within a specified range for the slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "close", "line_number": 38, "body": "def close(self):\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()", "is_method": true, "class_name": "Slot", "function_description": "Cancels any active, pending operation associated with this slot. This prevents the operation from executing if it was previously scheduled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "__repr__", "line_number": 42, "body": "def __repr__(self):\n        cls_name = self.__class__.__name__\n        return (f\"{cls_name}(concurrency={self.concurrency!r}, \"\n                f\"delay={self.delay:.2f}, \"\n                f\"randomize_delay={self.randomize_delay!r})\")", "is_method": true, "class_name": "Slot", "function_description": "Provides a clear, unambiguous string representation of a Slot object, showing its concurrency, delay, and randomization settings for debugging and inspection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "__str__", "line_number": 48, "body": "def __str__(self):\n        return (\n            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n            f\"len(transferring)={len(self.transferring)} \"\n            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n        )", "is_method": true, "class_name": "Slot", "function_description": "Generates a human-readable string representation of the `Slot` object's current state. It details concurrency settings, activity queues, and last seen timestamp for debugging or logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "fetch", "line_number": 87, "body": "def fetch(self, request, spider):\n        def _deactivate(response):\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd = self.middleware.download(self._enqueue_request, request, spider)\n        return dfd.addBoth(_deactivate)", "is_method": true, "class_name": "Downloader", "function_description": "Initiates and manages the asynchronous download of a web request using configured middleware, tracking its active status until completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "needs_backout", "line_number": 96, "body": "def needs_backout(self):\n        return len(self.active) >= self.total_concurrency", "is_method": true, "class_name": "Downloader", "function_description": "Checks if the Downloader has reached its concurrent operation capacity. It indicates whether new tasks should be paused or deferred to avoid overload."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_slot", "line_number": 99, "body": "def _get_slot(self, request, spider):\n        key = self._get_slot_key(request, spider)\n        if key not in self.slots:\n            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n\n        return key, self.slots[key]", "is_method": true, "class_name": "Downloader", "function_description": "Retrieves or initializes a download slot for a request, configuring its concurrency and delay. This manages polite crawling based on spider and global settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_slot_key", "line_number": 108, "body": "def _get_slot_key(self, request, spider):\n        if self.DOWNLOAD_SLOT in request.meta:\n            return request.meta[self.DOWNLOAD_SLOT]\n\n        key = urlparse_cached(request).hostname or ''\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key", "is_method": true, "class_name": "Downloader", "function_description": "Provides a slot key for a given download request, either from its metadata or derived from the hostname/IP address. This key likely facilitates managing concurrent downloads efficiently."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_enqueue_request", "line_number": 118, "body": "def _enqueue_request(self, request, spider):\n        key, slot = self._get_slot(request, spider)\n        request.meta[self.DOWNLOAD_SLOT] = key\n\n        def _deactivate(response):\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n                                    request=request,\n                                    spider=spider)\n        deferred = defer.Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred", "is_method": true, "class_name": "Downloader", "function_description": "Enqueues a download request from a spider into the downloader's internal system. It manages the request's state and prepares it for asynchronous processing, returning a deferred object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_process_queue", "line_number": 135, "body": "def _process_queue(self, spider, slot):\n        from twisted.internet import reactor\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break", "is_method": true, "class_name": "Downloader", "function_description": "Manages a download slot's request queue, initiating downloads while enforcing configured download delays and concurrency limits. It ensures requests are processed sequentially respecting these rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_download", "line_number": 160, "body": "def _download(self, slot, request, spider):\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives, remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(signal=signals.request_left_downloader,\n                                        request=request,\n                                        spider=spider)\n            return _\n\n        return dfd.addBoth(finish_transferring)", "is_method": true, "class_name": "Downloader", "function_description": "Manages the entire process of downloading a request by initiating the download, handling concurrency slots, and signaling its completion within the Downloader."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "close", "line_number": 192, "body": "def close(self):\n        self._slot_gc_loop.stop()\n        for slot in self.slots.values():\n            slot.close()", "is_method": true, "class_name": "Downloader", "function_description": "This method shuts down the Downloader and its managed resources. It stops background loops and closes all individual download slots, ensuring a clean shutdown and resource release."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_slot_gc", "line_number": 197, "body": "def _slot_gc(self, age=60):\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()", "is_method": true, "class_name": "Downloader", "function_description": "This internal Downloader method prunes inactive, stale download slots. It cleans up old, unused slot resources by removing and closing them, ensuring efficient resource management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_deactivate", "line_number": 88, "body": "def _deactivate(response):\n            self.active.remove(request)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Deactivates an ongoing download request by removing it from the Downloader's list of active operations. This helps manage the state and resources of current downloads."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_deactivate", "line_number": 122, "body": "def _deactivate(response):\n            slot.active.remove(request)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Removes a specific request from the active list within its designated download slot. This internal helper marks a request as no longer active after processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_downloaded", "line_number": 168, "body": "def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Notifies the system by dispatching a signal that a web response has been successfully downloaded. This allows other components to process the downloaded content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "finish_transferring", "line_number": 182, "body": "def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(signal=signals.request_left_downloader,\n                                        request=request,\n                                        spider=spider)\n            return _", "is_method": true, "class_name": "Downloader", "function_description": "Manages the completion of a download request, removing it from active transfers. It then signals the request's departure and prompts the downloader to process the next item in the queue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/tls.py", "function": "_identityVerifyingInfoCallback", "line_number": 45, "body": "def _identityVerifyingInfoCallback(self, connection, where, ret):\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                             self._hostnameASCII,\n                             connection.get_protocol_version_name(),\n                             connection.get_cipher_name(),\n                             )\n                server_cert = connection.get_peer_certificate()\n                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                             x509name_to_string(server_cert.get_issuer()),\n                             x509name_to_string(server_cert.get_subject()),\n                             )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug('SSL temp key: %s', key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n                        self._hostnameASCII, e))\n\n            except ValueError as e:\n                logger.warning(\n                    'Ignoring error while verifying certificate '\n                    'from host \"{}\" (exception: {})'.format(\n                        self._hostnameASCII, repr(e)))", "is_method": true, "class_name": "ScrapyClientTLSOptions", "function_description": "This callback verifies the server's identity during an SSL handshake by checking its certificate against the expected hostname. It also logs detailed SSL connection information for debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "load_context_factory_from_settings", "line_number": 120, "body": "def load_context_factory_from_settings(settings, crawler):\n    ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n    context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n    # try method-aware context factory\n    try:\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n        )\n        msg = \"\"\"\n            '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n            e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n            Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n            settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n        warnings.warn(msg)\n\n    return context_factory", "is_method": false, "function_description": "Creates and configures an SSL/TLS context factory instance from application settings for secure network operations. It dynamically loads the factory class and applies the specified TLS method."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "from_settings", "line_number": 37, "body": "def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "This class method creates and configures a `ScrapyClientContextFactory` instance. It initializes the instance with TLS/SSL settings, such as verbose logging and ciphers, retrieved directly from Scrapy's project settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "getCertificateOptions", "line_number": 42, "body": "def getCertificateOptions(self):\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super().__init__\n        return CertificateOptions(\n            verify=False,\n            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Configures SSL/TLS certificate options for Scrapy connections. It defaults to unverified connections for simplicity, allowing customization of the method and ciphers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "getContext", "line_number": 63, "body": "def getContext(self, hostname=None, port=None):\n        return self.getCertificateOptions().getContext()", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Provides an SSL/TLS context for secure client connections. This context, derived from certificate options, enables secure communication for Scrapy clients."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 66, "body": "def creatorForNetloc(self, hostname, port):\n        return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n                                      verbose_logging=self.tls_verbose_logging)", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Generates Scrapy client TLS configuration options for a given hostname and port, facilitating secure network connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 90, "body": "def creatorForNetloc(self, hostname, port):\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={'method': self._ssl_method},\n        )", "is_method": true, "class_name": "BrowserLikeContextFactory", "function_description": "Creates TLS client context options for a specific hostname and port. It configures the context to use the operating system's trusted root Certificate Authorities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 114, "body": "def creatorForNetloc(self, hostname, port):\n        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options", "is_method": true, "class_name": "AcceptableProtocolsContextFactory", "function_description": "This method creates a network context for a given host and port, then configures it to restrict the allowed communication protocols to a predefined set. It ensures connections only use acceptable security standards."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/datauri.py", "function": "download_request", "line_number": 12, "body": "def download_request(self, request, spider):\n        uri = parse_data_uri(request.url)\n        respcls = responsetypes.from_mimetype(uri.media_type)\n\n        resp_kwargs = {}\n        if (issubclass(respcls, TextResponse)\n                and uri.media_type.split('/')[0] == 'text'):\n            charset = uri.media_type_parameters.get('charset')\n            resp_kwargs['encoding'] = charset\n\n        return respcls(url=request.url, body=uri.data, **resp_kwargs)", "is_method": true, "class_name": "DataURIDownloadHandler", "function_description": "This method handles requests for Data URIs by parsing the URI's content. It then creates and returns a response object with the embedded data, treating it as a standard network response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "tunnel_request_data", "line_number": 165, "body": "def tunnel_request_data(host, port, proxy_auth_header=None):\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_unicode as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))\n    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\\r\\n'\n    tunnel_req += b'Host: ' + host_value + b'\\r\\n'\n    if proxy_auth_header:\n        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\\r\\n'\n    tunnel_req += b'\\r\\n'\n    return tunnel_req", "is_method": false, "function_description": "Generates the raw binary data for an HTTP `CONNECT` request. This allows clients to establish a tunnel through a proxy to a specified host and port, optionally with authentication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Factory method that instantiates an HTTP11DownloadHandler using settings and context from a given crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "download_request", "line_number": 55, "body": "def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request)", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Handles HTTP/1.1 download requests asynchronously. It provides the core functionality to fetch web content by delegating to a configured agent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "close", "line_number": 67, "body": "def close(self):\n        from twisted.internet import reactor\n        d = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "This method of HTTP11DownloadHandler closes all cached HTTP connections. It includes a timeout to prevent hanging, ensuring a reliable shutdown of network resources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "requestTunnel", "line_number": 113, "body": "def requestTunnel(self, protocol):\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse\n        self._protocol = protocol\n        return protocol", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Requests a network tunnel from a proxy. It sends the tunnel request data and configures the protocol to process the proxy's response for tunnel establishment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "processProxyResponse", "line_number": 122, "body": "def processProxyResponse(self, rcvd_bytes):\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        self._connectBuffer += rcvd_bytes\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b'\\r\\n\\r\\n' not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group('status')) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            if respm:\n                extra = {'status': int(respm.group('status')),\n                         'reason': respm.group('reason').strip()}\n            else:\n                extra = rcvd_bytes[:32]\n            self._tunnelReadyDeferred.errback(\n                TunnelError('Could not open CONNECT tunnel with proxy '\n                            f'{self._host}:{self._port} [{extra!r}]')\n            )", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Processes the proxy's HTTP CONNECT response to validate tunnel establishment. It initializes TLS and signals readiness for secure requests upon success, or raises an error if the tunnel fails."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectFailed", "line_number": 153, "body": "def connectFailed(self, reason):\n        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n        self._tunnelReadyDeferred.errback(reason)", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "This method handles a connection failure by propagating the given reason to the deferred object tracking the tunnel's readiness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connect", "line_number": 157, "body": "def connect(self, protocolFactory):\n        self._protocolFactory = protocolFactory\n        connectDeferred = super().connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Initiates a TCP connection, then requests and waits for a tunnel to be established over it. It provides a Deferred that signals when the tunneled connection is fully ready."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_getEndpoint", "line_number": 200, "body": "def _getEndpoint(self, uri):\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )", "is_method": true, "class_name": "TunnelingAgent", "function_description": "Creates and configures a `TunnelingTCP4ClientEndpoint` for a specific URI. This enables the agent to establish tunneled TCP connections through a proxy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_requestWithEndpoint", "line_number": 211, "body": "def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key = key + self._proxyConf\n        return super()._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )", "is_method": true, "class_name": "TunnelingAgent", "function_description": "This method forwards network requests, ensuring the connection key incorporates proxy configuration. It prevents misidentification and incorrect reuse of tunneled connections in the HTTP pool."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "request", "line_number": 238, "body": "def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )", "is_method": true, "class_name": "ScrapyProxyAgent", "function_description": "This method sends an HTTP request through the agent's configured proxy server. It enables Scrapy spiders to route their web requests via an intermediary."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_get_agent", "line_number": 273, "body": "def _get_agent(self, request, timeout):\n        from twisted.internet import reactor\n        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost = to_unicode(proxyHost)\n            omitConnectTunnel = b'noconnect' in proxyParams\n            if omitConnectTunnel:\n                warnings.warn(\n                    \"Using HTTPS proxies in the noconnect mode is deprecated. \"\n                    \"If you use Zyte Smart Proxy Manager (formerly Crawlera), \"\n                    \"it doesn't require this mode anymore, so you should \"\n                    \"update scrapy-crawlera to 1.3.0+ and remove '?noconnect' \"\n                    \"from the Zyte Smart Proxy Manager URL.\",\n                    ScrapyDeprecationWarning,\n                )\n            if scheme == b'https' and not omitConnectTunnel:\n                proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                proxyConf = (proxyHost, proxyPort, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            else:\n                return self._ProxyAgent(\n                    reactor=reactor,\n                    proxyURI=to_bytes(proxy, encoding='ascii'),\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Provides the correct Twisted HTTP client agent for a request. It selects an agent type based on proxy configuration, including support for HTTPS tunneling, and sets connection parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "download_request", "line_number": 319, "body": "def download_request(self, request):\n        from twisted.internet import reactor\n        timeout = request.meta.get('download_timeout') or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b'Proxy-Authorization')\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        else:\n            bodyproducer = None\n        start_time = time()\n        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d.addCallback(self._cb_bodyready, request)\n        d.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, url, timeout)\n        return d", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Initiates and manages the asynchronous HTTP download of a given web request using a configured agent. It handles request details, sets timeouts, and prepares the response for further processing by the Scrapy framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_timeout", "line_number": 346, "body": "def _cb_timeout(self, result, request, url, timeout):\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")", "is_method": true, "class_name": "ScrapyAgent", "function_description": "This callback method handles the outcome of a web request's timeout. It either confirms successful completion, cancelling the timeout, or raises a `TimeoutError` if the request exceeded its allotted time.\nProvides timeout handling for ScrapyAgent's web requests. It either confirms successful completion or raises a `TimeoutError` when the request exceeds its allotted time."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_latency", "line_number": 357, "body": "def _cb_latency(self, result, request, start_time):\n        request.meta['download_latency'] = time() - start_time\n        return result", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Calculates and records the download latency for a web request. This performance metric is stored in the request's metadata for subsequent processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_headers_from_twisted_response", "line_number": 362, "body": "def _headers_from_twisted_response(response):\n        headers = Headers()\n        if response.length != UNKNOWN_LENGTH:\n            headers[b'Content-Length'] = str(response.length).encode()\n        headers.update(response.headers.getAllRawHeaders())\n        return headers", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Converts headers from a Twisted network response into Scrapy's `Headers` object. This internal utility standardizes header representation for further processing by the Scrapy agent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_bodyready", "line_number": 369, "body": "def _cb_bodyready(self, txresponse, request):\n        headers_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.headers_received,\n            headers=self._headers_from_twisted_response(txresponse),\n            body_length=txresponse.length,\n            request=request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in headers_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                             {\"request\": request, \"handler\": handler.__qualname__})\n                txresponse._transport.stopProducing()\n                with suppress(AttributeError):\n                    txresponse._transport._producer.loseConnection()\n                return {\n                    \"txresponse\": txresponse,\n                    \"body\": b\"\",\n                    \"flags\": [\"download_stopped\"],\n                    \"certificate\": None,\n                    \"ip_address\": None,\n                    \"failure\": result if result.value.fail else None,\n                }\n\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return {\n                \"txresponse\": txresponse,\n                \"body\": b\"\",\n                \"flags\": None,\n                \"certificate\": None,\n                \"ip_address\": None,\n            }\n\n        maxsize = request.meta.get('download_maxsize', self._maxsize)\n        warnsize = request.meta.get('download_warnsize', self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n\n        if maxsize and expected_size > maxsize:\n            warning_msg = (\"Cancelling download of %(url)s: expected response \"\n                           \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n            warning_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n\n            logger.warning(warning_msg, warning_args)\n\n            txresponse._transport._producer.loseConnection()\n            raise defer.CancelledError(warning_msg % warning_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\"Expected response size (%(size)s) larger than \"\n                           \"download warn size (%(warnsize)s) in request %(request)s.\",\n                           {'size': expected_size, 'warnsize': warnsize, 'request': request})\n\n        def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d = defer.Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(\n                finished=d,\n                txresponse=txresponse,\n                request=request,\n                maxsize=maxsize,\n                warnsize=warnsize,\n                fail_on_dataloss=fail_on_dataloss,\n                crawler=self._crawler,\n            )\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Handles the readiness of a network response body within ScrapyAgent. It enforces download size limits, processes signals to stop downloads, and orchestrates the asynchronous retrieval of the response content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_bodydone", "line_number": 445, "body": "def _cb_bodydone(self, result, request, url):\n        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n        try:\n            version = result[\"txresponse\"].version\n            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n        except (AttributeError, TypeError, IndexError):\n            protocol = None\n        response = respcls(\n            url=url,\n            status=int(result[\"txresponse\"].code),\n            headers=headers,\n            body=result[\"body\"],\n            flags=result[\"flags\"],\n            certificate=result[\"certificate\"],\n            ip_address=result[\"ip_address\"],\n            protocol=protocol,\n        )\n        if result.get(\"failure\"):\n            result[\"failure\"].value.response = response\n            return result[\"failure\"]\n        return response", "is_method": true, "class_name": "ScrapyAgent", "function_description": "This internal ScrapyAgent callback transforms raw network request results into a structured Scrapy `Response` object. It provides the standardized response for further processing by spiders and middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "startProducing", "line_number": 476, "body": "def startProducing(self, consumer):\n        consumer.write(self.body)\n        return defer.succeed(None)", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "The method delivers the producer's stored body data to a given consumer. It provides a straightforward way to send fixed content for an asynchronous operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_finish_response", "line_number": 504, "body": "def _finish_response(self, flags=None, failure=None):\n        self._finished.callback({\n            \"txresponse\": self._txresponse,\n            \"body\": self._bodybuf.getvalue(),\n            \"flags\": flags,\n            \"certificate\": self._certificate,\n            \"ip_address\": self._ip_address,\n            \"failure\": failure,\n        })", "is_method": true, "class_name": "_ResponseReader", "function_description": "Finalizes response processing within `_ResponseReader` by aggregating all relevant data. It then delivers this complete response information, including body and status, via a designated callback."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectionMade", "line_number": 514, "body": "def connectionMade(self):\n        if self._certificate is None:\n            with suppress(AttributeError):\n                self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())\n\n        if self._ip_address is None:\n            self._ip_address = ipaddress.ip_address(self.transport._producer.getPeer().host)", "is_method": true, "class_name": "_ResponseReader", "function_description": "Upon connection establishment, this method initializes and stores the peer's SSL certificate and IP address. It provides essential connection metadata for subsequent handling or security validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "dataReceived", "line_number": 522, "body": "def dataReceived(self, bodyBytes):\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        bytes_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.bytes_received,\n            data=bodyBytes,\n            request=self._request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in bytes_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                             {\"request\": self._request, \"handler\": handler.__qualname__})\n                self.transport.stopProducing()\n                self.transport._producer.loseConnection()\n                failure = result if result.value.fail else None\n                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.warning(\"Received (%(bytes)s) bytes larger than download \"\n                           \"max size (%(maxsize)s) in request %(request)s.\",\n                           {'bytes': self._bytes_received,\n                            'maxsize': self._maxsize,\n                            'request': self._request})\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n            self._reached_warnsize = True\n            logger.warning(\"Received more bytes than download \"\n                           \"warn size (%(warnsize)s) in request %(request)s.\",\n                           {'warnsize': self._warnsize,\n                            'request': self._request})", "is_method": true, "class_name": "_ResponseReader", "function_description": "Processes incoming response data chunks, buffering them and tracking total bytes. It emits signals for received data, enforces download size limits, and can stop the download based on signal handler actions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectionLost", "line_number": 562, "body": "def connectionLost(self, reason):\n        if self._finished.called:\n            return\n\n        if reason.check(ResponseDone):\n            self._finish_response()\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finish_response(flags=[\"partial\"])\n            return\n\n        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n            if not self._fail_on_dataloss:\n                self._finish_response(flags=[\"dataloss\"])\n                return\n\n            elif not self._fail_on_dataloss_warned:\n                logger.warning(\"Got data loss in %s. If you want to process broken \"\n                               \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                               \" -- This message won't be shown in further requests\",\n                               self._txresponse.request.absoluteURI.decode())\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)", "is_method": true, "class_name": "_ResponseReader", "function_description": "Processes connection loss events for a response, determining its final state. It handles successful completion, partial responses, or failures due to data loss, respecting configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "cancel_delayed_call", "line_number": 80, "body": "def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Cancels a previously scheduled or delayed operation if it is still active. This prevents the operation from executing and returns the provided result."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cancel", "line_number": 423, "body": "def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()", "is_method": true, "class_name": "ScrapyAgent", "function_description": "This ScrapyAgent method immediately aborts an active web connection. It provides a forceful way to terminate ongoing communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/file.py", "function": "download_request", "line_number": 11, "body": "def download_request(self, request, spider):\n        filepath = file_uri_to_path(request.url)\n        with open(filepath, 'rb') as fo:\n            body = fo.read()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)", "is_method": true, "class_name": "FileDownloadHandler", "function_description": "Retrieves content from a local file based on the request URL and constructs a Response object. This is useful for local file handling or testing a spider's processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_get_handler", "line_number": 32, "body": "def _get_handler(self, scheme):\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = 'no handler available for that scheme'\n            return None\n\n        return self._load_handler(scheme)", "is_method": true, "class_name": "DownloadHandlers", "function_description": "The DownloadHandlers class uses this method to retrieve or lazily load the appropriate download handler for a given scheme, caching results for efficiency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_load_handler", "line_number": 46, "body": "def _load_handler(self, scheme, skip_lazy=False):\n        path = self._schemes[scheme]\n        try:\n            dhcls = load_object(path)\n            if skip_lazy and getattr(dhcls, 'lazy', True):\n                return None\n            dh = create_instance(\n                objcls=dhcls,\n                settings=self._crawler.settings,\n                crawler=self._crawler,\n            )\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                         {\"clspath\": path, \"scheme\": scheme},\n                         exc_info=True, extra={'crawler': self._crawler})\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n            return dh", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Internal utility method of DownloadHandlers that dynamically loads and instantiates a download handler for a specified scheme, managing lazy loading and setup errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "download_request", "line_number": 70, "body": "def download_request(self, request, spider):\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\")\n        return handler.download_request(request, spider)", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Routes download requests to the appropriate scheme-specific handler, providing a unified interface for initiating downloads across various protocols."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_close", "line_number": 78, "body": "def _close(self, *_a, **_kw):\n        for dh in self._handlers.values():\n            if hasattr(dh, 'close'):\n                yield dh.close()", "is_method": true, "class_name": "DownloadHandlers", "function_description": "This generator method iterates through all registered download handlers, yielding the result of closing each handler that possesses a `close` method for cleanup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "dataReceived", "line_number": 50, "body": "def dataReceived(self, data):\n        self.body.write(data)\n        self.size += len(data)", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Accumulates received data chunks into an internal buffer and tracks their total size, enabling the reconstruction of complete messages within the protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "filename", "line_number": 55, "body": "def filename(self):\n        return self.__filename", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Provides the filename associated with the data handled by this `ReceivedDataProtocol` instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "close", "line_number": 58, "body": "def close(self):\n        self.body.close() if self.filename else self.body.seek(0)", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Finalizes the handling of received data, either by closing the underlying file resource or by resetting the in-memory data stream to its beginning for potential reuse."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "from_crawler", "line_number": 79, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Creates an FTP download handler instance, configured using settings from a provided crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "download_request", "line_number": 82, "body": "def download_request(self, request, spider):\n        from twisted.internet import reactor\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n                                                  self.passive_mode)) else 0\n        creator = ClientCreator(reactor, FTPClient, user, password, passive=passive_mode)\n        dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Initiates an asynchronous FTP connection to download data based on a given request. It configures user credentials and passive mode for the connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "gotClient", "line_number": 93, "body": "def gotClient(self, client, request, filepath):\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        return client.retrieveFile(filepath, protocol).addCallbacks(\n            callback=self._build_response,\n            callbackArgs=(request, protocol),\n            errback=self._failed,\n            errbackArgs=(request,),\n        )", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Initiates an asynchronous FTP file download using the provided client and filepath. It configures a data reception protocol and sets up callbacks to manage the transfer's success or failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "_build_response", "line_number": 103, "body": "def _build_response(self, result, request, protocol):\n        self.result = result\n        respcls = responsetypes.from_args(url=request.url)\n        protocol.close()\n        body = protocol.filename or protocol.body.read()\n        headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "This method constructs a standardized response object for an FTP download operation. It encapsulates the downloaded file's details, such as filename, size, and body, into a structured HTTP-like response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "_failed", "line_number": 111, "body": "def _failed(self, result, request):\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(url=request.url, status=httpcode, body=to_bytes(message))\n        raise result.type(result.value)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Handles failed FTP operations by converting specific FTP command errors into an HTTP response with an appropriate status code and error message for the requester."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "from_crawler", "line_number": 33, "body": "def from_crawler(cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler) -> H2DownloadHandlerOrSubclass:\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "A factory method that instantiates an `H2DownloadHandler` (or subclass) using the provided `Crawler` object and its associated settings. It enables consistent handler creation within a crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "download_request", "line_number": 36, "body": "def download_request(self, request: Request, spider: Spider) -> Deferred:\n        agent = ScrapyH2Agent(\n            context_factory=self._context_factory,\n            pool=self._pool,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request, spider)", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "Initiates and manages an HTTP/2 download for a given request. It delegates the actual download process to a specialized ScrapyH2Agent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "close", "line_number": 44, "body": "def close(self) -> None:\n        self._pool.close_connections()", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "This method closes all active network connections managed by the handler's internal connection pool, ensuring proper resource release."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_get_agent", "line_number": 65, "body": "def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n        from twisted.internet import reactor\n        bind_address = request.meta.get('bindaddress') or self._bind_address\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxy_host = proxy_host.decode()\n            omit_connect_tunnel = b'noconnect' in proxy_params\n            if omit_connect_tunnel:\n                warnings.warn(\"Using HTTPS proxies in the noconnect mode is not supported by the \"\n                              \"downloader handler. If you use Crawlera, it doesn't require this \"\n                              \"mode anymore, so you should update scrapy-crawlera to 1.3.0+ \"\n                              \"and remove '?noconnect' from the Crawlera URL.\")\n\n            if scheme == b'https' and not omit_connect_tunnel:\n                # ToDo\n                raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')\n            return self._ProxyAgent(\n                reactor=reactor,\n                context_factory=self._context_factory,\n                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),\n                connect_timeout=timeout,\n                bind_address=bind_address,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            context_factory=self._context_factory,\n            connect_timeout=timeout,\n            bind_address=bind_address,\n            pool=self._pool,\n        )", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Provides the appropriate HTTP/2 agent (direct or proxy) for a Scrapy request, configured based on the request's metadata."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "download_request", "line_number": 100, "body": "def download_request(self, request: Request, spider: Spider) -> Deferred:\n        from twisted.internet import reactor\n        timeout = request.meta.get('download_timeout') or self._connect_timeout\n        agent = self._get_agent(request, timeout)\n\n        start_time = time()\n        d = agent.request(request, spider)\n        d.addCallback(self._cb_latency, request, start_time)\n\n        timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n        return d", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Initiates and manages an HTTP/2 download request for a given Scrapy request. It handles timeouts and tracks latency for efficient web content retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_cb_latency", "line_number": 114, "body": "def _cb_latency(response: Response, request: Request, start_time: float) -> Response:\n        request.meta['download_latency'] = time() - start_time\n        return response", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "This method calculates and records the download latency for a given Scrapy request. It stores this duration in the request's metadata, enabling performance tracking and analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_cb_timeout", "line_number": 119, "body": "def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl: DelayedCall) -> Response:\n        if timeout_cl.active():\n            timeout_cl.cancel()\n            return response\n\n        url = urldefrag(request.url)[0]\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Manages request timeouts within the ScrapyH2Agent. It cancels pending timeout errors upon successful response reception, or raises a TimeoutError if the request exceeds the specified duration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/s3.py", "function": "from_crawler", "line_number": 51, "body": "def from_crawler(cls, crawler, **kwargs):\n        return cls(crawler.settings, crawler=crawler, **kwargs)", "is_method": true, "class_name": "S3DownloadHandler", "function_description": "This class method serves as a factory to create an `S3DownloadHandler` instance, configuring it with settings and the crawler object for seamless integration into a crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/s3.py", "function": "download_request", "line_number": 54, "body": "def download_request(self, request, spider):\n        p = urlparse_cached(request)\n        scheme = 'https' if request.meta.get('is_secure') else 'http'\n        bucket = p.hostname\n        path = p.path + '?' + p.query if p.query else p.path\n        url = f'{scheme}://{bucket}.s3.amazonaws.com{path}'\n        if self.anon:\n            request = request.replace(url=url)\n        elif self._signer is not None:\n            import botocore.awsrequest\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url=f'{scheme}://s3.amazonaws.com/{bucket}{path}',\n                headers=request.headers.to_unicode_dict(),\n                data=request.body)\n            self._signer.add_auth(awsrequest)\n            request = request.replace(\n                url=url, headers=awsrequest.headers.items())\n        else:\n            signed_headers = self.conn.make_request(\n                method=request.method,\n                bucket=bucket,\n                key=unquote(p.path),\n                query_args=unquote(p.query),\n                headers=request.headers,\n                data=request.body,\n            )\n            request = request.replace(url=url, headers=signed_headers)\n        return self._download_http(request, spider)", "is_method": true, "class_name": "S3DownloadHandler", "function_description": "This method prepares and signs web requests for downloading content from Amazon S3 buckets, then initiates the download process. It handles different authentication methods for secure S3 access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "from_crawler", "line_number": 17, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "This class method constructs an `HTTP10DownloadHandler` instance, initializing it with settings and the crawler object. It provides a standard way to instantiate the handler within a crawler's context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "download_request", "line_number": 20, "body": "def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "Initiates an HTTP download for a given request. It returns a deferred object that resolves upon download completion or failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "_connect", "line_number": 26, "body": "def _connect(self, factory):\n        from twisted.internet import reactor\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b'https':\n            client_context_factory = create_instance(\n                objcls=self.ClientContextFactory,\n                settings=self._settings,\n                crawler=self._crawler,\n            )\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        else:\n            return reactor.connectTCP(host, port, factory)", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "This method initiates a network connection to a server, choosing between secure SSL for HTTPS or standard TCP for HTTP. It provides the underlying connection for download operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 44, "body": "def __str__(self) -> str:\n        return (f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\")", "is_method": true, "class_name": "InvalidNegotiatedProtocol", "function_description": "Provides a descriptive string representation for an `InvalidNegotiatedProtocol` error. It details the discrepancy between the expected and the actually received communication protocols for clearer debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 57, "body": "def __str__(self) -> str:\n        return f'Received GOAWAY frame from {self.remote_ip_address!r}'", "is_method": true, "class_name": "RemoteTerminatedConnection", "function_description": "Provides a human-readable string representation of a remote connection termination event, indicating a GOAWAY frame was received from a specific IP address."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 65, "body": "def __str__(self) -> str:\n        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"", "is_method": true, "class_name": "MethodNotAllowed405", "function_description": "Provides a human-readable string representation for an HTTP 405 Method Not Allowed error. It includes the remote IP address for clear error reporting and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "h2_connected", "line_number": 136, "body": "def h2_connected(self) -> bool:\n        \"\"\"Boolean to keep track of the connection status.\n        This is used while initiating pending streams to make sure\n        that we initiate stream only during active HTTP/2 Connection\n        \"\"\"\n        return bool(self.transport.connected) and self.metadata['settings_acknowledged']", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Reports if the HTTP/2 client connection is fully established and ready for operations. This ensures new streams are only initiated when active and settings are acknowledged."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "allowed_max_concurrent_streams", "line_number": 144, "body": "def allowed_max_concurrent_streams(self) -> int:\n        \"\"\"We keep total two streams for client (sending data) and\n        server side (receiving data) for a single request. To be safe\n        we choose the minimum. Since this value can change in event\n        RemoteSettingsChanged we make variable a property.\n        \"\"\"\n        return min(\n            self.conn.local_settings.max_concurrent_streams,\n            self.conn.remote_settings.max_concurrent_streams\n        )", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Calculates the effective maximum number of concurrent streams permitted for an HTTP/2 connection. This ensures compliance with both local and remote peer settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_send_pending_requests", "line_number": 155, "body": "def _send_pending_requests(self) -> None:\n        \"\"\"Initiate all pending requests from the deque following FIFO\n        We make sure that at any time {allowed_max_concurrent_streams}\n        streams are active.\n        \"\"\"\n        while (\n            self._pending_request_stream_pool\n            and self.metadata['active_streams'] < self.allowed_max_concurrent_streams\n            and self.h2_connected\n        ):\n            self.metadata['active_streams'] += 1\n            stream = self._pending_request_stream_pool.popleft()\n            stream.initiate_request()\n            self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "For the H2ClientProtocol, this method initiates pending requests from a queue. It manages concurrent active streams, ensuring the number of active streams never exceeds the allowed maximum."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "pop_stream", "line_number": 170, "body": "def pop_stream(self, stream_id: int) -> Stream:\n        \"\"\"Perform cleanup when a stream is closed\n        \"\"\"\n        stream = self.streams.pop(stream_id)\n        self.metadata['active_streams'] -= 1\n        self._send_pending_requests()\n        return stream", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "The `pop_stream` method of `H2ClientProtocol` performs necessary cleanup and state updates when an HTTP/2 stream is closed, returning the deactivated stream object. It ensures proper resource management for active connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_new_stream", "line_number": 178, "body": "def _new_stream(self, request: Request, spider: Spider) -> Stream:\n        \"\"\"Instantiates a new Stream object\n        \"\"\"\n        stream = Stream(\n            stream_id=next(self._stream_id_generator),\n            request=request,\n            protocol=self,\n            download_maxsize=getattr(spider, 'download_maxsize', self.metadata['default_download_maxsize']),\n            download_warnsize=getattr(spider, 'download_warnsize', self.metadata['default_download_warnsize']),\n        )\n        self.streams[stream.stream_id] = stream\n        return stream", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Creates and configures a new HTTP/2 stream within the protocol, assigning it a unique ID for managing a specific request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_write_to_transport", "line_number": 191, "body": "def _write_to_transport(self) -> None:\n        \"\"\" Write data to the underlying transport connection\n        from the HTTP2 connection instance if any\n        \"\"\"\n        # Reset the idle timeout as connection is still actively sending data\n        self.resetTimeout()\n\n        data = self.conn.data_to_send()\n        self.transport.write(data)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Writes buffered HTTP/2 data from the connection to the underlying network transport. It ensures active data transmission and resets the connection's idle timeout."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "request", "line_number": 201, "body": "def request(self, request: Request, spider: Spider) -> Deferred:\n        if not isinstance(request, Request):\n            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n\n        stream = self._new_stream(request, spider)\n        d = stream.get_response()\n\n        # Add the stream to the request pool\n        self._pending_request_stream_pool.append(stream)\n\n        # If we receive a request when connection is idle\n        # We need to initiate pending requests\n        self._send_pending_requests()\n        return d", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Sends an asynchronous HTTP/2 request, creating a new stream and returning a Deferred object for its future response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connectionMade", "line_number": 216, "body": "def connectionMade(self) -> None:\n        \"\"\"Called by Twisted when the connection is established. We can start\n        sending some data now: we should open with the connection preamble.\n        \"\"\"\n        # Initialize the timeout\n        self.setTimeout(self.IDLE_TIMEOUT)\n\n        destination = self.transport.getPeer()\n        self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n\n        # Initiate H2 Connection\n        self.conn.initiate_connection()\n        self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Immediately initiates the HTTP/2 protocol handshake upon connection establishment. It sends the connection preamble and sets an idle timeout, preparing for H2 communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_lose_connection_with_error", "line_number": 230, "body": "def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n        \"\"\"Helper function to lose the connection with the error sent as a\n        reason\"\"\"\n        self._conn_lost_errors += errors\n        self.transport.loseConnection()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "This helper method of H2ClientProtocol terminates the network connection. It records the provided errors as the reasons for the connection loss before closing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "handshakeCompleted", "line_number": 236, "body": "def handshakeCompleted(self) -> None:\n        \"\"\"\n        Close the connection if it's not made via the expected protocol\n        \"\"\"\n        if self.transport.negotiatedProtocol is not None and self.transport.negotiatedProtocol != PROTOCOL_NAME:\n            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n            self._lose_connection_with_error([InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Verifies the connection's negotiated protocol post-handshake. It closes the connection if the protocol does not match the expected HTTP/2 standard."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_check_received_data", "line_number": 244, "body": "def _check_received_data(self, data: bytes) -> None:\n        \"\"\"Checks for edge cases where the connection to remote fails\n        without raising an appropriate H2Error\n\n        Arguments:\n            data -- Data received from the remote\n        \"\"\"\n        if data.startswith(b'HTTP/2.0 405 Method Not Allowed'):\n            raise MethodNotAllowed405(self.metadata['ip_address'])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "This method validates raw received data for specific connection error patterns. It raises a `MethodNotAllowed405` exception when a \"Method Not Allowed\" response is detected, preventing unhandled failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "dataReceived", "line_number": 254, "body": "def dataReceived(self, data: bytes) -> None:\n        # Reset the idle timeout as connection is still actively receiving data\n        self.resetTimeout()\n\n        try:\n            self._check_received_data(data)\n            events = self.conn.receive_data(data)\n            self._handle_events(events)\n        except H2Error as e:\n            if isinstance(e, FrameTooLargeError):\n                # hyper-h2 does not drop the connection in this scenario, we\n                # need to abort the connection manually.\n                self._conn_lost_errors += [e]\n                self.transport.abortConnection()\n                return\n\n            # Save this error as ultimately the connection will be dropped\n            # internally by hyper-h2. Saved error will be passed to all the streams\n            # closed with the connection.\n            self._lose_connection_with_error([e])\n        finally:\n            self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles incoming raw data for the HTTP/2 client protocol, processing it into H2 events and managing connection state, including error handling. This ensures robust communication with the server."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "timeoutConnection", "line_number": 277, "body": "def timeoutConnection(self) -> None:\n        \"\"\"Called when the connection times out.\n        We lose the connection with TimeoutError\"\"\"\n\n        # Check whether there are open streams. If there are, we're going to\n        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n        # NO_ERROR.\n        if (\n            self.conn.open_outbound_streams > 0\n            or self.conn.open_inbound_streams > 0\n            or self.metadata['active_streams'] > 0\n        ):\n            error_code = ErrorCodes.PROTOCOL_ERROR\n        else:\n            error_code = ErrorCodes.NO_ERROR\n        self.conn.close_connection(error_code=error_code)\n        self._write_to_transport()\n\n        self._lose_connection_with_error([\n            TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")\n        ])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles an H2 connection timeout by determining the appropriate error code based on active streams. It closes the connection and signals a `TimeoutError` to the underlying transport."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connectionLost", "line_number": 299, "body": "def connectionLost(self, reason: Failure = connectionDone) -> None:\n        \"\"\"Called by Twisted when the transport connection is lost.\n        No need to write anything to transport here.\n        \"\"\"\n        # Cancel the timeout if not done yet\n        self.setTimeout(None)\n\n        # Notify the connection pool instance such that no new requests are\n        # sent over current connection\n        if not reason.check(connectionDone):\n            self._conn_lost_errors.append(reason)\n\n        self._conn_lost_deferred.callback(self._conn_lost_errors)\n\n        for stream in self.streams.values():\n            if stream.metadata['request_sent']:\n                close_reason = StreamCloseReason.CONNECTION_LOST\n            else:\n                close_reason = StreamCloseReason.INACTIVE\n            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n\n        self.metadata['active_streams'] -= len(self.streams)\n        self.streams.clear()\n        self._pending_request_stream_pool.clear()\n        self.conn.close_connection()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Manages graceful shutdown and cleanup for an HTTP/2 client connection upon loss. It notifies relevant components, closes active streams, and releases resources to maintain system integrity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_handle_events", "line_number": 325, "body": "def _handle_events(self, events: List[Event]) -> None:\n        \"\"\"Private method which acts as a bridge between the events\n        received from the HTTP/2 data and IH2EventsHandler\n\n        Arguments:\n            events -- A list of events that the remote peer triggered by sending data\n        \"\"\"\n        for event in events:\n            if isinstance(event, ConnectionTerminated):\n                self.connection_terminated(event)\n            elif isinstance(event, DataReceived):\n                self.data_received(event)\n            elif isinstance(event, ResponseReceived):\n                self.response_received(event)\n            elif isinstance(event, StreamEnded):\n                self.stream_ended(event)\n            elif isinstance(event, StreamReset):\n                self.stream_reset(event)\n            elif isinstance(event, WindowUpdated):\n                self.window_updated(event)\n            elif isinstance(event, SettingsAcknowledged):\n                self.settings_acknowledged(event)\n            elif isinstance(event, UnknownFrameReceived):\n                logger.warning('Unknown frame received: %s', event.frame)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Dispatches incoming HTTP/2 protocol events to their specific handler methods within the `H2ClientProtocol` class. It ensures proper processing of diverse connection, stream, and data-related events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connection_terminated", "line_number": 351, "body": "def connection_terminated(self, event: ConnectionTerminated) -> None:\n        self._lose_connection_with_error([\n            RemoteTerminatedConnection(self.metadata['ip_address'], event)\n        ])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles a remote connection termination event received by the H2ClientProtocol. It signals the local client to lose the connection, reporting the remote termination as an error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "data_received", "line_number": 356, "body": "def data_received(self, event: DataReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_data(event.data, event.flow_controlled_length)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Receives and dispatches incoming HTTP/2 data to the corresponding client-initiated stream. It ignores data for server-initiated streams unknown to the client."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "response_received", "line_number": 364, "body": "def response_received(self, event: ResponseReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_headers(event.headers)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Processes incoming HTTP/2 response headers for a specific stream, enabling the client protocol to manage and process server replies. This is a core part of handling HTTP/2 server communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "settings_acknowledged", "line_number": 372, "body": "def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n        self.metadata['settings_acknowledged'] = True\n\n        # Send off all the pending requests as now we have\n        # established a proper HTTP/2 connection\n        self._send_pending_requests()\n\n        # Update certificate when our HTTP/2 connection is established\n        self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Finalizes the HTTP/2 connection setup for the client, marking settings acknowledged. It enables pending requests to be sent and stores the peer's certificate for established communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "stream_ended", "line_number": 382, "body": "def stream_ended(self, event: StreamEnded) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.ENDED, from_protocol=True)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles the completion of client-initiated HTTP/2 streams. It closes the stream and removes it from active management upon receiving a `StreamEnded` event."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "stream_reset", "line_number": 390, "body": "def stream_reset(self, event: StreamReset) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.RESET, from_protocol=True)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles an HTTP/2 stream reset event received from the peer. It closes the affected stream, cleaning up its internal state and resources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "window_updated", "line_number": 398, "body": "def window_updated(self, event: WindowUpdated) -> None:\n        if event.stream_id != 0:\n            self.streams[event.stream_id].receive_window_update()\n        else:\n            # Send leftover data for all the streams\n            for stream in self.streams.values():\n                stream.receive_window_update()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Manages HTTP/2 flow control by processing a window update event. It updates the flow control window for a specific stream or the entire connection, enabling data transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "buildProtocol", "line_number": 414, "body": "def buildProtocol(self, addr) -> H2ClientProtocol:\n        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)", "is_method": true, "class_name": "H2ClientFactory", "function_description": "This factory method constructs and returns an H2ClientProtocol instance. It provides a configured HTTP/2 client protocol object ready to handle a new connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "acceptableProtocols", "line_number": 417, "body": "def acceptableProtocols(self) -> List[bytes]:\n        return [PROTOCOL_NAME]", "is_method": true, "class_name": "H2ClientFactory", "function_description": "Provides the list of network protocols that this H2 client factory is configured to accept. This ensures compatibility during connection negotiation for H2 clients."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 34, "body": "def __str__(self) -> str:\n        return f'InactiveStreamClosed: Connection was closed without sending the request {self.request!r}'", "is_method": true, "class_name": "InactiveStreamClosed", "function_description": "Generates a descriptive string representation for an `InactiveStreamClosed` error. It clearly indicates that the connection was closed before a request could be sent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 45, "body": "def __str__(self) -> str:\n        return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'", "is_method": true, "class_name": "InvalidHostname", "function_description": "Generates an informative string representation of an `InvalidHostname` error. It specifies the expected hostname or netloc versus the actual request for debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 154, "body": "def __str__(self) -> str:\n        return f'Stream(id={self.stream_id!r})'", "is_method": true, "class_name": "Stream", "function_description": "Provides a human-readable string representation of a Stream object, typically used for debugging or logging. It includes the stream's ID for identification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_log_warnsize", "line_number": 160, "body": "def _log_warnsize(self) -> bool:\n        \"\"\"Checks if we have received data which exceeds the download warnsize\n        and whether we have not already logged about it.\n\n        Returns:\n            True if both the above conditions hold true\n            False if any of the conditions is false\n        \"\"\"\n        content_length_header = int(self._response['headers'].get(b'Content-Length', -1))\n        return (\n            self._download_warnsize\n            and (\n                self._response['flow_controlled_size'] > self._download_warnsize\n                or content_length_header > self._download_warnsize\n            )\n            and not self.metadata['reached_warnsize']\n        )", "is_method": true, "class_name": "Stream", "function_description": "Determines if the stream's downloaded data size has exceeded a warning threshold and if this specific condition has not been previously logged. It provides a signal for triggering alerts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "get_response", "line_number": 178, "body": "def get_response(self) -> Deferred:\n        \"\"\"Simply return a Deferred which fires when response\n        from the asynchronous request is available\n        \"\"\"\n        return self._deferred_response", "is_method": true, "class_name": "Stream", "function_description": "Provides a Deferred object that will resolve with the response from an asynchronous request when available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "check_request_url", "line_number": 184, "body": "def check_request_url(self) -> bool:\n        # Make sure that we are sending the request to the correct URL\n        url = urlparse(self._request.url)\n        return (\n            url.netloc == str(self._protocol.metadata['uri'].host, 'utf-8')\n            or url.netloc == str(self._protocol.metadata['uri'].netloc, 'utf-8')\n            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n        )", "is_method": true, "class_name": "Stream", "function_description": "This method in the `Stream` class verifies if the request's URL matches the expected network location defined in the protocol's metadata. It ensures the request is targeting the correct and authorized destination."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_get_request_headers", "line_number": 193, "body": "def _get_request_headers(self) -> List[Tuple[str, str]]:\n        url = urlparse(self._request.url)\n\n        path = url.path\n        if url.query:\n            path += '?' + url.query\n\n        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n        # MUST include a value of '/'. The exception to this rule is an\n        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n        # a path component; these MUST include a \":path\" pseudo-header field\n        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n        if not path:\n            path = '*' if self._request.method == 'OPTIONS' else '/'\n\n        # Make sure pseudo-headers comes before all the other headers\n        headers = [\n            (':method', self._request.method),\n            (':authority', url.netloc),\n        ]\n\n        # The \":scheme\" and \":path\" pseudo-header fields MUST\n        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n        if self._request.method != 'CONNECT':\n            headers += [\n                (':scheme', self._protocol.metadata['uri'].scheme),\n                (':path', path),\n            ]\n\n        content_length = str(len(self._request.body))\n        headers.append(('Content-Length', content_length))\n\n        content_length_name = self._request.headers.normkey(b'Content-Length')\n        for name, values in self._request.headers.items():\n            for value in values:\n                value = str(value, 'utf-8')\n                if name == content_length_name:\n                    if value != content_length:\n                        logger.warning(\n                            'Ignoring bad Content-Length header %r of request %r, '\n                            'sending %r instead',\n                            value,\n                            self._request,\n                            content_length,\n                        )\n                    continue\n                headers.append((str(name, 'utf-8'), value))\n\n        return headers", "is_method": true, "class_name": "Stream", "function_description": "Generates a list of HTTP request headers, including pseudo-headers and Content-Length, for an outgoing request. It correctly formats headers according to HTTP specifications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "initiate_request", "line_number": 244, "body": "def initiate_request(self) -> None:\n        if self.check_request_url():\n            headers = self._get_request_headers()\n            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n            self.metadata['request_sent'] = True\n            self.send_data()\n        else:\n            # Close this stream calling the response errback\n            # Note that we have not sent any headers\n            self.close(StreamCloseReason.INVALID_HOSTNAME)", "is_method": true, "class_name": "Stream", "function_description": "This method initiates sending an HTTP request for the stream. It validates the request URL, transmitting headers and data if valid, otherwise closing the stream."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "send_data", "line_number": 255, "body": "def send_data(self) -> None:\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n         data as part of the request.\n\n         If the content length is 0 initially then we end the stream immediately and\n         wait for response data.\n\n         Warning: Only call this method when stream not closed from client side\n            and has initiated request already by sending HEADER frame. If not then\n            stream will raise ProtocolError (raise by h2 state machine).\n         \"\"\"\n        if self.metadata['stream_closed_local']:\n            raise StreamClosedError(self.stream_id)\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._protocol.conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send_size = min(window_size, self.metadata['remaining_content_length'])\n\n        # We now need to send a number of data frames.\n        while bytes_to_send_size > 0:\n            chunk_size = min(bytes_to_send_size, max_frame_size)\n\n            data_chunk_start_id = self.metadata['request_content_length'] - self.metadata['remaining_content_length']\n            data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]\n\n            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n\n            bytes_to_send_size = bytes_to_send_size - chunk_size\n            self.metadata['remaining_content_length'] = self.metadata['remaining_content_length'] - chunk_size\n\n        self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])\n\n        # End the stream if no more data needs to be send\n        if self.metadata['remaining_content_length'] == 0:\n            self._protocol.conn.end_stream(self.stream_id)", "is_method": true, "class_name": "Stream", "function_description": "Transmits the remaining request body data for an HTTP/2 stream, respecting flow control and frame size limits. It ensures all data is sent and closes the stream."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_window_update", "line_number": 300, "body": "def receive_window_update(self) -> None:\n        \"\"\"Flow control window size was changed.\n        Send data that earlier could not be sent as we were\n        blocked behind the flow control.\n        \"\"\"\n        if (\n            self.metadata['remaining_content_length']\n            and not self.metadata['stream_closed_server']\n            and self.metadata['request_sent']\n        ):\n            self.send_data()", "is_method": true, "class_name": "Stream", "function_description": "Handles flow control window updates for the stream. It attempts to send previously blocked data when the window size permits, ensuring efficient data transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_data", "line_number": 312, "body": "def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n        self._response['body'].write(data)\n        self._response['flow_controlled_size'] += flow_controlled_length\n\n        # We check maxsize here in case the Content-Length header was not received\n        if self._download_maxsize and self._response['flow_controlled_size'] > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata['reached_warnsize'] = True\n            warning_msg = (\n                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n                f'warn size ({self._download_warnsize}) in request {self._request}'\n            )\n            logger.warning(warning_msg)\n\n        # Acknowledge the data received\n        self._protocol.conn.acknowledge_received_data(\n            self._response['flow_controlled_size'],\n            self.stream_id\n        )", "is_method": true, "class_name": "Stream", "function_description": "This method processes incoming data for a stream, managing flow control and ensuring data size limits are respected. It acknowledges received bytes with the underlying protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_headers", "line_number": 335, "body": "def receive_headers(self, headers: List[HeaderTuple]) -> None:\n        for name, value in headers:\n            self._response['headers'][name] = value\n\n        # Check if we exceed the allowed max data size which can be received\n        expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n        if self._download_maxsize and expected_size > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata['reached_warnsize'] = True\n            warning_msg = (\n                f'Expected response size ({expected_size}) larger than '\n                f'download warn size ({self._download_warnsize}) in request {self._request}'\n            )\n            logger.warning(warning_msg)", "is_method": true, "class_name": "Stream", "function_description": "Processes and stores incoming response headers for the stream. It validates content size against configured limits, terminating the stream if the maximum size is exceeded."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "reset_stream", "line_number": 353, "body": "def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n        \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n        if self.metadata['stream_closed_local']:\n            raise StreamClosedError(self.stream_id)\n\n        # Clear buffer earlier to avoid keeping data in memory for a long time\n        self._response['body'].truncate(0)\n\n        self.metadata['stream_closed_local'] = True\n        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n        self.close(reason)", "is_method": true, "class_name": "Stream", "function_description": "Closes the network stream from the local end, signaling the remote peer of its termination and clearing internal buffers. This method provides a way to forcibly reset a stream.\nCloses the network stream from the local end, signaling the remote peer of its termination and clearing internal buffers. This method provides a way to forcibly reset a stream."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "close", "line_number": 365, "body": "def close(\n        self,\n        reason: StreamCloseReason,\n        errors: Optional[List[BaseException]] = None,\n        from_protocol: bool = False,\n    ) -> None:\n        \"\"\"Based on the reason sent we will handle each case.\n        \"\"\"\n        if self.metadata['stream_closed_server']:\n            raise StreamClosedError(self.stream_id)\n\n        if not isinstance(reason, StreamCloseReason):\n            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')\n\n        # Have default value of errors as an empty list as\n        # some cases can add a list of exceptions\n        errors = errors or []\n\n        if not from_protocol:\n            self._protocol.pop_stream(self.stream_id)\n\n        self.metadata['stream_closed_server'] = True\n\n        # We do not check for Content-Length or Transfer-Encoding in response headers\n        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n\n        # NOTE: Order of handling the events is important here\n        # As we immediately cancel the request when maxsize is exceeded while\n        # receiving DATA_FRAME's when we have received the headers (not\n        # having Content-Length)\n        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n            expected_size = int(self._response['headers'].get(\n                b'Content-Length',\n                self._response['flow_controlled_size'])\n            )\n            error_msg = (\n                f'Cancelling download of {self._request.url}: received response '\n                f'size ({expected_size}) larger than download max size ({self._download_maxsize})'\n            )\n            logger.error(error_msg)\n            self._deferred_response.errback(CancelledError(error_msg))\n\n        elif reason is StreamCloseReason.ENDED:\n            self._fire_response_deferred()\n\n        # Stream was abruptly ended here\n        elif reason is StreamCloseReason.CANCELLED:\n            # Client has cancelled the request. Remove all the data\n            # received and fire the response deferred with no flags set\n\n            # NOTE: The data is already flushed in Stream.reset_stream() called\n            # immediately when the stream needs to be cancelled\n\n            # There maybe no :status in headers, we make\n            # HTTP Status Code: 499 - Client Closed Request\n            self._response['headers'][':status'] = '499'\n            self._fire_response_deferred()\n\n        elif reason is StreamCloseReason.RESET:\n            self._deferred_response.errback(ResponseFailed([\n                Failure(\n                    f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n                    ProtocolError\n                )\n            ]))\n\n        elif reason is StreamCloseReason.CONNECTION_LOST:\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INACTIVE:\n            errors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n            self._deferred_response.errback(InvalidHostname(\n                self._request,\n                str(self._protocol.metadata['uri'].host, 'utf-8'),\n                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n            ))", "is_method": true, "class_name": "Stream", "function_description": "This method closes the stream, handling various termination reasons such as completion, cancellation, or errors. It updates stream state and triggers appropriate callbacks to signal its final status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_fire_response_deferred", "line_number": 447, "body": "def _fire_response_deferred(self) -> None:\n        \"\"\"Builds response from the self._response dict\n        and fires the response deferred callback with the\n        generated response instance\"\"\"\n\n        body = self._response['body'].getvalue()\n        response_cls = responsetypes.from_args(\n            headers=self._response['headers'],\n            url=self._request.url,\n            body=body,\n        )\n\n        response = response_cls(\n            url=self._request.url,\n            status=int(self._response['headers'][':status']),\n            headers=self._response['headers'],\n            body=body,\n            request=self._request,\n            certificate=self._protocol.metadata['certificate'],\n            ip_address=self._protocol.metadata['ip_address'],\n            protocol='h2',\n        )\n\n        self._deferred_response.callback(response)", "is_method": true, "class_name": "Stream", "function_description": "Assembles a complete HTTP response object from internal stream data. It then dispatches this response by invoking a deferred callback."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_cancel", "line_number": 143, "body": "def _cancel(_) -> None:\n            # Close this stream as gracefully as possible\n            # If the associated request is initiated we reset this stream\n            # else we directly call close() method\n            if self.metadata['request_sent']:\n                self.reset_stream(StreamCloseReason.CANCELLED)\n            else:\n                self.close(StreamCloseReason.CANCELLED)", "is_method": true, "class_name": "Stream", "function_description": "Gracefully cancels and closes the stream. It adapts its shutdown procedure based on whether an associated request has already been sent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_connection", "line_number": 31, "body": "def get_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n        if key in self._pending_requests:\n            # Received a request while connecting to remote\n            # Create a deferred which will fire with the H2ClientProtocol\n            # instance\n            d = Deferred()\n            self._pending_requests[key].append(d)\n            return d\n\n        # Check if we already have a connection to the remote\n        conn = self._connections.get(key, None)\n        if conn:\n            # Return this connection instance wrapped inside a deferred\n            return defer.succeed(conn)\n\n        # No connection is established for the given URI\n        return self._new_connection(key, uri, endpoint)", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "The `H2ConnectionPool.get_connection` method provides an H2 client connection. It reuses existing connections, waits for connections currently being established, or creates a new one to efficiently manage HTTP/2 resources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "_new_connection", "line_number": 49, "body": "def _new_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n        self._pending_requests[key] = deque()\n\n        conn_lost_deferred = Deferred()\n        conn_lost_deferred.addCallback(self._remove_connection, key)\n\n        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n        conn_d = endpoint.connect(factory)\n        conn_d.addCallback(self.put_connection, key)\n\n        d = Deferred()\n        self._pending_requests[key].append(d)\n        return d", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Initiates a new asynchronous HTTP/2 client connection for the pool. It sets up callbacks for connection establishment and loss, providing a mechanism for clients to await connection readiness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "put_connection", "line_number": 63, "body": "def put_connection(self, conn: H2ClientProtocol, key: Tuple) -> H2ClientProtocol:\n        self._connections[key] = conn\n\n        # Now as we have established a proper HTTP/2 connection\n        # we fire all the deferred's with the connection instance\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.callback(conn)\n\n        return conn", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Stores an H2 client connection in the pool. It then delivers this connection to all pending requests that were awaiting its availability."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "_remove_connection", "line_number": 75, "body": "def _remove_connection(self, errors: List[BaseException], key: Tuple) -> None:\n        self._connections.pop(key)\n\n        # Call the errback of all the pending requests for this connection\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.errback(errors)", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Removes a specific H2 connection from the pool. It notifies any pending requests associated with that connection about the encountered errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "close_connections", "line_number": 84, "body": "def close_connections(self) -> None:\n        \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n        Returns:\n            Deferred that fires when all connections have been closed\n        \"\"\"\n        for conn in self._connections.values():\n            conn.transport.abortConnection()", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Closes and removes all active HTTP/2 connections from the pool, ensuring proper network resource release."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_endpoint", "line_number": 110, "body": "def get_endpoint(self, uri: URI):\n        return self.endpoint_factory.endpointForURI(uri)", "is_method": true, "class_name": "H2Agent", "function_description": "This method retrieves the appropriate communication endpoint for a given URI. It enables the H2Agent to interact with specified remote resources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_key", "line_number": 113, "body": "def get_key(self, uri: URI) -> Tuple:\n        \"\"\"\n        Arguments:\n            uri - URI obtained directly from request URL\n        \"\"\"\n        return uri.scheme, uri.host, uri.port", "is_method": true, "class_name": "H2Agent", "function_description": "Extracts the scheme, host, and port from a URI to create a unique identifier. This key likely facilitates connection management or resource indexing within the agent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "request", "line_number": 120, "body": "def request(self, request: Request, spider: Spider) -> Deferred:\n        uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))\n        try:\n            endpoint = self.get_endpoint(uri)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n\n        key = self.get_key(uri)\n        d = self._pool.get_connection(key, uri, endpoint)\n        d.addCallback(lambda conn: conn.request(request, spider))\n        return d", "is_method": true, "class_name": "H2Agent", "function_description": "This method sends an HTTP/2 request by obtaining a connection from its pool. It dispatches the given request through this connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_endpoint", "line_number": 152, "body": "def get_endpoint(self, uri: URI):\n        return self.endpoint_factory.endpointForURI(self._proxy_uri)", "is_method": true, "class_name": "ScrapyProxyH2Agent", "function_description": "Provides the network endpoint configured for the Scrapy proxy. It obtains this endpoint from an endpoint factory using the agent's internal proxy URI."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_key", "line_number": 155, "body": "def get_key(self, uri: URI) -> Tuple:\n        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port", "is_method": true, "class_name": "ScrapyProxyH2Agent", "function_description": "Provides a unique identifier tuple for the HTTP proxy used by this agent, distinguishing it from the request's original URI."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "get_engine_status", "line_number": 7, "body": "def get_engine_status(engine):\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"engine.has_capacity()\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle(engine.spider)\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]\n        except Exception as e:\n            checks += [(test, f\"{type(e).__name__} (exception)\")]\n\n    return checks", "is_method": false, "function_description": "Provides a detailed operational status report for an engine. It evaluates various internal metrics and states, useful for monitoring performance and diagnosing issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "format_engine_status", "line_number": 37, "body": "def format_engine_status(engine=None):\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += f\"{test:<47} : {result}\\n\"\n    s += \"\\n\"\n\n    return s", "is_method": false, "function_description": "Generates a formatted, human-readable string summary of an execution engine's operational status checks, suitable for display or logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "print_engine_status", "line_number": 47, "body": "def print_engine_status(engine):\n    print(format_engine_status(engine))", "is_method": false, "function_description": "Prints the current status of a given engine to the console."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "send_catch_log", "line_number": 23, "body": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = (named.pop('dont_log', _IgnoredException), StopDownload)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender, *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses", "is_method": false, "function_description": "Dispatches a signal to all registered receivers. It gracefully handles and logs any exceptions from receivers, returning failures to ensure robust system operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "send_catch_log_deferred", "line_number": 49, "body": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender, *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d", "is_method": false, "function_description": "Dispatches a signal, asynchronously waiting for all signal handlers (even those returning deferreds) to complete. It consolidates handler results and logs any errors encountered."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "disconnect_all", "line_number": 75, "body": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)", "is_method": false, "function_description": "Removes all active handlers for a specified signal and sender. Primarily used for cleanup in testing to ensure a clean state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "logerror", "line_number": 54, "body": "def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure", "is_method": false, "function_description": "Provides a robust error logging service for signal handlers or receivers, selectively logging failures unless their type is explicitly excluded. It ensures critical errors are reported while allowing specific exceptions to be ignored."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_fingerprint", "line_number": 23, "body": "def request_fingerprint(\n    request: Request,\n    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n    keep_fragments: bool = False,\n):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (i.e. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    Also, servers usually ignore fragments in urls when handling requests,\n    so they are also ignored by default when calculating the fingerprint.\n    If you want to include them, set the keep_fragments argument to True\n    (for instance when handling requests with a headless browser).\n\n    \"\"\"\n    headers: Optional[Tuple[bytes, ...]] = None\n    if include_headers:\n        headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n    cache = _fingerprint_cache.setdefault(request, {})\n    cache_key = (headers, keep_fragments)\n    if cache_key not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n        fp.update(request.body or b'')\n        if headers:\n            for hdr in headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[cache_key] = fp.hexdigest()\n    return cache[cache_key]", "is_method": false, "function_description": "Calculates a unique, canonical hash (fingerprint) for an HTTP request to identify the resource it targets. Useful for identifying equivalent requests despite superficial differences like query order or session IDs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_authenticate", "line_number": 79, "body": "def request_authenticate(request: Request, username: str, password: str) -> None:\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)", "is_method": false, "function_description": "Configures an HTTP request for Basic Authentication by adding the necessary Authorization header. It enables the request to be sent with credentials for secure access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_httprepr", "line_number": 86, "body": "def request_httprepr(request: Request) -> bytes:\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s", "is_method": false, "function_description": "Provides a raw HTTP byte representation of a given request object. This is useful for inspection, debugging, or logging the structure of an HTTP request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "referer_str", "line_number": 103, "body": "def referer_str(request: Request) -> Optional[str]:\n    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n    referrer = request.headers.get('Referer')\n    if referrer is None:\n        return referrer\n    return to_unicode(referrer, errors='replace')", "is_method": false, "function_description": "This function safely extracts and returns the 'Referer' HTTP header from a request, converting it to a Unicode string suitable for logging or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "_getarg", "line_number": 30, "body": "def _getarg(request, name, default=None, type=str):\n    return type(request.args[name][0]) if name in request.args else default", "is_method": false, "function_description": "Provides a utility to safely extract a named argument from a web request's query parameters, with optional default values and type casting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "getChild", "line_number": 12, "body": "def getChild(self, name, request):\n        return self", "is_method": true, "class_name": "Root", "function_description": "This method serves as a base handler for child retrieval requests by always returning the current Root instance. It effectively treats the root as its own child."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "render", "line_number": 15, "body": "def render(self, request):\n        total = _getarg(request, b'total', 100, int)\n        show = _getarg(request, b'show', 10, int)\n        nlist = [random.randint(1, total) for _ in range(show)]\n        request.write(b\"<html><head></head><body>\")\n        args = request.args.copy()\n        for nl in nlist:\n            args['n'] = nl\n            argstr = urlencode(args, doseq=True)\n            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n                          .encode('utf8'))\n        request.write(b\"</body></html>\")\n        return b''", "is_method": true, "class_name": "Root", "function_description": "Generates an HTML web page dynamically displaying a list of clickable links. Each link leads to a '/follow' endpoint with an associated random number."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "_print_listening", "line_number": 40, "body": "def _print_listening():\n        httpHost = httpPort.getHost()\n        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")", "is_method": false, "function_description": "Prints the HTTP server's listening address to the console. It informs users where the server is accessible."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_is_from_any_domain", "line_number": 19, "body": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith(f'.{d}')) for d in domains)", "is_method": false, "function_description": "Determines if a URL originates from any of a specified list of domains. This function is useful for validating and filtering URLs by their source."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_is_from_spider", "line_number": 28, "body": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url, [spider.name] + list(getattr(spider, 'allowed_domains', [])))", "is_method": false, "function_description": "Checks if a given URL belongs to the specified web spider by validating its domain against the spider's name and its allowed domains."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_has_any_extension", "line_number": 33, "body": "def url_has_any_extension(url, extensions):\n    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions", "is_method": false, "function_description": "Checks if a given URL's file extension matches any in a specified collection of extensions. Useful for filtering or categorizing URLs by file type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "parse_url", "line_number": 37, "body": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_unicode(url, encoding))", "is_method": false, "function_description": "Ensures a URL input is returned as a parsed URL object, handling both string and already-parsed inputs for consistent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "escape_ajax", "line_number": 46, "body": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])", "is_method": false, "function_description": "Transforms URLs with `#!` fragments into `?_escaped_fragment_=` query parameters. This prepares dynamic web content for search engine crawling under Google's deprecated AJAX crawling scheme."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "add_http_if_no_scheme", "line_number": 75, "body": "def add_http_if_no_scheme(url):\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url", "is_method": false, "function_description": "Normalizes URLs by adding 'http://' as the default scheme if no scheme is present. This prepares the URL for standard web operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_posix_path", "line_number": 86, "body": "def _is_posix_path(string):\n    return bool(\n        re.match(\n            r'''\n            ^                   # start with...\n            (\n                \\.              # ...a single dot,\n                (\n                    \\. | [^/\\.]+  # optionally followed by\n                )?                # either a second dot or some characters\n                |\n                ~   # $HOME\n            )?      # optional match of \".\", \"..\" or \".blabla\"\n            /       # at least one \"/\" for a file path,\n            .       # and something after the \"/\"\n            ''',\n            string,\n            flags=re.VERBOSE,\n        )\n    )", "is_method": false, "function_description": "Determines if a string structurally resembles a POSIX-style file path. It validates the presence of path separators and common directory indicators."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_windows_path", "line_number": 108, "body": "def _is_windows_path(string):\n    return bool(\n        re.match(\n            r'''\n            ^\n            (\n                [a-z]:\\\\\n                | \\\\\\\\\n            )\n            ''',\n            string,\n            flags=re.IGNORECASE | re.VERBOSE,\n        )\n    )", "is_method": false, "function_description": "This utility function determines if a string starts with a Windows-style path, such as a drive letter or a UNC network share."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_filesystem_path", "line_number": 124, "body": "def _is_filesystem_path(string):\n    return _is_posix_path(string) or _is_windows_path(string)", "is_method": false, "function_description": "Determines if a given string represents a valid filesystem path, covering both POSIX and Windows formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "guess_scheme", "line_number": 128, "body": "def guess_scheme(url):\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n    http:// otherwise.\"\"\"\n    if _is_filesystem_path(url):\n        return any_to_uri(url)\n    return add_http_if_no_scheme(url)", "is_method": false, "function_description": "Adds a URL scheme (file:// or http://) to an input string if missing, based on whether it's a file path or a web address. It standardizes the input for further URI processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "strip_url", "line_number": 136, "body": "def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):\n\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):\n        netloc = netloc.split('@')[-1]\n    if strip_default_port and parsed_url.port:\n        if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n                                                    ('https', 443),\n                                                    ('ftp', 21)):\n            netloc = netloc.replace(f':{parsed_url.port}', '')\n    return urlunparse((\n        parsed_url.scheme,\n        netloc,\n        '/' if origin_only else parsed_url.path,\n        '' if origin_only else parsed_url.params,\n        '' if origin_only else parsed_url.query,\n        '' if strip_fragment else parsed_url.fragment\n    ))", "is_method": false, "function_description": "This function normalizes a URL string by stripping various components such as credentials, default ports, or fragments. It can also extract just the URL's origin, useful for canonicalization or privacy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "get_base_url", "line_number": 22, "body": "def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url, response.encoding)\n    return _baseurl_cache[response]", "is_method": false, "function_description": "This function extracts and returns the effective base URL from a web page response. It caches the result for efficiency, which is crucial for resolving relative URLs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "get_meta_refresh", "line_number": 33, "body": "def get_meta_refresh(\n    response: \"scrapy.http.response.text.TextResponse\",\n    ignore_tags: Optional[Iterable[str]] = ('script', 'noscript'),\n) -> Union[Tuple[None, None], Tuple[float, str]]:\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(\n            text, response.url, response.encoding, ignore_tags=ignore_tags)\n    return _metaref_cache[response]", "is_method": false, "function_description": "Extracts and caches the http-equiv refresh parameter (delay and URL) from a given web response. This helps web crawlers to automatically follow client-side redirects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "response_status_message", "line_number": 45, "body": "def response_status_message(status: Union[bytes, float, int, str]) -> str:\n    \"\"\"Return status code plus status text descriptive message\n    \"\"\"\n    status_int = int(status)\n    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n    return f'{status_int} {to_unicode(message)}'", "is_method": false, "function_description": "Generates a human-readable status message string for a given numerical status code. It combines the code with its standard descriptive text, defaulting to 'Unknown Status' if not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "response_httprepr", "line_number": 53, "body": "def response_httprepr(response: Response) -> bytes:\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    values = [\n        b\"HTTP/1.1 \",\n        to_bytes(str(response.status)),\n        b\" \",\n        to_bytes(http.RESPONSES.get(response.status, b'')),\n        b\"\\r\\n\",\n    ]\n    if response.headers:\n        values.extend([response.headers.to_string(), b\"\\r\\n\"])\n    values.extend([b\"\\r\\n\", response.body])\n    return b\"\".join(values)", "is_method": false, "function_description": "This function generates a byte string representing a raw HTTP response from a given Response object. It's primarily for reference or debugging purposes, showing what the response would look like in its raw form."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "open_in_browser", "line_number": 71, "body": "def open_in_browser(\n    response: Union[\"scrapy.http.response.html.HtmlResponse\", \"scrapy.http.response.text.TextResponse\"],\n    _openfunc: Callable[[str], Any] = webbrowser.open,\n) -> Any:\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = f'<head><base href=\"{response.url}\">'\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: \"\n                        f\"{response.__class__.__name__}\")\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(f\"file://{fname}\")", "is_method": false, "function_description": "Opens a Scrapy HTTP response in the local web browser for inspection. It handles HTML and text responses, ensuring correct link resolution for HTML content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/httpobj.py", "function": "urlparse_cached", "line_number": 13, "body": "def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]", "is_method": false, "function_description": "Parses URLs from request or response objects and caches the results. This offers efficient, repeated access to the parsed URL components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "iterate_spider_output", "line_number": 13, "body": "def iterate_spider_output(result):\n    if inspect.isasyncgen(result):\n        d = deferred_from_coro(collect_asyncgen(result))\n        d.addCallback(iterate_spider_output)\n        return d\n    elif inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(result)", "is_method": false, "function_description": "Converts diverse spider outputs, including asynchronous generators and coroutines, into a consistent iterable. This enables uniform processing of results from web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "iter_spider_classes", "line_number": 25, "body": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (i.e. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in vars(module).values():\n        if (\n            inspect.isclass(obj)\n            and issubclass(obj, Spider)\n            and obj.__module__ == module.__name__\n            and getattr(obj, 'name', None)\n        ):\n            yield obj", "is_method": false, "function_description": "Provides an iterator over Scrapy Spider classes defined directly within a module that possess a name attribute, enabling dynamic spider discovery. This service is crucial for loading and managing spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "spidercls_for_request", "line_number": 43, "body": "def spidercls_for_request(spider_loader, request, default_spidercls=None,\n                          log_none=False, log_multiple=False):\n    \"\"\"Return a spider class that handles the given Request.\n\n    This will look for the spiders that can handle the given request (using\n    the spider loader) and return a Spider class if (and only if) there is\n    only one Spider able to handle the Request.\n\n    If multiple spiders (or no spider) are found, it will return the\n    default_spidercls passed. It can optionally log if multiple or no spiders\n    are found.\n    \"\"\"\n    snames = spider_loader.find_by_request(request)\n    if len(snames) == 1:\n        return spider_loader.load(snames[0])\n\n    if len(snames) > 1 and log_multiple:\n        logger.error('More than one spider can handle: %(request)s - %(snames)s',\n                     {'request': request, 'snames': ', '.join(snames)})\n\n    if len(snames) == 0 and log_none:\n        logger.error('Unable to find spider that handles: %(request)s',\n                     {'request': request})\n\n    return default_spidercls", "is_method": false, "function_description": "Determines the single best spider class to handle a given web request using a spider loader. If multiple or no spiders match, it returns a default spider class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/template.py", "function": "render_templatefile", "line_number": 8, "body": "def render_templatefile(path, **kwargs):\n    with open(path, 'rb') as fp:\n        raw = fp.read().decode('utf8')\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n\n    if path.endswith('.tmpl'):\n        os.rename(path, render_path)\n\n    with open(render_path, 'wb') as fp:\n        fp.write(content.encode('utf8'))", "is_method": false, "function_description": "Renders a template file by substituting placeholders with provided arguments. The processed content is then saved to a new file, optionally renaming the original template."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/template.py", "function": "string_camelcase", "line_number": 26, "body": "def string_camelcase(string):\n    \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n\n    >>> string_camelcase('lost-pound')\n    'LostPound'\n\n    >>> string_camelcase('missing_images')\n    'MissingImages'\n\n    \"\"\"\n    return CAMELCASE_INVALID_CHARS.sub('', string.title())", "is_method": false, "function_description": "Provides a utility to transform an input string into CamelCase format. It capitalizes each word segment while removing common separator characters like hyphens or underscores."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__getitem__", "line_number": 22, "body": "def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Retrieves items from the dictionary using a provided key. It automatically normalizes the key to ensure case-insensitive lookup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 25, "body": "def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))", "is_method": true, "class_name": "CaselessDict", "function_description": "This method enables setting items in the dictionary, automatically normalizing the key for case-insensitivity and also normalizing the value before storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__delitem__", "line_number": 28, "body": "def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Provides case-insensitive deletion of items from the `CaselessDict` instance. It normalizes the provided key before removing the corresponding entry."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__contains__", "line_number": 31, "body": "def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Checks if a key is present in the dictionary using a case-insensitive comparison. It allows querying for keys without needing to match their exact casing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__copy__", "line_number": 35, "body": "def __copy__(self):\n        return self.__class__(self)", "is_method": true, "class_name": "CaselessDict", "function_description": "Creates a shallow copy of the `CaselessDict` instance. This allows for independent modification of the new dictionary without affecting the original."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "normkey", "line_number": 39, "body": "def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()", "is_method": true, "class_name": "CaselessDict", "function_description": "This method normalizes dictionary keys by converting them to lowercase. It facilitates case-insensitive key lookups within the `CaselessDict` class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "get", "line_number": 47, "body": "def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))", "is_method": true, "class_name": "CaselessDict", "function_description": "Provides case-insensitive retrieval of a value associated with the given key. It returns a normalized default value if the key is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "setdefault", "line_number": 50, "body": "def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))", "is_method": true, "class_name": "CaselessDict", "function_description": "Provides the standard `setdefault` functionality for a CaselessDict, allowing case-insensitive retrieval or insertion of a default value for a key."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "update", "line_number": 53, "body": "def update(self, seq):\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super().update(iseq)", "is_method": true, "class_name": "CaselessDict", "function_description": "This method updates the dictionary with new key-value pairs. It normalizes keys for case-insensitive handling and also normalizes values before the update operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "fromkeys", "line_number": 59, "body": "def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)", "is_method": true, "class_name": "CaselessDict", "function_description": "Creates a new `CaselessDict` instance from an iterable of keys, initializing each with an optional default value. This method allows for quick, case-insensitive dictionary population."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "pop", "line_number": 62, "body": "def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)", "is_method": true, "class_name": "CaselessDict", "function_description": "Removes and returns an item from the dictionary, treating the provided key in a case-insensitive manner."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 76, "body": "def __setitem__(self, key, value):\n        if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)\n        super().__setitem__(key, value)", "is_method": true, "class_name": "LocalCache", "function_description": "Adds a key-value pair to the cache. It automatically evicts older items if the cache reaches its defined size limit, ensuring capacity management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 99, "body": "def __setitem__(self, key, value):\n        try:\n            super().__setitem__(key, value)\n        except TypeError:\n            pass", "is_method": true, "class_name": "LocalWeakReferencedCache", "function_description": "Stores a key-value pair in the cache. It silently rejects values that cannot be weakly referenced by the underlying cache mechanism."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__getitem__", "line_number": 105, "body": "def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except (TypeError, KeyError):\n            return None", "is_method": true, "class_name": "LocalWeakReferencedCache", "function_description": "Enables dictionary-style retrieval for cache entries. It returns `None` if the key is not found or invalid, preventing `KeyError` or `TypeError`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__contains__", "line_number": 118, "body": "def __contains__(self, item):\n        return item not in self.seq", "is_method": true, "class_name": "SequenceExclude", "function_description": "Enables the `in` operator to check if an item is *not* present in the internal sequence, effectively defining an \"exclusion\" check for `SequenceExclude` objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_enable_windows_terminal_processing", "line_number": 12, "body": "def _enable_windows_terminal_processing():\n    # https://stackoverflow.com/a/36760881\n    kernel32 = ctypes.windll.kernel32\n    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))", "is_method": false, "function_description": "Configures the Windows console to enable processing of virtual terminal sequences, allowing for features like colored text and advanced formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_tty_supports_color", "line_number": 18, "body": "def _tty_supports_color():\n    if sys.platform != \"win32\":\n        return True\n\n    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n        return True\n\n    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n    # processing is enabled.\n    return _enable_windows_terminal_processing()", "is_method": false, "function_description": "Determines if the current terminal supports ANSI color codes for colored output. For specific Windows versions, it attempts to enable this capability for proper display."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_colorize", "line_number": 30, "body": "def _colorize(text, colorize=True):\n    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n        return text\n    try:\n        from pygments import highlight\n    except ImportError:\n        return text\n    else:\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n        return highlight(text, PythonLexer(), TerminalFormatter())", "is_method": false, "function_description": "Applies Python syntax highlighting to text for display in a terminal. It colorizes the output only if the environment supports it and the Pygments library is installed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "pformat", "line_number": 43, "body": "def pformat(obj, *args, **kwargs):\n    return _colorize(pformat_(obj), kwargs.pop('colorize', True))", "is_method": false, "function_description": "It provides a colorized, pretty-printed string representation of an object, useful for debugging or display."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "pprint", "line_number": 47, "body": "def pprint(obj, *args, **kwargs):\n    print(pformat(obj, *args, **kwargs))", "is_method": false, "function_description": "This function pretty-prints any Python object, providing a human-readable and structured representation. It's useful for debugging and inspecting complex data structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ossignal.py", "function": "install_shutdown_handlers", "line_number": 12, "body": "def install_shutdown_handlers(function, override_sigint=True):\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n    SIGINT handler won't be install if there is already a handler in place\n    (e.g.  Pdb)\n    \"\"\"\n    from twisted.internet import reactor\n    reactor._handleSignals()\n    signal.signal(signal.SIGTERM, function)\n    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, 'SIGBREAK'):\n        signal.signal(signal.SIGBREAK, function)", "is_method": false, "function_description": "Registers a specified function to execute when the program receives common shutdown signals like SIGTERM or SIGINT, enabling graceful application termination and resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "format_live_refs", "line_number": 34, "body": "def format_live_refs(ignore=NoneType):\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(),\n                             key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s", "is_method": false, "function_description": "Generates a tabular report of currently tracked objects by class. It provides counts of live references and the age of the oldest object for each class, useful for debugging and monitoring."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "print_live_refs", "line_number": 49, "body": "def print_live_refs(*a, **kw):\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))", "is_method": false, "function_description": "Prints detailed information about currently tracked objects (live references). Useful for debugging memory leaks and understanding object lifecycle."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "get_oldest", "line_number": 54, "body": "def get_oldest(class_name):\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]", "is_method": false, "function_description": "This function retrieves the oldest active object instance for a specified class name. It identifies the object with the earliest associated value from a collection of tracked live references."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "iter_all", "line_number": 63, "body": "def iter_all(class_name):\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()", "is_method": false, "function_description": "Provides a collection of identifiers for all active instances of a specified class, looked up by its name. Useful for introspection or object lifecycle management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "__new__", "line_number": 28, "body": "def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj", "is_method": true, "class_name": "object_ref", "function_description": "Creates a new `object_ref` instance and registers it in a `live_refs` dictionary with a timestamp. This allows tracking all active instances and their creation times."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "_parse_headers_and_cookies", "line_number": 37, "body": "def _parse_headers_and_cookies(parsed_args):\n    headers = []\n    cookies = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(':', 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == 'Cookie':\n            for name, morsel in SimpleCookie(val).items():\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(':', 1)\n        headers.append(('Authorization', basic_auth_header(user, password)))\n\n    return headers, cookies", "is_method": false, "function_description": "Transforms raw input arguments into properly formatted HTTP headers and a dictionary of cookies. It also processes basic authentication credentials, preparing structured data for making web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "curl_to_request_kwargs", "line_number": 57, "body": "def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n                                        cURL options are unknown. Otherwise\n                                        raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != 'curl':\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = f'Unrecognized options: {\", \".join(argv)}'\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = 'http://' + url\n\n    method = parsed_args.method or 'GET'\n\n    result = {'method': method.upper(), 'url': url}\n\n    headers, cookies = _parse_headers_and_cookies(parsed_args)\n\n    if headers:\n        result['headers'] = headers\n    if cookies:\n        result['cookies'] = cookies\n    if parsed_args.data:\n        result['body'] = parsed_args.data\n        if not parsed_args.method:\n            # if the \"data\" is specified but the \"method\" is not specified,\n            # the default method is 'POST'\n            result['method'] = 'POST'\n\n    return result", "is_method": false, "function_description": "Converts a cURL command string into a dictionary of keyword arguments for an HTTP request. This enables programmatic execution of cURL commands using an HTTP client library."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "error", "line_number": 11, "body": "def error(self, message):\n        error_msg = f'There was an error parsing the curl command: {message}'\n        raise ValueError(error_msg)", "is_method": true, "class_name": "CurlParser", "function_description": "This method of the `CurlParser` class provides a standardized way to report errors during curl command parsing. It raises a `ValueError` when the command cannot be processed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_fail", "line_number": 15, "body": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d", "is_method": false, "function_description": "Asynchronously triggers a Deferred's errback with a given failure after a brief delay. This allows the Twisted reactor to process other events before error handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_succeed", "line_number": 28, "body": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d", "is_method": false, "function_description": "Creates a Twisted Deferred that succeeds with a given result. Its callback is intentionally delayed until the next reactor loop, ensuring other I/O operations can proceed first."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_result", "line_number": 41, "body": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)", "is_method": false, "function_description": "This function normalizes various types of results into a `defer.Deferred` object. It ensures consistent asynchronous handling by converting successes, failures, or existing deferreds into a uniform type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "mustbe_deferred", "line_number": 50, "body": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except Exception:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)", "is_method": false, "function_description": "Provides a utility to wrap a function call, returning a Twisted Deferred that resolves asynchronously in the next reactor loop. This ensures consistent non-blocking execution for any callable."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "parallel", "line_number": 67, "body": "def parallel(iterable, count, callable, *args, **named):\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = task.Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])", "is_method": false, "function_description": "Orchestrates parallel execution of a callable across elements of an iterable. It limits the number of concurrent calls to a specified count."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_chain", "line_number": 78, "body": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d", "is_method": false, "function_description": "Orchestrates sequential execution of a series of functions using a Deferred pattern. It chains callbacks, passing the result of each to the next, starting with an initial input."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_chain_both", "line_number": 87, "body": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(\n            callback=cb, errback=eb,\n            callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw,\n        )\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d", "is_method": false, "function_description": "This function constructs and immediately triggers a Twisted Deferred object, chaining a sequence of paired callbacks and errbacks to process an initial input or failure asynchronously. It's used for defining robust asynchronous processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_parallel", "line_number": 103, "body": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d", "is_method": false, "function_description": "Orchestrates the parallel, asynchronous execution of multiple callbacks on a single input. It returns a Deferred that fires with all successful results, failing if any callback fails."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "iter_errback", "line_number": 113, "body": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)", "is_method": false, "function_description": "This function wraps an iterable to provide robust error handling during iteration. It yields each item, and if an exception occurs, it calls a specified error callback function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "deferred_from_coro", "line_number": 127, "body": "def deferred_from_coro(o):\n    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n    if isinstance(o, defer.Deferred):\n        return o\n    if asyncio.isfuture(o) or inspect.isawaitable(o):\n        if not is_asyncio_reactor_installed():\n            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n            return defer.ensureDeferred(o)\n        else:\n            # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n            return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n    return o", "is_method": false, "function_description": "This utility converts `asyncio` coroutines or Futures into Twisted `Deferred` objects, bridging asynchronous programming paradigms. It returns existing `Deferred` objects unchanged."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "deferred_f_from_coro_f", "line_number": 142, "body": "def deferred_f_from_coro_f(coro_f):\n    \"\"\" Converts a coroutine function into a function that returns a Deferred.\n\n    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n    This is useful for callback chains, as callback functions are called with the previous callback result.\n    \"\"\"\n    @wraps(coro_f)\n    def f(*coro_args, **coro_kwargs):\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n    return f", "is_method": false, "function_description": "Converts an asynchronous coroutine function into a regular function that returns a Deferred. This allows it to integrate seamlessly into existing callback chains."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "maybeDeferred_coro", "line_number": 154, "body": "def maybeDeferred_coro(f, *args, **kw):\n    \"\"\" Copy of defer.maybeDeferred that also converts coroutines to Deferreds. \"\"\"\n    try:\n        result = f(*args, **kw)\n    except:  # noqa: E722\n        return defer.fail(failure.Failure(captureVars=defer.Deferred.debug))\n\n    if isinstance(result, defer.Deferred):\n        return result\n    elif asyncio.isfuture(result) or inspect.isawaitable(result):\n        return deferred_from_coro(result)\n    elif isinstance(result, failure.Failure):\n        return defer.fail(result)\n    else:\n        return defer.succeed(result)", "is_method": false, "function_description": "Converts diverse function results, including asyncio coroutines, into Twisted Deferreds. It facilitates interoperability between Twisted and asyncio asynchronous code."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "f", "line_number": 149, "body": "def f(*coro_args, **coro_kwargs):\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))", "is_method": false, "function_description": "Transforms a coroutine function's invocation into a deferred object. It allows asynchronous management and future result handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "xmliter", "line_number": 14, "body": "def xmliter(obj, nodename):\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    nodename_patt = re.escape(nodename)\n\n    DOCUMENT_HEADER_RE = re.compile(r'<\\?xml[^>]+>\\s*', re.S)\n    HEADER_END_RE = re.compile(fr'<\\s*/{nodename_patt}\\s*>', re.S)\n    END_TAG_RE = re.compile(r'<\\s*/([^\\s>]+)\\s*>', re.S)\n    NAMESPACE_RE = re.compile(r'((xmlns[:A-Za-z]*)=[^>\\s]+)', re.S)\n    text = _body_or_str(obj)\n\n    document_header = re.search(DOCUMENT_HEADER_RE, text)\n    document_header = document_header.group().strip() if document_header else ''\n    header_end_idx = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end_idx[1]:].strip() if header_end_idx else ''\n    namespaces = {}\n    if header_end:\n        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n            tag = re.search(fr'<\\s*{tagname}.*?xmlns[:=][^>]*>', text[:header_end_idx[1]], re.S)\n            if tag:\n                namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))\n\n    r = re.compile(fr'<{nodename_patt}[\\s>].*?</{nodename_patt}>', re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = (\n            document_header\n            + match.group().replace(\n                nodename,\n                f'{nodename} {\" \".join(namespaces.values())}',\n                1\n            )\n            + header_end\n        )\n        yield Selector(text=nodetext, type='xml')", "is_method": false, "function_description": "Provides an iterator over a specified XML node type, yielding a Selector object for each matching node. This simplifies parsing and processing individual entries in XML feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "xmliter_lxml", "line_number": 56, "body": "def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n    from lxml import etree\n    reader = _StreamReader(obj)\n    tag = f'{{{namespace}}}{nodename}' if namespace else nodename\n    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n    selxpath = '//' + (f'{prefix}:{nodename}' if namespace else nodename)\n    for _, node in iterable:\n        nodetext = etree.tostring(node, encoding='unicode')\n        node.clear()\n        xs = Selector(text=nodetext, type='xml')\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]", "is_method": false, "function_description": "Provides an efficient iterator to parse large XML documents, yielding Scrapy-like Selector objects for specific nodes. It facilitates easy data extraction from complex XML structures, handling namespaces."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "csviter", "line_number": 96, "body": "def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n    \"\"\" Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n\n    def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]\n\n    lines = StringIO(_body_or_str(obj, unicode=True))\n\n    kwargs = {}\n    if delimiter:\n        kwargs[\"delimiter\"] = delimiter\n    if quotechar:\n        kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            row = next(csv_r)\n        except StopIteration:\n            return\n        headers = row_to_unicode(row)\n\n    for row in csv_r:\n        row = row_to_unicode(row)\n        if len(row) != len(headers):\n            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                           \"should be: %(csvheader)d)\",\n                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n                            'csvheader': len(headers)})\n            continue\n        else:\n            yield dict(zip(headers, row))", "is_method": false, "function_description": "Provides an iterator that converts CSV data from strings or responses into dictionaries. Each dictionary maps column headers to the corresponding row values for easy data access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_body_or_str", "line_number": 145, "body": "def _body_or_str(obj, unicode=True):\n    expected_types = (Response, str, bytes)\n    if not isinstance(obj, expected_types):\n        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n        raise TypeError(\n            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n        )\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        elif isinstance(obj, TextResponse):\n            return obj.text\n        else:\n            return obj.body.decode('utf-8')\n    elif isinstance(obj, str):\n        return obj if unicode else obj.encode('utf-8')\n    else:\n        return obj.decode('utf-8') if unicode else obj", "is_method": false, "function_description": "Normalizes input objects like HTTP responses, strings, or bytes into a consistent string or byte representation. It provides a unified way to extract content based on the desired encoding."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "read", "line_number": 81, "body": "def read(self, n=65535):\n        self.read = self._read_unicode if self._is_unicode else self._read_string\n        return self.read(n).lstrip()", "is_method": true, "class_name": "_StreamReader", "function_description": "This method reads data from the stream. On its first call, it dynamically selects the appropriate underlying reader (unicode or string) based on the stream's type, then returns the data with leading whitespace removed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_read_string", "line_number": 85, "body": "def _read_string(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e]", "is_method": true, "class_name": "_StreamReader", "function_description": "Reads a specified number of characters from an internal text buffer, advancing the reading pointer. It provides sequential access to string content, simulating a stream."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_read_unicode", "line_number": 90, "body": "def _read_unicode(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e].encode('utf-8')", "is_method": true, "class_name": "_StreamReader", "function_description": "Reads a specified number of Unicode characters from the internal text buffer, updates the stream position, and returns the segment as UTF-8 encoded bytes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "row_to_unicode", "line_number": 114, "body": "def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]", "is_method": false, "function_description": "Transforms a row of data by converting each field into its Unicode representation. This ensures consistent string encoding for data manipulation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "build_component_list", "line_number": 15, "body": "def build_component_list(compdict, custom=None, convert=update_classpath):\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n                             'please update your settings')\n\n    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n                                     'convert to the same '\n                                     'object, please update your settings'\n                                     )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(f'Invalid value {value} for component {name}, '\n                                 'please provide a real number or None instead')\n\n    # BEGIN Backward compatibility for old (base, custom) call signature\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    if custom is not None:\n        compdict.update(custom)\n    # END Backward compatibility\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]", "is_method": false, "function_description": "Constructs an ordered list of component identifiers from a dictionary mapping components to their priority values. It converts component names and validates order values before sorting the final list."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "arglist_to_dict", "line_number": 61, "body": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)", "is_method": false, "function_description": "This function converts a list of \"key=value\" formatted strings into a dictionary. It provides a convenient way to parse such structured data into a usable mapping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "closest_scrapy_cfg", "line_number": 68, "body": "def closest_scrapy_cfg(path='.', prevpath=None):\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if path == prevpath:\n        return ''\n    path = os.path.abspath(path)\n    cfgfile = os.path.join(path, 'scrapy.cfg')\n    if os.path.exists(cfgfile):\n        return cfgfile\n    return closest_scrapy_cfg(os.path.dirname(path), path)", "is_method": false, "function_description": "Locates the `scrapy.cfg` file by traversing up the directory tree from a given path. It returns the absolute path if found, or an empty string otherwise."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "init_env", "line_number": 81, "body": "def init_env(project='default', set_syspath=True):\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option('settings', project):\n        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = os.path.dirname(closest)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)", "is_method": false, "function_description": "Initializes the environment for a Scrapy project. It configures the Scrapy settings module and adds the project directory to `sys.path`, allowing project modules to be imported."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "get_config", "line_number": 96, "body": "def get_config(use_closest=True):\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg", "is_method": false, "function_description": "This function loads Scrapy configuration files from specified sources and returns them as a ConfigParser object. It provides a standardized way to access application settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "get_sources", "line_number": 104, "body": "def get_sources(use_closest=True):\n    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n    sources = [\n        '/etc/scrapy.cfg',\n        r'c:\\scrapy\\scrapy.cfg',\n        xdg_config_home + '/scrapy.cfg',\n        os.path.expanduser('~/.scrapy.cfg'),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources", "is_method": false, "function_description": "Provides a list of common and user-specific paths where `scrapy.cfg` configuration files are typically found. It helps applications locate their configuration across different environments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "feed_complete_default_values_from_settings", "line_number": 117, "body": "def feed_complete_default_values_from_settings(feed, settings):\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT'))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", dict())\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out", "is_method": false, "function_description": "This function populates a feed configuration dictionary with default values sourced from application settings, ensuring all necessary parameters are completely defined for consistent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "feed_process_params_from_cli", "line_number": 132, "body": "def feed_process_params_from_cli(settings, output, output_format=None,\n                                 overwrite_output=None):\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats = without_none_values(\n        settings.getwithbase('FEED_EXPORTERS')\n    ).keys()\n\n    def check_valid_format(output_format):\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    if output_format:\n        if len(output) == 1:\n            check_valid_format(output_format)\n            message = (\n                'The -t command line option is deprecated in favor of '\n                'specifying the output format within the output URI. See the '\n                'documentation of the -o and -O options for more information.',\n            )\n            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n            return {output[0]: {'format': output_format}}\n        else:\n            raise UsageError(\n                'The -t command-line option cannot be used if multiple output '\n                'URIs are specified'\n            )\n\n    result = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(':', 1)\n        except ValueError:\n            feed_uri = element\n            feed_format = os.path.splitext(element)[1].replace('.', '')\n        else:\n            if feed_uri == '-':\n                feed_uri = 'stdout:'\n        check_valid_format(feed_format)\n        result[feed_uri] = {'format': feed_format}\n        if overwrite:\n            result[feed_uri]['overwrite'] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict('FEEDS'))\n\n    return result", "is_method": false, "function_description": "Processes command-line parameters (output URI, format, overwrite) for feed export settings. It validates inputs for inconsistencies and returns a dictionary suitable for use as the application's FEEDS setting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_check_components", "line_number": 18, "body": "def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n                             'please update your settings')", "is_method": false, "function_description": "Validates a list of components, ensuring that no two components resolve to the same internal object after conversion. This prevents conflicts and ambiguous references in system configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_map_keys", "line_number": 23, "body": "def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n                                     'convert to the same '\n                                     'object, please update your settings'\n                                     )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in compdict.items()}", "is_method": false, "function_description": "Transforms keys of a dictionary or `BaseSettings` object using a conversion function. It ensures uniqueness of converted keys, preserving priorities when processing `BaseSettings` instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_validate_values", "line_number": 40, "body": "def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(f'Invalid value {value} for component {name}, '\n                                 'please provide a real number or None instead')", "is_method": false, "function_description": "Validates that all values within a given dictionary are either `None` or real numbers. It ensures data integrity by raising an error for invalid types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "check_valid_format", "line_number": 143, "body": "def check_valid_format(output_format):\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )", "is_method": false, "function_description": "Validates if a specified output format is supported. It raises an error with available options if the format is unrecognized, ensuring correct system usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_ipython_shell", "line_number": 5, "body": "def _embed_ipython_shell(namespace={}, banner=''):\n    \"\"\"Start an IPython Shell\"\"\"\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed\n        from IPython.terminal.ipapp import load_default_config\n    except ImportError:\n        from IPython.frontend.terminal.embed import InteractiveShellEmbed\n        from IPython.frontend.terminal.ipapp import load_default_config\n\n    @wraps(_embed_ipython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        config = load_default_config()\n        # Always use .instace() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config)\n        shell()\n    return wrapper", "is_method": false, "function_description": "Provides a function to embed and start an interactive IPython shell within a Python program. This enables dynamic debugging or state exploration during execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_bpython_shell", "line_number": 28, "body": "def _embed_bpython_shell(namespace={}, banner=''):\n    \"\"\"Start a bpython shell\"\"\"\n    import bpython\n\n    @wraps(_embed_bpython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        bpython.embed(locals_=namespace, banner=banner)\n    return wrapper", "is_method": false, "function_description": "Returns a function that, when called, starts an interactive bpython shell. It allows embedding the shell for debugging or interactive code exploration with a custom namespace."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_ptpython_shell", "line_number": 38, "body": "def _embed_ptpython_shell(namespace={}, banner=''):\n    \"\"\"Start a ptpython shell\"\"\"\n    import ptpython.repl\n\n    @wraps(_embed_ptpython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        print(banner)\n        ptpython.repl.embed(locals=namespace)\n    return wrapper", "is_method": false, "function_description": "This function creates and returns a callable that can launch an interactive `ptpython` shell. It allows customizing the shell's initial namespace and displaying a banner."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_standard_shell", "line_number": 49, "body": "def _embed_standard_shell(namespace={}, banner=''):\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n    try:  # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter  # noqa: F401\n        readline.parse_and_bind(\"tab:complete\")\n\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)\n    return wrapper", "is_method": false, "function_description": "The function returns a callable that launches an interactive Python shell. This provides a useful tool for debugging and direct code interaction within the program's context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "get_shell_embed_func", "line_number": 74, "body": "def get_shell_embed_func(shells=None, known_shells=None):\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None:  # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None:  # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue", "is_method": false, "function_description": "Returns the first available and functional shell-embedding capability from a prioritized list. It enables dynamic selection of an interactive Python environment or its integration mechanism."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "start_python_console", "line_number": 92, "body": "def start_python_console(namespace=None, banner='', shells=None):\n    \"\"\"Start Python console bound to the given namespace.\n    Readline support and tab completion will be used on Unix, if available.\n    \"\"\"\n    if namespace is None:\n        namespace = {}\n\n    try:\n        shell = get_shell_embed_func(shells)\n        if shell is not None:\n            shell(namespace=namespace, banner=banner)\n    except SystemExit:  # raised when using exit() in python code.interact\n        pass", "is_method": false, "function_description": "Starts an interactive Python console, optionally pre-loading it with a given namespace. This function provides a powerful tool for debugging or interactively exploring program states."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 15, "body": "def wrapper(namespace=namespace, banner=''):\n        config = load_default_config()\n        # Always use .instace() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config)\n        shell()", "is_method": false, "function_description": "Sets up and starts an embedded interactive shell with a specified namespace and configuration. It provides a fresh, tailored environment for interactive code execution and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 33, "body": "def wrapper(namespace=namespace, banner=''):\n        bpython.embed(locals_=namespace, banner=banner)", "is_method": false, "function_description": "Launches an embedded interactive bpython shell within the current program execution. It enables interactive debugging and inspection of the program's state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 43, "body": "def wrapper(namespace=namespace, banner=''):\n        print(banner)\n        ptpython.repl.embed(locals=namespace)", "is_method": false, "function_description": "Starts an interactive `ptpython` REPL session, initializing its local scope with a given namespace and optionally displaying a banner. Useful for interactive debugging or exploration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 61, "body": "def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)", "is_method": false, "function_description": "Launches an interactive Python console, allowing users to inspect or manipulate program state via a provided namespace. This is useful for debugging or interactive script exploration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "failure_to_exc_info", "line_number": 18, "body": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())", "is_method": false, "function_description": "Extracts exception type, value, and traceback object from a `Failure` instance. It converts custom failure representations into a standard `exc_info` format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "configure_logging", "line_number": 62, "body": "def configure_logging(settings=None, install_root_handler=True):\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver('twisted')\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool('LOG_STDOUT'):\n        sys.stdout = StreamLogger(logging.getLogger('stdout'))\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)", "is_method": false, "function_description": "Initializes and configures default logging for Scrapy applications. It routes warnings and Twisted logs, sets log levels, and can redirect standard output, providing a centralized logging setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "install_scrapy_root_handler", "line_number": 105, "body": "def install_scrapy_root_handler(settings):\n    global _scrapy_root_handler\n\n    if (_scrapy_root_handler is not None\n            and _scrapy_root_handler in logging.root.handlers):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)", "is_method": false, "function_description": "This function installs and configures the main logging handler for the Scrapy application's root logger. It ensures proper log handling based on Scrapy settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "get_scrapy_root_handler", "line_number": 116, "body": "def get_scrapy_root_handler():\n    return _scrapy_root_handler", "is_method": false, "function_description": "Provides access to the main Scrapy root handler object. This enables other parts of the application to interact with Scrapy's core functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "_get_handler", "line_number": 123, "body": "def _get_handler(settings):\n    \"\"\" Return a log handler object according to settings \"\"\"\n    filename = settings.get('LOG_FILE')\n    if filename:\n        encoding = settings.get('LOG_ENCODING')\n        handler = logging.FileHandler(filename, encoding=encoding)\n    elif settings.getbool('LOG_ENABLED'):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get('LOG_FORMAT'),\n        datefmt=settings.get('LOG_DATEFORMAT')\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get('LOG_LEVEL'))\n    if settings.getbool('LOG_SHORT_NAMES'):\n        handler.addFilter(TopLevelFormatter(['scrapy']))\n    return handler", "is_method": false, "function_description": "Configures and returns a logging handler based on application settings. It provides a ready-to-use handler to direct log messages to files, streams, or a null device.\nChain of Thought:\nThe user wants a concise description of the `_get_handler` function.\n\n1.  **Understand the function's purpose:** The docstring clearly states \"Return a log handler object according to settings\". The code confirms this by creating different `logging.Handler` types (`FileHandler`, `StreamHandler`, `NullHandler`) based on input `settings`. It then configures the formatter and level.\n2.  **Identify the core service:** This function's primary service is to *provide a correctly configured log handler*. It abstracts away the details of handler creation and configuration based on various logging preferences.\n3.  **Focus on WHAT, not HOW:** It *provides a log handler*, it doesn't *create a file* or *print to console* directly. It sets up an object that *will* do those things.\n4.  **Consider use cases:** Other parts of a logging system would call this to get the specific handler needed for the current application's logging setup.\n5.  **Drafting - first attempt:** \"This function generates and configures a Python logging handler (file, stream, or null) based on the provided settings. It serves as a utility for setting up logging destinations.\" (33 words, 2 sentences)\n6.  **Refinement for conciseness and impact (less than 40 words, max 2 sentences):**\n    *   \"Generates and configures a logging handler (file, stream, or null) based on provided settings.\" - Good, but can be more active.\n    *   \"Configures and returns a logging handler based on application settings.\" - This is very direct and covers the core action.\n    *   What service does it provide? \"It provides a ready-to-use handler to direct log messages to files, streams, or a null device.\" - This clarifies its utility.\n\n7.  **Final check against requirements:**\n    *   Concise, < 40 words, max 2 sentences? Yes (29 words, 2 sentences).\n    *   Clear, direct language? Yes.\n    *   \"Not Implemented\" / \"Not sure\"? No, it's clear and implemented.\n\nThe description \"Configures and returns a logging handler based on application settings. It provides a ready-to-use handler to direct log messages to files, streams, or a null device.\" fits all criteria.\nConfigures and returns a logging handler based on application settings. It provides a ready-to-use handler to direct log messages to files, streams, or a null device."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "log_scrapy_info", "line_number": 145, "body": "def log_scrapy_info(settings):\n    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n    versions = [\n        f\"{name} {version}\"\n        for name, version in scrapy_components_versions()\n        if name != \"Scrapy\"\n    ]\n    logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n    from twisted.internet import reactor\n    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n    from twisted.internet import asyncioreactor\n    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n        logger.debug(\n            \"Using asyncio event loop: %s.%s\",\n            reactor._asyncioEventloop.__module__,\n            reactor._asyncioEventloop.__class__.__name__,\n        )", "is_method": false, "function_description": "This function logs detailed startup information for a Scrapy project, including Scrapy and component versions, and the active Twisted networking reactor. It helps with debugging and understanding the execution environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "logformatter_adapter", "line_number": 197, "body": "def logformatter_adapter(logkws):\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {'level', 'msg', 'args'} <= set(logkws):\n        warnings.warn('Missing keys in LogFormatter method',\n                      ScrapyDeprecationWarning)\n\n    if 'format' in logkws:\n        warnings.warn('`format` key in LogFormatter methods has been '\n                      'deprecated, use `msg` instead',\n                      ScrapyDeprecationWarning)\n\n    level = logkws.get('level', logging.INFO)\n    message = logkws.get('format', logkws.get('msg'))\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = logkws if not logkws.get('args') else logkws['args']\n\n    return (level, message, args)", "is_method": false, "function_description": "Adapts a dictionary of log keywords into a tuple of positional arguments for `logger.log` calls. It ensures compatibility with standard logging and handles deprecated input formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "filter", "line_number": 39, "body": "def filter(self, record):\n        if any(record.name.startswith(logger + '.') for logger in self.loggers):\n            record.name = record.name.split('.', 1)[0]\n        return True", "is_method": true, "class_name": "TopLevelFormatter", "function_description": "Simplifies log record names for consistent formatting. If a record's name is a child of a configured top-level logger, it's truncated to the parent name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "write", "line_number": 176, "body": "def write(self, buf):\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())", "is_method": true, "class_name": "StreamLogger", "function_description": "This method writes a text buffer by logging each line as a separate message. It effectively redirects stream output to an internal logger at a specified log level."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "flush", "line_number": 180, "body": "def flush(self):\n        for h in self.logger.handlers:\n            h.flush()", "is_method": true, "class_name": "StreamLogger", "function_description": "Flushes all log handlers associated with the logger, ensuring any buffered log records are immediately written to their respective output destinations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "emit", "line_number": 192, "body": "def emit(self, record):\n        sname = f'log_count/{record.levelname}'\n        self.crawler.stats.inc_value(sname)", "is_method": true, "class_name": "LogCounterHandler", "function_description": "Increments a statistical counter based on the log record's level, providing a summary of logging activity for the crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "test_site", "line_number": 29, "body": "def test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)", "is_method": false, "function_description": "This function constructs a test web server site with various static content types and redirect rules. It provides a controlled environment for testing web server functionality or client interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "setUp", "line_number": 8, "body": "def setUp(self):\n        from twisted.internet import reactor\n        super().setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"", "is_method": true, "class_name": "SiteTest", "function_description": "Initializes a local test HTTP server using Twisted's reactor, providing a base URL for subsequent test interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "tearDown", "line_number": 14, "body": "def tearDown(self):\n        super().tearDown()\n        self.site.stopListening()", "is_method": true, "class_name": "SiteTest", "function_description": "Cleans up test resources after each test by stopping the site's network listener, ensuring a clean state for subsequent tests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "url", "line_number": 18, "body": "def url(self, path):\n        return urljoin(self.baseurl, path)", "is_method": true, "class_name": "SiteTest", "function_description": "This method constructs a full URL by joining a relative path with the `SiteTest` object's base URL. It provides a convenient way to generate specific resource URLs for site testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "render", "line_number": 23, "body": "def render(self, request):\n        content = util.Redirect.render(self, request)\n        return content.replace(b'http-equiv=\\\"refresh\\\"',\n            b'http-no-equiv=\\\"do-not-refresh-me\\\"')", "is_method": true, "class_name": "NoMetaRefreshRedirect", "function_description": "Modifies the rendered content of a redirect to disable the HTML meta refresh tag, preventing automatic client-side page refreshes or redirects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "deprecated", "line_number": 9, "body": "def deprecated(use_instead=None):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped\n\n    if callable(use_instead):\n        deco = deco(use_instead)\n        use_instead = None\n    return deco", "is_method": false, "function_description": "This decorator marks functions as deprecated. It issues a warning when a marked function is called, optionally suggesting an alternative to use instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "defers", "line_number": 30, "body": "def defers(func):\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)\n    return wrapped", "is_method": false, "function_description": "This decorator ensures a decorated function always returns a `Deferred` object. It facilitates integrating synchronous functions into asynchronous frameworks by standardizing their return type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "inthread", "line_number": 38, "body": "def inthread(func):\n    \"\"\"Decorator to call a function in a thread and return a deferred with the\n    result\n    \"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return threads.deferToThread(func, *a, **kw)\n    return wrapped", "is_method": false, "function_description": "Decorator that executes a function in a separate thread, returning a deferred object for asynchronous result retrieval. It allows offloading long-running operations to avoid blocking the main execution flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "deco", "line_number": 14, "body": "def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped", "is_method": false, "function_description": "This decorator marks a function as deprecated. It emits a warning when the decorated function is called, optionally suggesting an alternative to use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 33, "body": "def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)", "is_method": false, "function_description": "It wraps an arbitrary callable `func`, ensuring its execution always returns a Twisted Deferred object for consistent asynchronous handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 43, "body": "def wrapped(*a, **kw):\n        return threads.deferToThread(func, *a, **kw)", "is_method": false, "function_description": "Executes the associated function in a separate thread. This prevents the calling thread from blocking, ideal for asynchronous or long-running operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 16, "body": "def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)", "is_method": false, "function_description": "Notifies users that a function is deprecated by issuing a warning, optionally suggesting an alternative, then executes the original function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/job.py", "function": "job_dir", "line_number": 4, "body": "def job_dir(settings):\n    path = settings['JOBDIR']\n    if path and not os.path.exists(path):\n        os.makedirs(path)\n    return path", "is_method": false, "function_description": "Ensures a specified job directory exists, creating it if necessary. This function provides the guaranteed path for job-related file operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ftp.py", "function": "ftp_makedirs_cwd", "line_number": 7, "body": "def ftp_makedirs_cwd(ftp, path, first_call=True):\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)", "is_method": false, "function_description": "Creates a given directory path on an FTP server and changes the FTP connection's current directory to it, creating any necessary parent directories."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ftp.py", "function": "ftp_store_file", "line_number": 21, "body": "def ftp_store_file(\n        *, path, file, host, port,\n        username, password, use_active_mode=False, overwrite=True):\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        command = 'STOR' if overwrite else 'APPE'\n        ftp.storbinary(f'{command} {filename}', file)\n        file.close()", "is_method": false, "function_description": "Provides a service to securely upload a given file to a specified directory on a remote FTP server. It handles connection, authentication, and remote directory creation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "assert_gcs_environ", "line_number": 16, "body": "def assert_gcs_environ():\n    if 'GCS_PROJECT_ID' not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")", "is_method": false, "function_description": "Ensures the 'GCS_PROJECT_ID' environment variable is set. If not found, it raises `SkipTest`, preventing subsequent operations that require Google Cloud Storage configuration from running."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "skip_if_no_boto", "line_number": 21, "body": "def skip_if_no_boto():\n    if not is_botocore_available():\n        raise SkipTest('missing botocore library')", "is_method": false, "function_description": "Conditionally skips the current test if the `botocore` library is not installed. This prevents execution of tests that depend on AWS SDK components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_gcs_content_and_delete", "line_number": 26, "body": "def get_gcs_content_and_delete(bucket, path):\n    from google.cloud import storage\n    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob", "is_method": false, "function_description": "Retrieves content and ACL from a Google Cloud Storage object. It then immediately deletes the object, useful for 'consume-and-delete' workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_ftp_content_and_delete", "line_number": 37, "body": "def get_ftp_content_and_delete(\n        path, host, port, username,\n        password, use_active_mode=False):\n    from ftplib import FTP\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data = []\n\n    def buffer_data(data):\n        ftp_data.append(data)\n    ftp.retrbinary(f'RETR {path}', buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return \"\".join(ftp_data)", "is_method": false, "function_description": "Fetches a file's content from an FTP server and immediately deletes the original remote file. This function enables one-time processing and cleanup of data received via FTP."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_crawler", "line_number": 57, "body": "def get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(settings_dict)\n    return runner.create_crawler(spidercls or Spider)", "is_method": false, "function_description": "Provides an unconfigured Scrapy `Crawler` object. It allows specifying a custom spider class and settings, useful for embedding and running Scrapy within applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_pythonpath", "line_number": 69, "body": "def get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')", "is_method": false, "function_description": "Returns a PYTHONPATH string configured to ensure external processes can locate the current Scrapy installation. This facilitates running Scrapy-dependent scripts or commands in subprocesses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_testenv", "line_number": 76, "body": "def get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env", "is_method": false, "function_description": "Provides an OS environment dictionary configured for forked processes to correctly import the current Scrapy installation, facilitating testing and development."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "assert_samelines", "line_number": 85, "body": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)", "is_method": false, "function_description": "This function asserts that two texts have identical lines, regardless of line ending variations. It provides a platform-agnostic way to compare multi-line strings in test cases."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_from_asyncio_queue", "line_number": 92, "body": "def get_from_asyncio_queue(value):\n    q = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter", "is_method": false, "function_description": "Provides an awaitable object that, when awaited, asynchronously yields the supplied value. It utilizes an `asyncio.Queue` to wrap the value for future retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "mock_google_cloud_storage", "line_number": 99, "body": "def mock_google_cloud_storage():\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Client, Bucket, Blob\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)", "is_method": false, "function_description": "Provides pre-configured autospec mock objects for Google Cloud Storage Client, Bucket, and Blob classes. This facilitates isolated unit testing of GCS-dependent code."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "buffer_data", "line_number": 48, "body": "def buffer_data(data):\n        ftp_data.append(data)", "is_method": false, "function_description": "This function adds given data to an internal buffer. It accumulates data chunks for later processing or transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "listen_tcp", "line_number": 9, "body": "def listen_tcp(portrange, host, factory):\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    from twisted.internet import reactor\n    if len(portrange) > 2:\n        raise ValueError(f\"invalid portrange: {portrange}\")\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if not hasattr(portrange, '__iter__'):\n        return reactor.listenTCP(portrange, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1] + 1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise", "is_method": false, "function_description": "Establishes a TCP listening server. It attempts to bind to an available port within a specified range, providing a robust connection setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "install_reactor", "line_number": 53, "body": "def install_reactor(reactor_path, event_loop_path=None):\n    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n    import path. Also installs the asyncio event loop with the specified import\n    path if the asyncio reactor is enabled\"\"\"\n    reactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        with suppress(error.ReactorAlreadyInstalledError):\n            if event_loop_path is not None:\n                event_loop_class = load_object(event_loop_path)\n                event_loop = event_loop_class()\n                asyncio.set_event_loop(event_loop)\n            else:\n                event_loop = asyncio.get_event_loop()\n            asyncioreactor.install(eventloop=event_loop)\n    else:\n        *module, _ = reactor_path.split(\".\")\n        installer_path = module + [\"install\"]\n        installer = load_object(\".\".join(installer_path))\n        with suppress(error.ReactorAlreadyInstalledError):\n            installer()", "is_method": false, "function_description": "Configures and installs the Twisted event loop (reactor) for an application. It supports integrating with and setting up an asyncio event loop when using an asyncio-compatible reactor."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "verify_installed_reactor", "line_number": 75, "body": "def verify_installed_reactor(reactor_path):\n    \"\"\"Raises :exc:`Exception` if the installed\n    :mod:`~twisted.internet.reactor` does not match the specified import\n    path.\"\"\"\n    from twisted.internet import reactor\n    reactor_class = load_object(reactor_path)\n    if not isinstance(reactor, reactor_class):\n        msg = (\"The installed reactor \"\n               f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n               f\"match the requested one ({reactor_path})\")\n        raise Exception(msg)", "is_method": false, "function_description": "Checks if the installed Twisted reactor matches a specified import path. Raises an exception if the active reactor does not correspond to the expected type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "is_asyncio_reactor_installed", "line_number": 88, "body": "def is_asyncio_reactor_installed():\n    from twisted.internet import reactor\n    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)", "is_method": false, "function_description": "Checks if the Twisted event loop is configured to use the asyncio backend. Useful for verifying environment setup or conditional execution logic."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "schedule", "line_number": 39, "body": "def schedule(self, delay=0):\n        from twisted.internet import reactor\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)", "is_method": true, "class_name": "CallLaterOnce", "function_description": "This method schedules the `CallLaterOnce` instance to be called by the Twisted reactor after a delay. It ensures the action is scheduled only once."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "cancel", "line_number": 44, "body": "def cancel(self):\n        if self._call:\n            self._call.cancel()", "is_method": true, "class_name": "CallLaterOnce", "function_description": "Cancels a pending one-time call that was previously scheduled by the `CallLaterOnce` instance, preventing its execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "__call__", "line_number": 48, "body": "def __call__(self):\n        self._call = None\n        return self._func(*self._a, **self._kw)", "is_method": true, "class_name": "CallLaterOnce", "function_description": "Executes the wrapped function with its stored arguments. It also ensures the function runs only once by resetting its internal callable state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "attribute", "line_number": 8, "body": "def attribute(obj, oldattr, newattr, version='0.12'):\n    cname = obj.__class__.__name__\n    warnings.warn(\n        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n        ScrapyDeprecationWarning,\n        stacklevel=3)", "is_method": false, "function_description": "This function issues a deprecation warning for an object's old attribute, advising users to switch to a new attribute. It serves to inform developers about API changes and deprecated features."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "create_deprecated_class", "line_number": 17, "body": "def create_deprecated_class(\n    name,\n    new_class,\n    clsdict=None,\n    warn_category=ScrapyDeprecationWarning,\n    warn_once=True,\n    old_class_path=None,\n    new_class_path=None,\n    subclass_warn_message=\"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\"\n):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super().__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(f\"Error detecting parent module: {e!r}\")\n\n    return deprecated_cls", "is_method": false, "function_description": "This function creates a proxy class that acts as a deprecated version of a new class. It warns users when inherited from or instantiated, while maintaining `issubclass` and `isinstance` compatibility with the new class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "_clspath", "line_number": 123, "body": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return f'{cls.__module__}.{cls.__name__}'", "is_method": false, "function_description": "Returns the fully qualified name of a class (e.g., `module.ClassName`). This function allows an optional override to force a specific string representation instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "update_classpath", "line_number": 134, "body": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if isinstance(path, str) and path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path", "is_method": false, "function_description": "Updates a deprecated class path string to its new, correct location and issues a deprecation warning. This function helps migrate code using old module or class references."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "method_is_overridden", "line_number": 145, "body": "def method_is_overridden(subclass, base_class, method_name):\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base:\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__", "is_method": false, "function_description": "Determines if a method defined in a base class has been overridden in a given subclass. It provides a utility for runtime introspection of class hierarchies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__new__", "line_number": 58, "body": "def __new__(metacls, name, bases, clsdict_):\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls", "is_method": true, "class_name": "DeprecatedClass", "function_description": "This metaclass method creates a new class and, if not already set, registers it as the `deprecated_class`. It serves to identify and store the specific deprecated class for later reference."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__subclasscheck__", "line_number": 84, "body": "def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)", "is_method": true, "class_name": "DeprecatedClass", "function_description": "Modifies `issubclass()` behavior for a deprecated class. It ensures a class is considered a subclass if it inherits from either the deprecated class or its new replacement, aiding API migration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__call__", "line_number": 98, "body": "def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)", "is_method": true, "class_name": "DeprecatedClass", "function_description": "It issues a warning when a deprecated class is instantiated. This method informs users about the correct replacement class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "inside_project", "line_number": 16, "body": "def inside_project():\n    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n    if scrapy_module is not None:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(f\"Cannot import scrapy settings module {scrapy_module}: {exc}\")\n        else:\n            return True\n    return bool(closest_scrapy_cfg())", "is_method": false, "function_description": "This function checks if the current execution environment is within a Scrapy project. It determines this by looking for a Scrapy settings module or a `scrapy.cfg` file."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "project_data_dir", "line_number": 28, "body": "def project_data_dir(project='default'):\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = cfg.get(DATADIR_CFG_SECTION, project)\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n    if not exists(d):\n        os.makedirs(d)\n    return d", "is_method": false, "function_description": "This function finds or creates the designated data directory for the current project. It ensures a consistent, ready-to-use storage location for project-specific files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "data_path", "line_number": 45, "body": "def data_path(path, createdir=False):\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    if not isabs(path):\n        if inside_project():\n            path = join(project_data_dir(), path)\n        else:\n            path = join('.scrapy', path)\n    if createdir and not exists(path):\n        os.makedirs(path)\n    return path", "is_method": false, "function_description": "This function provides a standardized path within Scrapy's data directory, either project-specific or a default `.scrapy` folder. It can optionally create the necessary directory structure for storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "get_project_settings", "line_number": 60, "body": "def get_project_settings():\n    if ENVVAR not in os.environ:\n        project = os.environ.get('SCRAPY_PROJECT', 'default')\n        init_env(project)\n\n    settings = Settings()\n    settings_module_path = os.environ.get(ENVVAR)\n    if settings_module_path:\n        settings.setmodule(settings_module_path, priority='project')\n\n    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n                      k.startswith('SCRAPY_')}\n    valid_envvars = {\n        'CHECK',\n        'PROJECT',\n        'PYTHON_SHELL',\n        'SETTINGS_MODULE',\n    }\n    setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}\n    if setting_envvars:\n        setting_envvar_list = ', '.join(sorted(setting_envvars))\n        warnings.warn(\n            'Use of environment variables prefixed with SCRAPY_ to override '\n            'settings is deprecated. The following environment variables are '\n            f'currently defined: {setting_envvar_list}',\n            ScrapyDeprecationWarning\n        )\n    settings.setdict(scrapy_envvars, priority='project')\n\n    return settings", "is_method": false, "function_description": "Retrieves and configures Scrapy project settings. It initializes the environment and consolidates settings from default, module-defined, and environment variable sources into a comprehensive Settings object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/sitemap.py", "function": "sitemap_urls_from_robots", "line_number": 40, "body": "def sitemap_urls_from_robots(robots_text, base_url=None):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith('sitemap:'):\n            url = line.split(':', 1)[1].strip()\n            yield urljoin(base_url, url)", "is_method": false, "function_description": "Parses robots.txt content to yield all declared sitemap URLs, optionally resolving them against a base URL. This helps discover sitemaps for web crawling or SEO analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/sitemap.py", "function": "__iter__", "line_number": 23, "body": "def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d", "is_method": true, "class_name": "Sitemap", "function_description": "Enables iteration over the Sitemap, yielding dictionaries representing each URL entry. It parses location, modification data, and alternate links from the XML."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "ffi_buf_to_string", "line_number": 12, "body": "def ffi_buf_to_string(buf):\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))", "is_method": false, "function_description": "Converts a CFFI buffer object from a C library into a Python Unicode string. This facilitates safe interoperability between C data and Python string operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "x509name_to_string", "line_number": 16, "body": "def x509name_to_string(x509name):\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n\n    return ffi_buf_to_string(result_buffer)", "is_method": false, "function_description": "Converts an X.509 name object into a concise, single-line string. This provides a standardized human-readable representation for distinguished names from certificates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "get_temp_key_info", "line_number": 24, "body": "def get_temp_key_info(ssl_object):\n    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n        return None\n\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append('RSA')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append('DH')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append('ECDH')\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')\n    return ', '.join(key_info)", "is_method": false, "function_description": "Retrieves information about the temporary server-side cryptographic key used in an SSL/TLS session, including its type, curve (for EC), and bit length. This aids in understanding forward secrecy properties."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "get_openssl_version", "line_number": 57, "body": "def get_openssl_version():\n    system_openssl = OpenSSL.SSL.SSLeay_version(\n        OpenSSL.SSL.SSLEAY_VERSION\n    ).decode('ascii', errors='replace')\n    return f'{OpenSSL.version.__version__} ({system_openssl})'", "is_method": false, "function_description": "This function retrieves and formats the version information for both the pyOpenSSL library and the underlying OpenSSL system library. It provides combined version details useful for debugging or compatibility checks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "arg_to_iter", "line_number": 24, "body": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]", "is_method": false, "function_description": "This utility function converts various input types into a consistent iterable format. It handles `None` by returning an empty list and wraps single values into a list, ensuring uniform processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "load_object", "line_number": 38, "body": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        else:\n            raise TypeError(\"Unexpected argument type, expected string \"\n                            \"or object, got: %s\" % type(path))\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj", "is_method": false, "function_description": "Loads a Python object dynamically from its absolute string path, such as a class or function. It also returns callable objects directly, offering a flexible way to resolve runtime references."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "walk_modules", "line_number": 71, "body": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods", "is_method": false, "function_description": "This function recursively loads a specified Python module and all its contained submodules and packages, returning a list of all loaded module objects. It enables dynamic introspection and discovery of a module's entire structure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "extract_regex", "line_number": 93, "body": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n    warnings.warn(\n        \"scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2\n    )\n\n    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except Exception:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, str):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]", "is_method": false, "function_description": "This function extracts specific string content from text using a regular expression. It returns data based on named groups, numbered groups, or the full match, converting results to unicode and handling entities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "md5sum", "line_number": 122, "body": "def md5sum(file):\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()", "is_method": false, "function_description": "This function computes the MD5 checksum for a file-like object. It enables efficient content integrity verification or uniqueness checks without loading the entire file into memory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "rel_has_nofollow", "line_number": 139, "body": "def rel_has_nofollow(rel):\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and 'nofollow' in rel.split()", "is_method": false, "function_description": "Determines if a given HTML `rel` attribute string contains the 'nofollow' keyword. Useful for web crawling, SEO analysis, or link processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "create_instance", "line_number": 144, "body": "def create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, 'from_crawler'):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = 'from_crawler'\n    elif hasattr(objcls, 'from_settings'):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = 'from_settings'\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = '__new__'\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance", "is_method": false, "function_description": "This function creates an instance of a class, prioritizing `from_crawler` or `from_settings` constructors. It provides a robust way to instantiate framework components using available settings or crawler objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "set_environ", "line_number": 180, "body": "def set_environ(**kwargs):\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v", "is_method": false, "function_description": "This context manager temporarily sets specified environment variables. It ensures the original environment is fully restored upon exiting, preventing unintended global changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "walk_callable", "line_number": 197, "body": "def walk_callable(node):\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node", "is_method": false, "function_description": "Provides a specialized Abstract Syntax Tree (AST) traversal for a function's body. It walks all nodes within a function's main body, but explicitly skips any nested function definitions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "is_generator_with_return_value", "line_number": 216, "body": "def is_generator_with_return_value(callable):\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return _generator_callbacks_cache[callable]\n\n    def returns_none(return_node):\n        value = return_node.value\n        return value is None or isinstance(value, ast.NameConstant) and value.value is None\n\n    if inspect.isgeneratorfunction(callable):\n        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return _generator_callbacks_cache[callable]\n\n    _generator_callbacks_cache[callable] = False\n    return _generator_callbacks_cache[callable]", "is_method": false, "function_description": "Determines if a given callable is a generator function that explicitly returns a non-None value. This helps identify generators providing a final result beyond yielded values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "warn_on_generator_with_return_value", "line_number": 240, "body": "def warn_on_generator_with_return_value(spider, callable):\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                'different than None. This could lead to unexpected behaviour. Please see '\n                'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            'This will not prevent your code from working, but it prevents Scrapy from detecting '\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            'Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), '\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )", "is_method": false, "function_description": "Logs a warning when a generator function includes a return statement with a non-None value. This helps prevent unexpected behavior by identifying common generator pitfalls."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "returns_none", "line_number": 224, "body": "def returns_none(return_node):\n        value = return_node.value\n        return value is None or isinstance(value, ast.NameConstant) and value.value is None", "is_method": false, "function_description": "Determines if an Abstract Syntax Tree (AST) return node represents a `None` value. This aids static analysis tools in identifying functions that implicitly or explicitly return `None`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "read1", "line_number": 13, "body": "def read1(gzf, size=-1):\n    return gzf.read1(size)", "is_method": false, "function_description": "Reads bytes from a file-like object using its `read1` method. This allows efficient, non-blocking data retrieval, useful for buffered or compressed streams."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "gunzip", "line_number": 17, "body": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_list = []\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = f.read1(8196)\n            output_list.append(chunk)\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_list is empty and f.extrabuf\n            # contains the whole page content\n            if output_list or getattr(f, 'extrabuf', None):\n                try:\n                    output_list.append(f.extrabuf[-f.extrasize:])\n                finally:\n                    break\n            else:\n                raise\n    return b''.join(output_list)", "is_method": false, "function_description": "This function decompresses Gzip-compressed data, returning as much uncompressed content as possible. It robustly handles common errors, such as CRC checksum issues, to maximize data retrieval from potentially corrupt archives."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "gzip_magic_number", "line_number": 44, "body": "def gzip_magic_number(response):\n    return response.body[:3] == b'\\x1f\\x8b\\x08'", "is_method": false, "function_description": "Checks if the initial bytes of a response body match the GZIP magic number, indicating potential GZIP compression."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/serialize.py", "function": "default", "line_number": 16, "body": "def default(self, o):\n        if isinstance(o, set):\n            return list(o)\n        elif isinstance(o, datetime.datetime):\n            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif is_item(o):\n            return ItemAdapter(o).asdict()\n        elif isinstance(o, Request):\n            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n        elif isinstance(o, Response):\n            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n        else:\n            return super().default(o)", "is_method": true, "class_name": "ScrapyJSONEncoder", "function_description": "Extends JSON serialization by converting Scrapy-specific objects (like Items, Requests, Responses) and common types (sets, datetimes, decimals) into JSON-compatible formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "request_to_dict", "line_number": 11, "body": "def request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n        'flags': request.flags,\n        'cb_kwargs': request.cb_kwargs,\n    }\n    if type(request) is not Request:\n        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n    return d", "is_method": false, "function_description": "Converts a web request object into a dictionary containing all its key attributes. It transforms callable callbacks into string names for serialization or portable data representation, useful for logging or queueing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "request_from_dict", "line_number": 43, "body": "def request_from_dict(d, spider=None):\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    cb = d['callback']\n    if cb and spider:\n        cb = _get_method(spider, cb)\n    eb = d['errback']\n    if eb and spider:\n        eb = _get_method(spider, eb)\n    request_cls = load_object(d['_class']) if '_class' in d else Request\n    return request_cls(\n        url=to_unicode(d['url']),\n        callback=cb,\n        errback=eb,\n        method=d['method'],\n        headers=d['headers'],\n        body=d['body'],\n        cookies=d['cookies'],\n        meta=d['meta'],\n        encoding=d['_encoding'],\n        priority=d['priority'],\n        dont_filter=d['dont_filter'],\n        flags=d.get('flags'),\n        cb_kwargs=d.get('cb_kwargs'),\n    )", "is_method": false, "function_description": "This function constructs a web Request object from a dictionary, allowing dynamic creation of requests. It can resolve callback and errback methods by name if a spider object is provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "_find_method", "line_number": 73, "body": "def _find_method(obj, func):\n    # Only instance methods contain ``__func__``\n    if obj and hasattr(func, '__func__'):\n        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n        for name, obj_func in members:\n            # We need to use __func__ to access the original\n            # function object because instance method objects\n            # are generated each time attribute is retrieved from\n            # instance.\n            #\n            # Reference: The standard type hierarchy\n            # https://docs.python.org/3/reference/datamodel.html\n            if obj_func.__func__ is func.__func__:\n                return name\n    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")", "is_method": false, "function_description": "This helper function identifies and returns the name of an instance method on an object, given a reference to its underlying function object. It is useful for introspection or dynamic lookups."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "_get_method", "line_number": 90, "body": "def _get_method(obj, name):\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(f\"Method {name!r} not found in: {obj}\")", "is_method": false, "function_description": "This function retrieves a method by its string name from a given object. It provides a robust way to access methods dynamically, raising a specific error if the method is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/boto.py", "function": "is_botocore", "line_number": 7, "body": "def is_botocore():\n    \"\"\" Returns True if botocore is available, otherwise raises NotConfigured. Never returns False.\n\n    Previously, when boto was supported in addition to botocore, this returned False if boto was available\n    but botocore wasn't.\n    \"\"\"\n    message = (\n        'is_botocore() is deprecated and always returns True or raises an Exception, '\n        'so it cannot be used for checking if boto is available instead of botocore. '\n        'You can use scrapy.utils.boto.is_botocore_available() to check if botocore '\n        'is available.'\n    )\n    warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n    try:\n        import botocore  # noqa: F401\n        return True\n    except ImportError:\n        raise NotConfigured('missing botocore library')", "is_method": false, "function_description": "Verifies the availability of the `botocore` library, returning True if present or raising `NotConfigured` if missing. This function is deprecated."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/boto.py", "function": "is_botocore_available", "line_number": 27, "body": "def is_botocore_available():\n    try:\n        import botocore  # noqa: F401\n        return True\n    except ImportError:\n        return False", "is_method": false, "function_description": "This function checks for the presence of the `botocore` library, returning True if it can be imported, and False otherwise. It indicates whether AWS-related functionality is available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/versions.py", "function": "scrapy_components_versions", "line_number": 15, "body": "def scrapy_components_versions():\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\", platform.platform()),\n    ]", "is_method": false, "function_description": "Gathers and returns version information for Scrapy, its core dependencies, Python, and the platform. This provides a snapshot of the execution environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "flatten", "line_number": 18, "body": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))", "is_method": false, "function_description": "This function transforms a nested sequence into a single, flat list. It recursively extracts all elements from sub-sequences, providing a linearized representation of structured data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "iflatten", "line_number": 38, "body": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in iflatten(el):\n                yield el_\n        else:\n            yield el", "is_method": false, "function_description": "This function recursively flattens a nested iterable, yielding each element to provide an iterator for efficient traversal. It allows processing all items from a deeply structured collection sequentially."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "is_listlike", "line_number": 50, "body": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))", "is_method": false, "function_description": "Determines if an object is an iterable collection, excluding string and byte types. This utility helps validate function inputs that require sequence-like processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "unique", "line_number": 74, "body": "def unique(list_, key=lambda x: x):\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result", "is_method": false, "function_description": "This function efficiently removes duplicate items from a list while preserving their original order. It provides a unique sequence of elements, optionally based on a custom key for comparison."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_unicode", "line_number": 87, "body": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError('to_unicode must receive a bytes or str '\n                        f'object, got {type(text).__name__}')\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)", "is_method": false, "function_description": "Provides a utility to ensure any input text is a Unicode string. It decodes byte strings while returning already-unicode strings unchanged, preventing type errors in text processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_bytes", "line_number": 100, "body": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError('to_bytes must receive a str or bytes '\n                        f'object, got {type(text).__name__}')\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)", "is_method": false, "function_description": "Converts a string to its byte representation using a specified encoding, or returns the input directly if it's already bytes. This utility ensures data is in binary format for various operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_native_str", "line_number": 114, "body": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of ``text``. \"\"\"\n    return to_unicode(text, encoding, errors)", "is_method": false, "function_description": "Converts any given text input into the standard native string format. It ensures consistent string representation for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "re_rsearch", "line_number": 119, "body": "def re_rsearch(pattern, text, chunk_size=1024):\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = [match for match in pattern.finditer(chunk)]\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None", "is_method": false, "function_description": "This function performs an efficient reverse search for a regular expression pattern within a text. It finds the last occurrence of the pattern by searching in chunks from the end, returning its start and end positions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "memoizemethod_noargs", "line_number": 153, "body": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method", "is_method": false, "function_description": "Decorator to cache the result of an instance method that takes no additional arguments. It ensures the method computes its value only once per object for efficiency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "binary_is_text", "line_number": 172, "body": "def binary_is_text(data):\n    \"\"\" Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)", "is_method": false, "function_description": "Checks if a given byte sequence appears to contain only printable text characters, effectively distinguishing text from binary data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "_getargspec_py23", "line_number": 181, "body": "def _getargspec_py23(func):\n    \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n                                                        defaults)\n\n    Was identical to inspect.getargspec() in python2, but uses\n    inspect.getfullargspec() for python3 behind the scenes to avoid\n    DeprecationWarning.\n\n    >>> def f(a, b=2, *ar, **kw):\n    ...     pass\n\n    >>> _getargspec_py23(f)\n    ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n    \"\"\"\n    return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])", "is_method": false, "function_description": "Provides a consistent way to retrieve the argument specification of a Python function, ensuring compatibility across Python 2 and Python 3 environments. It returns a standard `ArgSpec` tuple."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "get_func_args", "line_number": 198, "body": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        spec = inspect.getfullargspec(func)\n        func_args = spec.args + spec.kwonlyargs\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError(f'{type(func)} is not callable')\n    if stripself:\n        func_args.pop(0)\n    return func_args", "is_method": false, "function_description": "Retrieves the argument names for any given Python callable, including functions, methods, classes, and partials. It provides a robust way to inspect a callable's signature for dynamic argument handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "get_spec", "line_number": 226, "body": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = _getargspec_py23(func)\n    elif hasattr(func, '__call__'):\n        spec = _getargspec_py23(func.__call__)\n    else:\n        raise TypeError(f'{type(func)} is not callable')\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs", "is_method": false, "function_description": "Retrieves a callable's expected positional arguments and keyword arguments with their default values. This function enables dynamic introspection and argument validation for any callable."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "equal_attributes", "line_number": 263, "body": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True", "is_method": false, "function_description": "Compares two objects by checking if a specified set of their attributes or callable-extracted values are equal. It provides a flexible way to define attribute-based equivalence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "retry_on_eintr", "line_number": 295, "body": "def retry_on_eintr(function, *args, **kw):\n    \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n    while True:\n        try:\n            return function(*args, **kw)\n        except IOError as e:\n            if e.errno != errno.EINTR:\n                raise", "is_method": false, "function_description": "This function executes a given function, automatically retrying its execution if it fails due to an `EINTR` (interrupted system call) `IOError`. It ensures reliable operation completion despite signal interruptions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "without_none_values", "line_number": 305, "body": "def without_none_values(iterable):\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    try:\n        return {k: v for k, v in iterable.items() if v is not None}\n    except AttributeError:\n        return type(iterable)((v for v in iterable if v is not None))", "is_method": false, "function_description": "This function provides a clean copy of an iterable or mapping by removing all entries that have `None` as their value. It is useful for data cleansing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "global_object_name", "line_number": 317, "body": "def global_object_name(obj):\n    \"\"\"\n    Return full name of a global object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__name__}\"", "is_method": false, "function_description": "Provides the full, globally qualified name of a Python object by combining its module path and its own name. This is useful for unambiguous identification or dynamic lookup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "_chunk_iter", "line_number": 133, "body": "def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)", "is_method": false, "function_description": "This generator yields progressively larger suffix chunks of a string, starting from its end and moving backwards to the beginning, along with their offsets."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "new_method", "line_number": 160, "body": "def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]", "is_method": false, "function_description": "This method caches the return value of an instance-specific computation. It ensures the underlying operation runs only once per object, improving efficiency for instance-dependent results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__getitem__", "line_number": 288, "body": "def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]", "is_method": true, "class_name": "WeakKeyCache", "function_description": "This method retrieves an item from the cache using dictionary-like access. If the key is not present, it automatically creates and stores the value via a default factory before returning it."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "garbage_collect", "line_number": 329, "body": "def garbage_collect():\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()", "is_method": false, "function_description": "This function explicitly triggers the Python garbage collector. It performs a double collection to ensure thorough cleanup of unreferenced objects and reclaim memory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "garbage_collect", "line_number": 334, "body": "def garbage_collect():\n        gc.collect()", "is_method": false, "function_description": "Forces the Python garbage collector to immediately clean up unreferenced objects, thereby reclaiming and freeing up memory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "extend", "line_number": 346, "body": "def extend(self, *iterables):\n        self.data = chain(self.data, chain.from_iterable(iterables))", "is_method": true, "class_name": "MutableChain", "function_description": "Extends the `MutableChain` instance by appending elements from one or more iterables. It lazily appends new data to the existing sequence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__iter__", "line_number": 349, "body": "def __iter__(self):\n        return self", "is_method": true, "class_name": "MutableChain", "function_description": "Enables direct iteration over the MutableChain's contents, allowing the object to be used in for-loops and other iteration contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__next__", "line_number": 352, "body": "def __next__(self):\n        return next(self.data)", "is_method": true, "class_name": "MutableChain", "function_description": "Allows the `MutableChain` to function as an iterator, providing the next item from its contained iterable data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "next", "line_number": 356, "body": "def next(self):\n        return self.__next__()", "is_method": true, "class_name": "MutableChain", "function_description": "Advances the chain's iterator to retrieve the next element in the sequence. This method provides the standard iteration capability for the chain."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "execute", "line_number": 13, "body": "def execute(self, args, check_code=True, settings=None):\n        from twisted.internet import reactor\n        env = os.environ.copy()\n        if settings is not None:\n            env['SCRAPY_SETTINGS_MODULE'] = settings\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred", "is_method": true, "class_name": "ProcessTest", "function_description": "Executes an external command as an asynchronous process using Twisted's reactor. It manages the environment and returns a deferred object, suitable for running Scrapy test commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "_process_finished", "line_number": 24, "body": "def _process_finished(self, pp, cmd, check_code):\n        if pp.exitcode and check_code:\n            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n            msg += f\"\\n>>> stdout <<<\\n{pp.out}\"\n            msg += \"\\n\"\n            msg += f\"\\n>>> stderr <<<\\n{pp.err}\"\n            raise RuntimeError(msg)\n        return pp.exitcode, pp.out, pp.err", "is_method": true, "class_name": "ProcessTest", "function_description": "Manages a completed subprocess, raising a RuntimeError if it exited with an error and checking is enabled. It returns the process's exit code, stdout, and stderr for further analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "outReceived", "line_number": 42, "body": "def outReceived(self, data):\n        self.out += data", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "This method of TestProcessProtocol accumulates received output data into an internal buffer. It serves to capture and store data streams, typically from a subprocess, for later analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "errReceived", "line_number": 45, "body": "def errReceived(self, data):\n        self.err += data", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "Accumulates incoming error data, typically from a subprocess's standard error, for later analysis within a testing protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "processEnded", "line_number": 48, "body": "def processEnded(self, status):\n        self.exitcode = status.value.exitCode\n        self.deferred.callback(self)", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "Registers the exit code of a completed process. It then signals the process's termination to any waiting asynchronous operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "sanitize_module_name", "line_number": 14, "body": "def sanitize_module_name(module_name):\n    \"\"\"Sanitize the given module name, by replacing dashes and points\n    with underscores and prefixing it with a letter if it doesn't start\n    with one\n    \"\"\"\n    module_name = module_name.replace('-', '_').replace('.', '_')\n    if module_name[0] not in string.ascii_letters:\n        module_name = \"a\" + module_name\n    return module_name", "is_method": false, "function_description": "This function transforms a given string into a valid Python module name format. It replaces special characters and ensures the name starts with a letter, preparing it for use in imports."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "syntax", "line_number": 30, "body": "def syntax(self):\n        return \"[options] <name> <domain>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the expected command-line syntax string for this Command, defining its required format for invocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "short_desc", "line_number": 33, "body": "def short_desc(self):\n        return \"Generate new spider using pre-defined templates\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief summary description of the command's functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "add_options", "line_number": 36, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                          help=\"List available templates\")\n        parser.add_option(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n                          help=\"Edit spider after creating it\")\n        parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n                          help=\"Dump template to standard output\")\n        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n                          help=\"Uses a custom template.\")\n        parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n                          help=\"If the spider already exists, overwrite it with the template\")", "is_method": true, "class_name": "Command", "function_description": "This method extends a command-line parser by registering specific options for spider creation and template management. It allows users to control how new spiders are generated and interact with available templates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "run", "line_number": 49, "body": "def run(self, args, opts):\n        if opts.list:\n            self._list_templates()\n            return\n        if opts.dump:\n            template_file = self._find_template(opts.dump)\n            if template_file:\n                with open(template_file, \"r\") as f:\n                    print(f.read())\n            return\n        if len(args) != 2:\n            raise UsageError()\n\n        name, domain = args[0:2]\n        module = sanitize_module_name(name)\n\n        if self.settings.get('BOT_NAME') == module:\n            print(\"Cannot create a spider with the same name as your project\")\n            return\n\n        if not opts.force and self._spider_exists(name):\n            return\n\n        template_file = self._find_template(opts.template)\n        if template_file:\n            self._genspider(module, name, domain, opts.template, template_file)\n            if opts.edit:\n                self.exitcode = os.system(f'scrapy edit \"{name}\"')", "is_method": true, "class_name": "Command", "function_description": "Provides the core functionality for a command-line tool to manage web scraping spiders. It can list or dump templates, or generate a new spider from a template, optionally opening it for editing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_genspider", "line_number": 78, "body": "def _genspider(self, module, name, domain, template_name, template_file):\n        \"\"\"Generate the spider module, based on the given template\"\"\"\n        capitalized_module = ''.join(s.capitalize() for s in module.split('_'))\n        tvars = {\n            'project_name': self.settings.get('BOT_NAME'),\n            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),\n            'module': module,\n            'name': name,\n            'domain': domain,\n            'classname': f'{capitalized_module}Spider'\n        }\n        if self.settings.get('NEWSPIDER_MODULE'):\n            spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n            spiders_dir = abspath(dirname(spiders_module.__file__))\n        else:\n            spiders_module = None\n            spiders_dir = \".\"\n        spider_file = f\"{join(spiders_dir, module)}.py\"\n        shutil.copyfile(template_file, spider_file)\n        render_templatefile(spider_file, **tvars)\n        print(f\"Created spider {name!r} using template {template_name!r} \",\n              end=('' if spiders_module else '\\n'))\n        if spiders_module:\n            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")", "is_method": true, "class_name": "Command", "function_description": "This method generates a new Scrapy spider module from a template file. It populates the new module with project details, spider name, and domain, providing a ready-to-use spider scaffold."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_find_template", "line_number": 103, "body": "def _find_template(self, template):\n        template_file = join(self.templates_dir, f'{template}.tmpl')\n        if exists(template_file):\n            return template_file\n        print(f\"Unable to find template: {template}\\n\")\n        print('Use \"scrapy genspider --list\" to see all available templates.')", "is_method": true, "class_name": "Command", "function_description": "Locates a specified template file within the command's template directory. It returns the full path if found, otherwise it indicates the template is missing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_list_templates", "line_number": 110, "body": "def _list_templates(self):\n        print(\"Available templates:\")\n        for filename in sorted(os.listdir(self.templates_dir)):\n            if filename.endswith('.tmpl'):\n                print(f\"  {splitext(filename)[0]}\")", "is_method": true, "class_name": "Command", "function_description": "Lists all available template names found in the command's templates directory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_spider_exists", "line_number": 116, "body": "def _spider_exists(self, name):\n        if not self.settings.get('NEWSPIDER_MODULE'):\n            # if run as a standalone command and file with same filename already exists\n            if exists(name + \".py\"):\n                print(f\"{abspath(name + '.py')} already exists\")\n                return True\n            return False\n\n        try:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n        else:\n            # if spider with same name exists\n            print(f\"Spider {name!r} already exists in module:\")\n            print(f\"  {spidercls.__module__}\")\n            return True\n\n        # a file with the same name exists in the target directory\n        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n        spiders_dir = dirname(spiders_module.__file__)\n        spiders_dir_abs = abspath(spiders_dir)\n        if exists(join(spiders_dir_abs, name + \".py\")):\n            print(f\"{join(spiders_dir_abs, (name + '.py'))} already exists\")\n            return True\n\n        return False", "is_method": true, "class_name": "Command", "function_description": "Checks if a spider or a spider file with the given name already exists within the project context, preventing naming conflicts during new spider creation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "templates_dir", "line_number": 145, "body": "def templates_dir(self):\n        return join(\n            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n            'spiders'\n        )", "is_method": true, "class_name": "Command", "function_description": "Provides the absolute path to the directory containing Scrapy spider templates. It defaults to the Scrapy installation's templates if a custom directory is not defined."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "syntax", "line_number": 9, "body": "def syntax(self):\n        return \"[options] <spider>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the standard command-line syntax string for this command, typically used for displaying help messages or usage instructions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "short_desc", "line_number": 12, "body": "def short_desc(self):\n        return \"Run a spider\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a concise, predefined summary of the command's primary action, typically for display in help messages or command listings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "run", "line_number": 15, "body": "def run(self, args, opts):\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\"running 'scrapy crawl' with more than one spider is not supported\")\n        spname = args[0]\n\n        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n        if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):\n            self.exitcode = 1\n        else:\n            self.crawler_process.start()\n\n            if (\n                self.crawler_process.bootstrap_failed\n                or hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception\n            ):\n                self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Orchestrates and executes a web crawling operation for a single specified Scrapy spider. It validates input, initiates the crawl process, and sets an exit code based on its success or failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/list.py", "function": "short_desc", "line_number": 9, "body": "def short_desc(self):\n        return \"List available spiders\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise description of the command's purpose, indicating that it lists available spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/list.py", "function": "run", "line_number": 12, "body": "def run(self, args, opts):\n        for s in sorted(self.crawler_process.spider_loader.list()):\n            print(s)", "is_method": true, "class_name": "Command", "function_description": "This method lists all available spiders managed by the crawler process. It provides a quick overview of registered web crawling agents."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "short_desc", "line_number": 7, "body": "def short_desc(self):\n        return \"Open URL in browser, as seen by Scrapy\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a concise, human-readable summary of the command's primary function. It describes the action performed by the Scrapy command."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "long_desc", "line_number": 10, "body": "def long_desc(self):\n        return \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a detailed, human-readable description of the command's functionality, suitable for help messages or documentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "add_options", "line_number": 13, "body": "def add_options(self, parser):\n        super().add_options(parser)\n        parser.remove_option(\"--headers\")", "is_method": true, "class_name": "Command", "function_description": "Configures the command's argument parser by including base class options but explicitly removing the `--headers` option from its available arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "_print_response", "line_number": 17, "body": "def _print_response(self, response, opts):\n        open_in_browser(response)", "is_method": true, "class_name": "Command", "function_description": "Opens the provided response data in a web browser. It offers a direct way to display command results visually."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "printSummary", "line_number": 12, "body": "def printSummary(self, start, stop):\n        write = self.stream.write\n        writeln = self.stream.writeln\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")", "is_method": true, "class_name": "TextTestResult", "function_description": "Prints a concise summary of test execution to the stream, including the number of tests run, elapsed time, and a breakdown of successes, failures, and errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "syntax", "line_number": 44, "body": "def syntax(self):\n        return \"[options] <spider>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the expected command-line syntax for this command, typically used in help messages or usage instructions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "short_desc", "line_number": 47, "body": "def short_desc(self):\n        return \"Check spider contracts\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable summary of the command's main purpose. Useful for displaying command descriptions in user interfaces or help messages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "add_options", "line_number": 50, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                          help=\"only list contracts, without checking them\")\n        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n                          help=\"print contract tests for all spiders\")", "is_method": true, "class_name": "Command", "function_description": "Extends a command-line argument parser with specific options. It adds flags to list contracts without checking and enable verbose output for spider contract tests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "run", "line_number": 57, "body": "def run(self, args, opts):\n        # load contracts\n        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        spider_loader = self.crawler_process.spider_loader\n\n        with set_environ(SCRAPY_CHECK='true'):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start_requests = lambda s: conman.from_spider(s, result)\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f'  * {method}')\n            else:\n                start = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start, stop)\n                self.exitcode = int(not result.wasSuccessful())", "is_method": true, "class_name": "Command", "function_description": "Executes validation \"contracts\" against Scrapy spiders, either listing testable methods or running spiders in a special check mode to report test results and ensure their proper functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "_import_file", "line_number": 10, "body": "def _import_file(filepath):\n    abspath = os.path.abspath(filepath)\n    dirname, file = os.path.split(abspath)\n    fname, fext = os.path.splitext(file)\n    if fext not in ('.py', '.pyw'):\n        raise ValueError(f\"Not a Python source file: {abspath}\")\n    if dirname:\n        sys.path = [dirname] + sys.path\n    try:\n        module = import_module(fname)\n    finally:\n        if dirname:\n            sys.path.pop(0)\n    return module", "is_method": false, "function_description": "Imports a Python module directly from a given file path. It temporarily modifies the system's import search path to facilitate this specific file import."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "syntax", "line_number": 31, "body": "def syntax(self):\n        return \"[options] <spider_file>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the standardized command-line syntax string for this Command object. This information is typically used for help messages or input validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "short_desc", "line_number": 34, "body": "def short_desc(self):\n        return \"Run a self-contained spider (without creating a project)\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable summary of the command's purpose, stating it runs a self-contained spider without requiring a project."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "long_desc", "line_number": 37, "body": "def long_desc(self):\n        return \"Run the spider defined in the given file\"", "is_method": true, "class_name": "Command", "function_description": "Returns a detailed description of the command's purpose. It specifies that the command is used to run a spider defined in a given file."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "run", "line_number": 40, "body": "def run(self, args, opts):\n        if len(args) != 1:\n            raise UsageError()\n        filename = args[0]\n        if not os.path.exists(filename):\n            raise UsageError(f\"File not found: {filename}\\n\")\n        try:\n            module = _import_file(filename)\n        except (ImportError, ValueError) as e:\n            raise UsageError(f\"Unable to load {filename!r}: {e}\\n\")\n        spclasses = list(iter_spider_classes(module))\n        if not spclasses:\n            raise UsageError(f\"No spider found in file: {filename}\\n\")\n        spidercls = spclasses.pop()\n\n        self.crawler_process.crawl(spidercls, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Executes a web crawling command by loading a specified Python file, identifying a spider class, and initiating the crawling process. It facilitates running custom spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "set_crawler", "line_number": 27, "body": "def set_crawler(self, crawler):\n        if hasattr(self, '_crawler'):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler = crawler", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Assigns a `Crawler` instance to the command, ensuring it is set only once. This links the command to the Scrapy engine for execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "long_desc", "line_number": 44, "body": "def long_desc(self):\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines, since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "This method of ScrapyCommand returns a long description for the command, falling back to the short description. It ensures the description is single-line for formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "help", "line_number": 51, "body": "def help(self):\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines, since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Returns the extensive, pre-formatted help description for a Scrapy command. This detailed text is displayed when the \"help\" command is used."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "add_options", "line_number": 58, "body": "def add_options(self, parser):\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        group = OptionGroup(parser, \"Global Options\")\n        group.add_option(\"--logfile\", metavar=\"FILE\",\n                         help=\"log file. if omitted stderr will be used\")\n        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n                         help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n        group.add_option(\"--nolog\", action=\"store_true\",\n                         help=\"disable logging completely\")\n        group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n                         help=\"write python cProfile stats to FILE\")\n        group.add_option(\"--pidfile\", metavar=\"FILE\",\n                         help=\"write process ID to FILE\")\n        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                         help=\"set/override setting (may be repeated)\")\n        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n        parser.add_option_group(group)", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Populates the command-line parser with global options for a Scrapy command. It allows users to configure aspects like logging, profiling, and settings directly from the CLI."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "process_options", "line_number": 79, "body": "def process_options(self, args, opts):\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set),\n                                  priority='cmdline')\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n            self.settings.set('LOG_FILE', opts.logfile, priority='cmdline')\n\n        if opts.loglevel:\n            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n            self.settings.set('LOG_LEVEL', opts.loglevel, priority='cmdline')\n\n        if opts.nolog:\n            self.settings.set('LOG_ENABLED', False, priority='cmdline')\n\n        if opts.pidfile:\n            with open(opts.pidfile, \"w\") as f:\n                f.write(str(os.getpid()) + os.linesep)\n\n        if opts.pdb:\n            failure.startDebugMode()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "This method processes command-line options and applies them to the Scrapy command's settings. It configures logging, PID file creation, and debugging behavior based on user input."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "add_options", "line_number": 115, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                          help=\"set spider argument (may be repeated)\")\n        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                          help=\"append scraped items to the end of FILE (use - for stdout)\")\n        parser.add_option(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n                          help=\"dump scraped items into FILE, overwriting any existing file\")\n        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                          help=\"format to use for dumping items\")", "is_method": true, "class_name": "BaseRunSpiderCommand", "function_description": "Adds command-line options to configure spider arguments, specify output files, and define the output format for Scrapy spider execution. This enables users to control spider behavior and data export via the CLI."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "process_options", "line_number": 126, "body": "def process_options(self, args, opts):\n        ScrapyCommand.process_options(self, args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                opts.output_format,\n                opts.overwrite_output,\n            )\n            self.settings.set('FEEDS', feeds, priority='cmdline')", "is_method": true, "class_name": "BaseRunSpiderCommand", "function_description": "This method processes command-line options for running a Scrapy spider. It converts spider arguments to a dictionary and configures output feeds based on user-provided flags."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "syntax", "line_number": 15, "body": "def syntax(self):\n        return \"[options] <url>\"", "is_method": true, "class_name": "Command", "function_description": "Returns a string defining the expected command-line syntax and arguments for this command."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "short_desc", "line_number": 18, "body": "def short_desc(self):\n        return \"Fetch a URL using the Scrapy downloader\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a concise, static string describing the command's primary action, which is to fetch a URL using the Scrapy downloader."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "long_desc", "line_number": 21, "body": "def long_desc(self):\n        return (\n            \"Fetch a URL using the Scrapy downloader and print its content\"\n            \" to stdout. You may want to use --nolog to disable logging\"\n        )", "is_method": true, "class_name": "Command", "function_description": "This method provides a detailed, human-readable description of the command's functionality. It explains the command fetches a URL using Scrapy and prints its content to stdout, along with a usage tip."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "add_options", "line_number": 27, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n                          help=\"print response HTTP headers instead of body\")\n        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                          help=\"do not handle HTTP 3xx status codes and print response as-is\")", "is_method": true, "class_name": "Command", "function_description": "This method extends the command-line parser by adding specific options for a Scrapy command. It allows users to control command behavior, such as specifying a spider, printing headers, or disabling redirects.\nThis method extends the command-line parser by adding specific options for a Scrapy command. It allows users to control command behavior, such as specifying a spider, printing headers, or disabling redirects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_headers", "line_number": 35, "body": "def _print_headers(self, headers, prefix):\n        for key, values in headers.items():\n            for value in values:\n                self._print_bytes(prefix + b' ' + key + b': ' + value)", "is_method": true, "class_name": "Command", "function_description": "Formats and outputs byte-based headers with a specified prefix. It serves to display or transmit structured header information, typically for network communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_response", "line_number": 40, "body": "def _print_response(self, response, opts):\n        if opts.headers:\n            self._print_headers(response.request.headers, b'>')\n            print('>')\n            self._print_headers(response.headers, b'<')\n        else:\n            self._print_bytes(response.body)", "is_method": true, "class_name": "Command", "function_description": "Displays the content of an operation's response. It can optionally print both request and response headers or just the response body."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_bytes", "line_number": 48, "body": "def _print_bytes(self, bytes_):\n        sys.stdout.buffer.write(bytes_ + b'\\n')", "is_method": true, "class_name": "Command", "function_description": "Outputs raw bytes directly to the standard output stream, appending a newline. This function is useful for displaying binary data or non-text content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "run", "line_number": 51, "body": "def run(self, args, opts):\n        if len(args) != 1 or not is_url(args[0]):\n            raise UsageError()\n        request = Request(args[0], callback=self._print_response,\n                          cb_kwargs={\"opts\": opts}, dont_filter=True)\n        # by default, let the framework handle redirects,\n        # i.e. command handles all codes expect 3xx\n        if not opts.no_redirect:\n            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n\n        spidercls = DefaultSpider\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        else:\n            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n        self.crawler_process.start()", "is_method": true, "class_name": "Command", "function_description": "Executes a web request or simple crawl from a command-line interface. It configures the request with a URL and specified options, then initiates the crawling process using an appropriate spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "syntax", "line_number": 27, "body": "def syntax(self):\n        return \"[options] <url>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the canonical command-line syntax string for the Command object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "short_desc", "line_number": 30, "body": "def short_desc(self):\n        return \"Parse URL (using its spider) and print the results\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable description of the command's primary action, typically used for help messages or command listings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_options", "line_number": 33, "body": "def add_options(self, parser):\n        BaseRunSpiderCommand.add_options(self, parser)\n        parser.add_option(\"--spider\", dest=\"spider\", default=None,\n                          help=\"use this spider without looking for one\")\n        parser.add_option(\"--pipelines\", action=\"store_true\",\n                          help=\"process items through pipelines\")\n        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n                          help=\"don't show links to follow (extracted requests)\")\n        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n                          help=\"don't show scraped items\")\n        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n                          help=\"avoid using pygments to colorize the output\")\n        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n                          help=\"use CrawlSpider rules to discover the callback\")\n        parser.add_option(\"-c\", \"--callback\", dest=\"callback\",\n                          help=\"use this callback for parsing, instead looking for a callback\")\n        parser.add_option(\"-m\", \"--meta\", dest=\"meta\",\n                          help=\"inject extra meta into the Request, it must be a valid raw json string\")\n        parser.add_option(\"--cbkwargs\", dest=\"cbkwargs\",\n                          help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n                          help=\"maximum depth for parsing requests [default: %default]\")\n        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                          help=\"print each depth level one by one\")", "is_method": true, "class_name": "Command", "function_description": "Adds specific command-line options to a parser, enabling detailed control over spider execution, item processing, and output characteristics for a command."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "max_level", "line_number": 59, "body": "def max_level(self):\n        max_items, max_requests = 0, 0\n        if self.items:\n            max_items = max(self.items)\n        if self.requests:\n            max_requests = max(self.requests)\n        return max(max_items, max_requests)", "is_method": true, "class_name": "Command", "function_description": "It determines the highest numerical \"level\" present across the command's `items` and `requests` attributes. This provides a unified maximum threshold or priority indicator for the command."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_items", "line_number": 67, "body": "def add_items(self, lvl, new_items):\n        old_items = self.items.get(lvl, [])\n        self.items[lvl] = old_items + new_items", "is_method": true, "class_name": "Command", "function_description": "Adds new items to a specific level within the command's internal collection. It appends them to existing items or initializes the list if the level is new."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_requests", "line_number": 71, "body": "def add_requests(self, lvl, new_reqs):\n        old_reqs = self.requests.get(lvl, [])\n        self.requests[lvl] = old_reqs + new_reqs", "is_method": true, "class_name": "Command", "function_description": "Adds new requests to a specific level within the command's internal request collection. It ensures new requests are appended to existing ones for that level."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_items", "line_number": 75, "body": "def print_items(self, lvl=None, colour=True):\n        if lvl is None:\n            items = [item for lst in self.items.values() for item in lst]\n        else:\n            items = self.items.get(lvl, [])\n\n        print(\"# Scraped Items \", \"-\" * 60)\n        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)", "is_method": true, "class_name": "Command", "function_description": "Displays scraped items managed by the Command object. It allows printing all items or filtering by a specific level, presented in a readable, formatted way."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_requests", "line_number": 84, "body": "def print_requests(self, lvl=None, colour=True):\n        if lvl is None:\n            if self.requests:\n                requests = self.requests[max(self.requests)]\n            else:\n                requests = []\n        else:\n            requests = self.requests.get(lvl, [])\n\n        print(\"# Requests \", \"-\" * 65)\n        display.pprint(requests, colorize=colour)", "is_method": true, "class_name": "Command", "function_description": "Displays a formatted list of requests stored within the command object. It allows viewing requests for a specific level or the latest ones for debugging or status monitoring."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_results", "line_number": 96, "body": "def print_results(self, opts):\n        colour = not opts.nocolour\n\n        if opts.verbose:\n            for level in range(1, self.max_level + 1):\n                print(f'\\n>>> DEPTH LEVEL: {level} <<<')\n                if not opts.noitems:\n                    self.print_items(level, colour)\n                if not opts.nolinks:\n                    self.print_requests(level, colour)\n        else:\n            print(f'\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<')\n            if not opts.noitems:\n                self.print_items(colour=colour)\n            if not opts.nolinks:\n                self.print_requests(colour=colour)", "is_method": true, "class_name": "Command", "function_description": "This method presents the results of a command's execution. It provides configurable, color-enabled output of items and requests, organized by processing depth level, with verbose or summarized views."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "run_callback", "line_number": 113, "body": "def run_callback(self, response, callback, cb_kwargs=None):\n        cb_kwargs = cb_kwargs or {}\n        items, requests = [], []\n\n        for x in iterate_spider_output(callback(response, **cb_kwargs)):\n            if is_item(x):\n                items.append(x)\n            elif isinstance(x, Request):\n                requests.append(x)\n        return items, requests", "is_method": true, "class_name": "Command", "function_description": "Executes a provided callback function with a response, classifying its generated output into data items and new requests for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "get_callback_from_rules", "line_number": 124, "body": "def get_callback_from_rules(self, spider, response):\n        if getattr(spider, 'rules', None):\n            for rule in spider.rules:\n                if rule.link_extractor.matches(response.url):\n                    return rule.callback or \"parse\"\n        else:\n            logger.error('No CrawlSpider rules found in spider %(spider)r, '\n                         'please specify a callback to use for parsing',\n                         {'spider': spider.name})", "is_method": true, "class_name": "Command", "function_description": "This method determines the appropriate callback function for processing a web response. It matches the response URL against the spider's defined crawling rules to direct parsing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "set_spidercls", "line_number": 134, "body": "def set_spidercls(self, url, opts):\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            try:\n                self.spidercls = spider_loader.load(opts.spider)\n            except KeyError:\n                logger.error('Unable to find spider: %(spider)s',\n                             {'spider': opts.spider})\n        else:\n            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n            if not self.spidercls:\n                logger.error('Unable to find spider for: %(url)s', {'url': url})\n\n        def _start_requests(spider):\n            yield self.prepare_request(spider, Request(url), opts)\n        self.spidercls.start_requests = _start_requests", "is_method": true, "class_name": "Command", "function_description": "Selects and configures the appropriate spider class for a crawl, based on provided options or a URL. It then sets the spider's initial starting request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "start_parsing", "line_number": 151, "body": "def start_parsing(self, url, opts):\n        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n        self.pcrawler = list(self.crawler_process.crawlers)[0]\n        self.crawler_process.start()\n\n        if not self.first_response:\n            logger.error('No response downloaded for: %(url)s',\n                         {'url': url})", "is_method": true, "class_name": "Command", "function_description": "Initiates and executes a web crawling process with a specific spider and arguments. It also verifies successful initial response download."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "prepare_request", "line_number": 160, "body": "def prepare_request(self, spider, request, opts):\n        def callback(response, **cb_kwargs):\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            # determine real callback\n            cb = response.meta['_callback']\n            if not cb:\n                if opts.callback:\n                    cb = opts.callback\n                elif opts.rules and self.first_response == response:\n                    cb = self.get_callback_from_rules(spider, response)\n\n                    if not cb:\n                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n                                     {'url': response.url, 'spider': spider.name})\n                        return\n                else:\n                    cb = 'parse'\n\n            if not callable(cb):\n                cb_method = getattr(spider, cb, None)\n                if callable(cb_method):\n                    cb = cb_method\n                else:\n                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n                                 {'callback': cb, 'spider': spider.name})\n                    return\n\n            # parse items and requests\n            depth = response.meta['_depth']\n\n            items, requests = self.run_callback(response, cb, cb_kwargs)\n            if opts.pipelines:\n                itemproc = self.pcrawler.engine.scraper.itemproc\n                for item in items:\n                    itemproc.process_item(item, spider)\n            self.add_items(depth, items)\n            self.add_requests(depth, requests)\n\n            scraped_data = items if opts.output else []\n            if depth < opts.depth:\n                for req in requests:\n                    req.meta['_depth'] = depth + 1\n                    req.meta['_callback'] = req.callback\n                    req.callback = callback\n                scraped_data += requests\n\n            return scraped_data\n\n        # update request meta if any extra meta was passed through the --meta/-m opts.\n        if opts.meta:\n            request.meta.update(opts.meta)\n\n        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.\n        if opts.cbkwargs:\n            request.cb_kwargs.update(opts.cbkwargs)\n\n        request.meta['_depth'] = 1\n        request.meta['_callback'] = request.callback\n        request.callback = callback\n        return request", "is_method": true, "class_name": "Command", "function_description": "Prepares a web request for a spider by configuring its callback. This callback manages response processing, item extraction, new request generation, and applies command-line options like depth and pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_options", "line_number": 224, "body": "def process_options(self, args, opts):\n        BaseRunSpiderCommand.process_options(self, args, opts)\n\n        self.process_request_meta(opts)\n        self.process_request_cb_kwargs(opts)", "is_method": true, "class_name": "Command", "function_description": "Processes command-line options for the `Command` class, extending base functionality. It configures request metadata and callback keyword arguments for spider execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_request_meta", "line_number": 230, "body": "def process_request_meta(self, opts):\n        if opts.meta:\n            try:\n                opts.meta = json.loads(opts.meta)\n            except ValueError:\n                raise UsageError(\"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                                 \"Example: --meta='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)", "is_method": true, "class_name": "Command", "function_description": "Validates and parses the 'meta' option, expecting a JSON string. It converts this string into a usable Python object for the command's metadata."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_request_cb_kwargs", "line_number": 238, "body": "def process_request_cb_kwargs(self, opts):\n        if opts.cbkwargs:\n            try:\n                opts.cbkwargs = json.loads(opts.cbkwargs)\n            except ValueError:\n                raise UsageError(\"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n                                 \"Example: --cbkwargs='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)", "is_method": true, "class_name": "Command", "function_description": "Validates and parses the `cbkwargs` command-line option, converting its JSON string value into a Python object for structured data input."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "run", "line_number": 246, "body": "def run(self, args, opts):\n        # parse arguments\n        if not len(args) == 1 or not is_url(args[0]):\n            raise UsageError()\n        else:\n            url = args[0]\n\n        # prepare spidercls\n        self.set_spidercls(url, opts)\n\n        if self.spidercls and opts.depth > 0:\n            self.start_parsing(url, opts)\n            self.print_results(opts)", "is_method": true, "class_name": "Command", "function_description": "Initiates and executes a web crawling and parsing operation for a given URL. It configures the crawler, performs parsing to a specified depth, and then displays the collected results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "callback", "line_number": 161, "body": "def callback(response, **cb_kwargs):\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            # determine real callback\n            cb = response.meta['_callback']\n            if not cb:\n                if opts.callback:\n                    cb = opts.callback\n                elif opts.rules and self.first_response == response:\n                    cb = self.get_callback_from_rules(spider, response)\n\n                    if not cb:\n                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n                                     {'url': response.url, 'spider': spider.name})\n                        return\n                else:\n                    cb = 'parse'\n\n            if not callable(cb):\n                cb_method = getattr(spider, cb, None)\n                if callable(cb_method):\n                    cb = cb_method\n                else:\n                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n                                 {'callback': cb, 'spider': spider.name})\n                    return\n\n            # parse items and requests\n            depth = response.meta['_depth']\n\n            items, requests = self.run_callback(response, cb, cb_kwargs)\n            if opts.pipelines:\n                itemproc = self.pcrawler.engine.scraper.itemproc\n                for item in items:\n                    itemproc.process_item(item, spider)\n            self.add_items(depth, items)\n            self.add_requests(depth, requests)\n\n            scraped_data = items if opts.output else []\n            if depth < opts.depth:\n                for req in requests:\n                    req.meta['_depth'] = depth + 1\n                    req.meta['_callback'] = req.callback\n                    req.callback = callback\n                scraped_data += requests\n\n            return scraped_data", "is_method": true, "class_name": "Command", "function_description": "Processes a web response, dynamically determining and executing the correct parsing callback. It handles extracted data items, new requests, and crawl depth, integrating with data pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "syntax", "line_number": 13, "body": "def syntax(self):\n        return \"[options]\"", "is_method": true, "class_name": "Command", "function_description": "Provides the string representation of a command's general syntax."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "short_desc", "line_number": 16, "body": "def short_desc(self):\n        return \"Get settings values\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a concise description of the command's primary function, suitable for display in help messages or command listings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "add_options", "line_number": 19, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\",\n                          help=\"print raw setting value\")\n        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a boolean\")\n        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as an integer\")\n        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a float\")\n        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a list\")", "is_method": true, "class_name": "Command", "function_description": "Adds command-line options to a parser, enabling users to retrieve and display Scrapy settings. It allows specifying the desired data type for interpreting the setting's value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "run", "line_number": 32, "body": "def run(self, args, opts):\n        settings = self.crawler_process.settings\n        if opts.get:\n            s = settings.get(opts.get)\n            if isinstance(s, BaseSettings):\n                print(json.dumps(s.copy_to_dict()))\n            else:\n                print(s)\n        elif opts.getbool:\n            print(settings.getbool(opts.getbool))\n        elif opts.getint:\n            print(settings.getint(opts.getint))\n        elif opts.getfloat:\n            print(settings.getfloat(opts.getfloat))\n        elif opts.getlist:\n            print(settings.getlist(opts.getlist))", "is_method": true, "class_name": "Command", "function_description": "Retrieves and displays specific configuration settings from the crawler process, allowing users to inspect various setting types via command-line options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "syntax", "line_number": 11, "body": "def syntax(self):\n        return \"[-v]\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command's expected syntax as a string. This is typically used for displaying usage instructions or help messages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "short_desc", "line_number": 14, "body": "def short_desc(self):\n        return \"Print Scrapy version\"", "is_method": true, "class_name": "Command", "function_description": "This method provides a short, human-readable description of the command's functionality, typically used for help messages or command listings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "add_options", "line_number": 17, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n                          help=\"also display twisted/python/platform info (useful for bug reports)\")", "is_method": true, "class_name": "Command", "function_description": "Provides the capability to add command-line options, including a verbose flag, to a parser. This enables users to request more detailed output for debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "run", "line_number": 22, "body": "def run(self, args, opts):\n        if opts.verbose:\n            versions = scrapy_components_versions()\n            width = max(len(n) for (n, _) in versions)\n            for name, version in versions:\n                print(f\"{name:<{width}} : {version}\")\n        else:\n            print(f\"Scrapy {scrapy.__version__}\")", "is_method": true, "class_name": "Command", "function_description": "This method displays the installed Scrapy version. Optionally, it lists detailed versions for all Scrapy components if verbose mode is enabled, useful for environment checks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_make_writable", "line_number": 26, "body": "def _make_writable(path):\n    current_permissions = os.stat(path).st_mode\n    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)", "is_method": false, "function_description": "Ensures the owner has write permissions for a given file or directory path. This is an internal utility to prepare paths for modification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "syntax", "line_number": 37, "body": "def syntax(self):\n        return \"<project_name> [project_dir]\"", "is_method": true, "class_name": "Command", "function_description": "Provides the standard command-line syntax string for this command. Useful for displaying usage information."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "short_desc", "line_number": 40, "body": "def short_desc(self):\n        return \"Create new project\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable description of the command's primary action, typically for display in a command-line interface or help message."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_is_valid_name", "line_number": 43, "body": "def _is_valid_name(self, project_name):\n        def _module_exists(module_name):\n            try:\n                import_module(module_name)\n                return True\n            except ImportError:\n                return False\n\n        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n            print('Error: Project names must begin with a letter and contain'\n                  ' only\\nletters, numbers and underscores')\n        elif _module_exists(project_name):\n            print(f'Error: Module {project_name!r} already exists')\n        else:\n            return True\n        return False", "is_method": true, "class_name": "Command", "function_description": "Validates a potential project name, checking if it adheres to Python identifier rules and doesn't conflict with existing modules. It ensures new project names are valid and unique."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_copytree", "line_number": 60, "body": "def _copytree(self, src, dst):\n        \"\"\"\n        Since the original function always creates the directory, to resolve\n        the issue a new function had to be created. It's a simple copy and\n        was reduced for this case.\n\n        More info at:\n        https://github.com/scrapy/scrapy/pull/2005\n        \"\"\"\n        ignore = IGNORE\n        names = os.listdir(src)\n        ignored_names = ignore(src, names)\n\n        if not os.path.exists(dst):\n            os.makedirs(dst)\n\n        for name in names:\n            if name in ignored_names:\n                continue\n\n            srcname = os.path.join(src, name)\n            dstname = os.path.join(dst, name)\n            if os.path.isdir(srcname):\n                self._copytree(srcname, dstname)\n            else:\n                copy2(srcname, dstname)\n                _make_writable(dstname)\n\n        copystat(src, dst)\n        _make_writable(dst)", "is_method": true, "class_name": "Command", "function_description": "This method recursively copies a directory tree from source to destination. It merges content into existing destinations, ensures writability, and supports ignoring specified items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "run", "line_number": 91, "body": "def run(self, args, opts):\n        if len(args) not in (1, 2):\n            raise UsageError()\n\n        project_name = args[0]\n        project_dir = args[0]\n\n        if len(args) == 2:\n            project_dir = args[1]\n\n        if exists(join(project_dir, 'scrapy.cfg')):\n            self.exitcode = 1\n            print(f'Error: scrapy.cfg already exists in {abspath(project_dir)}')\n            return\n\n        if not self._is_valid_name(project_name):\n            self.exitcode = 1\n            return\n\n        self._copytree(self.templates_dir, abspath(project_dir))\n        move(join(project_dir, 'module'), join(project_dir, project_name))\n        for paths in TEMPLATES_TO_RENDER:\n            path = join(*paths)\n            tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))\n            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n        print(f\"New Scrapy project '{project_name}', using template directory \"\n              f\"'{self.templates_dir}', created in:\")\n        print(f\"    {abspath(project_dir)}\\n\")\n        print(\"You can start your first spider with:\")\n        print(f\"    cd {project_dir}\")\n        print(\"    scrapy genspider example example.com\")", "is_method": true, "class_name": "Command", "function_description": "Creates a new Scrapy project from a template. It copies and customizes standard project files and directories, providing a ready-to-use project structure at the specified path."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "templates_dir", "line_number": 124, "body": "def templates_dir(self):\n        return join(\n            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n            'project'\n        )", "is_method": true, "class_name": "Command", "function_description": "Returns the file path for project templates, using a configured setting or defaulting to Scrapy's built-in templates. This provides the location for creating new project structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_module_exists", "line_number": 44, "body": "def _module_exists(module_name):\n            try:\n                import_module(module_name)\n                return True\n            except ImportError:\n                return False", "is_method": true, "class_name": "Command", "function_description": "Checks if a specified Python module is installed and can be successfully imported. This function allows commands to dynamically check for and adapt to available dependencies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "syntax", "line_number": 24, "body": "def syntax(self):\n        return \"[url|file]\"", "is_method": true, "class_name": "Command", "function_description": "Provides the expected argument syntax string for the command, useful for displaying usage information or validating input."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "short_desc", "line_number": 27, "body": "def short_desc(self):\n        return \"Interactive scraping console\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable description of the command's functionality, typically for display in help messages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "long_desc", "line_number": 30, "body": "def long_desc(self):\n        return (\"Interactive console for scraping the given url or file. \"\n                \"Use ./file.html syntax or full path for local file.\")", "is_method": true, "class_name": "Command", "function_description": "Provides a detailed usage description for an interactive console, explaining how to scrape content from URLs or local files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "add_options", "line_number": 34, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-c\", dest=\"code\",\n                          help=\"evaluate the code in the shell, print the result and exit\")\n        parser.add_option(\"--spider\", dest=\"spider\",\n                          help=\"use this spider\")\n        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                          help=\"do not handle HTTP 3xx status codes and print response as-is\")", "is_method": true, "class_name": "Command", "function_description": "Adds specific command-line options to a parser, enabling users to evaluate code, select a spider, and control HTTP redirect behavior for a Scrapy command.\nChain of Thought:\nThe `add_options` method is part of the `Command` class. It takes a `parser` object as input and calls `ScrapyCommand.add_options` on it, indicating it's extending a base set of options. Subsequently, it adds several specific command-line options like `-c` for code evaluation, `--spider` for spider selection, and `--no-redirect` for HTTP response handling.\n\nThe primary purpose is to define and register command-line arguments that users can pass to a Scrapy command. These options provide granular control over the command's execution, particularly related to interactive shell behavior, spider context, and network request handling.\n\nThis function serves as a configuration point for the command-line interface of a Scrapy command, allowing it to respond to user-specified flags and arguments.\n\nDescription should focus on what capabilities these options provide: code evaluation, spider selection, and HTTP redirect control. It's about enabling user control over the command's behavior.\n\nConstraint Checklist & Confidence Score:\n1. Concise (< 40 words): Yes\n2. Max 2 sentences: Yes\n3. Clear, direct language: Yes\n4. \"Not Implemented\" for placeholders: N/A\n5. \"Not sure\" for unclear: N/A\n\nConfidence Score: 5/5\n\nFinal description draft:\nAdds specific command-line options to a parser, enabling users to evaluate code, select a spider, and control HTTP redirect behavior for a Scrapy command.\nAdds specific command-line options to a parser, enabling users to evaluate code, select a spider, and control HTTP redirect behavior for a Scrapy command."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "run", "line_number": 49, "body": "def run(self, args, opts):\n        url = args[0] if args else None\n        if url:\n            # first argument may be a local file\n            url = guess_scheme(url)\n\n        spider_loader = self.crawler_process.spider_loader\n\n        spidercls = DefaultSpider\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        elif url:\n            spidercls = spidercls_for_request(spider_loader, Request(url),\n                                              spidercls, log_multiple=True)\n\n        # The crawler is created this way since the Shell manually handles the\n        # crawling engine, so the set up in the crawl method won't work\n        crawler = self.crawler_process._create_crawler(spidercls)\n        # The Shell class needs a persistent engine in the crawler\n        crawler.engine = crawler._create_engine()\n        crawler.engine.start()\n\n        self._start_crawler_thread()\n\n        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n        shell.start(url=url, redirect=not opts.no_redirect)", "is_method": true, "class_name": "Command", "function_description": "Launches an interactive Scrapy shell, preparing a crawler and its engine based on the provided URL or spider name for direct interaction and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "_start_crawler_thread", "line_number": 76, "body": "def _start_crawler_thread(self):\n        t = Thread(target=self.crawler_process.start,\n                   kwargs={'stop_after_crawl': False})\n        t.daemon = True\n        t.start()", "is_method": true, "class_name": "Command", "function_description": "Initiates a web crawling process in a separate, continuously running background thread. This enables non-blocking, persistent data collection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "syntax", "line_number": 13, "body": "def syntax(self):\n        return \"<spider>\"", "is_method": true, "class_name": "Command", "function_description": "Provides the specific command-line syntax required to execute this command, typically indicating expected arguments like a spider name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "short_desc", "line_number": 16, "body": "def short_desc(self):\n        return \"Edit spider\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable description of the command's purpose, typically for display in help messages or lists."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "long_desc", "line_number": 19, "body": "def long_desc(self):\n        return (\"Edit a spider using the editor defined in the EDITOR environment\"\n                \" variable or else the EDITOR setting\")", "is_method": true, "class_name": "Command", "function_description": "Provides a comprehensive description of the command's functionality, intended for display as part of help messages or documentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "_err", "line_number": 23, "body": "def _err(self, msg):\n        sys.stderr.write(msg + os.linesep)\n        self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Reports an error message to standard error and sets the command's exit code to indicate an operational failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "run", "line_number": 27, "body": "def run(self, args, opts):\n        if len(args) != 1:\n            raise UsageError()\n\n        editor = self.settings['EDITOR']\n        try:\n            spidercls = self.crawler_process.spider_loader.load(args[0])\n        except KeyError:\n            return self._err(f\"Spider not found: {args[0]}\")\n\n        sfile = sys.modules[spidercls.__module__].__file__\n        sfile = sfile.replace('.pyc', '.py')\n        self.exitcode = os.system(f'{editor} \"{sfile}\"')", "is_method": true, "class_name": "Command", "function_description": "Opens the source file of a specified spider using the configured external editor. It provides a convenient way to directly edit spider definitions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "short_desc", "line_number": 19, "body": "def short_desc(self):\n        return \"Run quick benchmark test\"", "is_method": true, "class_name": "Command", "function_description": "Provides a concise, human-readable description of the command's purpose."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "run", "line_number": 22, "body": "def run(self, args, opts):\n        with _BenchServer():\n            self.crawler_process.crawl(_BenchSpider, total=100000)\n            self.crawler_process.start()", "is_method": true, "class_name": "Command", "function_description": "Initiates and executes a web crawling benchmark. It starts a temporary server and a dedicated spider to crawl a specified number of items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "__enter__", "line_number": 30, "body": "def __enter__(self):\n        from scrapy.utils.test import get_testenv\n        pargs = [sys.executable, '-u', '-m', 'scrapy.utils.benchserver']\n        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE,\n                                     env=get_testenv())\n        self.proc.stdout.readline()", "is_method": true, "class_name": "_BenchServer", "function_description": "This method, when entering a `with` block, launches the Scrapy benchmark server in a subprocess. It prepares the server for performance testing by starting and waiting for it to become ready."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "__exit__", "line_number": 37, "body": "def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.wait()\n        time.sleep(0.2)", "is_method": true, "class_name": "_BenchServer", "function_description": "This method ensures the server's associated process is terminated and cleaned up when exiting the `_BenchServer` context. It prevents orphaned processes and guarantees proper resource release."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "start_requests", "line_number": 51, "body": "def start_requests(self):\n        qargs = {'total': self.total, 'show': self.show}\n        url = f'{self.baseurl}?{urlencode(qargs, doseq=True)}'\n        return [scrapy.Request(url, dont_filter=True)]", "is_method": true, "class_name": "_BenchSpider", "function_description": "This method generates the initial web request for the `_BenchSpider` to begin its crawling, configuring the URL with benchmark-specific parameters. It defines the starting point for data collection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "parse", "line_number": 56, "body": "def parse(self, response):\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)", "is_method": true, "class_name": "_BenchSpider", "function_description": "This method of `_BenchSpider` extracts links from a web page response. It then recursively schedules new requests to crawl those discovered links, enabling web traversal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/selector/unified.py", "function": "_st", "line_number": 14, "body": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    return st", "is_method": false, "function_description": "It determines a content type string, prioritizing an explicit 'st' value. If 'st' is not provided, it infers 'xml' or 'html' based on the response object type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/selector/unified.py", "function": "_response_from_text", "line_number": 20, "body": "def _response_from_text(text, st):\n    rt = XmlResponse if st == 'xml' else HtmlResponse\n    return rt(url='about:blank', encoding='utf-8',\n              body=to_bytes(text, 'utf-8'))", "is_method": false, "function_description": "Creates a Scrapy-like HTML or XML Response object from a given text string. This utility is ideal for testing or wrapping raw text for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "potential_domain_matches", "line_number": 90, "body": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]", "is_method": false, "function_description": "This function identifies all potential domain matches for a given host string. It provides a list suitable for checking cookie domain validity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "extract_cookies", "line_number": 22, "body": "def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)", "is_method": true, "class_name": "CookieJar", "function_description": "Extracts cookies from an HTTP response, considering the originating request. It populates the CookieJar with received cookies for subsequent use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "add_cookie_header", "line_number": 27, "body": "def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()", "is_method": true, "class_name": "CookieJar", "function_description": "Adds a \"Cookie\" header to an HTTP request by selecting relevant stored cookies. This ensures the request carries necessary session or state information back to the server."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "_cookies", "line_number": 60, "body": "def _cookies(self):\n        return self.jar._cookies", "is_method": true, "class_name": "CookieJar", "function_description": "Retrieves the internal collection of cookies from the CookieJar's underlying storage. This method provides direct access to the raw cookie data, likely for internal use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "clear_session_cookies", "line_number": 63, "body": "def clear_session_cookies(self, *args, **kwargs):\n        return self.jar.clear_session_cookies(*args, **kwargs)", "is_method": true, "class_name": "CookieJar", "function_description": "Removes all session-specific cookies currently stored within the cookie jar, ensuring a clean state for subsequent operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "clear", "line_number": 66, "body": "def clear(self, domain=None, path=None, name=None):\n        return self.jar.clear(domain, path, name)", "is_method": true, "class_name": "CookieJar", "function_description": "Clears cookies from the cookie jar. It can remove all cookies or selectively remove them by domain, path, or name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "__iter__", "line_number": 69, "body": "def __iter__(self):\n        return iter(self.jar)", "is_method": true, "class_name": "CookieJar", "function_description": "Allows the CookieJar object to be iterated over, making its contained cookies accessible directly in loops."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "__len__", "line_number": 72, "body": "def __len__(self):\n        return len(self.jar)", "is_method": true, "class_name": "CookieJar", "function_description": "Provides the count of cookies stored within the CookieJar instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_policy", "line_number": 75, "body": "def set_policy(self, pol):\n        return self.jar.set_policy(pol)", "is_method": true, "class_name": "CookieJar", "function_description": "Configures the cookie management policy for the CookieJar, determining rules for accepting or rejecting cookies. This allows precise control over cookie handling behavior.\nConfigures the cookie management policy for the `CookieJar` instance. This defines the rules for how cookies are accepted or rejected."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "make_cookies", "line_number": 78, "body": "def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)", "is_method": true, "class_name": "CookieJar", "function_description": "Processes an HTTP response and its originating request to extract and store cookies within the cookie jar. This updates the jar with received cookies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_cookie", "line_number": 83, "body": "def set_cookie(self, cookie):\n        self.jar.set_cookie(cookie)", "is_method": true, "class_name": "CookieJar", "function_description": "Adds a given cookie to the cookie jar's internal collection. This enables the CookieJar to store and manage web cookies for future requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_cookie_if_ok", "line_number": 86, "body": "def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))", "is_method": true, "class_name": "CookieJar", "function_description": "Conditionally adds a given cookie to the cookie jar. It only stores the cookie if it passes internal validation checks based on the request context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_full_url", "line_number": 126, "body": "def get_full_url(self):\n        return self.request.url", "is_method": true, "class_name": "WrappedRequest", "function_description": "Retrieves the complete URL string from the wrapped HTTP request object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_host", "line_number": 129, "body": "def get_host(self):\n        return urlparse_cached(self.request).netloc", "is_method": true, "class_name": "WrappedRequest", "function_description": "This method of `WrappedRequest` extracts the network location (host and port) from the encapsulated request. It provides the domain information for request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_type", "line_number": 132, "body": "def get_type(self):\n        return urlparse_cached(self.request).scheme", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the scheme (e.g., 'http', 'https') of the URL associated with the wrapped request. This identifies the protocol type of the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "is_unverifiable", "line_number": 135, "body": "def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)", "is_method": true, "class_name": "WrappedRequest", "function_description": "Determines if the request is unverifiable, indicating the user did not have an option to approve its URL. This status is critical for managing user consent and privacy in web interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_origin_req_host", "line_number": 145, "body": "def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname", "is_method": true, "class_name": "WrappedRequest", "function_description": "Extracts the hostname from the original request's URL. This provides the origin host for logging, routing, or security checks within the `WrappedRequest` context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "full_url", "line_number": 150, "body": "def full_url(self):\n        return self.get_full_url()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Retrieves the complete URL for the wrapped request object. It serves as a direct accessor for the full request URL."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "host", "line_number": 154, "body": "def host(self):\n        return self.get_host()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the host name associated with the wrapped request. It provides a simple accessor to the request's host."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "type", "line_number": 158, "body": "def type(self):\n        return self.get_type()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the type identifier of the wrapped request. This provides essential information about the request's nature for subsequent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "unverifiable", "line_number": 162, "body": "def unverifiable(self):\n        return self.is_unverifiable()", "is_method": true, "class_name": "WrappedRequest", "function_description": "This method indicates if the underlying request is considered unverifiable, providing a simple status check."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "origin_req_host", "line_number": 166, "body": "def origin_req_host(self):\n        return self.get_origin_req_host()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Provides the host of the original request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "has_header", "line_number": 169, "body": "def has_header(self, name):\n        return name in self.request.headers", "is_method": true, "class_name": "WrappedRequest", "function_description": "Determines if a named header is present in the underlying HTTP request. This provides a convenient way to check for specific request attributes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_header", "line_number": 172, "body": "def get_header(self, name, default=None):\n        return to_unicode(self.request.headers.get(name, default),\n                          errors='replace')", "is_method": true, "class_name": "WrappedRequest", "function_description": "Provides a safe way to retrieve a specific HTTP header from the wrapped request, returning it as a Unicode string or an optional default value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "header_items", "line_number": 176, "body": "def header_items(self):\n        return [\n            (to_unicode(k, errors='replace'),\n             [to_unicode(x, errors='replace') for x in v])\n            for k, v in self.request.headers.items()\n        ]", "is_method": true, "class_name": "WrappedRequest", "function_description": "Prepares and returns all HTTP request headers as a list of (name, value list) tuples, ensuring all strings are Unicode encoded for standardized consumption."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "add_unredirected_header", "line_number": 183, "body": "def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)", "is_method": true, "class_name": "WrappedRequest", "function_description": "Adds a header with the specified name and value to the internal HTTP request object. This header is intended to persist even if the request undergoes redirection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_all", "line_number": 195, "body": "def get_all(self, name, default=None):\n        return [to_unicode(v, errors='replace')\n                for v in self.response.headers.getlist(name)]", "is_method": true, "class_name": "WrappedResponse", "function_description": "Retrieves all values for a specified HTTP header from the wrapped response, converting them to unicode strings for convenient use.\nRetrieves all values for a specified HTTP header from the wrapped response, converting them to unicode strings for convenient use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "normkey", "line_number": 13, "body": "def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())", "is_method": true, "class_name": "Headers", "function_description": "Provides a canonical, byte-encoded representation of header keys. This ensures consistent handling and efficient lookup of header names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "normvalue", "line_number": 17, "body": "def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (str, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]", "is_method": true, "class_name": "Headers", "function_description": "Normalizes diverse input values into a consistent list of byte strings. This ensures header values are uniformly formatted for processing within the Headers class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "_tobytes", "line_number": 28, "body": "def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, str):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return str(x).encode(self.encoding)\n        else:\n            raise TypeError(f'Unsupported value type: {type(x)}')", "is_method": true, "class_name": "Headers", "function_description": "Converts input values like strings and integers into bytes, ensuring consistent byte representation for header operations within the `Headers` class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "__getitem__", "line_number": 38, "body": "def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)[-1]\n        except IndexError:\n            return None", "is_method": true, "class_name": "Headers", "function_description": "Enables dictionary-style access to header values. It retrieves the last value associated with a key, returning None if the key has no value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "get", "line_number": 44, "body": "def get(self, key, def_val=None):\n        try:\n            return super().get(key, def_val)[-1]\n        except IndexError:\n            return None", "is_method": true, "class_name": "Headers", "function_description": "Retrieves the last value associated with a specified header key. Returns `None` if the key exists but has no values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "getlist", "line_number": 50, "body": "def getlist(self, key, def_val=None):\n        try:\n            return super().__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []", "is_method": true, "class_name": "Headers", "function_description": "Provides a list of values for a specified header key. If the key is absent, it returns an empty list or a provided default value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "setlist", "line_number": 58, "body": "def setlist(self, key, list_):\n        self[key] = list_", "is_method": true, "class_name": "Headers", "function_description": "Assigns a provided list of values to a specified header key within the Headers object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "setlistdefault", "line_number": 61, "body": "def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)", "is_method": true, "class_name": "Headers", "function_description": "Provides a convenience method to ensure a header key exists, defaulting its value to an empty tuple if not found. It returns the key's current or newly set value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "appendlist", "line_number": 64, "body": "def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst", "is_method": true, "class_name": "Headers", "function_description": "Appends a value to the list of values for a specific key within the Headers object. Useful for managing multi-valued HTTP header fields."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "items", "line_number": 69, "body": "def items(self):\n        return ((k, self.getlist(k)) for k in self.keys())", "is_method": true, "class_name": "Headers", "function_description": "Provides an iterable view of all header names and their corresponding values as lists, facilitating comprehensive iteration over all headers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "values", "line_number": 72, "body": "def values(self):\n        return [self[k] for k in self.keys()]", "is_method": true, "class_name": "Headers", "function_description": "Provides a list of all header values stored within the Headers object, similar to a dictionary's values method."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "to_string", "line_number": 75, "body": "def to_string(self):\n        return headers_dict_to_raw(self)", "is_method": true, "class_name": "Headers", "function_description": "Converts the Headers object into its raw string representation. This is crucial for transmitting header information over a network or for debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "to_unicode_dict", "line_number": 78, "body": "def to_unicode_dict(self):\n        \"\"\" Return headers as a CaselessDict with unicode keys\n        and unicode values. Multiple values are joined with ','.\n        \"\"\"\n        return CaselessDict(\n            (to_unicode(key, encoding=self.encoding),\n             to_unicode(b','.join(value), encoding=self.encoding))\n            for key, value in self.items())", "is_method": true, "class_name": "Headers", "function_description": "Provides a case-insensitive dictionary representation of headers, converting all keys and values to unicode. Multiple header values are joined by a comma for a unified view."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "__copy__", "line_number": 87, "body": "def __copy__(self):\n        return self.__class__(self)", "is_method": true, "class_name": "Headers", "function_description": "This method creates a shallow copy of the current Headers instance. It provides a new, independent Headers object with the same content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/common.py", "function": "obsolete_setter", "line_number": 1, "body": "def obsolete_setter(setter, attrname):\n    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n        raise AttributeError(msg)\n    return newsetter", "is_method": false, "function_description": "Generates a special setter function that prevents direct assignment to an object's attribute. It informs users that the attribute is not modifiable and suggests using a dedicated `replace()` method instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/json_request.py", "function": "replace", "line_number": 39, "body": "def replace(self, *args, **kwargs):\n        body_passed = kwargs.get('body', None) is not None\n        data = kwargs.pop('data', None)\n        data_passed = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn('Both body and data passed. data will be ignored')\n\n        elif not body_passed and data_passed:\n            kwargs['body'] = self._dumps(data)\n\n        return super().replace(*args, **kwargs)", "is_method": true, "class_name": "JsonRequest", "function_description": "Prepares the request body for a JsonRequest replacement. It serializes Python data into a JSON string if provided, ensuring the correct body format for the parent's `replace` method."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/json_request.py", "function": "_dumps", "line_number": 52, "body": "def _dumps(self, data):\n        \"\"\"Convert to JSON \"\"\"\n        return json.dumps(data, **self._dumps_kwargs)", "is_method": true, "class_name": "JsonRequest", "function_description": "Serializes Python data structures into a JSON formatted string. This prepares data for transmission within JSON requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_form_url", "line_number": 61, "body": "def _get_form_url(form, url):\n    if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)", "is_method": false, "function_description": "Provides the absolute URL where a web form's data should be submitted. It resolves the final URL using an explicit override, the form's action attribute, or its base URL."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_urlencode", "line_number": 70, "body": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=True)", "is_method": false, "function_description": "Encodes a sequence of key-value pairs into a URL-friendly format. It prepares data by converting values to bytes and supports multiple values for the same key, useful for query strings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_form", "line_number": 77, "body": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(f\"No <form> element found in {response}\")\n\n    if formname is not None:\n        f = root.xpath(f'//form[@name=\"{formname}\"]')\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath(f'//form[@id=\"{formid}\"]')\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError(f'No <form> element found with {formxpath}')\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(f\"Form number {formnumber} not found in {response}\")\n        else:\n            return form", "is_method": false, "function_description": "This internal helper function identifies and retrieves a specific HTML form element from a web page's content. It supports finding forms by name, ID, XPath, or numerical index."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_inputs", "line_number": 119, "body": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    if not formdata:\n        formdata = ()\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[not(@type) or @type['\n                        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n                        ' and (../@checked or'\n                        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n                        namespaces={\n                            \"re\": \"http://exslt.org/regular-expressions\"})\n    values = [(k, '' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata_keys]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    if isinstance(formdata, dict):\n        formdata = formdata.items()\n\n    values.extend((k, v) for k, v in formdata if v is not None)\n    return values", "is_method": false, "function_description": "This function compiles form input values for submission, combining default HTML element values with provided data and clicked element information. It generates the complete payload for simulating a web form submission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_value", "line_number": 151, "body": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v", "is_method": false, "function_description": "This helper function extracts the name and the appropriate value from a given element. It provides specific handling for 'select' elements to ensure correct data retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_select_value", "line_number": 159, "body": "def _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags without options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath('.//option[@selected]')\n        v = [(o.get('value') or o.text or '').strip() for o in selected_options]\n    return n, v", "is_method": false, "function_description": "This function determines the selected value(s) of an HTML select element. It handles default browser behavior for single-selects and extracts all chosen options for multi-selects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_clickable", "line_number": 174, "body": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = list(form.xpath(\n        'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n        '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n        namespaces={\"re\": \"http://exslt.org/regular-expressions\"}\n    ))\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get('name'), el.get('value') or '')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get('name'), el.get('value') or '')\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = './/*' + ''.join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get('name'), el[0].get('value') or '')\n    elif len(el) > 1:\n        raise ValueError(f\"Multiple elements found ({el!r}) matching the \"\n                         f\"criteria in clickdata: {clickdata!r}\")\n    else:\n        raise ValueError(f'No clickable element matching clickdata: {clickdata!r}')", "is_method": false, "function_description": "This function locates a specific clickable element within a web form, such as a submit button. It identifies the element based on provided click data or defaults to the first available clickable element, returning its name and value."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "from_response", "line_number": 39, "body": "def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n\n        kwargs.setdefault('encoding', response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n\n        method = kwargs.pop('method', form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = 'GET'\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)", "is_method": true, "class_name": "FormRequest", "function_description": "This class method automatically generates a `FormRequest` object by parsing an HTML form found within an HTTP response, preparing it for subsequent programmatic submission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "cb_kwargs", "line_number": 47, "body": "def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs", "is_method": true, "class_name": "Request", "function_description": "This method provides and ensures access to a dictionary of keyword arguments intended for the request's callback, initializing it if not already present."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "meta", "line_number": 53, "body": "def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta", "is_method": true, "class_name": "Request", "function_description": "This method provides and ensures access to a mutable dictionary for storing arbitrary metadata associated with the request. It initializes the metadata dictionary if it doesn't already exist."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_get_url", "line_number": 58, "body": "def _get_url(self):\n        return self._url", "is_method": true, "class_name": "Request", "function_description": "Provides the URL associated with the Request object for internal use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_set_url", "line_number": 61, "body": "def _set_url(self, url):\n        if not isinstance(url, str):\n            raise TypeError(f'Request url must be str or unicode, got {type(url).__name__}')\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if (\n            '://' not in self._url\n            and not self._url.startswith('about:')\n            and not self._url.startswith('data:')\n        ):\n            raise ValueError(f'Missing scheme in request url: {self._url}')", "is_method": true, "class_name": "Request", "function_description": "Sets and validates the URL for the request object. It ensures the URL is a string, safely encoded, and includes a necessary scheme for a valid request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_get_body", "line_number": 77, "body": "def _get_body(self):\n        return self._body", "is_method": true, "class_name": "Request", "function_description": "This method retrieves the raw body content of the HTTP request, enabling other components to process the request's payload."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_set_body", "line_number": 80, "body": "def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)", "is_method": true, "class_name": "Request", "function_description": "This method sets the request's body content. It ensures the body is converted into a byte string for internal use, gracefully handling cases where no body is provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "encoding", "line_number": 89, "body": "def encoding(self):\n        return self._encoding", "is_method": true, "class_name": "Request", "function_description": "Retrieves the character encoding specified for the request. This information is crucial for correctly interpreting request body data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "__str__", "line_number": 92, "body": "def __str__(self):\n        return f\"<{self.method} {self.url}>\"", "is_method": true, "class_name": "Request", "function_description": "Provides a concise, human-readable string representation of an HTTP request object, displaying its method and URL for easy identification or debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "copy", "line_number": 97, "body": "def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()", "is_method": true, "class_name": "Request", "function_description": "Creates and returns a new, independent copy of the current Request object, preserving its original state. This is useful for making modifications without altering the original instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "replace", "line_number": 101, "body": "def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)", "is_method": true, "class_name": "Request", "function_description": "Generates a new Request object by copying existing attributes and applying specified overrides. This allows creating modified requests without altering the original."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "from_curl", "line_number": 112, "body": "def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)", "is_method": true, "class_name": "Request", "function_description": "This class method creates a Scrapy `Request` object directly from a cURL command string. It populates the HTTP method, URL, headers, cookies, and body, enabling easy conversion of cURL commands into usable requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "cb_kwargs", "line_number": 43, "body": "def cb_kwargs(self):\n        try:\n            return self.request.cb_kwargs\n        except AttributeError:\n            raise AttributeError(\n                \"Response.cb_kwargs not available, this response \"\n                \"is not tied to any request\"\n            )", "is_method": true, "class_name": "Response", "function_description": "Retrieves custom keyword arguments associated with the original request that generated this response. It allows accessing context-specific data for subsequent response processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "meta", "line_number": 53, "body": "def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )", "is_method": true, "class_name": "Response", "function_description": "Provides access to the metadata associated with the original request that generated this response. It ensures that only responses tied to a request can retrieve this information."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_get_url", "line_number": 62, "body": "def _get_url(self):\n        return self._url", "is_method": true, "class_name": "Response", "function_description": "Provides the URL from which this `Response` object was obtained. It allows retrieval of the source URL."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_set_url", "line_number": 65, "body": "def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError(f'{type(self).__name__} url must be str, '\n                            f'got {type(url).__name__}')", "is_method": true, "class_name": "Response", "function_description": "Sets the URL for the Response object, validating that the provided URL is a string."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_get_body", "line_number": 74, "body": "def _get_body(self):\n        return self._body", "is_method": true, "class_name": "Response", "function_description": "Provides access to the raw body content of the HTTP response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_set_body", "line_number": 77, "body": "def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body", "is_method": true, "class_name": "Response", "function_description": "Sets the internal byte representation of the HTTP response body. It validates that the provided body is in bytes format, converting `None` to an empty byte string."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "__str__", "line_number": 90, "body": "def __str__(self):\n        return f\"<{self.status} {self.url}>\"", "is_method": true, "class_name": "Response", "function_description": "Defines the string representation of a `Response` object. It provides a concise summary showing the response's status and URL for easy debugging or display."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "copy", "line_number": 95, "body": "def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()", "is_method": true, "class_name": "Response", "function_description": "Creates a new, independent copy of the Response object. This allows for modification without affecting the original instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "replace", "line_number": 99, "body": "def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in [\n            \"url\", \"status\", \"headers\", \"body\", \"request\", \"flags\", \"certificate\", \"ip_address\", \"protocol\",\n        ]:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)", "is_method": true, "class_name": "Response", "function_description": "Creates a new Response object that is a copy of the current instance, allowing specific attributes to be overridden. It provides a way to generate modified versions of the response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "urljoin", "line_number": 110, "body": "def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)", "is_method": true, "class_name": "Response", "function_description": "Combines a potentially relative URL with the Response's base URL to form a complete absolute URL. Useful for resolving links within the response's context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "text", "line_number": 116, "body": "def text(self):\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as str\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "Provides the response body as a string, specifically for `TextResponse` subclasses. For other response types, it explicitly indicates that the content is not text."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "css", "line_number": 122, "body": "def css(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "Not Implemented\nChain of Thought:\nThe function `css` within the `Response` class explicitly raises a `NotSupported` exception.\nThe docstring clarifies that this method is \"implemented only by responses whose content is text (subclasses of TextResponse).\"\nThis indicates that for the `Response` class itself, this `css` method is not functional; it's essentially a placeholder or a method meant to be overridden by more specific subclasses that handle text content.\nIts purpose in this specific class is to signify that CSS operations are not supported on a generic `Response` object.\nAccording to the instructions, I should return \"Not Implemented\" for \"placeholder functions, not implemented functions, or functions with no retrieval value\". This function fits the criteria of being a placeholder and not implemented for the base class, having no retrieval value (it raises an error).\nNot Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "follow", "line_number": 134, "body": "def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding='utf-8', priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n\n        .. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n\n        return Request(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "Response", "function_description": "Creates a `Request` object to follow a link found in the current response. It simplifies web crawling by automatically resolving relative URLs to absolute ones."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "follow_all", "line_number": 173, "body": "def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n                   cookies=None, meta=None, encoding='utf-8', priority=0,\n                   dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Generator[Request, None, None]\n        \"\"\"\n        .. versionadded:: 2.0\n\n        Return an iterable of :class:`~.Request` instances to follow all links\n        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n        not only absolute URLs.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if not hasattr(urls, '__iter__'):\n            raise TypeError(\"'urls' argument must be an iterable\")\n        return (\n            self.follow(\n                url=url,\n                callback=callback,\n                method=method,\n                headers=headers,\n                body=body,\n                cookies=cookies,\n                meta=meta,\n                encoding=encoding,\n                priority=priority,\n                dont_filter=dont_filter,\n                errback=errback,\n                cb_kwargs=cb_kwargs,\n                flags=flags,\n            )\n            for url in urls\n        )", "is_method": true, "class_name": "Response", "function_description": "Generates multiple new web requests from a list of URLs extracted from the current response. It simplifies batch processing of links for a web crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_url_from_selector", "line_number": 252, "body": "def _url_from_selector(sel):\n    # type: (parsel.Selector) -> str\n    if isinstance(sel.root, str):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, 'tag'):\n        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n    if sel.root.tag not in ('a', 'link'):\n        raise _InvalidSelector(\"Only <a> and <link> elements are supported; \"\n                               f\"got <{sel.root.tag}>\")\n    href = sel.root.get('href')\n    if href is None:\n        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n    return strip_html5_whitespace(href)", "is_method": false, "function_description": "Extracts and validates a URL from a `parsel.Selector` object. It specifically retrieves the `href` attribute from `<a>` or `<link>` HTML elements, or directly processes string results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_set_url", "line_number": 40, "body": "def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = to_unicode(url, self.encoding)\n        else:\n            super()._set_url(url)", "is_method": true, "class_name": "TextResponse", "function_description": "Provides a controlled way to set the internal URL for the response, ensuring string URLs are properly encoded to unicode using the specified encoding."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_set_body", "line_number": 46, "body": "def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, str):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - '\n                                f'{type(self).__name__} has no encoding')\n            self._body = body.encode(self._encoding)\n        else:\n            super()._set_body(body)", "is_method": true, "class_name": "TextResponse", "function_description": "Sets the internal response body, specifically handling string input by encoding it to bytes using the response's character set. It delegates non-string body types to the parent class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "replace", "line_number": 56, "body": "def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)", "is_method": true, "class_name": "TextResponse", "function_description": "Creates a new `TextResponse` instance with specified modifications. It automatically defaults to the original response's encoding if not explicitly provided, ensuring consistent text handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "encoding", "line_number": 61, "body": "def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()", "is_method": true, "class_name": "TextResponse", "function_description": "Provides the character encoding for the text response. It first checks for a declared encoding, then infers it from the body."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_declared_encoding", "line_number": 64, "body": "def _declared_encoding(self):\n        return (\n            self._encoding\n            or self._headers_encoding()\n            or self._body_declared_encoding()\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Provides the definitive character encoding for the response's text content. It determines the encoding by checking internal state, HTTP headers, and body declarations in a prioritized order."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "body_as_unicode", "line_number": 71, "body": "def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        warnings.warn('Response.body_as_unicode() is deprecated, '\n                      'please use Response.text instead.',\n                      ScrapyDeprecationWarning, stacklevel=2)\n        return self.text", "is_method": true, "class_name": "TextResponse", "function_description": "Returns the response body as unicode text. This method is deprecated; use `Response.text` for retrieving the response body as text."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "json", "line_number": 78, "body": "def json(self):\n        \"\"\"\n        .. versionadded:: 2.2\n\n        Deserialize a JSON document to a Python object.\n        \"\"\"\n        if self._cached_decoded_json is _NONE:\n            self._cached_decoded_json = json.loads(self.text)\n        return self._cached_decoded_json", "is_method": true, "class_name": "TextResponse", "function_description": "Deserializes the TextResponse's content, if it's a JSON document, into a Python object for easy structured data access. It caches the decoded object for efficiency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "text", "line_number": 89, "body": "def text(self):\n        \"\"\" Body as unicode \"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = f'charset={benc}'\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody", "is_method": true, "class_name": "TextResponse", "function_description": "Provides the response body as a Unicode string, converting the raw content using the detected encoding. It caches the result for efficient subsequent access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "urljoin", "line_number": 99, "body": "def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)", "is_method": true, "class_name": "TextResponse", "function_description": "Provides an absolute URL by joining a given URL with the TextResponse's base URL. This is crucial for resolving relative links found within web content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_headers_encoding", "line_number": 105, "body": "def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_unicode(content_type))", "is_method": true, "class_name": "TextResponse", "function_description": "This method retrieves and processes the 'Content-Type' header from the response. It determines the appropriate character encoding for the text response content, ensuring proper display."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_body_inferred_encoding", "line_number": 109, "body": "def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_unicode(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                                          auto_detect_fun=self._auto_detect_fun,\n                                          default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc", "is_method": true, "class_name": "TextResponse", "function_description": "Determines and caches the character encoding of the TextResponse's body. It uses content headers and auto-detection to ensure the response text is correctly decoded for use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_auto_detect_fun", "line_number": 119, "body": "def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)", "is_method": true, "class_name": "TextResponse", "function_description": "This internal method attempts to auto-detect the character encoding of raw text by trying common encodings. It returns the first encoding that successfully decodes the input without errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_body_declared_encoding", "line_number": 128, "body": "def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)", "is_method": true, "class_name": "TextResponse", "function_description": "Retrieves the character encoding declared within the HTML body of the text response. This helps ensure the response content is decoded correctly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "selector", "line_number": 132, "body": "def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector", "is_method": true, "class_name": "TextResponse", "function_description": "Provides a cached Scrapy Selector object associated with the response. This enables efficient parsing and data extraction from the response's content using CSS or XPath expressions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "xpath", "line_number": 138, "body": "def xpath(self, query, **kwargs):\n        return self.selector.xpath(query, **kwargs)", "is_method": true, "class_name": "TextResponse", "function_description": "Allows executing XPath queries directly on the `TextResponse` object's content. This facilitates extracting specific data from structured text such as HTML or XML."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "css", "line_number": 141, "body": "def css(self, query):\n        return self.selector.css(query)", "is_method": true, "class_name": "TextResponse", "function_description": "Enables extracting specific elements or data from the text response's content by applying a CSS selector query."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "follow", "line_number": 144, "body": "def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding=None, priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be not only an absolute URL, but also\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super().follow(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Generates a new web request to follow a specified URL, intelligently handling various URL formats extracted from the current response for convenient web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "follow_all", "line_number": 185, "body": "def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n                   cookies=None, meta=None, encoding=None, priority=0,\n                   dont_filter=False, errback=None, cb_kwargs=None, flags=None,\n                   css=None, xpath=None):\n        # type: (...) -> Generator[Request, None, None]\n        \"\"\"\n        A generator that produces :class:`~.Request` instances to follow all\n        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n        ``__init__`` method, except that each ``urls`` element does not need to be\n        an absolute URL, it can be any of the following:\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n\n        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n        selectors from which links cannot be obtained (for instance, anchor tags without an\n        ``href`` attribute)\n        \"\"\"\n        arguments = [x for x in (urls, css, xpath) if x is not None]\n        if len(arguments) != 1:\n            raise ValueError(\n                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n            )\n        if not urls:\n            if css:\n                urls = self.css(css)\n            if xpath:\n                urls = self.xpath(xpath)\n        if isinstance(urls, parsel.SelectorList):\n            selectors = urls\n            urls = []\n            for sel in selectors:\n                with suppress(_InvalidSelector):\n                    urls.append(_url_from_selector(sel))\n        return super().follow_all(\n            urls=urls,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Generates new web requests to follow all links found in the current response. It simplifies link extraction by supporting URLs, CSS selectors, or XPath expressions from the response content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_matches", "line_number": 50, "body": "def _matches(url, regexs):\n    return any(r.search(url) for r in regexs)", "is_method": false, "function_description": "Checks if a given URL matches any of the provided regular expressions. This is useful for URL pattern matching or filtering."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_is_valid_url", "line_number": 54, "body": "def _is_valid_url(url):\n    return url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}", "is_method": false, "function_description": "Determines if a given string starts with a recognized URL scheme (http, https, file, or ftp). It serves as a preliminary check for URL protocol validity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "__new__", "line_number": 62, "body": "def __new__(cls, *args, **kwargs):\n        from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n        if issubclass(cls, FilteringLinkExtractor) and not issubclass(cls, LxmlLinkExtractor):\n            warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                 'please use scrapy.linkextractors.LinkExtractor instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "This method issues a deprecation warning when `FilteringLinkExtractor` or its specific subclasses are instantiated, advising users to use `LinkExtractor` instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_link_allowed", "line_number": 94, "body": "def _link_allowed(self, link):\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n            return False\n        if self.restrict_text and not _matches(link.text, self.restrict_text):\n            return False\n        return True", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Determines if a link is permissible by evaluating its URL, domain, extension, and text against the extractor's configured allow and deny rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "matches", "line_number": 112, "body": "def matches(self, url):\n\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Determines if a given URL satisfies the configured domain and regex-based inclusion/exclusion rules. This method provides a URL filtering capability."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_process_links", "line_number": 123, "body": "def _process_links(self, links):\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        links = self.link_extractor._process_links(links)\n        return links", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Provides a core utility to filter a list of links based on predefined rules. It also optionally canonicalizes URLs and delegates further processing to an internal extractor."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_extract_links", "line_number": 131, "body": "def _extract_links(self, *args, **kwargs):\n        return self.link_extractor._extract_links(*args, **kwargs)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "This method delegates the actual link extraction process to an encapsulated link extractor object. It serves as the core link retrieval mechanism for the filtering class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_nons", "line_number": 25, "body": "def _nons(tag):\n    if isinstance(tag, str):\n        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n            return tag.split('}')[-1]\n    return tag", "is_method": false, "function_description": "This helper function normalizes XML/XHTML tag strings by removing their namespace prefix. It provides the local name of an element for further processing or comparison."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_identity", "line_number": 32, "body": "def _identity(x):\n    return x", "is_method": false, "function_description": "A utility function that simply returns its input value unchanged. It provides an identity mapping, useful when a no-operation transformation is required."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_canonicalize_link_url", "line_number": 36, "body": "def _canonicalize_link_url(link):\n    return canonicalize_url(link.url, keep_fragments=True)", "is_method": false, "function_description": "Canonicalizes the URL extracted from a link object, providing a standardized format suitable for storage, comparison, or further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_iter_links", "line_number": 51, "body": "def _iter_links(self, document):\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield (el, attrib, attribs[attrib])", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "This method iterates an lxml document, identifying and yielding element attributes that are potential links. It uses internal rules to scan for specific tags and attribute names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_extract_links", "line_number": 61, "body": "def _extract_links(self, selector, response_url, response_encoding, base_url):\n        links = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            url = safe_url_string(url, encoding=response_encoding)\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(url, _collect_string_content(el) or '',\n                        nofollow=rel_has_nofollow(el.get('rel')))\n            links.append(link)\n        return self._deduplicate_if_needed(links)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Extracts and normalizes all discoverable links from a parsed HTML/XML document represented by a selector. It resolves relative URLs and returns a deduplicated list of Link objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "extract_links", "line_number": 84, "body": "def extract_links(self, response):\n        base_url = get_base_url(response)\n        return self._extract_links(response.selector, response.url, response.encoding, base_url)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Extracts all links from a given web response. It gathers contextual information like the base URL and selector for the underlying link parsing mechanism."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_process_links", "line_number": 88, "body": "def _process_links(self, links):\n        \"\"\" Normalize and filter extracted links\n\n        The subclass should override it if neccessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Normalizes and filters a list of extracted links, applying deduplication by default. This method serves as an override point for subclasses to implement custom link processing logic."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_deduplicate_if_needed", "line_number": 95, "body": "def _deduplicate_if_needed(self, links):\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "This helper method conditionally removes duplicate links from a list. It ensures that only unique links are returned if the extractor is configured to do so."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "extract_links", "line_number": 142, "body": "def extract_links(self, response):\n        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n        specified :class:`response <scrapy.http.Response>`.\n\n        Only links that match the settings passed to the ``__init__`` method of\n        the link extractor are returned.\n\n        Duplicate links are omitted.\n        \"\"\"\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [\n                subdoc\n                for x in self.restrict_xpaths\n                for subdoc in response.xpath(x)\n            ]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        return unique_list(all_links)", "is_method": true, "class_name": "LxmlLinkExtractor", "function_description": "Provides the service of extracting a filtered and unique list of web links from an HTML response. It uses the extractor's configured rules to discover new URLs for web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_identity", "line_number": 17, "body": "def _identity(x):\n    return x", "is_method": false, "function_description": "A simple utility function that returns its input value unchanged, serving as a no-operation (no-op) transformation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_identity_process_request", "line_number": 21, "body": "def _identity_process_request(request, response):\n    return request", "is_method": false, "function_description": "Serves as a no-op processor that passes through the original request object unmodified. Useful as a default or neutral step in request-response pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_get_method", "line_number": 25, "body": "def _get_method(method, spider):\n    if callable(method):\n        return method\n    elif isinstance(method, str):\n        return getattr(spider, method, None)", "is_method": false, "function_description": "Resolves a method reference by returning a callable object. It accepts a direct callable or a string name to be retrieved from the provided spider instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_compile", "line_number": 55, "body": "def _compile(self, spider):\n        self.callback = _get_method(self.callback, spider)\n        self.errback = _get_method(self.errback, spider)\n        self.process_links = _get_method(self.process_links, spider)\n        self.process_request = _get_method(self.process_request, spider)", "is_method": true, "class_name": "Rule", "function_description": "This internal Rule method prepares and resolves its callback and processing functions. It ensures these handlers are correctly configured for the provided spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_parse", "line_number": 70, "body": "def _parse(self, response, **kwargs):\n        return self._parse_response(\n            response=response,\n            callback=self.parse_start_url,\n            cb_kwargs=kwargs,\n            follow=True,\n        )", "is_method": true, "class_name": "CrawlSpider", "function_description": "Initiates the parsing of a web response for the `CrawlSpider`. It sets `parse_start_url` as the primary callback and enables the spider to follow discovered links."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_build_request", "line_number": 84, "body": "def _build_request(self, rule_index, link):\n        return Request(\n            url=link.url,\n            callback=self._callback,\n            errback=self._errback,\n            meta=dict(rule=rule_index, link_text=link.text),\n        )", "is_method": true, "class_name": "CrawlSpider", "function_description": "This method constructs a Scrapy Request object for a given link, embedding rule-specific metadata and setting essential callback functions for web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_requests_to_follow", "line_number": 92, "body": "def _requests_to_follow(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        seen = set()\n        for rule_index, rule in enumerate(self._rules):\n            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                     if lnk not in seen]\n            for link in rule.process_links(links):\n                seen.add(link)\n                request = self._build_request(rule_index, link)\n                yield rule.process_request(request, response)", "is_method": true, "class_name": "CrawlSpider", "function_description": "For a CrawlSpider, this method processes an HTML response to identify and generate subsequent web requests to follow. It applies configured link extraction and processing rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_callback", "line_number": 104, "body": "def _callback(self, response):\n        rule = self._rules[response.meta['rule']]\n        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)", "is_method": true, "class_name": "CrawlSpider", "function_description": "This method processes a web response within the `CrawlSpider` by identifying the relevant parsing rule. It then dispatches the response to an internal handler for execution of the rule's callback."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_errback", "line_number": 108, "body": "def _errback(self, failure):\n        rule = self._rules[failure.request.meta['rule']]\n        return self._handle_failure(failure, rule.errback)", "is_method": true, "class_name": "CrawlSpider", "function_description": "Handles errors from failed requests by routing the `failure` object to the appropriate error callback defined by the associated crawling rule."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_parse_response", "line_number": 112, "body": "def _parse_response(self, response, callback, cb_kwargs, follow=True):\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            cb_res = self.process_results(response, cb_res)\n            for request_or_item in iterate_spider_output(cb_res):\n                yield request_or_item\n\n        if follow and self._follow_links:\n            for request_or_item in self._requests_to_follow(response):\n                yield request_or_item", "is_method": true, "class_name": "CrawlSpider", "function_description": "Core method for a `CrawlSpider` that processes a web response. It applies a callback to extract data or new requests, and optionally generates further crawl requests by following links."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_handle_failure", "line_number": 123, "body": "def _handle_failure(self, failure, errback):\n        if errback:\n            results = errback(failure) or ()\n            for request_or_item in iterate_spider_output(results):\n                yield request_or_item", "is_method": true, "class_name": "CrawlSpider", "function_description": "Provides a mechanism to handle crawl failures by invoking a user-defined error callback. It allows the spider to process errors and generate subsequent requests or items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_compile_rules", "line_number": 129, "body": "def _compile_rules(self):\n        self._rules = []\n        for rule in self.rules:\n            self._rules.append(copy.copy(rule))\n            self._rules[-1]._compile(self)", "is_method": true, "class_name": "CrawlSpider", "function_description": "This method initializes and compiles the spider's defined crawling rules. It prepares each rule for execution by linking it to the spider instance, making them ready for use during a crawl."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "from_crawler", "line_number": 136, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n        return spider", "is_method": true, "class_name": "CrawlSpider", "function_description": "Initializes a `CrawlSpider` instance, configuring its link-following behavior based on the crawler's settings. It enables or disables automatic link extraction and following."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "process_results", "line_number": 28, "body": "def process_results(self, response, results):\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (items or requests).\n        \"\"\"\n        return results", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "This method provides a customizable final processing step for extracted items or requests from an XML feed. It allows preparing data, such as setting GUIDs, before passing results to the web scraping framework's core."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_nodes", "line_number": 51, "body": "def parse_nodes(self, response, nodes):\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either an item, a request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            for result_item in self.process_results(response, ret):\n                yield result_item", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "This mandatory method for `XMLFeedSpider` processes a collection of XML nodes from a response. It orchestrates individual node parsing and result post-processing, yielding extracted items or requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_parse", "line_number": 64, "body": "def _parse(self, response, **kwargs):\n        if not hasattr(self, 'parse_node'):\n            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n\n        response = self.adapt_response(response)\n        if self.iterator == 'iternodes':\n            nodes = self._iternodes(response)\n        elif self.iterator == 'xml':\n            selector = Selector(response, type='xml')\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f'//{self.itertag}')\n        elif self.iterator == 'html':\n            selector = Selector(response, type='html')\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f'//{self.itertag}')\n        else:\n            raise NotSupported('Unsupported node iterator')\n\n        return self.parse_nodes(response, nodes)", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "This internal method of XMLFeedSpider extracts structured data nodes from a web response. It uses configured iteration strategies (like XPath) to prepare these nodes for further processing by `parse_nodes`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_iternodes", "line_number": 84, "body": "def _iternodes(self, response):\n        for node in xmliter(response, self.itertag):\n            self._register_namespaces(node)\n            yield node", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Provides an iterable of specific XML nodes from a response, ensuring namespaces are registered for each node for correct processing by the spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_register_namespaces", "line_number": 89, "body": "def _register_namespaces(self, selector):\n        for (prefix, uri) in self.namespaces:\n            selector.register_namespace(prefix, uri)", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "The method registers predefined XML namespaces with a given selector. This enables the selector to correctly interpret and navigate namespaced XML content from feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_rows", "line_number": 119, "body": "def parse_rows(self, response):\n        \"\"\"Receives a response and a dict (representing each row) with a key for\n        each provided (or detected) header of the CSV file.  This spider also\n        gives the opportunity to override adapt_response and\n        process_results methods for pre and post-processing purposes.\n        \"\"\"\n\n        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n            ret = iterate_spider_output(self.parse_row(response, row))\n            for result_item in self.process_results(response, ret):\n                yield result_item", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "The core method for `CSVFeedSpider` that iterates and processes rows from a CSV response. It orchestrates row-level parsing and result post-processing, enabling flexible data extraction."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_parse", "line_number": 131, "body": "def _parse(self, response, **kwargs):\n        if not hasattr(self, 'parse_row'):\n            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n        response = self.adapt_response(response)\n        return self.parse_rows(response)", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "The `_parse` method of `CSVFeedSpider` orchestrates the parsing of a CSV response. It adapts the incoming data and then processes it row by row, ensuring the necessary `parse_row` method is configured."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "logger", "line_number": 35, "body": "def logger(self):\n        logger = logging.getLogger(self.name)\n        return logging.LoggerAdapter(logger, {'spider': self})", "is_method": true, "class_name": "Spider", "function_description": "Provides a specialized logger for the spider instance. It automatically adds spider-specific context to log messages, simplifying debugging and traceability."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "log", "line_number": 39, "body": "def log(self, message, level=logging.DEBUG, **kw):\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)", "is_method": true, "class_name": "Spider", "function_description": "Provides a unified logging mechanism for the Spider, enabling it to record messages at specified severity levels for debugging and operational insights."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "from_crawler", "line_number": 49, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider", "is_method": true, "class_name": "Spider", "function_description": "This class method constructs a Spider instance, linking it to the provided Crawler. It ensures the spider is properly initialized within the crawling framework's context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "_set_crawler", "line_number": 54, "body": "def _set_crawler(self, crawler):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)", "is_method": true, "class_name": "Spider", "function_description": "Configures the spider by associating it with its crawling engine and settings. It also connects the spider's closure method to the crawler's shutdown signal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "start_requests", "line_number": 59, "body": "def start_requests(self):\n        cls = self.__class__\n        if not self.start_urls and hasattr(self, 'start_url'):\n            raise AttributeError(\n                \"Crawling could not start: 'start_urls' not found \"\n                \"or empty (but found 'start_url' attribute instead, \"\n                \"did you miss an 's'?)\")\n        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n            warnings.warn(\n                \"Spider.make_requests_from_url method is deprecated; it \"\n                \"won't be called in future Scrapy releases. Please \"\n                \"override Spider.start_requests method instead \"\n                f\"(see {cls.__module__}.{cls.__name__}).\",\n            )\n            for url in self.start_urls:\n                yield self.make_requests_from_url(url)\n        else:\n            for url in self.start_urls:\n                yield Request(url, dont_filter=True)", "is_method": true, "class_name": "Spider", "function_description": "Generates the initial `Request` objects for the spider, defining the starting URLs for the crawling process. It serves as the entry point for the spider's web scraping activity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "_parse", "line_number": 89, "body": "def _parse(self, response, **kwargs):\n        return self.parse(response, **kwargs)", "is_method": true, "class_name": "Spider", "function_description": "This internal method serves as an entry point for the Scrapy engine to invoke the `parse` method, initiating the spider's response processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "update_settings", "line_number": 96, "body": "def update_settings(cls, settings):\n        settings.setdict(cls.custom_settings or {}, priority='spider')", "is_method": true, "class_name": "Spider", "function_description": "Configures a spider by applying its `custom_settings` to the provided settings object. These settings are given 'spider' priority, ensuring specific overrides for the spider's operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "handles_request", "line_number": 100, "body": "def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)", "is_method": true, "class_name": "Spider", "function_description": "Verifies if a given web request's URL falls within this Spider's defined scope. This determines if the Spider should process the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "close", "line_number": 104, "body": "def close(spider, reason):\n        closed = getattr(spider, 'closed', None)\n        if callable(closed):\n            return closed(reason)", "is_method": true, "class_name": "Spider", "function_description": "Executes the spider's custom 'closed' callback, if defined, allowing it to perform cleanup or finalization tasks based on the shutdown reason."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "__str__", "line_number": 109, "body": "def __str__(self):\n        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"", "is_method": true, "class_name": "Spider", "function_description": "Provides a human-readable string representation of a Spider object, including its name and memory address. This is primarily used for debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "regex", "line_number": 87, "body": "def regex(x):\n    if isinstance(x, str):\n        return re.compile(x)\n    return x", "is_method": false, "function_description": "This utility converts a string into a compiled regular expression object. If the input is already a regex pattern, it is returned as is, ensuring normalized patterns for further use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "iterloc", "line_number": 93, "body": "def iterloc(it, alt=False):\n    for d in it:\n        yield d['loc']\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and 'alternate' in d:\n            yield from d['alternate']", "is_method": false, "function_description": "Iterates through structured data, extracting and yielding primary 'loc' values from each item. It can optionally yield 'alternate' URLs, providing a comprehensive stream of link addresses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "start_requests", "line_number": 29, "body": "def start_requests(self):\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)", "is_method": true, "class_name": "SitemapSpider", "function_description": "This method initiates the web crawling process for the SitemapSpider. It generates the initial requests for all configured sitemap URLs, which will then be parsed to discover pages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "sitemap_filter", "line_number": 33, "body": "def sitemap_filter(self, entries):\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        for entry in entries:\n            yield entry", "is_method": true, "class_name": "SitemapSpider", "function_description": "This method provides a customizable hook for filtering sitemap entries before processing. It allows developers to define rules based on entry attributes like last modification date."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "_parse_sitemap", "line_number": 41, "body": "def _parse_sitemap(self, response):\n        if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n                               {'response': response}, extra={'spider': self})\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == 'sitemapindex':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == 'urlset':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break", "is_method": true, "class_name": "SitemapSpider", "function_description": "This method recursively parses sitemap and `robots.txt` responses. It discovers other sitemaps or extracts and queues URLs for crawling based on defined patterns."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "_get_sitemap_body", "line_number": 66, "body": "def _get_sitemap_body(self, response):\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        elif gzip_magic_number(response):\n            return gunzip(response.body)\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n            return response.body", "is_method": true, "class_name": "SitemapSpider", "function_description": "Extracts and decompresses the sitemap content from an HTTP response, supporting plain XML and gzipped formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "start_requests", "line_number": 8, "body": "def start_requests(self):\n        self._postinit_reqs = super().start_requests()\n        return iterate_spider_output(self.init_request())", "is_method": true, "class_name": "InitSpider", "function_description": "This method initiates the spider's crawling by generating initial requests. It combines requests from the parent class with custom initial requests defined by the `InitSpider`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "initialized", "line_number": 12, "body": "def initialized(self, response=None):\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop('_postinit_reqs')", "is_method": true, "class_name": "InitSpider", "function_description": "This method serves as a callback for the final initialization request, signaling the spider's readiness. It returns a list of requests or tasks to be executed after initialization is complete."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "init_request", "line_number": 18, "body": "def init_request(self):\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()", "is_method": true, "class_name": "InitSpider", "function_description": "Defines the initial request(s) an `InitSpider` must complete to be considered ready for crawling. It uses `self.initialized` to signal the spider's readiness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "from_crawler", "line_number": 13, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n            raise NotConfigured\n        return cls(crawler.stats)", "is_method": true, "class_name": "DownloaderStats", "function_description": "Factory method for DownloaderStats. It instantiates DownloaderStats, providing the crawler's statistics collector, only if downloader statistics are enabled in the crawler's settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_request", "line_number": 18, "body": "def process_request(self, request, spider):\n        self.stats.inc_value('downloader/request_count', spider=spider)\n        self.stats.inc_value(f'downloader/request_method_count/{request.method}', spider=spider)\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)", "is_method": true, "class_name": "DownloaderStats", "function_description": "This method records various statistics for each web request, including total count, breakdown by HTTP method, and request bytes. It provides detailed metrics for monitoring downloader activity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_response", "line_number": 24, "body": "def process_response(self, request, response, spider):\n        self.stats.inc_value('downloader/response_count', spider=spider)\n        self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)\n        reslen = len(response_httprepr(response))\n        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n        return response", "is_method": true, "class_name": "DownloaderStats", "function_description": "This method records various statistics for each processed HTTP response from a downloader. It tracks total responses, counts by status code, and the total bytes transferred."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_exception", "line_number": 31, "body": "def process_exception(self, request, exception, spider):\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value('downloader/exception_count', spider=spider)\n        self.stats.inc_value(f'downloader/exception_type_count/{ex_class}', spider=spider)", "is_method": true, "class_name": "DownloaderStats", "function_description": "Records and categorizes exceptions encountered during the download process. It tracks counts for general and specific exception types to monitor downloader health.\nRecords and categorizes exceptions encountered during the download process. It tracks counts for general and specific exception types to monitor downloader health."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "Initializes the `DownloadTimeoutMiddleware` instance from a crawler's settings. It connects the middleware to the `spider_opened` signal for proper lifecycle management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "spider_opened", "line_number": 21, "body": "def spider_opened(self, spider):\n        self._timeout = getattr(spider, 'download_timeout', self._timeout)", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "This method configures the middleware's download timeout. It sets the timeout based on the `spider`'s `download_timeout` attribute, if provided, otherwise uses its default."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "process_request", "line_number": 24, "body": "def process_request(self, request, spider):\n        if self._timeout:\n            request.meta.setdefault('download_timeout', self._timeout)", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "Configures a default download timeout for outgoing requests, ensuring a time limit is applied if not already set."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "from_crawler", "line_number": 24, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n            raise NotConfigured\n        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n        return cls(auth_encoding)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Initializes the `HttpProxyMiddleware` for a web crawler. It configures the proxy using crawler settings and raises an error if the proxy is not enabled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_basic_auth_header", "line_number": 30, "body": "def _basic_auth_header(self, username, password):\n        user_pass = to_bytes(\n            f'{unquote(username)}:{unquote(password)}',\n            encoding=self.auth_encoding)\n        return base64.b64encode(user_pass)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "This HttpProxyMiddleware utility generates the Base64-encoded HTTP Basic Authentication header value from a given username and password. It prepares credentials for proxy authentication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_get_proxy", "line_number": 36, "body": "def _get_proxy(self, url, orig_type):\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Parses a given proxy URL to extract connection details and generate basic authentication headers. It provides the formatted proxy configuration needed for outgoing HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "process_request", "line_number": 47, "body": "def process_request(self, request, spider):\n        # ignore if proxy is already set\n        if 'proxy' in request.meta:\n            if request.meta['proxy'] is None:\n                return\n            # extract credentials if present\n            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n            request.meta['proxy'] = proxy_url\n            if creds and not request.headers.get('Proxy-Authorization'):\n                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n            return\n        elif not self.proxies:\n            return\n\n        parsed = urlparse_cached(request)\n        scheme = parsed.scheme\n\n        # 'no_proxy' is only supported by http schemes\n        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n            return\n\n        if scheme in self.proxies:\n            self._set_proxy(request, scheme)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "This middleware method applies an HTTP or HTTPS proxy to an outgoing web request. It configures the proxy based on rules, handling pre-set proxies, bypasses, and authentication for the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_set_proxy", "line_number": 71, "body": "def _set_proxy(self, request, scheme):\n        creds, proxy = self.proxies[scheme]\n        request.meta['proxy'] = proxy\n        if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "This method configures a given HTTP request to route through a specified proxy server. It sets the proxy address and includes authentication headers if credentials are provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "from_crawler", "line_number": 26, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Provides a factory method for the middleware, initializing it with settings obtained from the provided crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "_redirect", "line_number": 29, "body": "def _redirect(self, redirected, request, spider, reason):\n        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n        redirects = request.meta.get('redirect_times', 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta['redirect_times'] = redirects\n            redirected.meta['redirect_ttl'] = ttl - 1\n            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + [request.url]\n            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + [reason]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                         {'reason': reason, 'redirected': redirected, 'request': request},\n                         extra={'spider': spider})\n            return redirected\n        else:\n            logger.debug(\"Discarding %(request)s: max redirections reached\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest(\"max redirections reached\")", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Manages web request redirections by updating the new request's metadata and enforcing a maximum redirect limit. It prevents infinite loops by stopping redirection chains once the threshold is met."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "_redirect_request_using_get", "line_number": 49, "body": "def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Transforms an incoming request into a GET request for a specified redirect URL. It clears the request body and relevant headers, preparing it for a redirect."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "process_response", "line_number": 62, "body": "def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n            or response.status in request.meta.get('handle_httpstatus_list', [])\n            or request.meta.get('handle_httpstatus_all', False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)", "is_method": true, "class_name": "RedirectMiddleware", "function_description": "Provides a core service for `RedirectMiddleware` by processing HTTP responses to automatically follow redirect URLs, unless configured otherwise. This ensures proper navigation for web crawlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "process_response", "line_number": 99, "body": "def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or request.method == 'HEAD'\n            or not isinstance(response, HtmlResponse)\n        ):\n            return response\n\n        interval, url = get_meta_refresh(response,\n                                         ignore_tags=self._ignore_tags)\n        if url and interval < self._maxdelay:\n            redirected = self._redirect_request_using_get(request, url)\n            return self._redirect(redirected, request, spider, 'meta refresh')\n\n        return response", "is_method": true, "class_name": "MetaRefreshMiddleware", "function_description": "This middleware method processes HTML responses to detect and handle meta refresh directives. If a valid meta refresh is found, it generates a new request to follow the redirection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "from_crawler", "line_number": 13, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.settings['USER_AGENT'])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Configures and initializes the UserAgentMiddleware for a crawler. It retrieves the user agent from settings and connects the middleware to the spider_opened signal for proper operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "spider_opened", "line_number": 18, "body": "def spider_opened(self, spider):\n        self.user_agent = getattr(spider, 'user_agent', self.user_agent)", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Configures the middleware's user agent upon spider initialization. It uses the spider's specified user agent if available, otherwise retains its own."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "process_request", "line_number": 21, "body": "def process_request(self, request, spider):\n        if self.user_agent:\n            request.headers.setdefault(b'User-Agent', self.user_agent)", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Sets the User-Agent header for outgoing web requests if configured. This identifies the client making the request, useful for web scraping or API interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "get_retry_request", "line_number": 38, "body": "def get_retry_request(\n    request: Request,\n    *,\n    spider: Spider,\n    reason: Union[str, Exception] = 'unspecified',\n    max_retry_times: Optional[int] = None,\n    priority_adjust: Optional[int] = None,\n    logger: Logger = retry_logger,\n    stats_base_key: str = 'retry',\n):\n    \"\"\"\n    Returns a new :class:`~scrapy.Request` object to retry the specified\n    request, or ``None`` if retries of the specified request have been\n    exhausted.\n\n    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n    follows::\n\n        def parse(self, response):\n            if not response.text:\n                new_request_or_none = get_retry_request(\n                    response.request,\n                    spider=self,\n                    reason='empty',\n                )\n                return new_request_or_none\n\n    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n    retry request. It is used to access the :ref:`settings <topics-settings>`\n    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n    :func:`logging.debug`).\n\n    *reason* is a string or an :class:`Exception` object that indicates the\n    reason why the request needs to be retried. It is used to name retry stats.\n\n    *max_retry_times* is a number that determines the maximum number of times\n    that *request* can be retried. If not specified or ``None``, the number is\n    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n    is read from the :setting:`RETRY_TIMES` setting.\n\n    *priority_adjust* is a number that determines how the priority of the new\n    request changes in relation to *request*. If not specified, the number is\n    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n\n    *logger* is the logging.Logger object to be used when logging messages\n\n    *stats_base_key* is a string to be used as the base key for the\n    retry-related job stats\n    \"\"\"\n    settings = spider.crawler.settings\n    stats = spider.crawler.stats\n    retry_times = request.meta.get('retry_times', 0) + 1\n    if max_retry_times is None:\n        max_retry_times = request.meta.get('max_retry_times')\n        if max_retry_times is None:\n            max_retry_times = settings.getint('RETRY_TIMES')\n    if retry_times <= max_retry_times:\n        logger.debug(\n            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n            {'request': request, 'retry_times': retry_times, 'reason': reason},\n            extra={'spider': spider}\n        )\n        new_request = request.copy()\n        new_request.meta['retry_times'] = retry_times\n        new_request.dont_filter = True\n        if priority_adjust is None:\n            priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n        new_request.priority = request.priority + priority_adjust\n\n        if callable(reason):\n            reason = reason()\n        if isinstance(reason, Exception):\n            reason = global_object_name(reason.__class__)\n\n        stats.inc_value(f'{stats_base_key}/count')\n        stats.inc_value(f'{stats_base_key}/reason_count/{reason}')\n        return new_request\n    else:\n        stats.inc_value(f'{stats_base_key}/max_reached')\n        logger.error(\n            \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n            \"%(reason)s\",\n            {'request': request, 'retry_times': retry_times, 'reason': reason},\n            extra={'spider': spider},\n        )\n        return None", "is_method": false, "function_description": "Generates a new request object to retry a previously failed web request, managing retry counts and priority. It returns None when the maximum retry limit for the request has been exhausted."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "from_crawler", "line_number": 144, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "RetryMiddleware", "function_description": "This class method creates and initializes a `RetryMiddleware` instance. It configures the middleware using settings obtained directly from a given crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "process_response", "line_number": 147, "body": "def process_response(self, request, response, spider):\n        if request.meta.get('dont_retry', False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response", "is_method": true, "class_name": "RetryMiddleware", "function_description": "The method determines if a web response needs to be retried based on its HTTP status code. It initiates a retry attempt for the original request if necessary."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "process_exception", "line_number": 155, "body": "def process_exception(self, request, exception, spider):\n        if (\n            isinstance(exception, self.EXCEPTIONS_TO_RETRY)\n            and not request.meta.get('dont_retry', False)\n        ):\n            return self._retry(request, exception, spider)", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Identifies specific exceptions during request processing. If the exception is retryable and not explicitly disabled, it triggers a retry for the failed request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "_retry", "line_number": 162, "body": "def _retry(self, request, reason, spider):\n        max_retry_times = request.meta.get('max_retry_times', self.max_retry_times)\n        priority_adjust = request.meta.get('priority_adjust', self.priority_adjust)\n        return get_retry_request(\n            request,\n            reason=reason,\n            spider=spider,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Provides the core logic for generating a new request that can be retried. It customizes retry parameters based on the original request's metadata or middleware settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_tar", "line_number": 31, "body": "def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "This method of `DecompressionMiddleware` checks if a response body is a tar archive. If so, it decompresses and extracts the first contained file, returning a new response with the extracted content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_zip", "line_number": 42, "body": "def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "This method checks if a response body is a ZIP archive. If so, it decompresses the first file and returns a new response with the extracted content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_gzip", "line_number": 54, "body": "def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "It attempts to decompress a Gzip-encoded response body. If successful, it provides the decompressed content and an updated response, otherwise it returns nothing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_bzip2", "line_number": 64, "body": "def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Attempts to decompress a response body using Bzip2. If successful, it returns a new response with the decompressed content, enabling the middleware to handle Bzip2 encoded responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "process_response", "line_number": 73, "body": "def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in self._formats.items():\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "This method of `DecompressionMiddleware` decompresses the HTTP response body if it's compressed. It provides a readable, uncompressed response for subsequent processing stages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COOKIES_ENABLED'):\n            raise NotConfigured\n        return cls(crawler.settings.getbool('COOKIES_DEBUG'))", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Initializes the CookiesMiddleware instance, ensuring it's only created if cookie handling is enabled in the crawler's settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "process_request", "line_number": 26, "body": "def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        for cookie in self._get_request_cookies(jar, request):\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "This method processes an outgoing web request by updating its associated cookie jar. It ensures the request includes the correct `Cookie` header for proper session management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "process_response", "line_number": 40, "body": "def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Extracts cookies from HTTP responses and stores them in the associated cookie jar. This enables consistent session management for subsequent requests within a web scraping or request pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_debug_cookie", "line_number": 52, "body": "def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = [to_unicode(c, errors='replace')\n                  for c in request.headers.getlist('Cookie')]\n            if cl:\n                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                logger.debug(msg, extra={'spider': spider})", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "This method, within `CookiesMiddleware`, provides debugging information by conditionally logging outgoing cookie headers for a request. It helps to inspect which cookies are being sent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_debug_set_cookie", "line_number": 61, "body": "def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = [to_unicode(c, errors='replace')\n                  for c in response.headers.getlist('Set-Cookie')]\n            if cl:\n                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                logger.debug(msg, extra={'spider': spider})", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Logs received `Set-Cookie` headers from a response for debugging purposes when the middleware's debug mode is enabled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_format_cookie", "line_number": 70, "body": "def _format_cookie(self, cookie, request):\n        \"\"\"\n        Given a dict consisting of cookie components, return its string representation.\n        Decode from bytes if necessary.\n        \"\"\"\n        decoded = {}\n        for key in (\"name\", \"value\", \"path\", \"domain\"):\n            if cookie.get(key) is None:\n                if key in (\"name\", \"value\"):\n                    msg = \"Invalid cookie found in request {}: {} ('{}' is missing)\"\n                    logger.warning(msg.format(request, cookie, key))\n                    return\n                continue\n            if isinstance(cookie[key], str):\n                decoded[key] = cookie[key]\n            else:\n                try:\n                    decoded[key] = cookie[key].decode(\"utf8\")\n                except UnicodeDecodeError:\n                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n                                   request, cookie)\n                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n\n        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n        for key, value in decoded.items():  # path, domain\n            cookie_str += f\"; {key.capitalize()}={value}\"\n        return cookie_str", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Converts a cookie dictionary into its standard HTTP string format, decoding byte values and validating essential components for proper HTTP header inclusion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_get_request_cookies", "line_number": 98, "body": "def _get_request_cookies(self, jar, request):\n        \"\"\"\n        Extract cookies from the Request.cookies attribute\n        \"\"\"\n        if not request.cookies:\n            return []\n        elif isinstance(request.cookies, dict):\n            cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n        else:\n            cookies = request.cookies\n        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n        response = Response(request.url, headers={\"Set-Cookie\": formatted})\n        return jar.make_cookies(response, request)", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "This internal method extracts and formats cookies from a request object. It prepares these cookies for processing and management by a cookie jar within the middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "from_crawler", "line_number": 35, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "A class method that acts as a factory, instantiating the middleware with access to the crawler object. This enables the middleware to be properly configured within the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "process_request", "line_number": 38, "body": "def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        d = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "This middleware method processes incoming web requests to enforce `robots.txt` crawling rules. It asynchronously checks permissions, allowing a bypass if explicitly configured."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "process_request_2", "line_number": 45, "body": "def process_request_2(self, rp, request, spider):\n        if rp is None:\n            return\n\n        useragent = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b'User-Agent', self._default_useragent)\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            self.crawler.stats.inc_value('robotstxt/forbidden')\n            raise IgnoreRequest(\"Forbidden by robots.txt\")", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "The `RobotsTxtMiddleware` method validates if a web request's URL is permitted by robots.txt rules for its user agent. It prevents forbidden requests, enforcing crawling etiquette."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "robot_parser", "line_number": 58, "body": "def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc, spider)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n            self.crawler.stats.inc_value('robotstxt/request_count')\n\n        if isinstance(self._parsers[netloc], Deferred):\n            d = Deferred()\n\n            def cb(result):\n                d.callback(result)\n                return result\n            self._parsers[netloc].addCallback(cb)\n            return d\n        else:\n            return self._parsers[netloc]", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Initiates and manages the asynchronous retrieval and parsing of a domain's `robots.txt` file. It ensures the file is fetched only once, caching the result to determine crawling permissions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_logerror", "line_number": 87, "body": "def _logerror(self, failure, request, spider):\n        if failure.type is not IgnoreRequest:\n            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                         {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "This method logs significant download errors for a web crawling request, excluding those intentionally ignored. It centralizes error reporting for unhandled request failures within the middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_parse_robots", "line_number": 95, "body": "def _parse_robots(self, response, netloc, spider):\n        self.crawler.stats.inc_value('robotstxt/response_count')\n        self.crawler.stats.inc_value(f'robotstxt/response_status_count/{response.status}')\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Parses `robots.txt` content from a web response and stores the resulting crawling rules for a specific domain. It updates internal statistics and makes these rules available to the crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_robots_error", "line_number": 103, "body": "def _robots_error(self, failure, netloc):\n        if failure.type is not IgnoreRequest:\n            key = f'robotstxt/exception_count/{failure.type}'\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Handles errors during `robots.txt` parsing for a network location. It logs the exception, cleans up the parser, and signals failure to waiting operations, ensuring graceful recovery."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "cb", "line_number": 79, "body": "def cb(result):\n                d.callback(result)\n                return result", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Executes a specific deferred callback with a provided result, then returns the result to allow for further chained operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "from_crawler", "line_number": 34, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n            raise NotConfigured\n        try:\n            return cls(stats=crawler.stats)\n        except TypeError:\n            warnings.warn(\n                \"HttpCompressionMiddleware subclasses must either modify \"\n                \"their '__init__' method to support a 'stats' parameter or \"\n                \"reimplement the 'from_crawler' method.\",\n                ScrapyDeprecationWarning,\n            )\n            result = cls()\n            result.stats = crawler.stats\n            return result", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Provides a factory method to instantiate the HTTP compression middleware. It only creates an instance if compression is enabled in the crawler settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "process_request", "line_number": 50, "body": "def process_request(self, request, spider):\n        request.headers.setdefault('Accept-Encoding',\n                                   b\", \".join(ACCEPTED_ENCODINGS))", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Configures outgoing HTTP requests by adding the 'Accept-Encoding' header, informing servers about supported compression methods for efficient data transfer."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "process_response", "line_number": 54, "body": "def process_response(self, request, response, spider):\n\n        if request.method == 'HEAD':\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist('Content-Encoding')\n            if content_encoding:\n                encoding = content_encoding.pop()\n                decoded_body = self._decode(response.body, encoding.lower())\n                if self.stats:\n                    self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n                    self.stats.inc_value('httpcompression/response_count', spider=spider)\n                respcls = responsetypes.from_args(\n                    headers=response.headers, url=response.url, body=decoded_body\n                )\n                kwargs = dict(cls=respcls, body=decoded_body)\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs['encoding'] = None\n                response = response.replace(**kwargs)\n                if not content_encoding:\n                    del response.headers['Content-Encoding']\n\n        return response", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Provides the capability to decompress HTTP response bodies. It decodes content based on the 'Content-Encoding' header, ensuring downstream components receive uncompressed data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "_decode", "line_number": 80, "body": "def _decode(self, body, encoding):\n        if encoding == b'gzip' or encoding == b'x-gzip':\n            body = gunzip(body)\n\n        if encoding == b'deflate':\n            try:\n                body = zlib.decompress(body)\n            except zlib.error:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                body = zlib.decompress(body, -15)\n        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n            body = brotli.decompress(body)\n        if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:\n            # Using its streaming API since its simple API could handle only cases\n            # where there is content size data embedded in the frame\n            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n            body = reader.read()\n        return body", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "A private method of `HttpCompressionMiddleware`, it decompresses an HTTP response body based on its compression encoding (gzip, deflate, brotli, zstd), making the original content available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/defaultheaders.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n        return cls(headers.items())", "is_method": true, "class_name": "DefaultHeadersMiddleware", "function_description": "Factory method for `DefaultHeadersMiddleware` that initializes the instance by extracting default request headers from the provided crawler's settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/defaultheaders.py", "function": "process_request", "line_number": 20, "body": "def process_request(self, request, spider):\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)", "is_method": true, "class_name": "DefaultHeadersMiddleware", "function_description": "Applies default HTTP headers to an outgoing request, adding them only if they are not already present."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "from_crawler", "line_number": 46, "body": "def from_crawler(cls: Type[HttpCacheMiddlewareTV], crawler: Crawler) -> HttpCacheMiddlewareTV:\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "This class method initializes and integrates the HTTP cache middleware with a web crawler. It connects the middleware's lifecycle events to crawler signals for proper operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "spider_opened", "line_number": 52, "body": "def spider_opened(self, spider: Spider) -> None:\n        self.storage.open_spider(spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Prepares the HTTP cache storage component when a new web spider is opened, enabling it to manage cached responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "spider_closed", "line_number": 55, "body": "def spider_closed(self, spider: Spider) -> None:\n        self.storage.close_spider(spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Notifies the cache storage that a web spider has completed its operation. This allows the caching system to perform necessary cleanup or finalization tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_request", "line_number": 58, "body": "def process_request(self, request: Request, spider: Spider) -> Optional[Response]:\n        if request.meta.get('dont_cache', False):\n            return None\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta['_dont_cache'] = True  # flag as uncacheable\n            return None\n\n        # Look for cached response and check if expired\n        cachedresponse = self.storage.retrieve_response(spider, request)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/miss', spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value('httpcache/ignore', spider=spider)\n                raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n            return None  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append('cached')\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value('httpcache/hit', spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta['cached_response'] = cachedresponse\n\n        return None", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "This `HttpCacheMiddleware` method retrieves and validates cached responses for incoming requests. It optimizes performance by returning a fresh cached response if available, avoiding redundant network fetches."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_response", "line_number": 88, "body": "def process_response(self, request: Request, response: Response, spider: Spider) -> Response:\n        if request.meta.get('dont_cache', False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if 'cached' in response.flags or '_dont_cache' in request.meta:\n            request.meta.pop('_dont_cache', None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if 'Date' not in response.headers:\n            response.headers['Date'] = formatdate(usegmt=True)\n\n        # Do not validate first-hand responses\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/firsthand', spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value('httpcache/revalidate', spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value('httpcache/invalidate', spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Manages HTTP response caching within the middleware. It decides whether to store new responses, serve cached ones, or revalidate them against the server based on defined policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_exception", "line_number": 117, "body": "def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Optional[Response]:\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n            return cachedresponse\n        return None", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Recovers from download exceptions by serving a cached response for the `HttpCacheMiddleware`. This prevents request failures when a valid cached entry exists."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "_cache_response", "line_number": 126, "body": "def _cache_response(\n        self, spider: Spider, response: Response, request: Request, cachedresponse: Optional[Response]\n    ) -> None:\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value('httpcache/store', spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value('httpcache/uncacheable', spider=spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "This method evaluates if an HTTP response should be cached based on policy. If cacheable, it stores the response and updates caching statistics for the spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "_has_ajaxcrawlable_meta", "line_number": 70, "body": "def _has_ajaxcrawlable_meta(text):\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if 'fragment' not in text:\n        return False\n    if 'content' not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None", "is_method": false, "function_description": "Checks an HTML string for the presence of the `meta name=\"fragment\" content=\"!\"` tag. This determines if the page adheres to older AJAX crawlability specifications for search engines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "from_crawler", "line_number": 30, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "Initializes the `AjaxCrawlMiddleware` instance using the Scrapy project settings retrieved from the crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "process_response", "line_number": 33, "body": "def process_response(self, request, response, spider):\n\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != 'GET':\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if 'ajax_crawlable' in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url + '#!')\n        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n                     extra={'spider': spider})\n\n        ajax_crawl_request.meta['ajax_crawlable'] = True\n        return ajax_crawl_request", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "This middleware method processes HTML responses to detect AJAX-crawlable content. If found, it modifies the request to fetch the AJAX-crawlable URL, enabling the spider to retrieve dynamically loaded page content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "_has_ajax_crawlable_variant", "line_number": 57, "body": "def _has_ajax_crawlable_variant(self, response):\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[:self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "Checks if a web page is AJAX crawlable based on Google's guidelines. This method helps the `AjaxCrawlMiddleware` identify pages that can be indexed without hash fragments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Provides an initialized `HttpAuthMiddleware` instance, connecting its `spider_opened` method to the crawler's signal system. This integrates the middleware, enabling it to react when a spider is opened."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "spider_opened", "line_number": 26, "body": "def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            if not hasattr(spider, 'http_auth_domain'):\n                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n                              'problems if the spider makes requests to several different domains. http_auth_domain '\n                              'will be set to the domain of the first request, please set it to the correct value '\n                              'explicitly.',\n                              category=ScrapyDeprecationWarning)\n                self.domain_unset = True\n            else:\n                self.domain = spider.http_auth_domain\n                self.domain_unset = False", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Initializes HTTP Basic Authentication for a spider upon startup. It configures the authentication header using spider credentials and manages the associated domain for secure application."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "process_request", "line_number": 42, "body": "def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            domain = urlparse_cached(request).hostname\n            if self.domain_unset:\n                self.domain = domain\n                self.domain_unset = False\n            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                request.headers[b'Authorization'] = auth", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Injects an `Authorization` header into outgoing HTTP requests when it's missing. This applies to requests targeting a specific domain, possibly inferred from the first request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "get_settings_priority", "line_number": 19, "body": "def get_settings_priority(priority):\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, str):\n        return SETTINGS_PRIORITIES[priority]\n    else:\n        return priority", "is_method": false, "function_description": "Converts string-based priority levels to their corresponding numerical values or returns numerical priorities directly. This standardizes priority values for internal processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "iter_default_settings", "line_number": 453, "body": "def iter_default_settings():\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)", "is_method": false, "function_description": "Provides an iterator over the default settings, yielding each setting's name and its corresponding value. This allows easy access to predefined configuration parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "overridden_settings", "line_number": 460, "body": "def overridden_settings(settings):\n    \"\"\"Return a dict of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value", "is_method": false, "function_description": "This function identifies and yields configuration settings that have been explicitly changed from their default values. It's useful for understanding what custom settings are in effect."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "set", "line_number": 46, "body": "def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority", "is_method": true, "class_name": "SettingsAttribute", "function_description": "Updates the attribute's value and priority if the provided priority is higher or equal to the current setting's priority. This ensures a robust, priority-based configuration system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__str__", "line_number": 54, "body": "def __str__(self):\n        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"", "is_method": true, "class_name": "SettingsAttribute", "function_description": "Provides a user-friendly string representation of a `SettingsAttribute` object, displaying its value and priority for debugging or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__getitem__", "line_number": 88, "body": "def __getitem__(self, opt_name):\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides dictionary-like access to retrieve a setting's value by name. It returns `None` if the specified setting is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__contains__", "line_number": 93, "body": "def __contains__(self, name):\n        return name in self.attributes", "is_method": true, "class_name": "BaseSettings", "function_description": "Checks if a specific attribute name is recognized by or present within the settings object. It enables convenient checking using the `in` operator."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "get", "line_number": 96, "body": "def get(self, name, default=None):\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return self[name] if self[name] is not None else default", "is_method": true, "class_name": "BaseSettings", "function_description": "Retrieves a setting value by name from the configuration. If the setting is not found or its value is `None`, it provides a specified default."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getbool", "line_number": 108, "body": "def getbool(self, name, default=False):\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\"Supported values for boolean settings \"\n                             \"are 0/1, True/False, '0'/'1', \"\n                             \"'True'/'False' and 'true'/'false'\")", "is_method": true, "class_name": "BaseSettings", "function_description": "This method retrieves a setting value and converts it to a boolean. It consistently interprets various string and numeric forms (e.g., '1', 'True', '0', 'False') as True or False."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getint", "line_number": 136, "body": "def getint(self, name, default=0):\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return int(self.get(name, default))", "is_method": true, "class_name": "BaseSettings", "function_description": "Retrieves a configuration setting from the `BaseSettings` object, returning its value as an integer. It provides a default if the setting is not found, ensuring type safety."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getfloat", "line_number": 148, "body": "def getfloat(self, name, default=0.0):\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return float(self.get(name, default))", "is_method": true, "class_name": "BaseSettings", "function_description": "Retrieves a named setting from the `BaseSettings` object and casts its value to a float. It provides a default float if the setting is not found, ensuring safe access to numerical configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getlist", "line_number": 160, "body": "def getlist(self, name, default=None):\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, str):\n            value = value.split(',')\n        return list(value)", "is_method": true, "class_name": "BaseSettings", "function_description": "This method retrieves a configuration setting by name, ensuring its value is returned as a list. It automatically splits comma-separated strings into list items, providing a consistent format for settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getdict", "line_number": 179, "body": "def getdict(self, name, default=None):\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, str):\n            value = json.loads(value)\n        return dict(value)", "is_method": true, "class_name": "BaseSettings", "function_description": "Retrieves a setting value and converts it into a dictionary. It automatically handles JSON string parsing and conversion of nested `BaseSettings` instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getwithbase", "line_number": 200, "body": "def getwithbase(self, name):\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: str\n        \"\"\"\n        compbs = BaseSettings()\n        compbs.update(self[name + '_BASE'])\n        compbs.update(self[name])\n        return compbs", "is_method": true, "class_name": "BaseSettings", "function_description": "This BaseSettings method merges a dictionary-like setting with its `_BASE` counterpart. It creates a composed view where specific settings override base values, useful for hierarchical configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getpriority", "line_number": 212, "body": "def getpriority(self, name):\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: str\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority", "is_method": true, "class_name": "BaseSettings", "function_description": "Retrieves the numerical priority value of a specified setting. It returns `None` if the setting does not exist, useful for managing configuration precedence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "maxpriority", "line_number": 224, "body": "def maxpriority(self):\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(self.getpriority(name) for name in self)\n        else:\n            return get_settings_priority('default')", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides the numerical value of the highest priority among all stored settings. Returns a default priority if no settings are currently present."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__setitem__", "line_number": 236, "body": "def __setitem__(self, name, value):\n        self.set(name, value)", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides dictionary-style assignment to update configuration settings. It internally delegates the actual setting operation to the `set` method for controlled management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "set", "line_number": 239, "body": "def set(self, name, value, priority='project'):\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: str\n\n        :param value: the value to associate with the setting\n        :type value: object\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides a service to store or update a named configuration setting within BaseSettings. It incorporates a priority system to manage hierarchical application settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "setdict", "line_number": 267, "body": "def setdict(self, values, priority='project'):\n        self.update(values, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Updates the settings object with a dictionary of new values. It allows for priority-based merging, acting as a convenient alias for the `update` method."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "setmodule", "line_number": 270, "body": "def setmodule(self, module, priority='project'):\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: types.ModuleType or str\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, str):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Loads all uppercase variables from a given module as settings into the `BaseSettings` object. It assigns a specified priority to all these loaded settings for bulk configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "update", "line_number": 292, "body": "def update(self, values, priority='project'):\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, str):\n            values = json.loads(values)\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in values.items():\n                    self.set(name, value, values.getpriority(name))\n            else:\n                for name, value in values.items():\n                    self.set(name, value, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Updates multiple settings within the BaseSettings object. It efficiently stores key-value pairs from various sources (dict, JSON, or other settings) at a specified priority level."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "delete", "line_number": 325, "body": "def delete(self, name, priority='project'):\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= self.getpriority(name):\n            del self.attributes[name]", "is_method": true, "class_name": "BaseSettings", "function_description": "Removes a configuration setting by name from the BaseSettings instance. It only deletes the setting if the specified priority is high enough to override its current priority."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__delitem__", "line_number": 331, "body": "def __delitem__(self, name):\n        self._assert_mutability()\n        del self.attributes[name]", "is_method": true, "class_name": "BaseSettings", "function_description": "Removes a named setting from the BaseSettings object. It ensures the object is mutable before allowing the deletion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_assert_mutability", "line_number": 335, "body": "def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")", "is_method": true, "class_name": "BaseSettings", "function_description": "Ensures that a `BaseSettings` object remains immutable if marked as frozen. It prevents modifications by raising a TypeError upon any alteration attempt."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "copy", "line_number": 339, "body": "def copy(self):\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)", "is_method": true, "class_name": "BaseSettings", "function_description": "Creates a deep copy of the settings object, returning an independent instance. This allows modifications without affecting the original settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "freeze", "line_number": 351, "body": "def freeze(self):\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True", "is_method": true, "class_name": "BaseSettings", "function_description": "This method makes the current settings immutable, preventing any further changes. It provides a mechanism to lock configuration after initialization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "frozencopy", "line_number": 361, "body": "def frozencopy(self):\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides an immutable copy of the settings object. This ensures the returned settings cannot be modified, useful for fixed configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__iter__", "line_number": 371, "body": "def __iter__(self):\n        return iter(self.attributes)", "is_method": true, "class_name": "BaseSettings", "function_description": "Enables iteration over the attributes of a `BaseSettings` object, treating it as a collection. This allows direct looping over the stored settings or configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__len__", "line_number": 374, "body": "def __len__(self):\n        return len(self.attributes)", "is_method": true, "class_name": "BaseSettings", "function_description": "This method allows a `BaseSettings` object to report the total number of attributes it manages. It enables the use of the `len()` function directly on instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_to_dict", "line_number": 377, "body": "def _to_dict(self):\n        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n                for k, v in self.items()}", "is_method": true, "class_name": "BaseSettings", "function_description": "Converts the BaseSettings object and its nested BaseSettings instances into a deep, standard Python dictionary representation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "copy_to_dict", "line_number": 381, "body": "def copy_to_dict(self):\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides a mutable, dictionary representation of the current settings. This copy is independent, useful for debugging, logging, or serialization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_repr_pretty_", "line_number": 397, "body": "def _repr_pretty_(self, p, cycle):\n        if cycle:\n            p.text(repr(self))\n        else:\n            p.text(pformat(self.copy_to_dict()))", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides a pretty-printed, dictionary-like representation for BaseSettings instances. This enhances readability in interactive environments by formatting settings clearly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__len__", "line_number": 411, "body": "def __len__(self):\n        return len(self.o)", "is_method": true, "class_name": "_DictProxy", "function_description": "Enables the `len()` built-in function to return the size of the underlying proxied dictionary. This allows `_DictProxy` instances to behave like a dictionary when queried for their length."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__getitem__", "line_number": 414, "body": "def __getitem__(self, k):\n        return self.o[k]", "is_method": true, "class_name": "_DictProxy", "function_description": "Enables dictionary-like item retrieval using `obj[key]` syntax. It provides direct access to elements of the proxied object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__setitem__", "line_number": 417, "body": "def __setitem__(self, k, v):\n        self.settings.set(k, v, priority=self.priority)\n        self.o[k] = v", "is_method": true, "class_name": "_DictProxy", "function_description": "Enables dictionary-style item assignment, simultaneously updating an internal object and a prioritized settings store."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__delitem__", "line_number": 421, "body": "def __delitem__(self, k):\n        del self.o[k]", "is_method": true, "class_name": "_DictProxy", "function_description": "Enables deletion of key-value pairs from the wrapped dictionary-like object via the proxy interface."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__iter__", "line_number": 424, "body": "def __iter__(self, k, v):\n        return iter(self.o)", "is_method": true, "class_name": "_DictProxy", "function_description": "Makes the `_DictProxy` object iterable. It allows direct iteration over the wrapped internal object `self.o`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "build_storage", "line_number": 33, "body": "def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n    argument_names = get_func_args(builder)\n    if 'feed_options' in argument_names:\n        kwargs['feed_options'] = feed_options\n    else:\n        warnings.warn(\n            \"{} does not support the 'feed_options' keyword argument. Add a \"\n            \"'feed_options' parameter to its signature to remove this \"\n            \"warning. This parameter will become mandatory in a future \"\n            \"version of Scrapy.\"\n            .format(builder.__qualname__),\n            category=ScrapyDeprecationWarning\n        )\n    return builder(*preargs, uri, *args, **kwargs)", "is_method": false, "function_description": "Facilitates the creation of a storage object by invoking a builder. It intelligently passes `feed_options` if supported, otherwise warns about future API changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 56, "body": "def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"", "is_method": true, "class_name": "IFeedStorage", "function_description": "Prepares and opens the storage for a given spider, returning a file-like object suitable for data exporters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 60, "body": "def store(file):\n        \"\"\"Store the given file stream\"\"\"", "is_method": true, "class_name": "IFeedStorage", "function_description": "Enables `IFeedStorage` to save an incoming file stream to persistent storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 67, "body": "def open(self, spider):\n        path = spider.crawler.settings['FEED_TEMPDIR']\n        if path and not os.path.isdir(path):\n            raise OSError('Not a Directory: ' + str(path))\n\n        return NamedTemporaryFile(prefix='feed-', dir=path)", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Provides a named temporary file for feed storage, ensuring the configured temporary directory is valid. This file is ready for subsequent writing operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 74, "body": "def store(self, file):\n        return threads.deferToThread(self._store_in_thread, file)", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Asynchronously stores a file by offloading the operation to a separate thread. This prevents blocking the caller's execution while the file is being stored."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 94, "body": "def open(self, spider):\n        return self._stdout", "is_method": true, "class_name": "StdoutFeedStorage", "function_description": "Provides access to the standard output stream for data storage. It allows a spider to write scraped items directly to stdout."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 109, "body": "def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, self.write_mode)", "is_method": true, "class_name": "FileFeedStorage", "function_description": "Opens a file, ensuring its parent directories exist, and returns the file object. It prepares the file for writing or appending data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 142, "body": "def from_crawler(cls, crawler, uri, *, feed_options=None):\n        return build_storage(\n            cls,\n            uri,\n            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n            feed_options=feed_options,\n        )", "is_method": true, "class_name": "S3FeedStorage", "function_description": "This class method initializes S3 feed storage, automatically configuring it by extracting necessary AWS credentials and ACL settings from the provided crawler's settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 152, "body": "def _store_in_thread(self, file):\n        file.seek(0)\n        kwargs = {'ACL': self.acl} if self.acl else {}\n        self.s3_client.put_object(\n            Bucket=self.bucketname, Key=self.keyname, Body=file,\n            **kwargs)\n        file.close()", "is_method": true, "class_name": "S3FeedStorage", "function_description": "Provides the core functionality to upload a file's content to an Amazon S3 bucket. It ensures data persistence by storing the file under a specified key in the cloud."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 171, "body": "def from_crawler(cls, crawler, uri):\n        return cls(\n            uri,\n            crawler.settings['GCS_PROJECT_ID'],\n            crawler.settings['FEED_STORAGE_GCS_ACL'] or None\n        )", "is_method": true, "class_name": "GCSFeedStorage", "function_description": "A factory method that creates a GCSFeedStorage instance, configuring it with Google Cloud Storage settings from the provided crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 178, "body": "def _store_in_thread(self, file):\n        file.seek(0)\n        from google.cloud.storage import Client\n        client = Client(project=self.project_id)\n        bucket = client.get_bucket(self.bucket_name)\n        blob = bucket.blob(self.blob_name)\n        blob.upload_from_file(file, predefined_acl=self.acl)", "is_method": true, "class_name": "GCSFeedStorage", "function_description": "The method uploads the contents of a file-like object to a specified Google Cloud Storage blob. It ensures the data is stored with predefined access control permissions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 200, "body": "def from_crawler(cls, crawler, uri, *, feed_options=None):\n        return build_storage(\n            cls,\n            uri,\n            crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE'),\n            feed_options=feed_options,\n        )", "is_method": true, "class_name": "FTPFeedStorage", "function_description": "Initializes an FTP feed storage instance, using a crawler's settings and a URI. It streamlines the creation of FTP storage configured for data feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 208, "body": "def _store_in_thread(self, file):\n        ftp_store_file(\n            path=self.path, file=file, host=self.host,\n            port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.use_active_mode,\n            overwrite=self.overwrite,\n        )", "is_method": true, "class_name": "FTPFeedStorage", "function_description": "This private helper method uploads a given file to a configured FTP server. It facilitates efficient, possibly threaded, storage of data feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "start_exporting", "line_number": 232, "body": "def start_exporting(self):\n        if not self._exporting:\n            self.exporter.start_exporting()\n            self._exporting = True", "is_method": true, "class_name": "_FeedSlot", "function_description": "Initiates the data export process for this feed slot. It ensures the export begins only once by delegating the task to its internal exporter."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "finish_exporting", "line_number": 237, "body": "def finish_exporting(self):\n        if self._exporting:\n            self.exporter.finish_exporting()\n            self._exporting = False", "is_method": true, "class_name": "_FeedSlot", "function_description": "Concludes an active data export for this feed slot by instructing its internal exporter to finish. It also resets the slot's export state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 246, "body": "def from_crawler(cls, crawler):\n        exporter = cls(crawler)\n        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n        return exporter", "is_method": true, "class_name": "FeedExporter", "function_description": "Initializes a `FeedExporter` instance, connecting it to the `crawler`'s signals for spider lifecycle and item scraping events. This sets up the exporter to automatically process data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open_spider", "line_number": 289, "body": "def open_spider(self, spider):\n        for uri, feed_options in self.feeds.items():\n            uri_params = self._get_uri_params(spider, feed_options['uri_params'])\n            self.slots.append(self._start_new_batch(\n                batch_id=1,\n                uri=uri % uri_params,\n                feed_options=feed_options,\n                spider=spider,\n                uri_template=uri,\n            ))", "is_method": true, "class_name": "FeedExporter", "function_description": "This method initializes the export process for all configured data feeds when a spider starts. It prepares individual export batches and their URIs for data delivery."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "close_spider", "line_number": 300, "body": "def close_spider(self, spider):\n        deferred_list = []\n        for slot in self.slots:\n            d = self._close_slot(slot, spider)\n            deferred_list.append(d)\n        return defer.DeferredList(deferred_list) if deferred_list else None", "is_method": true, "class_name": "FeedExporter", "function_description": "Manages the asynchronous closing of all associated data export slots when a spider finishes. It ensures proper finalization of all export operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_close_slot", "line_number": 307, "body": "def _close_slot(self, slot, spider):\n        if not slot.itemcount and not slot.store_empty:\n            # We need to call slot.storage.store nonetheless to get the file\n            # properly closed.\n            return defer.maybeDeferred(slot.storage.store, slot.file)\n        slot.finish_exporting()\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': slot.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        d = defer.maybeDeferred(slot.storage.store, slot.file)\n\n        # Use `largs=log_args` to copy log_args into function's scope\n        # instead of using `log_args` from the outer scope\n        d.addCallback(\n            self._handle_store_success, log_args, logfmt, spider, type(slot.storage).__name__\n        )\n        d.addErrback(\n            self._handle_store_error, log_args, logfmt, spider, type(slot.storage).__name__\n        )\n        return d", "is_method": true, "class_name": "FeedExporter", "function_description": "Finalizes and stores the data for a specific export feed slot. It handles success/error logging and ensures proper file closure, even for empty feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_handle_store_error", "line_number": 329, "body": "def _handle_store_error(self, f, largs, logfmt, spider, slot_type):\n        logger.error(\n            logfmt % \"Error storing\", largs,\n            exc_info=failure_to_exc_info(f), extra={'spider': spider}\n        )\n        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")", "is_method": true, "class_name": "FeedExporter", "function_description": "Logs storage errors encountered during feed export operations. It records detailed error information and increments a failure counter for tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_handle_store_success", "line_number": 336, "body": "def _handle_store_success(self, f, largs, logfmt, spider, slot_type):\n        logger.info(\n            logfmt % \"Stored\", largs, extra={'spider': spider}\n        )\n        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")", "is_method": true, "class_name": "FeedExporter", "function_description": "Logs a successful feed export operation and increments the corresponding success counter. It provides feedback and statistics for completed feed storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_start_new_batch", "line_number": 342, "body": "def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n        :param batch_id: sequence number of current batch\n        :param uri: uri of the new batch to start\n        :param feed_options: dict with parameters of feed\n        :param spider: user spider\n        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n        \"\"\"\n        storage = self._get_storage(uri, feed_options)\n        file = storage.open(spider)\n        exporter = self._get_exporter(\n            file=file,\n            format=feed_options['format'],\n            fields_to_export=feed_options['fields'],\n            encoding=feed_options['encoding'],\n            indent=feed_options['indent'],\n            **feed_options['item_export_kwargs'],\n        )\n        slot = _FeedSlot(\n            file=file,\n            exporter=exporter,\n            storage=storage,\n            uri=uri,\n            format=feed_options['format'],\n            store_empty=feed_options['store_empty'],\n            batch_id=batch_id,\n            uri_template=uri_template,\n        )\n        if slot.store_empty:\n            slot.start_exporting()\n        return slot", "is_method": true, "class_name": "FeedExporter", "function_description": "Prepares and initializes a new export batch for the `FeedExporter` by setting up the output file and configuring the data exporter. It returns a `_FeedSlot` object to manage this batch."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "item_scraped", "line_number": 376, "body": "def item_scraped(self, item, spider):\n        slots = []\n        for slot in self.slots:\n            slot.start_exporting()\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n            if (\n                self.feeds[slot.uri_template]['batch_item_count']\n                and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']\n            ):\n                uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                self._close_slot(slot, spider)\n                slots.append(self._start_new_batch(\n                    batch_id=slot.batch_id + 1,\n                    uri=slot.uri_template % uri_params,\n                    feed_options=self.feeds[slot.uri_template],\n                    spider=spider,\n                    uri_template=slot.uri_template,\n                ))\n            else:\n                slots.append(slot)\n        self.slots = slots", "is_method": true, "class_name": "FeedExporter", "function_description": "Receives a scraped item and exports it through all configured feed slots. It also manages batching, initiating new export files when an item count threshold is met for a slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_load_components", "line_number": 400, "body": "def _load_components(self, setting_prefix):\n        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d", "is_method": true, "class_name": "FeedExporter", "function_description": "This internal method dynamically loads and instantiates Python objects specified in configuration settings. It returns a dictionary of these loaded components, gracefully handling unconfigured items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_exporter_supported", "line_number": 410, "body": "def _exporter_supported(self, format):\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})", "is_method": true, "class_name": "FeedExporter", "function_description": "Checks if a specified feed export format is supported by the exporter. It returns true if the format is recognized, otherwise logs an error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_settings_are_valid", "line_number": 415, "body": "def _settings_are_valid(self):\n        \"\"\"\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n                logger.error(\n                    '%(batch_time)s or %(batch_id)d must be in the feed URI ({}) if FEED_EXPORT_BATCH_ITEM_COUNT '\n                    'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '\n                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n                    ''.format(uri_template)\n                )\n                return False\n        return True", "is_method": true, "class_name": "FeedExporter", "function_description": "This method validates feed export configurations for `FeedExporter`. It ensures that URIs for batched exports include time or ID placeholders, preventing output file conflicts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_storage_supported", "line_number": 431, "body": "def _storage_supported(self, uri, feed_options):\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages:\n            try:\n                self._get_storage(uri, feed_options)\n                return True\n            except NotConfigured as e:\n                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n                             \"Reason: %(reason)s\",\n                             {'scheme': scheme, 'reason': str(e)})\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})", "is_method": true, "class_name": "FeedExporter", "function_description": "Checks if the given URI's storage scheme is recognized and configured for feed export. It validates if the specific storage backend is available and ready for use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_instance", "line_number": 445, "body": "def _get_instance(self, objcls, *args, **kwargs):\n        return create_instance(\n            objcls, self.settings, getattr(self, 'crawler', None),\n            *args, **kwargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "Provides a standardized way for the exporter to create instances of specified classes. It injects essential settings and crawler context into the new objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_exporter", "line_number": 450, "body": "def _get_exporter(self, file, format, *args, **kwargs):\n        return self._get_instance(self.exporters[format], file, *args, **kwargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "Provides an internal utility to retrieve and instantiate the correct feed exporter object based on a specified format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_storage", "line_number": 453, "body": "def _get_storage(self, uri, feed_options):\n        \"\"\"Fork of create_instance specific to feed storage classes\n\n        It supports not passing the *feed_options* parameters to classes that\n        do not support it, and issuing a deprecation warning instead.\n        \"\"\"\n        feedcls = self.storages[urlparse(uri).scheme]\n        crawler = getattr(self, 'crawler', None)\n\n        def build_instance(builder, *preargs):\n            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)\n\n        if crawler and hasattr(feedcls, 'from_crawler'):\n            instance = build_instance(feedcls.from_crawler, crawler)\n            method_name = 'from_crawler'\n        elif hasattr(feedcls, 'from_settings'):\n            instance = build_instance(feedcls.from_settings, self.settings)\n            method_name = 'from_settings'\n        else:\n            instance = build_instance(feedcls)\n            method_name = '__new__'\n        if instance is None:\n            raise TypeError(\"%s.%s returned None\" % (feedcls.__qualname__, method_name))\n        return instance", "is_method": true, "class_name": "FeedExporter", "function_description": "This method instantiates the appropriate feed storage class for data export based on a given URI. It handles different initialization methods to provide the required storage object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_uri_params", "line_number": 478, "body": "def _get_uri_params(self, spider, uri_params, slot=None):\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        utc_now = datetime.utcnow()\n        params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n        params['batch_time'] = utc_now.isoformat().replace(':', '-')\n        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n        uripar_function(params, spider)\n        return params", "is_method": true, "class_name": "FeedExporter", "function_description": "Generates a dictionary of dynamic parameters for feed export URIs. It populates this with spider attributes, timestamps, and batch information, allowing external custom modifications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "build_instance", "line_number": 462, "body": "def build_instance(builder, *preargs):\n            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "This method constructs a storage instance for the `FeedExporter` using a provided builder and configuration. It delegates the actual creation process to an external `build_storage` function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "from_crawler", "line_number": 32, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "StackTraceDump", "function_description": "This class method serves as an alternative constructor, creating a `StackTraceDump` instance using a provided crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "dump_stacktrace", "line_number": 35, "body": "def dump_stacktrace(self, signum, frame):\n        log_args = {\n            'stackdumps': self._thread_stacks(),\n            'enginestatus': format_engine_status(self.crawler.engine),\n            'liverefs': format_live_refs(),\n        }\n        logger.info(\"Dumping stack trace and engine status\\n\"\n                    \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n                    log_args, extra={'crawler': self.crawler})", "is_method": true, "class_name": "StackTraceDump", "function_description": "Logs a detailed snapshot of the application's state for debugging, including thread stack traces, engine status, and live object references. It provides crucial insights for diagnosing issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "_thread_stacks", "line_number": 45, "body": "def _thread_stacks(self):\n        id2name = dict((th.ident, th.name) for th in threading.enumerate())\n        dumps = ''\n        for id_, frame in sys._current_frames().items():\n            name = id2name.get(id_, '')\n            dump = ''.join(traceback.format_stack(frame))\n            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n        return dumps", "is_method": true, "class_name": "StackTraceDump", "function_description": "This method collects and formats the full stack trace for all currently active threads in the process. It provides a snapshot of what each thread is executing, useful for debugging concurrent issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "_enter_debugger", "line_number": 63, "body": "def _enter_debugger(self, signum, frame):\n        Pdb().set_trace(frame.f_back)", "is_method": true, "class_name": "Debugger", "function_description": "This internal method within the Debugger class initiates an interactive Python debugger session. It pauses program execution, allowing on-demand inspection from the calling frame."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj", "is_method": true, "class_name": "SpiderState", "function_description": "Factory method for SpiderState, initializing it from crawler settings to manage spider state. It connects to spider lifecycle signals for proper state handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "spider_closed", "line_number": 26, "body": "def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=4)", "is_method": true, "class_name": "SpiderState", "function_description": "Persists the spider's current state to a file when it closes, provided a job directory is configured. This allows for resuming or auditing the spider's progress."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "spider_opened", "line_number": 31, "body": "def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}", "is_method": true, "class_name": "SpiderState", "function_description": "Loads a spider's saved operational state from a file when it opens, or initializes an empty state. This enables spiders to resume previous runs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "statefn", "line_number": 39, "body": "def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')", "is_method": true, "class_name": "SpiderState", "function_description": "Provides the standardized file path for the spider's state within its job directory. This enables other functions to consistently access or persist the spider's operational state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memdebug.py", "function": "from_crawler", "line_number": 20, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n            raise NotConfigured\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "MemoryDebugger", "function_description": "Initializes the MemoryDebugger for a crawler if enabled, connecting it to monitor memory usage and respond to spider lifecycle events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memdebug.py", "function": "spider_closed", "line_number": 27, "body": "def spider_closed(self, spider, reason):\n        gc.collect()\n        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n        for cls, wdict in live_refs.items():\n            if not wdict:\n                continue\n            self.stats.set_value(f'memdebug/live_refs/{cls.__name__}', len(wdict), spider=spider)", "is_method": true, "class_name": "MemoryDebugger", "function_description": "As part of the MemoryDebugger, this method collects garbage and logs memory usage statistics upon spider completion to aid in identifying memory issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "from_crawler", "line_number": 41, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "CloseSpider", "function_description": "Factory method for `CloseSpider` that instantiates the class, passing the `Crawler` object to its constructor for proper initialization within the crawling process.\nFactory method for `CloseSpider` that instantiates the class, passing the `Crawler` object to its constructor for proper initialization within the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "error_count", "line_number": 44, "body": "def error_count(self, failure, response, spider):\n        self.counter['errorcount'] += 1\n        if self.counter['errorcount'] == self.close_on['errorcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_errorcount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors spider errors and triggers its closure if the accumulated error count reaches a predefined limit, preventing excessive resource usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "page_count", "line_number": 49, "body": "def page_count(self, response, request, spider):\n        self.counter['pagecount'] += 1\n        if self.counter['pagecount'] == self.close_on['pagecount']:\n            self.crawler.engine.close_spider(spider, 'closespider_pagecount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors the number of pages a spider processes. It automatically closes the spider when a configured page count limit is reached."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "spider_opened", "line_number": 54, "body": "def spider_opened(self, spider):\n        from twisted.internet import reactor\n        self.task = reactor.callLater(self.close_on['timeout'],\n                                      self.crawler.engine.close_spider, spider,\n                                      reason='closespider_timeout')", "is_method": true, "class_name": "CloseSpider", "function_description": "Schedules an automatic spider closure for a specified timeout when a spider starts, preventing indefinite execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "item_scraped", "line_number": 60, "body": "def item_scraped(self, item, spider):\n        self.counter['itemcount'] += 1\n        if self.counter['itemcount'] == self.close_on['itemcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_itemcount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors the number of items scraped by a spider. It automatically closes the spider if a configured item count limit is reached, controlling crawl scope."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "spider_closed", "line_number": 65, "body": "def spider_closed(self, spider):\n        task = getattr(self, 'task', False)\n        if task and task.active():\n            task.cancel()", "is_method": true, "class_name": "CloseSpider", "function_description": "This method of `CloseSpider` cancels any active background task associated with the spider upon its closure. It ensures proper resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "from_crawler", "line_number": 44, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "MemoryUsage", "function_description": "Provides an alternative constructor for `MemoryUsage` objects. It initializes a new instance using a given `crawler` object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "get_virtual_size", "line_number": 47, "body": "def get_virtual_size(self):\n        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != 'darwin':\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size", "is_method": true, "class_name": "MemoryUsage", "function_description": "This method retrieves the current process's maximum resident set size, converting it to bytes for consistent memory usage measurement across platforms."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "engine_started", "line_number": 54, "body": "def engine_started(self):\n        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n        self.tasks = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)", "is_method": true, "class_name": "MemoryUsage", "function_description": "This method, called at engine startup, initializes memory usage tracking for the crawler. It records initial memory and schedules periodic monitoring for current usage, defined limits, and warning thresholds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "engine_stopped", "line_number": 69, "body": "def engine_stopped(self):\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()", "is_method": true, "class_name": "MemoryUsage", "function_description": "Ensures all currently active tasks managed by the memory usage system are stopped upon engine shutdown."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "update", "line_number": 74, "body": "def update(self):\n        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())", "is_method": true, "class_name": "MemoryUsage", "function_description": "Updates the crawler's statistics with the current maximum virtual memory usage observed. This tracks the peak memory consumption during operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_check_limit", "line_number": 77, "body": "def _check_limit(self):\n        if self.get_virtual_size() > self.limit:\n            self.crawler.stats.set_value('memusage/limit_reached', 1)\n            mem = self.limit/1024/1024\n            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n                         {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                    f\"memory usage exceeded {mem}M at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/limit_notified', 1)\n\n            open_spiders = self.crawler.engine.open_spiders\n            if open_spiders:\n                for spider in open_spiders:\n                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n            else:\n                self.crawler.stop()", "is_method": true, "class_name": "MemoryUsage", "function_description": "Monitors a crawler's memory usage and triggers an emergency shutdown if a predefined limit is exceeded, preventing excessive resource consumption."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_check_warning", "line_number": 98, "body": "def _check_warning(self):\n        if self.warned: # warn only once\n            return\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value('memusage/warning_reached', 1)\n            mem = self.warning/1024/1024\n            logger.warning(\"Memory usage reached %(memusage)dM\",\n                           {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                    f\"memory usage reached {mem}M at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/warning_notified', 1)\n            self.warned = True", "is_method": true, "class_name": "MemoryUsage", "function_description": "This method monitors system memory, issuing a single warning via log and optional email if a virtual memory threshold is surpassed. It provides an alert service for high memory usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_send_report", "line_number": 115, "body": "def _send_report(self, rcpts, subject):\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        stats = self.crawler.stats\n        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup')/1024/1024}M\\r\\n\"\n        s += f\"Maximum memory usage          : {stats.get_value('memusage/max')/1024/1024}M\\r\\n\"\n        s += f\"Current memory usage          : {self.get_virtual_size()/1024/1024}M\\r\\n\"\n\n        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)", "is_method": true, "class_name": "MemoryUsage", "function_description": "Sends a detailed email report containing current, startup, and maximum memory usage, along with the crawler engine's operational status, to specified recipients. This provides a comprehensive overview for monitoring.\nSends a detailed email report containing current, startup, and maximum memory usage, along with the crawler engine's operational status, to specified recipients. This provides a comprehensive overview for monitoring."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n        if not interval:\n            raise NotConfigured\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "LogStats", "function_description": "Initializes and configures a `LogStats` instance for a crawler. It retrieves logging interval settings and connects to spider lifecycle signals to enable statistics logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "spider_opened", "line_number": 30, "body": "def spider_opened(self, spider):\n        self.pagesprev = 0\n        self.itemsprev = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)", "is_method": true, "class_name": "LogStats", "function_description": "Initializes tracking counters and schedules a recurring task to periodically log the spider's progress and statistics once it starts operating."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "log", "line_number": 37, "body": "def log(self, spider):\n        items = self.stats.get_value('item_scraped_count', 0)\n        pages = self.stats.get_value('response_received_count', 0)\n        irate = (items - self.itemsprev) * self.multiplier\n        prate = (pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = pages, items\n\n        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n        log_args = {'pages': pages, 'pagerate': prate,\n                    'items': items, 'itemrate': irate}\n        logger.info(msg, log_args, extra={'spider': spider})", "is_method": true, "class_name": "LogStats", "function_description": "Logs the ongoing progress of a web spider, reporting total pages crawled, items scraped, and their respective rates per minute for monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "spider_closed", "line_number": 50, "body": "def spider_closed(self, spider, reason):\n        if self.task and self.task.running:\n            self.task.stop()", "is_method": true, "class_name": "LogStats", "function_description": "Responds to a spider closing event by stopping any associated running task. This ensures proper cleanup and resource release after the spider completes its work."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "parse_cachecontrol", "line_number": 347, "body": "def parse_cachecontrol(header):\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b','):\n        key, sep, val = directive.strip().partition(b'=')\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives", "is_method": false, "function_description": "This function parses an HTTP `Cache-Control` header string into a dictionary of directives, enabling structured interpretation of web caching rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "rfc1123_to_epoch", "line_number": 367, "body": "def rfc1123_to_epoch(date_str):\n    try:\n        date_str = to_unicode(date_str, encoding='ascii')\n        return mktime_tz(parsedate_tz(date_str))\n    except Exception:\n        return None", "is_method": false, "function_description": "Converts an RFC 1123 formatted date string into a Unix epoch timestamp. This function provides a standardized numerical representation for date comparisons or storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_request", "line_number": 29, "body": "def should_cache_request(self, request):\n        return urlparse_cached(request).scheme not in self.ignore_schemes", "is_method": true, "class_name": "DummyPolicy", "function_description": "Determines if a request should be cached by evaluating its URL scheme against a list of ignored schemes. It provides a simple policy for caching decisions within the `DummyPolicy`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_response", "line_number": 32, "body": "def should_cache_response(self, response, request):\n        return response.status not in self.ignore_http_codes", "is_method": true, "class_name": "DummyPolicy", "function_description": "Determines if an HTTP response should be cached based on its status code. It allows caching unless the response's status code is configured to be ignored."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_valid", "line_number": 38, "body": "def is_cached_response_valid(self, cachedresponse, response, request):\n        return True", "is_method": true, "class_name": "DummyPolicy", "function_description": "This method from DummyPolicy always validates cached responses as valid. It bypasses actual validation logic, effectively trusting all cached data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_parse_cachecontrol", "line_number": 54, "body": "def _parse_cachecontrol(self, r):\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b'Cache-Control', b'')\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Provides parsed and cached `Cache-Control` directives from an HTTP message. It filters out policy-ignored response directives, aiding HTTP caching logic."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_request", "line_number": 64, "body": "def should_cache_request(self, request):\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Any other is eligible for caching\n        return True", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if an HTTP request should be cached according to RFC 2616 policy. It checks for ignored schemes and the 'no-store' Cache-Control directive."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_response", "line_number": 74, "body": "def should_cache_response(self, response, request):\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        elif response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        elif self.always_store:\n            return True\n        # Any hint on response expiration is good\n        elif b'max-age' in cc or b'Expires' in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        elif response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        elif response.status in (200, 203, 401):\n            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        else:\n            return False", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if a given HTTP response is cacheable according to RFC 2616 guidelines. It evaluates various HTTP headers and status codes to assess caching eligibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_fresh", "line_number": 103, "body": "def is_cached_response_fresh(self, cachedresponse, request):\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b'no-cache' in cc or b'no-cache' in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b'max-stale']\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if a cached HTTP response is fresh for a given request, adhering to RFC 2616 caching rules. It evaluates `Cache-Control` directives like `no-cache` and `max-stale`."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_valid", "line_number": 142, "body": "def is_cached_response_valid(self, cachedresponse, response, request):\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b'must-revalidate' not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if a cached HTTP response remains valid for use, either due to a new server's 304 status or as a fallback during server errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_set_conditional_validators", "line_number": 153, "body": "def _set_conditional_validators(self, request, cachedresponse):\n        if b'Last-Modified' in cachedresponse.headers:\n            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n\n        if b'ETag' in cachedresponse.headers:\n            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']", "is_method": true, "class_name": "RFC2616Policy", "function_description": "This method prepares an HTTP request for conditional revalidation by adding `If-Modified-Since` or `If-None-Match` headers based on a cached response. This enables efficient server-side revalidation of cached content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_get_max_age", "line_number": 160, "body": "def _get_max_age(self, cc):\n        try:\n            return max(0, int(cc[b'max-age']))\n        except (KeyError, ValueError):\n            return None", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Extracts and validates the 'max-age' value from cache-control data. It determines the maximum allowable age for a cached resource, critical for HTTP caching policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_compute_freshness_lifetime", "line_number": 166, "body": "def _compute_freshness_lifetime(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n\n        # Try HTTP/1.0 Expires header\n        if b'Expires' in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute fresshness lifetime\n        return 0", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Computes the maximum duration an HTTP response remains fresh in a cache. It applies HTTP/1.1 cache-control policies and heuristics based on response headers to determine validity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_compute_current_age", "line_number": 197, "body": "def _compute_current_age(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n        if now > date:\n            currentage = now - date\n\n        if b'Age' in response.headers:\n            try:\n                age = int(response.headers[b'Age'])\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Computes the current age of an HTTP response, factoring in its 'Date' and 'Age' headers. This is a core utility method for the RFC2616Policy to assess cache freshness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "open_spider", "line_number": 225, "body": "def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, f'{spider.name}.db')\n        self.db = self.dbmodule.open(dbpath, 'c')\n\n        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Prepares a DBM database file to serve as the persistent cache storage for a given web spider's operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "close_spider", "line_number": 231, "body": "def close_spider(self, spider):\n        self.db.close()", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Closes the underlying DBM database connection used for cache storage. Ensures proper resource cleanup when a spider finishes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "retrieve_response", "line_number": 234, "body": "def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "This method retrieves cached HTTP responses for a specific spider and request. It reconstructs the stored data (URL, status, headers, body) into a proper Response object if available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "store_response", "line_number": 246, "body": "def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        self.db[f'{key}_data'] = pickle.dumps(data, protocol=4)\n        self.db[f'{key}_time'] = str(time())", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Persists an HTTP response's status, URL, headers, and body, along with a timestamp, into the DBM cache. This enables caching of web requests for later retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_read_data", "line_number": 257, "body": "def _read_data(self, spider, request):\n        key = self._request_key(request)\n        db = self.db\n        tkey = f'{key}_time'\n        if tkey not in db:\n            return  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        return pickle.loads(db[f'{key}_data'])", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Retrieves cached data for a request from the DBM storage, returning it only if it exists and has not expired."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_request_key", "line_number": 270, "body": "def _request_key(self, request):\n        return request_fingerprint(request)", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Generates a unique key from a given request object for use in caching. This key serves as an identifier to store or retrieve cached responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "retrieve_response", "line_number": 289, "body": "def retrieve_response(self, spider, request):\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return  # not cached\n        rpath = self._get_request_path(spider, request)\n        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n            body = f.read()\n        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n            rawheaders = f.read()\n        url = metadata.get('response_url')\n        status = metadata['status']\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Retrieves a cached web response from the filesystem storage. It reconstructs and returns the full response object, or None if not found in the cache."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "store_response", "line_number": 306, "body": "def store_response(self, spider, request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n            f.write(request.body)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Stores a web spider's request and its response to the filesystem. This method provides caching for later retrieval, enabling offline processing or replay."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_get_request_path", "line_number": 331, "body": "def _get_request_path(self, spider, request):\n        key = request_fingerprint(request)\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Calculates the unique filesystem path where a web request's cached data is stored or retrieved, based on the spider and request fingerprint."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_read_meta", "line_number": 335, "body": "def _read_meta(self, spider, request):\n        rpath = self._get_request_path(spider, request)\n        metapath = os.path.join(rpath, 'pickled_meta')\n        if not os.path.exists(metapath):\n            return  # not found\n        mtime = os.stat(metapath).st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return  # expired\n        with self._open(metapath, 'rb') as f:\n            return pickle.load(f)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Retrieves pickled request metadata from the filesystem cache. It checks for existence and expiration before loading the data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "from_crawler", "line_number": 22, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Provides a standard factory method to instantiate AutoThrottle, injecting the crawler object for its operational context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_spider_opened", "line_number": 25, "body": "def _spider_opened(self, spider):\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Initializes the auto-throttling mechanism for a spider upon its opening. It configures the minimum, maximum, and initial download delays."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_min_delay", "line_number": 30, "body": "def _min_delay(self, spider):\n        s = self.crawler.settings\n        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))", "is_method": true, "class_name": "AutoThrottle", "function_description": "Determines the minimum download delay for a given spider. It prioritizes the spider's specific delay setting over the global default."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_max_delay", "line_number": 34, "body": "def _max_delay(self, spider):\n        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')", "is_method": true, "class_name": "AutoThrottle", "function_description": "Retrieves the maximum allowed delay value for the AutoThrottle mechanism from the crawler's settings. This defines the upper limit for request delays."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_start_delay", "line_number": 37, "body": "def _start_delay(self, spider):\n        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))", "is_method": true, "class_name": "AutoThrottle", "function_description": "This method determines the initial delay for the auto-throttling system, ensuring a minimum delay is applied while respecting a configured start delay."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_response_downloaded", "line_number": 40, "body": "def _response_downloaded(self, response, request, spider):\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get('download_latency')\n        if latency is None or slot is None:\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    'slot': key, 'concurrency': conc,\n                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n                    'latency': latency * 1000, 'size': size\n                },\n                extra={'spider': spider}\n            )", "is_method": true, "class_name": "AutoThrottle", "function_description": "Updates the automatic crawling delay for a given target after a response is downloaded. It uses the response's download latency to dynamically adjust future request pacing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_get_slot", "line_number": 64, "body": "def _get_slot(self, request, spider):\n        key = request.meta.get('download_slot')\n        return key, self.crawler.engine.downloader.slots.get(key)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Retrieves the download slot key and object associated with a request. This aids in managing specific download concurrency and speed for the AutoThrottle system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_adjust_delay", "line_number": 68, "body": "def _adjust_delay(self, slot, latency, response):\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay", "is_method": true, "class_name": "AutoThrottle", "function_description": "This method dynamically adjusts the request delay for a server based on its response latency and a target concurrency. It optimizes the request rate to avoid overwhelming the server, with special handling for error responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "from_crawler", "line_number": 61, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "TelnetConsole", "function_description": "This class method constructs a TelnetConsole instance, specifically initializing it using a provided crawler object. It serves as an alternative constructor for the console."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "start_listening", "line_number": 64, "body": "def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\"Telnet console listening on %(host)s:%(port)d\",\n                    {'host': h.host, 'port': h.port},\n                    extra={'crawler': self.crawler})", "is_method": true, "class_name": "TelnetConsole", "function_description": "Activates the Telnet console by initiating listening for incoming TCP connections. This makes the console remotely accessible on a specified network port."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "stop_listening", "line_number": 71, "body": "def stop_listening(self):\n        self.port.stopListening()", "is_method": true, "class_name": "TelnetConsole", "function_description": "Instructs the Telnet console to stop listening for new incoming connections on its configured network port. This halts the console's ability to accept new client connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "protocol", "line_number": 74, "body": "def protocol(self):\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode('utf8')\n                    and credentials.checkPassword(self.password.encode('utf8'))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(\n            telnet.AuthenticatingTelnetProtocol,\n            Portal()\n        )", "is_method": true, "class_name": "TelnetConsole", "function_description": "Sets up a Telnet server protocol, providing user authentication and an interactive Python console (Manhole) upon successful login."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "_get_telnet_vars", "line_number": 97, "body": "def _get_telnet_vars(self):\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        telnet_vars = {\n            'engine': self.crawler.engine,\n            'spider': self.crawler.engine.spider,\n            'slot': self.crawler.engine.slot,\n            'crawler': self.crawler,\n            'extensions': self.crawler.extensions,\n            'stats': self.crawler.stats,\n            'settings': self.crawler.settings,\n            'est': lambda: print_engine_status(self.crawler.engine),\n            'p': pprint.pprint,\n            'prefs': print_live_refs,\n            'help': \"This is Scrapy telnet console. For more info see: \"\n                    \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars", "is_method": true, "class_name": "TelnetConsole", "function_description": "Populates a dictionary with core Scrapy objects and debugging utilities. This enables interactive inspection and control of the running crawler through the Telnet console."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "login", "line_number": 78, "body": "def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode('utf8')\n                    and credentials.checkPassword(self.password.encode('utf8'))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)", "is_method": true, "class_name": "Portal", "function_description": "Authenticates user credentials for the Portal service. Upon successful login, it establishes a Telnet protocol session, granting access to an interactive shell."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/statsmailer.py", "function": "from_crawler", "line_number": 19, "body": "def from_crawler(cls, crawler):\n        recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n        if not recipients:\n            raise NotConfigured\n        mail = MailSender.from_settings(crawler.settings)\n        o = cls(crawler.stats, recipients, mail)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "StatsMailer", "function_description": "Initializes and configures the `StatsMailer` from crawler settings. It ensures crawl statistics are mailed to specified recipients upon spider completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/statsmailer.py", "function": "spider_closed", "line_number": 28, "body": "def spider_closed(self, spider):\n        spider_stats = self.stats.get_stats(spider)\n        body = \"Global stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)", "is_method": true, "class_name": "StatsMailer", "function_description": "Upon a web spider's completion, this method collects global and spider-specific scraping statistics, then dispatches them via email for reporting and analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o", "is_method": true, "class_name": "CoreStats", "function_description": "Configures and returns a `CoreStats` object that automatically collects and updates statistics by listening to a crawler's various operational events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "spider_opened", "line_number": 25, "body": "def spider_opened(self, spider):\n        self.start_time = datetime.utcnow()\n        self.stats.set_value('start_time', self.start_time, spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Records the exact start time when a spider begins its operation. This facilitates tracking its duration and activity within the CoreStats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "spider_closed", "line_number": 29, "body": "def spider_closed(self, spider, reason):\n        finish_time = datetime.utcnow()\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n        self.stats.set_value('finish_time', finish_time, spider=spider)\n        self.stats.set_value('finish_reason', reason, spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "This method records the elapsed time, finish time, and the reason for completion when a web spider finishes its operation. It stores these essential metrics for tracking spider performance and status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "item_scraped", "line_number": 37, "body": "def item_scraped(self, item, spider):\n        self.stats.inc_value('item_scraped_count', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Registers that a data item has been successfully scraped by a specific web spider. It increments a dedicated counter, enabling real-time tracking of collected items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "response_received", "line_number": 40, "body": "def response_received(self, spider):\n        self.stats.inc_value('response_received_count', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Increments a counter for the number of responses received by a specific web crawler, providing statistics on its activity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "item_dropped", "line_number": 43, "body": "def item_dropped(self, item, spider, exception):\n        reason = exception.__class__.__name__\n        self.stats.inc_value('item_dropped_count', spider=spider)\n        self.stats.inc_value(f'item_dropped_reasons_count/{reason}', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "This method updates core statistics when a data item is dropped during processing. It records the total count and the specific exception reason for the drop."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "from_crawler", "line_number": 24, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "This class method serves as an alternative constructor for `HttpErrorMiddleware`. It instantiates the middleware using settings provided by a `crawler` object, simplifying integration into a crawler framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "process_spider_input", "line_number": 31, "body": "def process_spider_input(self, response, spider):\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if meta.get('handle_httpstatus_all', False):\n            return\n        if 'handle_httpstatus_list' in meta:\n            allowed_statuses = meta['handle_httpstatus_list']\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, 'Ignoring non-200 response')", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "Controls the processing of non-successful HTTP responses. It determines whether to pass them to the spider or raise an error, based on configured status code handling rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "process_spider_exception", "line_number": 47, "body": "def process_spider_exception(self, response, exception, spider):\n        if isinstance(exception, HttpError):\n            spider.crawler.stats.inc_value('httperror/response_ignored_count')\n            spider.crawler.stats.inc_value(\n                f'httperror/response_ignored_status_count/{response.status}'\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {'response': response}, extra={'spider': spider},\n            )\n            return []", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "Manages and logs `HttpError` exceptions from spider responses. This prevents problematic HTTP status codes from being processed further by the crawler, updating relevant statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "from_settings", "line_number": 21, "body": "def from_settings(cls, settings):\n        maxlength = settings.getint('URLLENGTH_LIMIT')\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Constructs a `UrlLengthMiddleware` instance by retrieving the 'URLLENGTH_LIMIT' setting. It ensures the URL length limit is properly defined before configuring and instantiating the middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "process_spider_output", "line_number": 27, "body": "def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.info(\n                    \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                    {'maxlength': self.maxlength, 'url': request.url},\n                    extra={'spider': spider}\n                )\n                spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Filters the spider's output to exclude requests with URLs exceeding a configured maximum length. This prevents excessively long URLs from being processed further, helping manage crawl scope."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "_filter", "line_number": 28, "body": "def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.info(\n                    \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                    {'maxlength': self.maxlength, 'url': request.url},\n                    extra={'spider': spider}\n                )\n                spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                return False\n            else:\n                return True", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Filters incoming requests based on URL length. It prevents processing of URLs exceeding a maximum limit, logging and counting the ignored ones."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "_load_policy_class", "line_number": 271, "body": "def _load_policy_class(policy, warning_only=False):\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return load_object(policy)\n    except ValueError:\n        try:\n            return _policy_classes[policy.lower()]\n        except KeyError:\n            msg = f\"Could not load referrer policy {policy!r}\"\n            if not warning_only:\n                raise RuntimeError(msg)\n            else:\n                warnings.warn(msg, RuntimeWarning)\n                return None", "is_method": false, "function_description": "Loads a policy class, resolving the input string as an importable object path or a standard W3C referrer policy name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "stripped_referrer", "line_number": 40, "body": "def stripped_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Provides a referrer URL stripped according to policy. It returns the stripped URL only if its scheme is allowed, preventing disclosure for restricted schemes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "origin_referrer", "line_number": 44, "body": "def origin_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Determines the origin-based referrer for a given URL, respecting policies that prevent referrer generation for certain schemes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "strip_url", "line_number": 48, "body": "def strip_url(self, url, origin_only=False):\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(url,\n                         strip_credentials=True,\n                         strip_fragment=True,\n                         strip_default_port=True,\n                         origin_only=origin_only)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Strips sensitive components like credentials and fragments from a URL, optionally retaining only the origin. This ensures compliance with W3C Referrer Policy for privacy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "origin", "line_number": 70, "body": "def origin(self, url):\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "This method extracts the serialized origin (scheme, host, path) from a given URL. It provides essential origin information for implementing referrer policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "potentially_trustworthy", "line_number": 74, "body": "def potentially_trustworthy(self, url):\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in ('data',):\n            return False\n        return self.tls_protected(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Evaluates if a URL is potentially trustworthy by checking its scheme and TLS protection, explicitly *not* adhering to W3C secure context standards."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "tls_protected", "line_number": 81, "body": "def tls_protected(self, url):\n        return urlparse(url).scheme in ('https', 'ftps')", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "This method determines if a given URL uses a TLS-protected scheme, such as HTTPS or FTPS. It's used to validate URL security for referrer policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 95, "body": "def referrer(self, response_url, request_url):\n        return None", "is_method": true, "class_name": "NoReferrerPolicy", "function_description": "This method of the `NoReferrerPolicy` ensures that no referrer information is ever returned, thereby enforcing a strict no-referrer privacy policy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 115, "body": "def referrer(self, response_url, request_url):\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)", "is_method": true, "class_name": "NoReferrerWhenDowngradePolicy", "function_description": "Determines if a referrer URL can be sent according to the 'No Referrer When Downgrade' policy. It prevents sending the referrer when navigating from HTTPS to HTTP."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 132, "body": "def referrer(self, response_url, request_url):\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)", "is_method": true, "class_name": "SameOriginPolicy", "function_description": "This method determines if a referrer URL can be safely disclosed according to the Same-Origin Policy. It returns a stripped referrer only if the response and request URLs share the same origin."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 148, "body": "def referrer(self, response_url, request_url):\n        return self.origin_referrer(response_url)", "is_method": true, "class_name": "OriginPolicy", "function_description": "Provides the referrer URL for a given response, applying the `OriginPolicy`'s rules. This service helps determine source integrity for web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 167, "body": "def referrer(self, response_url, request_url):\n        if (\n            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)", "is_method": true, "class_name": "StrictOriginPolicy", "function_description": "Determines whether to provide the origin referrer based on the response URL's TLS protection and the request URL's trustworthiness. It enforces a strict origin policy for referrer disclosure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 188, "body": "def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        else:\n            return origin", "is_method": true, "class_name": "OriginWhenCrossOriginPolicy", "function_description": "Computes the `Referer` header value following an \"origin-when-cross-origin\" policy. It provides the full referrer for same-origin requests, otherwise just the origin."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 215, "body": "def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        elif (\n            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)", "is_method": true, "class_name": "StrictOriginWhenCrossOriginPolicy", "function_description": "Calculates the HTTP Referer header value according to the Strict-Origin-When-Cross-Origin policy. This limits referrer information shared with cross-origin destinations for security and privacy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 241, "body": "def referrer(self, response_url, request_url):\n        return self.stripped_referrer(response_url)", "is_method": true, "class_name": "UnsafeUrlPolicy", "function_description": "Provides the referrer URL for a given response, applying the policy's stripping rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "from_crawler", "line_number": 300, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('REFERER_ENABLED'):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Initializes the `RefererMiddleware` for a Scrapy crawler, enabling it conditionally. It connects the middleware to intercept scheduled requests, primarily for managing Referer headers during redirections.\nInitializes the `RefererMiddleware` for a Scrapy crawler, enabling it conditionally. It connects the middleware to intercept scheduled requests, primarily for managing Referer headers during redirections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "policy", "line_number": 310, "body": "def policy(self, resp_or_url, request):\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get('referrer_policy')\n        if policy_name is None:\n            if isinstance(resp_or_url, Response):\n                policy_header = resp_or_url.headers.get('Referrer-Policy')\n                if policy_header is not None:\n                    policy_name = to_unicode(policy_header.decode('latin1'))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()", "is_method": true, "class_name": "RefererMiddleware", "function_description": "This method of `RefererMiddleware` determines the active Referrer-Policy for a request. It prioritizes policies from request meta or parent response headers, falling back to the default if not specified or invalid."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "process_spider_output", "line_number": 335, "body": "def process_spider_output(self, response, result, spider):\n        def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r\n        return (_set_referer(r) for r in result or ())", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Adds the 'Referer' header to new requests generated by the spider. It uses a defined policy to determine the referrer based on the current response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "request_scheduled", "line_number": 344, "body": "def request_scheduled(self, request, spider):\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get('redirect_urls', [])\n        if redirected_urls:\n            request_referrer = request.headers.get('Referer')\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url)\n                if policy_referrer != request_referrer:\n                    if policy_referrer is None:\n                        request.headers.pop('Referer')\n                    else:\n                        request.headers['Referer'] = policy_referrer", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Corrects the 'Referer' header of a redirected request according to a defined referrer policy. This ensures accurate referrer information for subsequent requests after redirection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "_set_referer", "line_number": 336, "body": "def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r", "is_method": true, "class_name": "RefererMiddleware", "function_description": "This method sets or updates the 'Referer' HTTP header on a Request object, deriving its value from the middleware's configured policy. It ensures outgoing requests correctly indicate their origin."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "from_crawler", "line_number": 23, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Initializes and configures the `OffsiteMiddleware` instance for a given crawler. It connects the middleware to the `spider_opened` signal, preparing it for spider lifecycle events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "process_spider_output", "line_number": 28, "body": "def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\n                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                            {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "This method filters outgoing web requests generated by a spider, preventing offsite requests that are not explicitly allowed. It processes and yields only valid requests, tracking filtered ones."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "should_follow", "line_number": 45, "body": "def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Determines if a request's hostname matches the middleware's configured regular expression. This indicates whether the crawler should follow the link, preventing offsite navigation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "get_host_regex", "line_number": 51, "body": "def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('')  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n                message = (\"allowed_domains accepts only domains, not URLs. \"\n                           f\"Ignoring URL entry {domain} in allowed_domains.\")\n                warnings.warn(message, URLWarning)\n            elif port_pattern.search(domain):\n                message = (\"allowed_domains accepts only domains without ports. \"\n                           f\"Ignoring entry {domain} in allowed_domains.\")\n                warnings.warn(message, PortWarning)\n            else:\n                domains.append(re.escape(domain))\n        regex = fr'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Generates a regular expression to validate hostnames against a spider's allowed domains. This defines the 'offsite policy' for web crawling, preventing visits to external hosts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "spider_opened", "line_number": 75, "body": "def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Prepares the OffsiteMiddleware for a spider's operation. It sets up domain filtering rules and initializes domain tracking to manage offsite requests effectively."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "from_crawler", "line_number": 23, "body": "def from_crawler(cls, crawler):\n        settings = crawler.settings\n        maxdepth = settings.getint('DEPTH_LIMIT')\n        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n        prio = settings.getint('DEPTH_PRIORITY')\n        return cls(maxdepth, crawler.stats, verbose, prio)", "is_method": true, "class_name": "DepthMiddleware", "function_description": "This class method initializes and configures a DepthMiddleware instance for a web crawler. It extracts depth-related settings like limit and priority directly from the provided crawler's configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "process_spider_output", "line_number": 30, "body": "def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value(f'request_depth_count/{depth}',\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True\n\n        # base case (depth=0)\n        if 'depth' not in response.meta:\n            response.meta['depth'] = 0\n            if self.verbose_stats:\n                self.stats.inc_value('request_depth_count/0', spider=spider)\n\n        return (r for r in result or () if _filter(r))", "is_method": true, "class_name": "DepthMiddleware", "function_description": "This method of `DepthMiddleware` assigns and manages the crawling depth for new requests generated by a spider. It filters out requests exceeding a configured maximum depth, ensuring controlled recursion and updating depth-related statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "_filter", "line_number": 31, "body": "def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value(f'request_depth_count/{depth}',\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True", "is_method": true, "class_name": "DepthMiddleware", "function_description": "Manages web request depth by incrementing depth, adjusting priority, and filtering requests exceeding a maximum depth. It also updates depth-related statistics for the spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "_create_testcase", "line_number": 171, "body": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return f\"[{spider}] {method.__name__} ({desc})\"\n\n    name = f'{spider}_{method.__name__}'\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)", "is_method": false, "function_description": "Dynamically creates a custom `unittest.TestCase` instance for a specific method. It customizes the test case's string representation for clear identification in test reports, useful for contract testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "add_pre_hook", "line_number": 22, "body": "def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n\n            request.callback = wrapper\n\n        return request", "is_method": true, "class_name": "Contract", "function_description": "This method modifies a request's callback to execute a `pre_process` method and report its outcome to a test results object, before the original callback runs. It provides a mechanism for enforcing pre-conditions or contract validation within a request's lifecycle."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "add_post_hook", "line_number": 45, "body": "def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request", "is_method": true, "class_name": "Contract", "function_description": "Adds a post-execution hook to a request's callback. This hook applies `self.post_process` to the output and records its validation result for a test runner."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "tested_methods_from_spidercls", "line_number": 80, "body": "def tested_methods_from_spidercls(self, spidercls):\n        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n        methods = []\n        for key, value in getmembers(spidercls):\n            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                methods.append(key)\n\n        return methods", "is_method": true, "class_name": "ContractsManager", "function_description": "Extracts method names from a spider class that are marked with '@' in their docstrings, signifying they are subject to contract testing. It provides a list of these method names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "extract_contracts", "line_number": 89, "body": "def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts", "is_method": true, "class_name": "ContractsManager", "function_description": "Parses a method's docstring to identify and instantiate specific contract objects. It extracts structured contract definitions, identified by '@' annotations, enabling dynamic enforcement or generation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "from_spider", "line_number": 102, "body": "def from_spider(self, spider, results):\n        requests = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, 'contract')\n                results.addError(case, sys.exc_info())\n\n        return requests", "is_method": true, "class_name": "ContractsManager", "function_description": "Processes a spider's designated methods to create a list of test requests for contract validation. It captures and reports any errors encountered during this generation process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "from_method", "line_number": 114, "body": "def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs['dont_filter'] = True\n            kwargs['callback'] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove('self')\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request", "is_method": true, "class_name": "ContractsManager", "function_description": "Constructs and configures a Request object for a given method, applying contract-defined request class, argument adjustments, and pre/post-processing hooks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "_clean_req", "line_number": 148, "body": "def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper", "is_method": true, "class_name": "ContractsManager", "function_description": "This method enhances a request by wrapping its callback and errback functions. It ensures output standardization and robustly captures/records all execution errors for reporting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "cb_wrapper", "line_number": 154, "body": "def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())", "is_method": true, "class_name": "ContractsManager", "function_description": "Provides a robust wrapper for executing callback functions, processing their output. It catches and records any execution errors for debugging or test validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "eb_wrapper", "line_number": 162, "body": "def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)", "is_method": true, "class_name": "ContractsManager", "function_description": "This function processes a failure object, extracting exception details. It then records this error with a generated test case in a results object, typically for error reporting or testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "__str__", "line_number": 175, "body": "def __str__(_self):\n            return f\"[{spider}] {method.__name__} ({desc})\"", "is_method": true, "class_name": "ContractTestCase", "function_description": "Generates a descriptive string for a `ContractTestCase` instance, including details like the spider, method name, and test description. This aids in logging and debugging test outcomes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "wrapper", "line_number": 27, "body": "def wrapper(response, **cb_kwargs):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response, **cb_kwargs)))", "is_method": true, "class_name": "Contract", "function_description": "This method wraps a data processing callback, first performing and logging the results of a pre-validation step. It then consistently executes and returns the main callback's output."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "wrapper", "line_number": 50, "body": "def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output", "is_method": true, "class_name": "Contract", "function_description": "This method executes a provided callback, processes its output, and runs a post-processing test against the results. It reports the test outcome (success, failure, or error)."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "adjust_request_args", "line_number": 18, "body": "def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args", "is_method": true, "class_name": "UrlContract", "function_description": "This method inserts the URL from the `UrlContract` instance into a provided dictionary of request arguments, preparing them for network operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "adjust_request_args", "line_number": 32, "body": "def adjust_request_args(self, args):\n        args['cb_kwargs'] = json.loads(' '.join(self.args))\n        return args", "is_method": true, "class_name": "CallbackKeywordArgumentsContract", "function_description": "This method injects dynamically parsed callback keyword arguments into a request argument dictionary. It prepares the arguments for subsequent callback execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "post_process", "line_number": 78, "body": "def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if self.obj_type_verifier(x):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = f'{self.min_bound}..{self.max_bound}'\n\n            raise ContractFail(f\"Returned {occurrences} {self.obj_name}, expected {expected}\")", "is_method": true, "class_name": "ReturnsContract", "function_description": "This method of `ReturnsContract` validates the quantity of specific objects within a function's output. It ensures the count adheres to predefined bounds, raising an error if the contract is violated."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "post_process", "line_number": 102, "body": "def post_process(self, output):\n        for x in output:\n            if is_item(x):\n                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                if missing:\n                    missing_fields = \", \".join(missing)\n                    raise ContractFail(f\"Missing fields: {missing_fields}\")", "is_method": true, "class_name": "ScrapesContract", "function_description": "This method of `ScrapesContract` validates that each item in the output contains all required fields. It raises a `ContractFail` exception if any item is missing essential data, ensuring data integrity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_handle_statuses", "line_number": 49, "body": "def _handle_statuses(self, allow_redirects):\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))", "is_method": true, "class_name": "MediaPipeline", "function_description": "Sets the MediaPipeline's behavior for HTTP redirect status codes. If redirects are allowed, it configures the pipeline to exclude 3xx responses from specific handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_key_for_pipe", "line_number": 54, "body": "def _key_for_pipe(self, key, base_class_name=None, settings=None):\n        \"\"\"\n        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n        'IMAGES'\n        >>> class MyPipe(MediaPipeline):\n        ...     pass\n        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n        'MYPIPE_IMAGES'\n        \"\"\"\n        class_name = self.__class__.__name__\n        formatted_key = f\"{class_name.upper()}_{key}\"\n        if (\n            not base_class_name\n            or class_name == base_class_name\n            or settings and not settings.get(formatted_key)\n        ):\n            return key\n        return formatted_key", "is_method": true, "class_name": "MediaPipeline", "function_description": "This method generates a class-specific key for internal lookup or configuration. It enables subclasses to have distinct, namespaced keys while allowing fallback to a base key."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "from_crawler", "line_number": 74, "body": "def from_crawler(cls, crawler):\n        try:\n            pipe = cls.from_settings(crawler.settings)\n        except AttributeError:\n            pipe = cls()\n        pipe.crawler = crawler\n        return pipe", "is_method": true, "class_name": "MediaPipeline", "function_description": "This class method creates a MediaPipeline instance, configuring it from a given crawler's settings if available. It links the pipeline back to the crawler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "open_spider", "line_number": 82, "body": "def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Initializes spider-specific information within the media pipeline. This sets up the pipeline's context for processing items from the given spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "process_item", "line_number": 85, "body": "def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info, item) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=True)\n        return dfd.addCallback(self.item_completed, item, info)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Coordinates the asynchronous downloading and processing of media files for a scraped item. It initiates media requests and signals their completion within the pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_process_request", "line_number": 92, "body": "def _process_request(self, request, info, item):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n        dfd.addCallback(self._check_media_to_download, request, info, item=item)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Handles asynchronous media download requests within the pipeline. It manages caching, prevents duplicate downloads, and notifies all waiting consumers upon completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_make_compatible", "line_number": 121, "body": "def _make_compatible(self):\n        \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n        methods = [\n            \"file_path\", \"media_to_download\", \"media_downloaded\",\n            \"file_downloaded\", \"image_downloaded\", \"get_images\"\n        ]\n\n        for method_name in methods:\n            method = getattr(self, method_name, None)\n            if callable(method):\n                setattr(self, method_name, self._compatible(method))", "is_method": true, "class_name": "MediaPipeline", "function_description": "This method ensures specified `MediaPipeline` methods are backward compatible, enabling older code to interact seamlessly with updated implementations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_compatible", "line_number": 133, "body": "def _compatible(self, func):\n        \"\"\"Wrapper for overridable methods to allow backwards compatibility\"\"\"\n        self._check_signature(func)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if self._expects_item[func.__name__]:\n                return func(*args, **kwargs)\n\n            kwargs.pop('item', None)\n            return func(*args, **kwargs)\n\n        return wrapper", "is_method": true, "class_name": "MediaPipeline", "function_description": "This method provides a compatibility wrapper for overridable functions. It ensures older function signatures remain functional by conditionally removing an 'item' argument when not expected."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_check_signature", "line_number": 147, "body": "def _check_signature(self, func):\n        sig = signature(func)\n        self._expects_item[func.__name__] = True\n\n        if 'item' not in sig.parameters:\n            old_params = str(sig)[1:-1]\n            new_params = old_params + \", *, item=None\"\n            warn(f'{func.__name__}(self, {old_params}) is deprecated, '\n                 f'please use {func.__name__}(self, {new_params})',\n                 ScrapyDeprecationWarning, stacklevel=2)\n            self._expects_item[func.__name__] = False", "is_method": true, "class_name": "MediaPipeline", "function_description": "This internal method validates the signature of pipeline methods, issuing a deprecation warning if the `item` parameter is missing. It tracks whether each method expects an `item` for internal use.\nThis internal method validates the signature of pipeline methods, issuing a deprecation warning if the `item` parameter is missing. It tracks whether each method expects an `item` for internal use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_modify_media_request", "line_number": 159, "body": "def _modify_media_request(self, request):\n        if self.handle_httpstatus_list:\n            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n        else:\n            request.meta['handle_httpstatus_all'] = True", "is_method": true, "class_name": "MediaPipeline", "function_description": "Configures a media request's metadata to define how HTTP status codes should be handled, either for a specified list or for all statuses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_check_media_to_download", "line_number": 165, "body": "def _check_media_to_download(self, result, request, info, item):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            self._modify_media_request(request)\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd", "is_method": true, "class_name": "MediaPipeline", "function_description": "This internal MediaPipeline method orchestrates media download. It either uses a custom download function or the crawler engine, setting up callbacks for successful completion or failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_cache_result_and_execute_waiters", "line_number": 182, "body": "def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # What happens when the media_downloaded callback raises an\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that the original\n            # StopIteration exception (which in turn contains the failed\n            # Response and by extension, the original Request) gets encapsulated\n            # within the FileException context.\n            #\n            # Originally, Scrapy was using twisted.internet.defer.returnValue\n            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n            # encapsulating the returned Response in a _DefGen_Return exception\n            # instead of a StopIteration.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the encapsulated exception when it is a StopIteration instance\n            #\n            # This problem does not occur in Python 2.7 since we don't have\n            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n            context = getattr(result.value, '__context__', None)\n            if isinstance(context, StopIteration):\n                setattr(result.value, '__context__', None)\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Caches the outcome of a media download, cleaning up failed results to prevent memory leaks. It then notifies all operations that were waiting for this media item's completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_downloaded", "line_number": 228, "body": "def media_downloaded(self, response, request, info, *, item=None):\n        \"\"\"Handler for success downloads\"\"\"\n        return response", "is_method": true, "class_name": "MediaPipeline", "function_description": "Serves as the designated handler within the MediaPipeline for successful media download events. It passes the download response onward for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_failed", "line_number": 232, "body": "def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure", "is_method": true, "class_name": "MediaPipeline", "function_description": "Handles failed media download events within the pipeline. It processes and returns the encountered failure information for further handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "item_completed", "line_number": 236, "body": "def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item", "is_method": true, "class_name": "MediaPipeline", "function_description": "Finalizes processing for an item in a media pipeline after all media requests are done. It logs any failed results if configured and returns the item."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "wrapper", "line_number": 138, "body": "def wrapper(*args, **kwargs):\n            if self._expects_item[func.__name__]:\n                return func(*args, **kwargs)\n\n            kwargs.pop('item', None)\n            return func(*args, **kwargs)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Conditionally removes the 'item' keyword argument for a wrapped function if it's not expected. Ensures functions within the MediaPipeline receive only relevant arguments, preventing errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "from_settings", "line_number": 91, "body": "def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['IMAGES_STORE']\n        return cls(store_uri, settings=settings)", "is_method": true, "class_name": "ImagesPipeline", "function_description": "This class method initializes the ImagesPipeline, configuring its S3, GCS, and FTP image storage backends with necessary credentials and policies from application settings before creating an instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "file_downloaded", "line_number": 113, "body": "def file_downloaded(self, response, request, info, *, item=None):\n        return self.image_downloaded(response, request, info, item=item)", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Delegates the handling of a downloaded file to the `image_downloaded` method, effectively serving as an alias within the image processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "image_downloaded", "line_number": 116, "body": "def image_downloaded(self, response, request, info, *, item=None):\n        checksum = None\n        for path, image, buf in self.get_images(response, request, info, item=item):\n            if checksum is None:\n                buf.seek(0)\n                checksum = md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path, buf, info,\n                meta={'width': width, 'height': height},\n                headers={'Content-Type': 'image/jpeg'})\n        return checksum", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Handles a newly downloaded image by calculating its checksum and persisting it to storage. It stores image dimensions as metadata for efficient retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "get_images", "line_number": 129, "body": "def get_images(self, response, request, info, *, item=None):\n        path = self.file_path(request, response=response, info=info, item=item)\n        orig_image = self._Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\"Image too small \"\n                                 f\"({width}x{height} < \"\n                                 f\"{self.min_width}x{self.min_height})\")\n\n        image, buf = self.convert_image(orig_image)\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n            thumb_image, thumb_buf = self.convert_image(image, size)\n            yield thumb_path, thumb_image, thumb_buf", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Processes a raw image from a web response, validating its dimensions. It generates the main image and various thumbnails, yielding their paths and binary data for storage or further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "convert_image", "line_number": 147, "body": "def convert_image(self, image, size=None):\n        if image.format == 'PNG' and image.mode == 'RGBA':\n            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode != 'RGB':\n            image = image.convert('RGB')\n\n        if size:\n            image = image.copy()\n            image.thumbnail(size, self._Image.ANTIALIAS)\n\n        buf = BytesIO()\n        image.save(buf, 'JPEG')\n        return image, buf", "is_method": true, "class_name": "ImagesPipeline", "function_description": "This method processes an image by converting it to RGB format and optionally resizing it. It then saves the processed image as a JPEG into a byte buffer for pipeline use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "get_media_requests", "line_number": 168, "body": "def get_media_requests(self, item, info):\n        urls = ItemAdapter(item).get(self.images_urls_field, [])\n        return [Request(u) for u in urls]", "is_method": true, "class_name": "ImagesPipeline", "function_description": "This method extracts image URLs from a scraped item and converts them into web requests, enabling the pipeline to download the associated media files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "item_completed", "line_number": 172, "body": "def item_completed(self, results, item, info):\n        with suppress(KeyError):\n            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n        return item", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Updates the scraped item by adding a list of successfully downloaded images to a designated field, ensuring the item reflects processed media results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "file_path", "line_number": 177, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return f'full/{image_guid}.jpg'", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Generates a unique file path for a downloaded image based on its URL. This method provides the storage location for images processed by the ImagesPipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "thumb_path", "line_number": 181, "body": "def thumb_path(self, request, thumb_id, response=None, info=None):\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return f'thumbs/{thumb_id}/{thumb_guid}.jpg'", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Generates a standardized file path for image thumbnails. This path includes a unique identifier derived from the original image URL and the requested thumbnail size, enabling organized storage and retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 49, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(os.path.dirname(absolute_path), info)\n        with open(absolute_path, 'wb') as f:\n            f.write(buf.getvalue())", "is_method": true, "class_name": "FSFilesStore", "function_description": "This `FSFilesStore` method persists binary data from a buffer to a specified file on the local filesystem. It handles directory creation for reliable storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 55, "body": "def stat_file(self, path, info):\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = os.path.getmtime(absolute_path)\n        except os.error:\n            return {}\n\n        with open(absolute_path, 'rb') as f:\n            checksum = md5sum(f)\n\n        return {'last_modified': last_modified, 'checksum': checksum}", "is_method": true, "class_name": "FSFilesStore", "function_description": "Retrieves essential metadata for a file, including its last modification time and content checksum, useful for integrity checks or change detection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_filesystem_path", "line_number": 67, "body": "def _get_filesystem_path(self, path):\n        path_comps = path.split('/')\n        return os.path.join(self.basedir, *path_comps)", "is_method": true, "class_name": "FSFilesStore", "function_description": "This internal method converts a logical file store path into a complete, absolute path on the underlying filesystem. It provides the necessary system-specific path for file operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_mkdir", "line_number": 71, "body": "def _mkdir(self, dirname, domain=None):\n        seen = self.created_directories[domain] if domain else set()\n        if dirname not in seen:\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n            seen.add(dirname)", "is_method": true, "class_name": "FSFilesStore", "function_description": "Ensures a specified directory exists on the filesystem. It efficiently avoids redundant creation attempts by tracking already processed directories within a given domain."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 110, "body": "def stat_file(self, path, info):\n        def _onsuccess(boto_key):\n            checksum = boto_key['ETag'].strip('\"')\n            last_modified = boto_key['LastModified']\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {'checksum': checksum, 'last_modified': modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)", "is_method": true, "class_name": "S3FilesStore", "function_description": "This method retrieves essential metadata for a file stored in S3, including its checksum and last modification timestamp. It provides crucial information for verifying file integrity and freshness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_boto_key", "line_number": 119, "body": "def _get_boto_key(self, path):\n        key_name = f'{self.prefix}{path}'\n        return threads.deferToThread(\n            self.s3_client.head_object,\n            Bucket=self.bucket,\n            Key=key_name)", "is_method": true, "class_name": "S3FilesStore", "function_description": "Asynchronously checks for the existence and retrieves metadata of a specified S3 object. It returns a deferred object representing the result of the S3 HEAD request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 126, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f'{self.prefix}{path}'\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return threads.deferToThread(\n            self.s3_client.put_object,\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra)", "is_method": true, "class_name": "S3FilesStore", "function_description": "Uploads a file, represented by a buffer, to the S3 bucket associated with the store. This method handles key naming, metadata, and performs the upload asynchronously."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_headers_to_botocore_kwargs", "line_number": 142, "body": "def _headers_to_botocore_kwargs(self, headers):\n        \"\"\" Convert headers to botocore keyword agruments.\n        \"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaselessDict({\n            'Content-Type': 'ContentType',\n            'Cache-Control': 'CacheControl',\n            'Content-Disposition': 'ContentDisposition',\n            'Content-Encoding': 'ContentEncoding',\n            'Content-Language': 'ContentLanguage',\n            'Content-Length': 'ContentLength',\n            'Content-MD5': 'ContentMD5',\n            'Expires': 'Expires',\n            'X-Amz-Grant-Full-Control': 'GrantFullControl',\n            'X-Amz-Grant-Read': 'GrantRead',\n            'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',\n            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',\n            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',\n            'X-Amz-Request-Payer': 'RequestPayer',\n            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',\n            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',\n            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',\n            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',\n            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',\n            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',\n            'X-Amz-Storage-Class': 'StorageClass',\n            'X-Amz-Tagging': 'Tagging',\n            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n        })\n        extra = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            else:\n                extra[kwarg] = value\n        return extra", "is_method": true, "class_name": "S3FilesStore", "function_description": "Converts common HTTP-like headers into the specific keyword arguments required by `botocore` for S3 API calls. This ensures proper interpretation of S3 object metadata and control options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 215, "body": "def stat_file(self, path, info):\n        def _onsuccess(blob):\n            if blob:\n                checksum = blob.md5_hash\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {'checksum': checksum, 'last_modified': last_modified}\n            else:\n                return {}\n\n        return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)", "is_method": true, "class_name": "GCSFilesStore", "function_description": "The `GCSFilesStore` method asynchronously retrieves a file's checksum and last modification timestamp from Google Cloud Storage, providing essential metadata for integrity checks or synchronization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_content_type", "line_number": 226, "body": "def _get_content_type(self, headers):\n        if headers and 'Content-Type' in headers:\n            return headers['Content-Type']\n        else:\n            return 'application/octet-stream'", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Provides the MIME content type for a file, extracting it from headers or supplying a default. This ensures proper file handling within the GCS file store."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 232, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        blob = self.bucket.blob(self.prefix + path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY\n        )", "is_method": true, "class_name": "GCSFilesStore", "function_description": "This method uploads file content from a buffer to Google Cloud Storage, applying specified metadata and access policies asynchronously. It provides a service for storing files remotely."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 261, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        path = f'{self.basedir}/{path}'\n        return threads.deferToThread(\n            ftp_store_file, path=path, file=buf,\n            host=self.host, port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.USE_ACTIVE_MODE\n        )", "is_method": true, "class_name": "FTPFilesStore", "function_description": "This method asynchronously stores a file to the configured FTP server. It takes a file buffer and path, deferring the actual transfer to a separate thread for non-blocking operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 269, "body": "def stat_file(self, path, info):\n        def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary(f'RETR {file_path}', m.update)\n                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n        return threads.deferToThread(_stat_file, path)", "is_method": true, "class_name": "FTPFilesStore", "function_description": "This method asynchronously connects to an FTP server to retrieve metadata for a specified file, including its last modification time and MD5 checksum. It provides file status information for integrity checks or change detection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "from_settings", "line_number": 348, "body": "def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)", "is_method": true, "class_name": "FilesPipeline", "function_description": "This class method initializes a `FilesPipeline` instance, configuring various cloud and FTP file storage backends with credentials and policies extracted from application settings. It centralizes the setup of diverse storage options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_store", "line_number": 370, "body": "def _get_store(self, uri):\n        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n            scheme = 'file'\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)", "is_method": true, "class_name": "FilesPipeline", "function_description": "This method dynamically creates and returns the appropriate file storage backend instance. It determines the correct store based on the URI scheme, including support for absolute local paths."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_to_download", "line_number": 378, "body": "def media_to_download(self, request, info, *, item=None):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}\n\n        path = self.file_path(request, info=info, item=item)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f:\n            logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n        )\n        return dfd", "is_method": true, "class_name": "FilesPipeline", "function_description": "This `FilesPipeline` method determines if a requested media file is already downloaded and remains valid based on its age and expiry settings. It prevents unnecessary re-downloads of up-to-date files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_failed", "line_number": 416, "body": "def media_failed(self, failure, request, info):\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                'File (unknown-error): Error downloading %(medianame)s from '\n                '%(request)s referred in <%(referer)s>: %(exception)s',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer, 'exception': failure.value},\n                extra={'spider': info.spider}\n            )\n\n        raise FileException", "is_method": true, "class_name": "FilesPipeline", "function_description": "Handles media download failures within the `FilesPipeline`. It logs unexpected errors with detailed information, then raises a `FileException` to signal the definitive failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_downloaded", "line_number": 429, "body": "def media_downloaded(self, response, request, info, *, item=None):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('download-error')\n\n        if not response.body:\n            logger.warning(\n                'File (empty-content): Empty file from %(request)s referred '\n                'in <%(referer)s>: no-content',\n                {'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('empty-content')\n\n        status = 'cached' if 'cached' in response.flags else 'downloaded'\n        logger.debug(\n            'File (%(status)s): Downloaded file from %(request)s referred in '\n            '<%(referer)s>',\n            {'status': status, 'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                'File (error): Error processing file from %(request)s '\n                'referred in <%(referer)s>: %(errormsg)s',\n                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n                extra={'spider': info.spider}, exc_info=True\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                'File (unknown-error): Error processing file from %(request)s '\n                'referred in <%(referer)s>',\n                {'request': request, 'referer': referer},\n                exc_info=True, extra={'spider': info.spider}\n            )\n            raise FileException(str(exc))\n\n        return {'url': request.url, 'path': path, 'checksum': checksum, 'status': status}", "is_method": true, "class_name": "FilesPipeline", "function_description": "This method of the `FilesPipeline` validates and processes a media file after it's downloaded. It handles download errors, determines the storage path, calculates a checksum, and returns essential file metadata."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "inc_stats", "line_number": 482, "body": "def inc_stats(self, spider, status):\n        spider.crawler.stats.inc_value('file_count', spider=spider)\n        spider.crawler.stats.inc_value(f'file_status_count/{status}', spider=spider)", "is_method": true, "class_name": "FilesPipeline", "function_description": "Increments file processing statistics for a spider, tracking both the total number of files and counts by specific processing status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "get_media_requests", "line_number": 487, "body": "def get_media_requests(self, item, info):\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        return [Request(u) for u in urls]", "is_method": true, "class_name": "FilesPipeline", "function_description": "This method generates requests for media files from the provided item. It is a key step in a file downloading pipeline, enabling the retrieval of associated media from scraped data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "file_downloaded", "line_number": 491, "body": "def file_downloaded(self, response, request, info, *, item=None):\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum", "is_method": true, "class_name": "FilesPipeline", "function_description": "Handles a successfully downloaded file within the pipeline. It calculates an MD5 checksum and persists the file content to the configured storage system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "item_completed", "line_number": 499, "body": "def item_completed(self, results, item, info):\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item", "is_method": true, "class_name": "FilesPipeline", "function_description": "For a `FilesPipeline`, this method filters and assigns successful file operation results to a designated field within the processed item. This integrates file outcomes directly into the data item for subsequent processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "file_path", "line_number": 504, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        media_ext = os.path.splitext(request.url)[1]\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = ''\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = mimetypes.guess_extension(media_type)\n        return f'full/{media_guid}{media_ext}'", "is_method": true, "class_name": "FilesPipeline", "function_description": "Generates a unique, standardized file path for a media URL, including an identifier and a determined file extension. This method is used by a files pipeline to organize downloaded content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 111, "body": "def _onsuccess(boto_key):\n            checksum = boto_key['ETag'].strip('\"')\n            last_modified = boto_key['LastModified']\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {'checksum': checksum, 'last_modified': modified_stamp}", "is_method": true, "class_name": "S3FilesStore", "function_description": "Processes S3 object metadata from a successful operation, extracting its ETag (checksum) and last modification timestamp into a standardized format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 216, "body": "def _onsuccess(blob):\n            if blob:\n                checksum = blob.md5_hash\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {'checksum': checksum, 'last_modified': last_modified}\n            else:\n                return {}", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Provides essential metadata (checksum and last modified time) from a Google Cloud Storage blob object upon successful operations. It helps track file integrity and changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_stat_file", "line_number": 270, "body": "def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary(f'RETR {file_path}', m.update)\n                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}", "is_method": true, "class_name": "FTPFilesStore", "function_description": "Connects to an FTP server to retrieve a file's last modified timestamp and MD5 checksum. Returns an empty dictionary if the file is not found or accessible."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 379, "body": "def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}", "is_method": true, "class_name": "FilesPipeline", "function_description": "Checks if a file processed by the pipeline is still valid based on its age and an expiration policy. It signals if re-download is required or confirms its uptodate status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "_get_mwlist_from_settings", "line_number": 17, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "For the `ItemPipelineManager`, this helper method constructs the list of item processing pipelines from the application's configured settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "_add_middleware", "line_number": 20, "body": "def _add_middleware(self, pipe):\n        super(ItemPipelineManager, self)._add_middleware(pipe)\n        if hasattr(pipe, 'process_item'):\n            self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "As part of ItemPipelineManager, this method integrates a pipeline component, registering its item processing method for subsequent execution within the pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "process_item", "line_number": 25, "body": "def process_item(self, item, spider):\n        return self._process_chain('process_item', item, spider)", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "Orchestrates the processing of a scraped item by dispatching it through the configured item pipeline. This ensures the item undergoes all necessary post-extraction stages."}]