[{"file": "./dataset/RepoExec/test-apps/scrapy/extras/qpsclient.py", "function": "parse", "line_number": 54, "body": "def parse(self, response):\n        pass", "is_method": true, "class_name": "QPSSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_get_mwlist_from_settings", "line_number": 24, "body": "def _get_mwlist_from_settings(cls, settings):\n        raise NotImplementedError", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "open", "line_number": 17, "body": "def open(self):  # can return deferred\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "close", "line_number": 20, "body": "def close(self, reason):  # can return a deferred\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "log", "line_number": 23, "body": "def log(self, request, spider):  # log that a request has been filtered\n        pass", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "open_spider", "line_number": 41, "body": "def open_spider(self, spider):\n        pass", "is_method": true, "class_name": "StatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "_persist_stats", "line_number": 50, "body": "def _persist_stats(self, stats, spider):\n        pass", "is_method": true, "class_name": "StatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_value", "line_number": 69, "body": "def set_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_stats", "line_number": 72, "body": "def set_stats(self, stats, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "inc_value", "line_number": 75, "body": "def inc_value(self, key, count=1, start=0, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "max_value", "line_number": 78, "body": "def max_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "min_value", "line_number": 81, "body": "def min_value(self, key, value, spider=None):\n        pass", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "from_crawler", "line_number": 76, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n            return cls()", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 33, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n        pass", "is_method": true, "class_name": "RobotParser", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 46, "body": "def allowed(self, url, user_agent):\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: str\n\n        :param user_agent: User agent\n        :type user_agent: str\n        \"\"\"\n        pass", "is_method": true, "class_name": "RobotParser", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "cancel", "line_number": 58, "body": "def cancel(self):\n        raise NotImplementedError()", "is_method": true, "class_name": "HostResolution", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolutionComplete", "line_number": 77, "body": "def resolutionComplete(self):\n        self.resolutionReceiver.resolutionComplete()\n        if self.addresses:\n            dnscache[self.hostName] = self.addresses", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_command", "line_number": 149, "body": "def _run_command(cmd, args, opts):\n    if opts.profile:\n        _run_command_profiled(cmd, args, opts)\n    else:\n        cmd.run(args, opts)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_restore_callbacks", "line_number": 178, "body": "def _restore_callbacks(result):\n        request.callback = request_callback\n        request.errback = request_errback\n        return result", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 44, "body": "def export_item(self, item):\n        raise NotImplementedError", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 51, "body": "def start_exporting(self):\n        pass", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 54, "body": "def finish_exporting(self):\n        pass", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_callback_output", "line_number": 119, "body": "def process_callback_output(result):\n            return self._process_callback_output(response, spider, result)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_spider_exception", "line_number": 122, "body": "def process_spider_exception(_failure):\n            return self._process_spider_exception(response, spider, _failure)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_deactivate", "line_number": 88, "body": "def _deactivate(response):\n            self.active.remove(request)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_deactivate", "line_number": 122, "body": "def _deactivate(response):\n            slot.active.remove(request)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_downloaded", "line_number": 168, "body": "def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response", "is_method": true, "class_name": "Downloader", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "pauseProducing", "line_number": 480, "body": "def pauseProducing(self):\n        pass", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "stopProducing", "line_number": 483, "body": "def stopProducing(self):\n        pass", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "_print_listening", "line_number": 40, "body": "def _print_listening():\n        httpHost = httpPort.getHost()\n        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "f", "line_number": 149, "body": "def f(*coro_args, **coro_kwargs):\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 33, "body": "def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 16, "body": "def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "buffer_data", "line_number": 48, "body": "def buffer_data(data):\n        ftp_data.append(data)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "_chunk_iter", "line_number": 133, "body": "def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "new_method", "line_number": 160, "body": "def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "syntax", "line_number": 32, "body": "def syntax(self):\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "short_desc", "line_number": 38, "body": "def short_desc(self):\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "run", "line_number": 104, "body": "def run(self, args, opts):\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "update_vars", "line_number": 43, "body": "def update_vars(self, vars):\n        \"\"\"You can use this function to update the Scrapy objects that will be\n        available in the shell\n        \"\"\"\n        pass", "is_method": true, "class_name": "Command", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "acquire", "line_number": 110, "body": "def acquire(self):\n        pass", "is_method": true, "class_name": "_DummyLock", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "release", "line_number": 113, "body": "def release(self):\n        pass", "is_method": true, "class_name": "_DummyLock", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/common.py", "function": "newsetter", "line_number": 2, "body": "def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n        raise AttributeError(msg)", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_identity_process_request", "line_number": 21, "body": "def _identity_process_request(request, response):\n    return request", "is_method": false, "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "parse_start_url", "line_number": 78, "body": "def parse_start_url(self, response, **kwargs):\n        return []", "is_method": true, "class_name": "CrawlSpider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "parse", "line_number": 92, "body": "def parse(self, response, **kwargs):\n        raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')", "is_method": true, "class_name": "Spider", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__iter__", "line_number": 424, "body": "def __iter__(self, k, v):\n        return iter(self.o)", "is_method": true, "class_name": "_DictProxy", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 77, "body": "def _store_in_thread(self, file):\n        raise NotImplementedError", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 97, "body": "def store(self, file):\n        pass", "is_method": true, "class_name": "StdoutFeedStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "build_instance", "line_number": 462, "body": "def build_instance(builder, *preargs):\n            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "close_spider", "line_number": 286, "body": "def close_spider(self, spider):\n        pass", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 37, "body": "def referrer(self, response_url, request_url):\n        raise NotImplementedError()", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "eb_wrapper", "line_number": 162, "body": "def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)", "is_method": true, "class_name": "ContractsManager", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "__str__", "line_number": 175, "body": "def __str__(_self):\n            return f\"[{spider}] {method.__name__} ({desc})\"", "is_method": true, "class_name": "ContractTestCase", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "wrapper", "line_number": 50, "body": "def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output", "is_method": true, "class_name": "Contract", "function_description": "Not sure"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_to_download", "line_number": 220, "body": "def media_to_download(self, request, info, *, item=None):\n        \"\"\"Check request before starting download\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "get_media_requests", "line_number": 224, "body": "def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "file_path", "line_number": 249, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n        pass", "is_method": true, "class_name": "MediaPipeline", "function_description": "Not Implemented"}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 379, "body": "def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}", "is_method": true, "class_name": "FilesPipeline", "function_description": "Not sure"}]