[{"file": "./dataset/RepoExec/test-apps/scrapy/setup.py", "function": "has_environment_marker_platform_impl_support", "line_number": 10, "body": "def has_environment_marker_platform_impl_support():\n    \"\"\"Code extracted from 'pytest/setup.py'\n    https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31\n\n    The first known release to support environment marker with range operators\n    it is 18.5, see:\n    https://setuptools.readthedocs.io/en/latest/history.html#id235\n    \"\"\"\n    return parse_version(setuptools_version) >= parse_version('18.5')", "is_method": false, "function_description": "Utility function that checks if the current setuptools version supports environment markers with range operators, enabling conditional dependency specifications based on environment markers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "_py_files", "line_number": 10, "body": "def _py_files(folder):\n    return (str(p) for p in Path(folder).rglob('*.py'))", "is_method": false, "function_description": "Generator function that yields paths of all Python files within a given folder and its subdirectories, enabling recursive file discovery for processing or analysis tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "chdir", "line_number": 30, "body": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()", "is_method": false, "function_description": "Function that switches the current working directory to a pytest-provided temporary directory, facilitating isolated file system operations during testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_collection_modifyitems", "line_number": 35, "body": "def pytest_collection_modifyitems(session, config, items):\n    # Avoid executing tests when executing `--flake8` flag (pytest-flake8)\n    try:\n        from pytest_flake8 import Flake8Item\n        if config.getoption('--flake8'):\n            items[:] = [item for item in items if isinstance(item, Flake8Item)]\n    except ImportError:\n        pass", "is_method": false, "function_description": "Modifies pytest's collected test items to run only flake8 checks when the `--flake8` flag is used, preventing execution of other tests during flake8 linting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_addoption", "line_number": 45, "body": "def pytest_addoption(parser):\n    parser.addoption(\n        \"--reactor\",\n        default=\"default\",\n        choices=[\"default\", \"asyncio\"],\n    )", "is_method": false, "function_description": "Adds a command-line option to select the reactor implementation for pytest, enabling users to specify between \"default\" and \"asyncio\" event loops during test runs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "reactor_pytest", "line_number": 54, "body": "def reactor_pytest(request):\n    if not request.cls:\n        # doctests\n        return\n    request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n    return request.cls.reactor_pytest", "is_method": false, "function_description": "Utility function for pytest that sets and returns a testing configuration flag on the test class based on a command-line option \"--reactor\". It enables conditional test behavior related to reactor settings during test execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "only_asyncio", "line_number": 63, "body": "def only_asyncio(request, reactor_pytest):\n    if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n        pytest.skip('This test is only run with --reactor=asyncio')", "is_method": false, "function_description": "Function that conditionally skips a test unless it is run using the asyncio reactor, enforcing test execution only in the asyncio environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/conftest.py", "function": "pytest_configure", "line_number": 68, "body": "def pytest_configure(config):\n    if config.getoption(\"--reactor\") == \"asyncio\":\n        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")", "is_method": false, "function_description": "Function to conditionally configure the test environment to use the asyncio reactor for Twisted when the corresponding pytest option is set. It enables integration of asyncio event loop with Twisted during test execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conf.py", "function": "setup", "line_number": 315, "body": "def setup(app):\n    app.connect('autodoc-skip-member', maybe_skip_member)", "is_method": false, "function_description": "This function registers a handler to the app that determines whether to skip members during automatic documentation generation. It integrates custom filtering logic into the app's autodoc process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conf.py", "function": "maybe_skip_member", "line_number": 319, "body": "def maybe_skip_member(app, what, name, obj, skip, options):\n    if not skip:\n        # autodocs was generating a text \"alias of\" for the following members\n        # https://github.com/sphinx-doc/sphinx/issues/4422\n        return name in {'default_item_class', 'default_selector_class'}\n    return skip", "is_method": false, "function_description": "This function determines whether to skip certain members during documentation generation, specifically forcing inclusion of particular members to avoid unwanted \"alias of\" text in autodocs output."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conftest.py", "function": "load_response", "line_number": 11, "body": "def load_response(url, filename):\n    input_path = os.path.join(os.path.dirname(__file__), '_tests', filename)\n    with open(input_path, 'rb') as input_file:\n        return HtmlResponse(url, body=input_file.read())", "is_method": false, "function_description": "This function loads an HTML response from a local test file corresponding to a given URL, facilitating testing or simulation of web responses without actual network requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/conftest.py", "function": "setup", "line_number": 17, "body": "def setup(namespace):\n    namespace['load_response'] = load_response", "is_method": false, "function_description": "This function adds the load_response function to a given namespace, enabling dynamic extension or injection of that capability into the namespace's scope. It facilitates modular setup by binding specific functions to provided contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/utils/linkfix.py", "function": "main", "line_number": 18, "body": "def main():\n\n    # Used for remembering the file (and its contents)\n    # so we don't have to open the same file again.\n    _filename = None\n    _contents = None\n\n    # A regex that matches standard linkcheck output lines\n    line_re = re.compile(r'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n\n    # Read lines from the linkcheck output file\n    try:\n        with open(\"build/linkcheck/output.txt\") as out:\n            output_lines = out.readlines()\n    except IOError:\n        print(\"linkcheck output not found; please run linkcheck first.\")\n        exit(1)\n\n    # For every line, fix the respective file\n    for line in output_lines:\n        match = re.match(line_re, line)\n\n        if match:\n            newfilename = match.group(1)\n            errortype = match.group(2)\n\n            # Broken links can't be fixed and\n            # I am not sure what do with the local ones.\n            if errortype.lower() in [\"broken\", \"local\"]:\n                print(\"Not Fixed: \" + line)\n            else:\n                # If this is a new file\n                if newfilename != _filename:\n\n                    # Update the previous file\n                    if _filename:\n                        with open(_filename, \"w\") as _file:\n                            _file.write(_contents)\n\n                    _filename = newfilename\n\n                    # Read the new file to memory\n                    with open(_filename) as _file:\n                        _contents = _file.read()\n\n                _contents = _contents.replace(match.group(3), match.group(4))\n        else:\n            # We don't understand what the current line means!\n            print(\"Not Understood: \" + line)", "is_method": false, "function_description": "Script function that parses a linkcheck output file to identify fixable link errors, then updates the corresponding source files by replacing incorrect links with corrected ones. It skips broken or local link errors and reports unrecognized lines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "is_setting_index", "line_number": 17, "body": "def is_setting_index(node):\n    if node.tagname == 'index':\n        # index entries for setting directives look like:\n        # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n        entry_type, info, refid = node['entries'][0][:3]\n        return entry_type == 'pair' and info.endswith('; setting')\n    return False", "is_method": false, "function_description": "Function that checks if a given node represents an index entry specifically for a setting directive, identifying it by the node's tag and entry attributes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "get_setting_target", "line_number": 26, "body": "def get_setting_target(node):\n    # target nodes are placed next to the node in the doc tree\n    return node.parent[node.parent.index(node) + 1]", "is_method": false, "function_description": "Returns the sibling node immediately following the given node within its parent's children, facilitating traversal or retrieval of related nodes in a document tree structure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "get_setting_name_and_refid", "line_number": 31, "body": "def get_setting_name_and_refid(node):\n    \"\"\"Extract setting name from directive index node\"\"\"\n    entry_type, info, refid = node['entries'][0][:3]\n    return info.replace('; setting', ''), refid", "is_method": false, "function_description": "This function extracts and returns the setting name and reference ID from a directive index node, supporting retrieval of specific settings metadata for documentation or configuration tools."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "collect_scrapy_settings_refs", "line_number": 37, "body": "def collect_scrapy_settings_refs(app, doctree):\n    env = app.builder.env\n\n    if not hasattr(env, 'scrapy_all_settings'):\n        env.scrapy_all_settings = []\n\n    for node in doctree.traverse(is_setting_index):\n        targetnode = get_setting_target(node)\n        assert isinstance(targetnode, nodes.target), \"Next node is not a target\"\n\n        setting_name, refid = get_setting_name_and_refid(node)\n\n        env.scrapy_all_settings.append({\n            'docname': env.docname,\n            'setting_name': setting_name,\n            'refid': refid,\n        })", "is_method": false, "function_description": "Function that collects and records references to Scrapy settings found in a documentation tree, storing their names and locations for later use during the documentation build process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "make_setting_element", "line_number": 56, "body": "def make_setting_element(setting_data, app, fromdocname):\n    refnode = make_refnode(app.builder, fromdocname,\n                           todocname=setting_data['docname'],\n                           targetid=setting_data['refid'],\n                           child=nodes.Text(setting_data['setting_name']))\n    p = nodes.paragraph()\n    p += refnode\n\n    item = nodes.list_item()\n    item += p\n    return item", "is_method": false, "function_description": "Creates a list item node containing a reference link to a specific setting within documentation, facilitating structured and navigable settings listings in generated docs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "replace_settingslist_nodes", "line_number": 69, "body": "def replace_settingslist_nodes(app, doctree, fromdocname):\n    env = app.builder.env\n\n    for node in doctree.traverse(settingslist_node):\n        settings_list = nodes.bullet_list()\n        settings_list.extend([make_setting_element(d, app, fromdocname)\n                              for d in sorted(env.scrapy_all_settings,\n                                              key=itemgetter('setting_name'))\n                              if fromdocname != d['docname']])\n        node.replace_self(settings_list)", "is_method": false, "function_description": "This function replaces custom settings list nodes in a document tree with sorted bullet lists of settings, excluding those from the current document. It facilitates dynamic insertion of global configuration summaries in documentation builds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "setup", "line_number": 81, "body": "def setup(app):\n    app.add_crossref_type(\n        directivename = \"setting\",\n        rolename      = \"setting\",\n        indextemplate = \"pair: %s; setting\",\n    )\n    app.add_crossref_type(\n        directivename = \"signal\",\n        rolename      = \"signal\",\n        indextemplate = \"pair: %s; signal\",\n    )\n    app.add_crossref_type(\n        directivename = \"command\",\n        rolename      = \"command\",\n        indextemplate = \"pair: %s; command\",\n    )\n    app.add_crossref_type(\n        directivename = \"reqmeta\",\n        rolename      = \"reqmeta\",\n        indextemplate = \"pair: %s; reqmeta\",\n    )\n    app.add_role('source', source_role)\n    app.add_role('commit', commit_role)\n    app.add_role('issue', issue_role)\n    app.add_role('rev', rev_role)\n\n    app.add_node(settingslist_node)\n    app.add_directive('settingslist', SettingsListDirective)\n\n    app.connect('doctree-read', collect_scrapy_settings_refs)\n    app.connect('doctree-resolved', replace_settingslist_nodes)", "is_method": false, "function_description": "Setup function that configures custom cross-reference types, roles, nodes, and directives for documentation processing within an application, enabling structured referencing and enhanced linking capabilities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "source_role", "line_number": 114, "body": "def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Creates a hyperlink node pointing to a specific GitHub source file URL based on provided text. This function is useful for adding references to source code locations within a document."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "issue_role", "line_number": 121, "body": "def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/issues/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'issue ' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Creates a reference node linking to a specific Scrapy GitHub issue, facilitating issue referencing within parsed text or documentation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "commit_role", "line_number": 128, "body": "def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'https://github.com/scrapy/scrapy/commit/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'commit ' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Creates a reference node linking a commit identifier to its GitHub URL, facilitating commit citation in documentation with customizable display options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "rev_role", "line_number": 135, "body": "def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = 'http://hg.scrapy.org/scrapy/changeset/' + text\n    set_classes(options)\n    node = nodes.reference(rawtext, 'r' + text, refuri=ref, **options)\n    return [node], []", "is_method": false, "function_description": "Creates a reference node linking to a specific Scrapy changeset URL based on given text, suitable for use in documentation or markup processing to generate clickable change references."}, {"file": "./dataset/RepoExec/test-apps/scrapy/docs/_ext/scrapydocs.py", "function": "run", "line_number": 13, "body": "def run(self):\n        return [settingslist_node('')]", "is_method": true, "class_name": "SettingsListDirective", "function_description": "Returns a list containing a single settings list node, serving as the directive's output for processing configuration or settings-related content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qpsclient.py", "function": "start_requests", "line_number": 37, "body": "def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += f'?latency={self.latency}'\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace('localhost', f'127.0.0.{x + 1}') for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1", "is_method": true, "class_name": "QPSSpider", "function_description": "Generates an infinite sequence of HTTP requests targeting configured URLs, optionally distributing requests across multiple local IP addresses based on slot count and latency settings. Useful for load testing or crawling with concurrent connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "_reset_stats", "line_number": 17, "body": "def _reset_stats(self):\n        self.tail.clear()\n        self.start = self.lastmark = self.lasttime = time()", "is_method": true, "class_name": "Root", "function_description": "Private method in the Root class that resets internal timing and marker statistics, clearing tracked data to prepare for new measurements or state tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "getChild", "line_number": 21, "body": "def getChild(self, request, name):\n        return self", "is_method": true, "class_name": "Root", "function_description": "Returns the current instance regardless of input, potentially serving as a placeholder or default behavior for retrieving a child element by name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "render", "line_number": 24, "body": "def render(self, request):\n        now = time()\n        delta = now - self.lasttime\n\n        # reset stats on high iter-request times caused by client restarts\n        if delta > 3: # seconds\n            self._reset_stats()\n            return ''\n\n        self.tail.appendleft(delta)\n        self.lasttime = now\n        self.concurrent += 1\n\n        if now - self.lastmark >= 3:\n            self.lastmark = now\n            qps = len(self.tail) / sum(self.tail)\n            print(f'samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}')\n\n        if 'latency' in request.args:\n            latency = float(request.args['latency'][0])\n            reactor.callLater(latency, self._finish, request)\n            return NOT_DONE_YET\n\n        self.concurrent -= 1\n        return ''", "is_method": true, "class_name": "Root", "function_description": "Provides basic request handling with concurrent request tracking, calculates and logs queries-per-second metrics, and supports delayed response completion based on latency request parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/extras/qps-bench-server.py", "function": "_finish", "line_number": 50, "body": "def _finish(self, request):\n        self.concurrent -= 1\n        if not request.finished and not request._disconnected:\n            request.finish()", "is_method": true, "class_name": "Root", "function_description": "Internal method of the Root class that decrements concurrency count and ensures a given request is properly finished if it is still active and connected."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "crawl", "line_number": 80, "body": "def crawl(self, *args, **kwargs):\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        self.crawling = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield defer.maybeDeferred(self.engine.start)\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n            raise", "is_method": true, "class_name": "Crawler", "function_description": "Provides an asynchronous crawling process that manages spider creation, engine initialization, and execution control while ensuring only one crawl runs at a time and handling cleanup on errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_spider", "line_number": 97, "body": "def _create_spider(self, *args, **kwargs):\n        return self.spidercls.from_crawler(self, *args, **kwargs)", "is_method": true, "class_name": "Crawler", "function_description": "Internal helper of Crawler that instantiates a spider object using class method from_crawler with passed arguments, enabling spider creation tied to the crawler context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_engine", "line_number": 100, "body": "def _create_engine(self):\n        return ExecutionEngine(self, lambda _: self.stop())", "is_method": true, "class_name": "Crawler", "function_description": "Private method of the Crawler class that initializes an execution engine configured to stop the crawler when triggered. It supports controlled execution flow management within the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "stop", "line_number": 104, "body": "def stop(self):\n        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        if self.crawling:\n            self.crawling = False\n            yield defer.maybeDeferred(self.engine.stop)", "is_method": true, "class_name": "Crawler", "function_description": "Provides a method to gracefully stop the crawling process, signaling completion via a deferred trigger once the crawler halts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_get_spider_loader", "line_number": 132, "body": "def _get_spider_loader(settings):\n        \"\"\" Get SpiderLoader instance from settings \"\"\"\n        cls_path = settings.get('SPIDER_LOADER_CLASS')\n        loader_cls = load_object(cls_path)\n        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n        try:\n            verifyClass(ISpiderLoader, loader_cls)\n        except excs:\n            warnings.warn(\n                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n                'Please add all missing methods to avoid unexpected runtime errors.',\n                category=ScrapyDeprecationWarning, stacklevel=2\n            )\n        return loader_cls.from_settings(settings.frozencopy())", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Returns a configured SpiderLoader instance based on given settings, verifying interface compliance and warning about partial implementations to ensure correct spider management in the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "spiders", "line_number": 159, "body": "def spiders(self):\n        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n                      \"CrawlerRunner.spider_loader.\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        return self.spider_loader", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Deprecated property accessor in CrawlerRunner that warns users about the renaming of the spiders attribute to spider_loader and returns the updated spider_loader attribute for backward compatibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "crawl", "line_number": 165, "body": "def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Core method of the CrawlerRunner class that initiates and manages the execution of a crawl process using a given crawler or spider class, returning a deferred object that signals when the crawl completes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_crawl", "line_number": 194, "body": "def _crawl(self, crawler, *args, **kwargs):\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result\n\n        return d.addBoth(_done)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Internal method of CrawlerRunner that manages a crawler's lifecycle by starting its crawl process, tracking its activity, and updating internal state upon completion or failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "create_crawler", "line_number": 207, "body": "def create_crawler(self, crawler_or_spidercls):\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Creates and returns a Crawler object based on the input, which can be an existing Crawler, a Spider subclass, or a spider name string. This method standardizes crawler instantiation for flexible crawling setups."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_create_crawler", "line_number": 226, "body": "def _create_crawler(self, spidercls):\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Creates and returns a Crawler instance for a given spider class, loading it by name if necessary. This enables dynamic instantiation of crawlers based on spider definitions within the crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "stop", "line_number": 231, "body": "def stop(self):\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return defer.DeferredList([c.stop() for c in list(self.crawlers)])", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Method of CrawlerRunner that stops all active crawling jobs simultaneously and returns a deferred notifying when all have completed, enabling coordinated shutdown of multiple crawlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "join", "line_number": 240, "body": "def join(self):\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Core method of CrawlerRunner that provides a deferred triggering once all managed crawlers have finished their execution, enabling synchronization with asynchronous crawler processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_handle_twisted_reactor", "line_number": 250, "body": "def _handle_twisted_reactor(self):\n        if self.settings.get(\"TWISTED_REACTOR\"):\n            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Internal utility of CrawlerRunner that verifies the installation of a specified Twisted reactor based on configuration, ensuring the correct asynchronous event loop is set up before crawling operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_signal_shutdown", "line_number": 285, "body": "def _signal_shutdown(self, signum, _):\n        from twisted.internet import reactor\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                    {'signame': signame})\n        reactor.callFromThread(self._graceful_stop_reactor)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Internal method of CrawlerProcess that handles shutdown signals by initiating a graceful reactor stop, ensuring orderly termination upon receiving specific OS signals."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_signal_kill", "line_number": 293, "body": "def _signal_kill(self, signum, _):\n        from twisted.internet import reactor\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n                    {'signame': signame})\n        reactor.callFromThread(self._stop_reactor)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Handles a repeated termination signal to force an immediate unclean shutdown of the Twisted reactor within the CrawlerProcess. This ensures the crawler stops promptly when receiving multiple kill signals."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "start", "line_number": 301, "body": "def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n        \"\"\"\n        from twisted.internet import reactor\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(self._stop_reactor)\n\n        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n        resolver.install_on_reactor()\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n        reactor.run(installSignalHandlers=False)", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Starts and manages the Twisted reactor for crawling, configuring thread pools and DNS caching. Optionally stops the reactor after all crawlers complete, facilitating controlled asynchronous crawl execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_graceful_stop_reactor", "line_number": 329, "body": "def _graceful_stop_reactor(self):\n        d = self.stop()\n        d.addBoth(self._stop_reactor)\n        return d", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Internal method of CrawlerProcess that initiates a graceful shutdown sequence and ensures the reactor stops properly. It manages asynchronous stop operations to cleanly terminate the process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_stop_reactor", "line_number": 334, "body": "def _stop_reactor(self, _=None):\n        from twisted.internet import reactor\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass", "is_method": true, "class_name": "CrawlerProcess", "function_description": "Internal utility of CrawlerProcess that attempts to stop the Twisted reactor safely, handling cases where it may have already been stopped or is shutting down."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_handle_twisted_reactor", "line_number": 341, "body": "def _handle_twisted_reactor(self):\n        if self.settings.get(\"TWISTED_REACTOR\"):\n            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n        super()._handle_twisted_reactor()", "is_method": true, "class_name": "CrawlerProcess", "function_description": "This internal method configures the Twisted reactor based on settings, ensuring the appropriate event loop integration for asynchronous operations in the crawling process. It enables customization of the reactor for flexible networking behavior."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/crawler.py", "function": "_done", "line_number": 199, "body": "def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result", "is_method": true, "class_name": "CrawlerRunner", "function_description": "Internal utility method in CrawlerRunner that updates tracking sets when a crawler finishes, flags bootstrap failure if the crawler lacks a spider attribute, and returns the given result."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "from_settings", "line_number": 28, "body": "def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                mw = create_instance(mwcls, settings, crawler)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': pprint.pformat(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Factory method of MiddlewareManager that initializes and returns middleware instances based on configuration settings, handling disabled components and logging enabled ones for tracking middleware setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings, crawler)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Factory method of MiddlewareManager that instantiates an object using settings from a crawler, facilitating integration with the crawling framework\u2019s configuration system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_add_middleware", "line_number": 55, "body": "def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].appendleft(mw.close_spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Private method of MiddlewareManager that registers middleware callbacks for spider lifecycle events, enabling dynamic extension of open and close behaviors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_parallel", "line_number": 61, "body": "def _process_parallel(self, methodname, obj, *args):\n        return process_parallel(self.methods[methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Internal helper of MiddlewareManager that concurrently applies a specified method from registered middleware to a given object with provided arguments. It facilitates parallel processing to enhance middleware execution efficiency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_chain", "line_number": 64, "body": "def _process_chain(self, methodname, obj, *args):\n        return process_chain(self.methods[methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Internal helper method in MiddlewareManager that executes a sequence of middleware functions for a given method name, passing the target object and arguments through the chain to process or transform them."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "_process_chain_both", "line_number": 67, "body": "def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n        return process_chain_both(self.methods[cb_methodname],\n                                  self.methods[eb_methodname], obj, *args)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Internal helper method in MiddlewareManager that invokes paired callback and error-handling methods sequentially, enabling coordinated processing of middleware actions with shared arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "open_spider", "line_number": 71, "body": "def open_spider(self, spider):\n        return self._process_parallel('open_spider', spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "MiddlewareManager method that triggers the 'open_spider' event across all middleware components concurrently, facilitating parallel initialization or setup when a spider starts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/middleware.py", "function": "close_spider", "line_number": 74, "body": "def close_spider(self, spider):\n        return self._process_parallel('close_spider', spider)", "is_method": true, "class_name": "MiddlewareManager", "function_description": "Core method of MiddlewareManager that triggers the 'close_spider' process across all managed middleware components in parallel when a spider finishes its execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "connect", "line_number": 10, "body": "def connect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.connect(receiver, signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Core method of SignalManager that connects a receiver function to a specified signal, enabling event-driven communication by registering callbacks for signal dispatching."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "disconnect", "line_number": 27, "body": "def disconnect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.disconnect(receiver, signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Disconnect a receiver from a specified signal, stopping it from being notified when the signal is sent. This method manages event listeners by undoing previous signal connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "send_catch_log", "line_number": 36, "body": "def send_catch_log(self, signal, **kwargs):\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Utility method of SignalManager that sends a signal while catching and logging any exceptions raised by its handlers, ensuring robust signal dispatching with error reporting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "send_catch_log_deferred", "line_number": 46, "body": "def send_catch_log_deferred(self, signal, **kwargs):\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Service method of SignalManager that sends a signal supporting asynchronous handlers returning Deferreds, catches and logs exceptions, and returns a Deferred that fires after all handlers complete. Useful for robust, asynchronous event handling with error logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/signalmanager.py", "function": "disconnect_all", "line_number": 60, "body": "def disconnect_all(self, signal, **kwargs):\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.disconnect_all(signal, **kwargs)", "is_method": true, "class_name": "SignalManager", "function_description": "Utility method in SignalManager that disconnects all receiver functions from a specified signal, allowing cleanup or reconfiguration of signal handlers associated with the manager's sender."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "from_settings", "line_number": 11, "body": "def from_settings(cls, settings):\n        return cls()", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "Factory method in BaseDupeFilter that creates an instance using provided settings, serving as a standardized initialization interface."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_seen", "line_number": 14, "body": "def request_seen(self, request):\n        return False", "is_method": true, "class_name": "BaseDupeFilter", "function_description": "This method always indicates that a given request has not been seen before, effectively disabling duplicate filtering. It serves as a default implementation for duplicate detection in request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "from_settings", "line_number": 42, "body": "def from_settings(cls, settings):\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(job_dir(settings), debug)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Factory method of RFPDupeFilter that creates an instance using configuration settings, including a debug flag and job directory path."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_seen", "line_number": 46, "body": "def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + '\\n')", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Method of RFPDupeFilter that checks if a request has been seen before by fingerprinting it, preventing duplicate processing and optionally logging fingerprints to a file for persistence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "request_fingerprint", "line_number": 54, "body": "def request_fingerprint(self, request):\n        return request_fingerprint(request)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Returns a unique fingerprint identifier for a given request, facilitating duplicate detection in the RFPDupeFilter."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "close", "line_number": 57, "body": "def close(self, reason):\n        if self.file:\n            self.file.close()", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "This method ensures that any open file resource used by RFPDupeFilter is properly closed, supporting clean resource management when the filter is no longer needed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/dupefilters.py", "function": "log", "line_number": 61, "body": "def log(self, request, spider):\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {'request': request, 'referer': referer_str(request)}\n            self.logger.debug(msg, args, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request: %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False\n\n        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)", "is_method": true, "class_name": "RFPDupeFilter", "function_description": "Provides logging for filtered duplicate requests within the RFPDupeFilter, supporting debug display and limiting repeated duplicate logs while tracking filtering statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__instancecheck__", "line_number": 27, "body": "def __instancecheck__(cls, instance):\n        if cls is BaseItem:\n            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__instancecheck__(instance)", "is_method": true, "class_name": "_BaseItemMeta", "function_description": "Overrides instance check to warn when using a deprecated base class, guiding users to update to the preferred class while maintaining standard instance checking behavior."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 39, "body": "def __new__(cls, *args, **kwargs):\n        if issubclass(cls, BaseItem) and not issubclass(cls, (Item, DictItem)):\n            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls, *args, **kwargs)", "is_method": true, "class_name": "BaseItem", "function_description": "Constructor override in BaseItem class that warns about deprecation when instantiating subclasses not derived from Item or DictItem, guiding users towards updated class usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 56, "body": "def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super().__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        return super().__new__(mcs, class_name, bases, new_attrs)", "is_method": true, "class_name": "ItemMeta", "function_description": "Custom metaclass method that dynamically constructs a new class, aggregates Field instances into a fields dictionary, and manages class inheritance to support advanced attribute and metadata handling for ItemMeta-based classes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__new__", "line_number": 81, "body": "def __new__(cls, *args, **kwargs):\n        if issubclass(cls, DictItem) and not issubclass(cls, Item):\n            warn('scrapy.item.DictItem is deprecated, please use scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls, *args, **kwargs)", "is_method": true, "class_name": "DictItem", "function_description": "Constructor override of the DictItem class that issues a deprecation warning when instantiated, guiding users to prefer the Item class instead. It helps manage backward compatibility during the transition from DictItem to Item."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__getitem__", "line_number": 93, "body": "def __getitem__(self, key):\n        return self._values[key]", "is_method": true, "class_name": "DictItem", "function_description": "Provides dictionary-like access to stored values by key, enabling retrieval of items within the DictItem container."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__setitem__", "line_number": 96, "body": "def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")", "is_method": true, "class_name": "DictItem", "function_description": "Overrides item assignment to allow setting values only for predefined fields, ensuring keys are restricted to existing dictionary fields in DictItem instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__delitem__", "line_number": 102, "body": "def __delitem__(self, key):\n        del self._values[key]", "is_method": true, "class_name": "DictItem", "function_description": "Overrides item deletion to remove an entry from the internal dictionary of stored values in the DictItem class. This enables standard dictionary-like deletion behavior for DictItem instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__getattr__", "line_number": 105, "body": "def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n        raise AttributeError(name)", "is_method": true, "class_name": "DictItem", "function_description": "Intercepts attribute access and directs users to access dict item fields using item[name] syntax instead of attributes, enforcing correct field value retrieval in DictItem instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__setattr__", "line_number": 110, "body": "def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n        super().__setattr__(name, value)", "is_method": true, "class_name": "DictItem", "function_description": "Overrides attribute setting to enforce using item assignment syntax for non-private fields, ensuring controlled modification of dictionary-like items in the DictItem class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__len__", "line_number": 115, "body": "def __len__(self):\n        return len(self._values)", "is_method": true, "class_name": "DictItem", "function_description": "Returns the number of items stored in the DictItem instance, enabling users to quickly determine its size."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__iter__", "line_number": 118, "body": "def __iter__(self):\n        return iter(self._values)", "is_method": true, "class_name": "DictItem", "function_description": "Enables iteration over the values stored in a DictItem instance, allowing users to loop through its contents directly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "keys", "line_number": 123, "body": "def keys(self):\n        return self._values.keys()", "is_method": true, "class_name": "DictItem", "function_description": "Returns the set of keys from the internal dictionary values of the DictItem instance, enabling access to its stored keys."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "__repr__", "line_number": 126, "body": "def __repr__(self):\n        return pformat(dict(self))", "is_method": true, "class_name": "DictItem", "function_description": "Provides a readable string representation of the DictItem instance by formatting its dictionary contents for easy inspection and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "copy", "line_number": 129, "body": "def copy(self):\n        return self.__class__(self)", "is_method": true, "class_name": "DictItem", "function_description": "Creates and returns a new instance of DictItem that is a copy of the current object, enabling duplication of dictionary-like items with the same data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/item.py", "function": "deepcopy", "line_number": 132, "body": "def deepcopy(self):\n        \"\"\"Return a :func:`~copy.deepcopy` of this item.\n        \"\"\"\n        return deepcopy(self)", "is_method": true, "class_name": "DictItem", "function_description": "Returns a deep copy of the current DictItem instance, providing an independent duplicate with no shared references."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extension.py", "function": "_get_mwlist_from_settings", "line_number": 15, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('EXTENSIONS'))", "is_method": true, "class_name": "ExtensionManager", "function_description": "Utility method in ExtensionManager that extracts and constructs a list of middleware extensions from configuration settings for further processing or loading."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_mimetype", "line_number": 41, "body": "def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = f\"{mimetype.split('/')[0]}/*\"\n            return self.classes.get(basetype, Response)", "is_method": true, "class_name": "ResponseTypes", "function_description": "Provides the best matching Response class based on a given MIME type, supporting exact and broad MIME type matching for flexible response handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_content_type", "line_number": 51, "body": "def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_unicode(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)", "is_method": true, "class_name": "ResponseTypes", "function_description": "Method of ResponseTypes that selects the appropriate HTTP response class based on the Content-Type header and optionally content encoding, facilitating correct handling of different HTTP response formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_content_disposition", "line_number": 59, "body": "def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_unicode(\n                content_disposition, encoding='latin-1', errors='replace'\n            ).split(';')[1].split('=')[1].strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response", "is_method": true, "class_name": "ResponseTypes", "function_description": "Method of ResponseTypes that extracts a filename from a Content-Disposition header string and returns the corresponding response type based on that filename. It facilitates determining response handling from HTTP header metadata."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_headers", "line_number": 68, "body": "def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-Type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls", "is_method": true, "class_name": "ResponseTypes", "function_description": "Determines and returns the best matching Response subclass based on HTTP headers, facilitating appropriate handling of different response types in HTTP communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_filename", "line_number": 81, "body": "def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response", "is_method": true, "class_name": "ResponseTypes", "function_description": "Returns the most suitable Response class based on the file's MIME type deduced from its filename, defaulting to a generic Response if the type is unknown or encoded. This supports dynamic response handling based on file types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_body", "line_number": 89, "body": "def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')", "is_method": true, "class_name": "ResponseTypes", "function_description": "Method of ResponseTypes that heuristically determines the response type based on the initial content of a body, supporting use cases where standard detection methods are insufficient or unavailable."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/responsetypes.py", "function": "from_args", "line_number": 105, "body": "def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls", "is_method": true, "class_name": "ResponseTypes", "function_description": "Determines the most suitable Response subclass based on provided headers, URL, filename, or body arguments. This enables dynamic selection of response types for handling various input data in a flexible way."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_value", "line_number": 16, "body": "def get_value(self, key, default=None, spider=None):\n        return self._stats.get(key, default)", "is_method": true, "class_name": "StatsCollector", "function_description": "Utility method of StatsCollector that retrieves a stored statistic value by key, returning a default if the key is missing. It provides convenient access to collected stats data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_stats", "line_number": 19, "body": "def get_stats(self, spider=None):\n        return self._stats", "is_method": true, "class_name": "StatsCollector", "function_description": "Returns the current collected statistics, optionally filtered by a spider. This function provides access to internal tracking metrics useful for monitoring scraping or processing activities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_value", "line_number": 22, "body": "def set_value(self, key, value, spider=None):\n        self._stats[key] = value", "is_method": true, "class_name": "StatsCollector", "function_description": "Stores or updates a statistical value identified by a key within the StatsCollector, optionally scoped by a spider context. This method provides a simple interface for recording metrics or tracking data points."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "set_stats", "line_number": 25, "body": "def set_stats(self, stats, spider=None):\n        self._stats = stats", "is_method": true, "class_name": "StatsCollector", "function_description": "Sets the internal statistics data of the StatsCollector, optionally related to a specific spider. This method allows updating or replacing the current stats for later retrieval or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "inc_value", "line_number": 28, "body": "def inc_value(self, key, count=1, start=0, spider=None):\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count", "is_method": true, "class_name": "StatsCollector", "function_description": "Utility method of StatsCollector that increments a numeric statistic identified by a key, allowing optional initialization and customizable step size, supporting flexible tracking of counts or metrics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "max_value", "line_number": 32, "body": "def max_value(self, key, value, spider=None):\n        self._stats[key] = max(self._stats.setdefault(key, value), value)", "is_method": true, "class_name": "StatsCollector", "function_description": "Core utility method of the StatsCollector class that updates a statistic by storing the maximum observed value for a given key, optionally scoped by a spider. It ensures only the highest value is recorded for monitoring or analysis purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "min_value", "line_number": 35, "body": "def min_value(self, key, value, spider=None):\n        self._stats[key] = min(self._stats.setdefault(key, value), value)", "is_method": true, "class_name": "StatsCollector", "function_description": "Utility method of StatsCollector that updates a statistic by storing the minimum value encountered for a given key, supporting optional grouping by an entity like a spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "clear_stats", "line_number": 38, "body": "def clear_stats(self, spider=None):\n        self._stats.clear()", "is_method": true, "class_name": "StatsCollector", "function_description": "Clears all collected statistical data, optionally resetting across spiders if applicable. It provides a simple way to reset the internal statistics storage for fresh data collection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "close_spider", "line_number": 44, "body": "def close_spider(self, spider, reason):\n        if self._dump:\n            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                        extra={'spider': spider})\n        self._persist_stats(self._stats, spider)", "is_method": true, "class_name": "StatsCollector", "function_description": "Final method of the StatsCollector class that logs and saves collected crawling statistics when a spider finishes, supporting data persistence and post-run analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "_persist_stats", "line_number": 60, "body": "def _persist_stats(self, stats, spider):\n        self.spider_stats[spider.name] = stats", "is_method": true, "class_name": "MemoryStatsCollector", "function_description": "Private method of MemoryStatsCollector that stores collected statistics for a given spider, associating the stats with the spider's name for later retrieval or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/statscollectors.py", "function": "get_value", "line_number": 66, "body": "def get_value(self, key, default=None, spider=None):\n        return default", "is_method": true, "class_name": "DummyStatsCollector", "function_description": "Returns a default value for a given key, without performing any data retrieval or calculation. It serves as a placeholder method in the DummyStatsCollector class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_with_mkdir", "line_number": 14, "body": "def _with_mkdir(queue_class):\n\n    class DirectoriesCreated(queue_class):\n\n        def __init__(self, path, *args, **kwargs):\n            dirname = os.path.dirname(path)\n            if not os.path.exists(dirname):\n                os.makedirs(dirname, exist_ok=True)\n\n            super().__init__(path, *args, **kwargs)\n\n    return DirectoriesCreated", "is_method": false, "function_description": "Decorator factory function that wraps a queue class to ensure the directory for a given path exists before initialization, enabling safe file-path handling in queue-based operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_serializable_queue", "line_number": 28, "body": "def _serializable_queue(queue_class, serialize, deserialize):\n\n    class SerializableQueue(queue_class):\n\n        def push(self, obj):\n            s = serialize(obj)\n            super().push(s)\n\n        def pop(self):\n            s = super().pop()\n            if s:\n                return deserialize(s)\n\n    return SerializableQueue", "is_method": false, "function_description": "Decorator function that extends a queue class to automatically serialize objects on push and deserialize them on pop, enabling transparent storage and retrieval of complex data types within queue structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_scrapy_serialization_queue", "line_number": 44, "body": "def _scrapy_serialization_queue(queue_class):\n\n    class ScrapyRequestQueue(queue_class):\n\n        def __init__(self, crawler, key):\n            self.spider = crawler.spider\n            super().__init__(key)\n\n        @classmethod\n        def from_crawler(cls, crawler, key, *args, **kwargs):\n            return cls(crawler, key)\n\n        def push(self, request):\n            request = request_to_dict(request, self.spider)\n            return super().push(request)\n\n        def pop(self):\n            request = super().pop()\n\n            if not request:\n                return None\n\n            request = request_from_dict(request, self.spider)\n            return request\n\n    return ScrapyRequestQueue", "is_method": false, "function_description": "Creates a customized queue class that serializes and deserializes Scrapy requests for reliable storage and retrieval within a crawling process. It ensures requests are properly converted when pushed to or popped from the queue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_scrapy_non_serialization_queue", "line_number": 72, "body": "def _scrapy_non_serialization_queue(queue_class):\n\n    class ScrapyRequestQueue(queue_class):\n        @classmethod\n        def from_crawler(cls, crawler, *args, **kwargs):\n            return cls()\n\n    return ScrapyRequestQueue", "is_method": false, "function_description": "Utility function that creates a specialized queue class with a simplified factory method for instantiation from a crawler, facilitating integration with Scrapy's request handling system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "_pickle_serialize", "line_number": 82, "body": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=4)\n    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n    # TypeError is raised from parsel.Selector\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e)) from e", "is_method": false, "function_description": "Utility function that serializes a Python object using pickle with error handling, ensuring consistent byte output or raising a clear exception on failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "push", "line_number": 32, "body": "def push(self, obj):\n            s = serialize(obj)\n            super().push(s)", "is_method": true, "class_name": "SerializableQueue", "function_description": "Utility method of the SerializableQueue class that serializes an object before adding it to the queue, ensuring all queued items are stored in a consistent, serialized format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "pop", "line_number": 36, "body": "def pop(self):\n            s = super().pop()\n            if s:\n                return deserialize(s)", "is_method": true, "class_name": "SerializableQueue", "function_description": "This method removes and returns the first item from the queue after deserializing it, providing a seamless way to retrieve stored objects in their original form. It enables easy access to deserialized queue elements for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "from_crawler", "line_number": 53, "body": "def from_crawler(cls, crawler, key, *args, **kwargs):\n            return cls(crawler, key)", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Class method that initializes an instance of ScrapyRequestQueue using a crawler and a key, facilitating integration with Scrapy's crawler infrastructure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "push", "line_number": 56, "body": "def push(self, request):\n            request = request_to_dict(request, self.spider)\n            return super().push(request)", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Utility method in ScrapyRequestQueue that converts a request object to a dictionary format before adding it to the queue, ensuring consistent request storage and processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/squeues.py", "function": "pop", "line_number": 60, "body": "def pop(self):\n            request = super().pop()\n\n            if not request:\n                return None\n\n            request = request_from_dict(request, self.spider)\n            return request", "is_method": true, "class_name": "ScrapyRequestQueue", "function_description": "Returns the next request from the queue as a Scrapy Request object, or None if the queue is empty. It converts stored request data into executable requests for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "decode_robotstxt", "line_number": 11, "body": "def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n    try:\n        if to_native_str_type:\n            robotstxt_body = to_unicode(robotstxt_body)\n        else:\n            robotstxt_body = robotstxt_body.decode('utf-8')\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\n            \"Failure while parsing robots.txt. File either contains garbage or \"\n            \"is in an encoding other than UTF-8, treating it as an empty file.\",\n            exc_info=sys.exc_info(),\n            extra={'spider': spider},\n        )\n        robotstxt_body = ''\n    return robotstxt_body", "is_method": false, "function_description": "Function that decodes the raw content of a robots.txt file into a string, handling encoding issues gracefully by returning an empty string if decoding fails. It ensures proper text format for further processing in web crawling contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 67, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "PythonRobotParser", "function_description": "Factory method of PythonRobotParser that creates an instance using crawler data and the robots.txt content, facilitating parser initialization within web crawling contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 72, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(user_agent, url)", "is_method": true, "class_name": "PythonRobotParser", "function_description": "Determines if a specified user agent is permitted to access a given URL based on the robot exclusion rules. This method helps enforce web crawling or scraping restrictions defined by a site's robots.txt."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 85, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "ReppyRobotParser", "function_description": "Factory method of ReppyRobotParser that creates an instance from crawler data and robots.txt content, facilitating integration with web crawling frameworks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 90, "body": "def allowed(self, url, user_agent):\n        return self.rp.allowed(url, user_agent)", "is_method": true, "class_name": "ReppyRobotParser", "function_description": "This method checks if a given URL is accessible to a specified user agent according to the site's robots.txt rules. It enables applications to respect web crawling permissions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 103, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "RerpRobotParser", "function_description": "Factory method of RerpRobotParser that initializes an instance using crawler data and the robots.txt content, linking the parser to a specific spider if available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 108, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)", "is_method": true, "class_name": "RerpRobotParser", "function_description": "Determines if a given user agent is permitted to access a specific URL based on the robot exclusion rules. It provides access control decisions for web crawlers within the RerpRobotParser context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "from_crawler", "line_number": 122, "body": "def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o", "is_method": true, "class_name": "ProtegoRobotParser", "function_description": "Factory method in ProtegoRobotParser that creates an instance using crawler data and robots.txt content, facilitating parser initialization aligned with the crawling context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/robotstxt.py", "function": "allowed", "line_number": 127, "body": "def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)", "is_method": true, "class_name": "ProtegoRobotParser", "function_description": "Determines if a given URL is accessible for a specified user agent according to the parser's robots.txt rules. This function enables respectful web crawling by checking URL access permissions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_to_bytes_or_none", "line_number": 29, "body": "def _to_bytes_or_none(text):\n    if text is None:\n        return None\n    return to_bytes(text)", "is_method": false, "function_description": "Helper function that converts a given text to bytes or returns None if the input is None, useful for consistent byte representation in data processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "from_settings", "line_number": 50, "body": "def from_settings(cls, settings):\n        return cls(\n            smtphost=settings['MAIL_HOST'],\n            mailfrom=settings['MAIL_FROM'],\n            smtpuser=settings['MAIL_USER'],\n            smtppass=settings['MAIL_PASS'],\n            smtpport=settings.getint('MAIL_PORT'),\n            smtptls=settings.getbool('MAIL_TLS'),\n            smtpssl=settings.getbool('MAIL_SSL'),\n        )", "is_method": true, "class_name": "MailSender", "function_description": "Factory method of the MailSender class that creates an instance configured using mail-related settings from a given configuration source. It simplifies initializing MailSender with standardized email server credentials and connection options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "send", "line_number": 61, "body": "def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n        from twisted.internet import reactor\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split('/', 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg['From'] = self.mailfrom\n        msg['To'] = COMMASPACE.join(to)\n        msg['Date'] = formatdate(localtime=True)\n        msg['Subject'] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg['Cc'] = COMMASPACE.join(cc)\n\n        if charset:\n            msg.set_charset(charset)\n\n        if attachs:\n            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split('/'))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header('Content-Disposition', 'attachment', filename=attach_name)\n                msg.attach(part)\n        else:\n            msg.set_payload(body)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                          'mailattachs': len(attachs)})\n            return\n\n        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n        dfd.addCallbacks(\n            callback=self._sent_ok,\n            errback=self._sent_failed,\n            callbackArgs=[to, cc, subject, len(attachs)],\n            errbackArgs=[to, cc, subject, len(attachs)],\n        )\n        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n        return dfd", "is_method": true, "class_name": "MailSender", "function_description": "Service method of MailSender that composes and asynchronously sends an email with optional CC recipients, attachments, and customizable MIME types, supporting callbacks and debug mode for integration in email dispatch workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sent_ok", "line_number": 114, "body": "def _sent_ok(self, result, to, cc, subject, nattachs):\n        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                    'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                     'mailattachs': nattachs})", "is_method": true, "class_name": "MailSender", "function_description": "Private method in MailSender that logs successful email sending details including recipients, subject, and attachment count for monitoring email delivery status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sent_failed", "line_number": 120, "body": "def _sent_failed(self, failure, to, cc, subject, nattachs):\n        errstr = str(failure.value)\n        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n                     'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n                     '- %(mailerr)s',\n                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                      'mailattachs': nattachs, 'mailerr': errstr})", "is_method": true, "class_name": "MailSender", "function_description": "Logs detailed error information when an email fails to send, capturing recipients, subject, attachment count, and the specific failure reason for diagnostic purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/mail.py", "function": "_sendmail", "line_number": 128, "body": "def _sendmail(self, to_addrs, msg):\n        # Import twisted.mail here because it is not available in python3\n        from twisted.internet import reactor\n        from twisted.mail.smtp import ESMTPSenderFactory\n        msg = BytesIO(msg)\n        d = defer.Deferred()\n        factory = ESMTPSenderFactory(\n            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n        )\n        factory.noisy = False\n\n        if self.smtpssl:\n            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d", "is_method": true, "class_name": "MailSender", "function_description": "Core internal method of the MailSender class that asynchronously sends an email message to specified recipients using SMTP with optional SSL/TLS encryption and authentication settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "crawled", "line_number": 55, "body": "def crawled(self, request, response, spider):\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = f' {str(request.flags)}' if request.flags else ''\n        response_flags = f' {str(response.flags)}' if response.flags else ''\n        return {\n            'level': logging.DEBUG,\n            'msg': CRAWLEDMSG,\n            'args': {\n                'status': response.status,\n                'request': request,\n                'request_flags': request_flags,\n                'referer': referer_str(request),\n                'response_flags': response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                'flags': response_flags\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Returns a structured log entry with detailed information about a successfully crawled webpage, including HTTP status, request and response flags, and referer, to facilitate consistent and informative crawling logs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "scraped", "line_number": 73, "body": "def scraped(self, item, response, spider):\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            'level': logging.DEBUG,\n            'msg': SCRAPEDMSG,\n            'args': {\n                'src': src,\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Provides a structured log entry when a spider successfully scrapes an item, including debug-level details about the item and its response source for tracking scraping events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "dropped", "line_number": 88, "body": "def dropped(self, item, exception, response, spider):\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        return {\n            'level': logging.WARNING,\n            'msg': DROPPEDMSG,\n            'args': {\n                'exception': exception,\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Logs a warning message detailing when an item is dropped during the item pipeline processing, including the associated exception and item information for troubleshooting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "item_error", "line_number": 99, "body": "def item_error(self, item, exception, response, spider):\n        \"\"\"Logs a message when an item causes an error while it is passing\n        through the item pipeline.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            'level': logging.ERROR,\n            'msg': ITEMERRORMSG,\n            'args': {\n                'item': item,\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Provides a standardized error log message when an item fails processing in the item pipeline, helping to track and debug item-related issues during scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "spider_error", "line_number": 113, "body": "def spider_error(self, failure, request, response, spider):\n        \"\"\"Logs an error message from a spider.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            'level': logging.ERROR,\n            'msg': SPIDERERRORMSG,\n            'args': {\n                'request': request,\n                'referer': referer_str(request),\n            }\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Provides a standardized error log entry for spider failures, capturing request details to aid debugging and monitoring in web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "download_error", "line_number": 127, "body": "def download_error(self, failure, request, spider, errmsg=None):\n        \"\"\"Logs a download error message from a spider (typically coming from\n        the engine).\n\n        .. versionadded:: 2.0\n        \"\"\"\n        args = {'request': request}\n        if errmsg:\n            msg = DOWNLOADERRORMSG_LONG\n            args['errmsg'] = errmsg\n        else:\n            msg = DOWNLOADERRORMSG_SHORT\n        return {\n            'level': logging.ERROR,\n            'msg': msg,\n            'args': args,\n        }", "is_method": true, "class_name": "LogFormatter", "function_description": "Provides a standardized error log entry for download failures in spiders, including optional error messages, to facilitate consistent and detailed logging within the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/logformatter.py", "function": "from_crawler", "line_number": 146, "body": "def from_crawler(cls, crawler):\n        return cls()", "is_method": true, "class_name": "LogFormatter", "function_description": "Constructor method for LogFormatter that creates an instance from a crawler object, enabling integration with crawling frameworks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "from_crawler", "line_number": 25, "body": "def from_crawler(cls, crawler, reactor):\n        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Factory method of CachingThreadedResolver that initializes an instance configured based on crawler settings, enabling DNS caching with customizable cache size and timeout parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "install_on_reactor", "line_number": 32, "body": "def install_on_reactor(self):\n        self.reactor.installResolver(self)", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Registers the resolver instance with the reactor to enable asynchronous name resolution within an event-driven system. This integrates the resolver into the reactor's workflow for handling resolution requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "getHostByName", "line_number": 35, "body": "def getHostByName(self, name, timeout=None):\n        if name in dnscache:\n            return defer.succeed(dnscache[name])\n        # in Twisted<=16.6, getHostByName() is always called with\n        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n        # so the input argument above is simply overridden\n        # to enforce Scrapy's DNS_TIMEOUT setting's value\n        timeout = (self.timeout,)\n        d = super().getHostByName(name, timeout)\n        if dnscache.limit:\n            d.addCallback(self._cache_result, name)\n        return d", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Provides asynchronous DNS resolution with caching support, returning cached results immediately or performing a lookup with a configurable timeout, improving network request efficiency within the CachingThreadedResolver context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "_cache_result", "line_number": 48, "body": "def _cache_result(self, result, name):\n        dnscache[name] = result\n        return result", "is_method": true, "class_name": "CachingThreadedResolver", "function_description": "Private helper method in CachingThreadedResolver that stores DNS resolution results in a cache by name and returns the cached result for efficient future retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolutionBegan", "line_number": 69, "body": "def resolutionBegan(self, resolution):\n        self.resolutionReceiver.resolutionBegan(resolution)\n        self.resolution = resolution", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "Method in _CachingResolutionReceiver that delegates the start of a resolution process to an internal receiver and stores the current resolution state for later use. It provides a way to track and propagate resolution initiation events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "addressResolved", "line_number": 73, "body": "def addressResolved(self, address):\n        self.resolutionReceiver.addressResolved(address)\n        self.addresses.append(address)", "is_method": true, "class_name": "_CachingResolutionReceiver", "function_description": "This method forwards a resolved address to another component and stores it internally. It supports tracking and handling of resolved addresses within the caching resolution workflow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "from_crawler", "line_number": 96, "body": "def from_crawler(cls, crawler, reactor):\n        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size)", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "Class method that creates an instance of CachingHostnameResolver using crawler settings to enable and size a DNS cache, supporting configurable hostname resolution caching."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "install_on_reactor", "line_number": 103, "body": "def install_on_reactor(self):\n        self.reactor.installNameResolver(self)", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "Registers the CachingHostnameResolver as the active name resolver in the reactor, enabling it to handle hostname resolution requests within the reactor's event-driven environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/resolver.py", "function": "resolveHostName", "line_number": 106, "body": "def resolveHostName(\n        self, resolutionReceiver, hostName, portNumber=0, addressTypes=None, transportSemantics=\"TCP\"\n    ):\n        try:\n            addresses = dnscache[hostName]\n        except KeyError:\n            return self.original_resolver.resolveHostName(\n                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                hostName,\n                portNumber,\n                addressTypes,\n                transportSemantics,\n            )\n        else:\n            resolutionReceiver.resolutionBegan(HostResolution(hostName))\n            for addr in addresses:\n                resolutionReceiver.addressResolved(addr)\n            resolutionReceiver.resolutionComplete()\n            return resolutionReceiver", "is_method": true, "class_name": "CachingHostnameResolver", "function_description": "Provides cached or delegated hostname resolution by returning stored addresses when available, otherwise performing a fresh DNS lookup and notifying the receiver throughout the resolution process. Enables efficient hostname-to-address mapping with fallback to the original resolver."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "from_settings", "line_number": 6, "body": "def from_settings(settings):\n        \"\"\"Return an instance of the class for the given settings\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Constructs and returns an ISpiderLoader instance configured based on the provided settings, enabling customized spider loading behavior according to specific configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "load", "line_number": 9, "body": "def load(spider_name):\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Utility method of ISpiderLoader that returns the Spider class matching a given spider name, facilitating dynamic spider retrieval; raises KeyError if the spider name does not exist."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "list", "line_number": 13, "body": "def list():\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Utility method of the ISpiderLoader class that returns the names of all spiders available in the current project, facilitating discovery and management of spider components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/interfaces.py", "function": "find_by_request", "line_number": 17, "body": "def find_by_request(request):\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"", "is_method": true, "class_name": "ISpiderLoader", "function_description": "Returns a list of spider names capable of processing the specified request, enabling dynamic selection of appropriate spiders for handling different web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__eq__", "line_number": 38, "body": "def __eq__(self, other):\n        return (\n            self.url == other.url\n            and self.text == other.text\n            and self.fragment == other.fragment\n            and self.nofollow == other.nofollow\n        )", "is_method": true, "class_name": "Link", "function_description": "Provides equality comparison for Link objects by checking if all key attributes (url, text, fragment, nofollow) are identical between two instances. This supports correct behavior in collections or comparisons involving Link instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__hash__", "line_number": 46, "body": "def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)", "is_method": true, "class_name": "Link", "function_description": "Overrides the hash function to provide a unique hash value for a Link object based on its URL, text, fragment, and nofollow attributes. This enables Link instances to be used effectively in sets and as dictionary keys."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/link.py", "function": "__repr__", "line_number": 49, "body": "def __repr__(self):\n        return (\n            f'Link(url={self.url!r}, text={self.text!r}, '\n            f'fragment={self.fragment!r}, nofollow={self.nofollow!r})'\n        )", "is_method": true, "class_name": "Link", "function_description": "Provides a string representation of the Link object showing its URL, text, fragment, and nofollow attributes for easy debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_check_name_duplicates", "line_number": 26, "body": "def _check_name_duplicates(self):\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend([\n                f\"  {cls} named {name!r} (in {mod})\"\n                for mod, cls in locations\n                if len(locations) > 1\n            ])\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )", "is_method": true, "class_name": "SpiderLoader", "function_description": "Checks for duplicate spider names within the SpiderLoader and issues a warning listing them to prevent potential conflicts or unexpected behavior when multiple spiders share the same name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_load_spiders", "line_number": 43, "body": "def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls", "is_method": true, "class_name": "SpiderLoader", "function_description": "Internal method of SpiderLoader that discovers spider classes within a module and registers them by name, facilitating the dynamic loading and tracking of available spiders for web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "_load_all_spiders", "line_number": 48, "body": "def _load_all_spiders(self):\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError:\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()", "is_method": true, "class_name": "SpiderLoader", "function_description": "Core method of the SpiderLoader class that loads spider classes from all specified modules, handling import errors based on configuration and ensuring no duplicate spider names exist. It supports dynamic discovery of spider components for web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "from_settings", "line_number": 66, "body": "def from_settings(cls, settings):\n        return cls(settings)", "is_method": true, "class_name": "SpiderLoader", "function_description": "Utility class method of SpiderLoader that initializes an instance using provided settings, enabling flexible creation based on configuration inputs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "load", "line_number": 69, "body": "def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")", "is_method": true, "class_name": "SpiderLoader", "function_description": "Utility method of SpiderLoader that retrieves the Spider class corresponding to a given spider name, raising an error if the name is not found. It enables dynamic access to spider implementations by their identifiers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "find_by_request", "line_number": 79, "body": "def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items()\n            if cls.handles_request(request)\n        ]", "is_method": true, "class_name": "SpiderLoader", "function_description": "Method of SpiderLoader that returns all spider names capable of processing a specific request, facilitating request routing to appropriate spider handlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiderloader.py", "function": "list", "line_number": 88, "body": "def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())", "is_method": true, "class_name": "SpiderLoader", "function_description": "Returns a list of all spider names available in the project, enabling other functions to access or manage available spiders dynamically."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_iter_command_classes", "line_number": 17, "body": "def _iter_command_classes(module_name):\n    # TODO: add `name` attribute to commands and and merge this function with\n    # scrapy.utils.spider.iter_spider_classes\n    for module in walk_modules(module_name):\n        for obj in vars(module).values():\n            if (\n                inspect.isclass(obj)\n                and issubclass(obj, ScrapyCommand)\n                and obj.__module__ == module.__name__\n                and not obj == ScrapyCommand\n            ):\n                yield obj", "is_method": false, "function_description": "Private generator function that iterates through modules under a given name to yield all subclasses of ScrapyCommand defined within those modules. It supports dynamic discovery of command classes for extension or plugin use in Scrapy projects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_from_module", "line_number": 31, "body": "def _get_commands_from_module(module, inproject):\n    d = {}\n    for cmd in _iter_command_classes(module):\n        if inproject or not cmd.requires_project:\n            cmdname = cmd.__module__.split('.')[-1]\n            d[cmdname] = cmd()\n    return d", "is_method": false, "function_description": "Utility function that extracts and instantiates command classes from a module, optionally filtering commands based on their project dependency status for flexible command management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_from_entry_points", "line_number": 40, "body": "def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n    cmds = {}\n    for entry_point in pkg_resources.iter_entry_points(group):\n        obj = entry_point.load()\n        if inspect.isclass(obj):\n            cmds[entry_point.name] = obj()\n        else:\n            raise Exception(f\"Invalid entry point {entry_point.name}\")\n    return cmds", "is_method": false, "function_description": "This function discovers and instantiates command classes registered under a specified entry point group, collecting them into a dictionary by their entry point names for dynamic command loading and execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_get_commands_dict", "line_number": 51, "body": "def _get_commands_dict(settings, inproject):\n    cmds = _get_commands_from_module('scrapy.commands', inproject)\n    cmds.update(_get_commands_from_entry_points(inproject))\n    cmds_module = settings['COMMANDS_MODULE']\n    if cmds_module:\n        cmds.update(_get_commands_from_module(cmds_module, inproject))\n    return cmds", "is_method": false, "function_description": "Utility function that collects and consolidates command definitions from default modules, entry points, and project-specific modules based on the given settings and context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_pop_command_name", "line_number": 60, "body": "def _pop_command_name(argv):\n    i = 0\n    for arg in argv[1:]:\n        if not arg.startswith('-'):\n            del argv[i]\n            return arg\n        i += 1", "is_method": false, "function_description": "Extracts and removes the first non-option argument (command name) from a list of command-line arguments, facilitating command parsing in CLI applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_header", "line_number": 69, "body": "def _print_header(settings, inproject):\n    version = scrapy.__version__\n    if inproject:\n        print(f\"Scrapy {version} - project: {settings['BOT_NAME']}\\n\")\n    else:\n        print(f\"Scrapy {version} - no active project\\n\")", "is_method": false, "function_description": "Utility function that prints the Scrapy version along with the current project name if available, or indicates no active project otherwise."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_commands", "line_number": 77, "body": "def _print_commands(settings, inproject):\n    _print_header(settings, inproject)\n    print(\"Usage:\")\n    print(\"  scrapy <command> [options] [args]\\n\")\n    print(\"Available commands:\")\n    cmds = _get_commands_dict(settings, inproject)\n    for cmdname, cmdclass in sorted(cmds.items()):\n        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n    if not inproject:\n        print()\n        print(\"  [ more ]      More commands available when run from project directory\")\n    print()\n    print('Use \"scrapy <command> -h\" to see more info about a command')", "is_method": false, "function_description": "Utility function that displays the usage instructions and lists available Scrapy commands, adapting output based on whether it's run inside a project directory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_print_unknown_command", "line_number": 92, "body": "def _print_unknown_command(settings, cmdname, inproject):\n    _print_header(settings, inproject)\n    print(f\"Unknown command: {cmdname}\\n\")\n    print('Use \"scrapy\" to see available commands')", "is_method": false, "function_description": "Utility function that displays an error message for unrecognized commands, including a header and guidance to list available commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_print_help", "line_number": 98, "body": "def _run_print_help(parser, func, *a, **kw):\n    try:\n        func(*a, **kw)\n    except UsageError as e:\n        if str(e):\n            parser.error(str(e))\n        if e.print_help:\n            parser.print_help()\n        sys.exit(2)", "is_method": false, "function_description": "Utility function that executes a command function, handling usage errors by displaying error messages or help information through a parser before exiting the program."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "execute", "line_number": 109, "body": "def execute(argv=None, settings=None):\n    if argv is None:\n        argv = sys.argv\n\n    if settings is None:\n        settings = get_project_settings()\n        # set EDITOR from environment if available\n        try:\n            editor = os.environ['EDITOR']\n        except KeyError:\n            pass\n        else:\n            settings['EDITOR'] = editor\n\n    inproject = inside_project()\n    cmds = _get_commands_dict(settings, inproject)\n    cmdname = _pop_command_name(argv)\n    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n                                   conflict_handler='resolve')\n    if not cmdname:\n        _print_commands(settings, inproject)\n        sys.exit(0)\n    elif cmdname not in cmds:\n        _print_unknown_command(settings, cmdname, inproject)\n        sys.exit(2)\n\n    cmd = cmds[cmdname]\n    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n    parser.description = cmd.long_desc()\n    settings.setdict(cmd.default_settings, priority='command')\n    cmd.settings = settings\n    cmd.add_options(parser)\n    opts, args = parser.parse_args(args=argv[1:])\n    _run_print_help(parser, cmd.process_options, args, opts)\n\n    cmd.crawler_process = CrawlerProcess(settings)\n    _run_print_help(parser, _run_command, cmd, args, opts)\n    sys.exit(cmd.exitcode)", "is_method": false, "function_description": "Function that processes command-line arguments and executes the corresponding command with configured settings, managing error handling, option parsing, and command lifecycle for a CLI application."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/cmdline.py", "function": "_run_command_profiled", "line_number": 156, "body": "def _run_command_profiled(cmd, args, opts):\n    if opts.profile:\n        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n    loc = locals()\n    p = cProfile.Profile()\n    p.runctx('cmd.run(args, opts)', globals(), loc)\n    if opts.profile:\n        p.dump_stats(opts.profile)", "is_method": false, "function_description": "Internal utility that executes a command with optional profiling, enabling performance measurement and saving detailed stats when profiling is enabled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "_path_safe", "line_number": 9, "body": "def _path_safe(text):\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n                             for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n    return '-'.join([pathable_slot, unique_slot])", "is_method": false, "function_description": "Utility function that converts a string into a filesystem-safe format by replacing unsafe characters and appending a unique hash to prevent collisions. It ensures safe and unique filenames for storing or referencing text-based data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n        return cls(crawler, downstream_queue_cls, key, startprios)", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Factory method of ScrapyPriorityQueue that initializes an instance using crawler settings, downstream queue class, a priority key, and optional starting priorities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "init_prios", "line_number": 63, "body": "def init_prios(self, startprios):\n        if not startprios:\n            return\n\n        for priority in startprios:\n            self.queues[priority] = self.qfactory(priority)\n\n        self.curprio = min(startprios)", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Initializes priority queues with given priorities, setting up internal queues and establishing the current lowest priority for task processing in the ScrapyPriorityQueue class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "qfactory", "line_number": 72, "body": "def qfactory(self, key):\n        return create_instance(self.downstream_queue_cls,\n                               None,\n                               self.crawler,\n                               self.key + '/' + str(key))", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Creates and returns an instance of the downstream queue class configured with a composite key and crawler context. This supports generating specialized sub-queues within the ScrapyPriorityQueue system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "priority", "line_number": 78, "body": "def priority(self, request):\n        return -request.priority", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Returns the negated priority of a request, enabling the priority queue to process higher-priority requests first. This function supports efficient task scheduling based on request priority."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "push", "line_number": 81, "body": "def push(self, request):\n        priority = self.priority(request)\n        if priority not in self.queues:\n            self.queues[priority] = self.qfactory(priority)\n        q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Adds a request to the priority queue based on its priority, creating a new sub-queue if needed and updating the current highest priority for efficient request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pop", "line_number": 90, "body": "def pop(self):\n        if self.curprio is None:\n            return\n        q = self.queues[self.curprio]\n        m = q.pop()\n        if not q:\n            del self.queues[self.curprio]\n            q.close()\n            prios = [p for p, q in self.queues.items() if q]\n            self.curprio = min(prios) if prios else None\n        return m", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Removes and returns the next item from the highest-priority queue, updating the current priority level and cleaning up empty queues as needed. It provides prioritized retrieval of tasks or elements in a ScrapyPriorityQueue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "close", "line_number": 102, "body": "def close(self):\n        active = []\n        for p, q in self.queues.items():\n            active.append(p)\n            q.close()\n        return active", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Closes all priority queues in the ScrapyPriorityQueue instance and returns a list of their priority levels that were active before closing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__len__", "line_number": 109, "body": "def __len__(self):\n        return sum(len(x) for x in self.queues.values()) if self.queues else 0", "is_method": true, "class_name": "ScrapyPriorityQueue", "function_description": "Returns the total number of items across all priority queues in the ScrapyPriorityQueue, providing a quick way to check the queue's current size."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "stats", "line_number": 118, "body": "def stats(self, possible_slots):\n        return [(self._active_downloads(slot), slot)\n                for slot in possible_slots]", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Returns a list pairing each given slot with its number of active downloads, providing insight into current download activity across specified slots."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "get_slot_key", "line_number": 122, "body": "def get_slot_key(self, request):\n        return self.downloader._get_slot_key(request, None)", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Utility method in DownloaderInterface that retrieves a slot key associated with a given request by delegating to an internal downloader's slot key retrieval function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "_active_downloads", "line_number": 125, "body": "def _active_downloads(self, slot):\n        \"\"\" Return a number of requests in a Downloader for a given slot \"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)", "is_method": true, "class_name": "DownloaderInterface", "function_description": "Returns the count of active download requests for a specified slot, allowing the DownloaderInterface to monitor ongoing download activity per slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "from_crawler", "line_number": 139, "body": "def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n        return cls(crawler, downstream_queue_cls, key, startprios)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Factory method for DownloaderAwarePriorityQueue that initializes an instance using crawler context and configuration, setting up priority keys and optional starting priorities."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pqfactory", "line_number": 164, "body": "def pqfactory(self, slot, startprios=()):\n        return ScrapyPriorityQueue(self.crawler,\n                                   self.downstream_queue_cls,\n                                   self.key + '/' + _path_safe(slot),\n                                   startprios)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Factory method of DownloaderAwarePriorityQueue that creates and returns a ScrapyPriorityQueue instance configured with a specific slot and initial priorities. It facilitates priority queue management for different downloader slots within crawling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "pop", "line_number": 170, "body": "def pop(self):\n        stats = self._downloader_interface.stats(self.pqueues)\n\n        if not stats:\n            return\n\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Provides a priority-based pop operation that selects and removes the next request from the downloader queues based on downloader statistics, enabling prioritized request handling in download management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "push", "line_number": 183, "body": "def push(self, request):\n        slot = self._downloader_interface.get_slot_key(request)\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory(slot)\n        queue = self.pqueues[slot]\n        queue.push(request)", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Adds a request to a priority queue associated with its corresponding downloader slot, organizing requests for efficient, slot-aware processing. This supports prioritized handling of tasks based on their downloader contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "close", "line_number": 190, "body": "def close(self):\n        active = {slot: queue.close()\n                  for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Service method of DownloaderAwarePriorityQueue that closes all internal priority queues, clears them, and returns a dictionary indicating the closure status of each queue slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__len__", "line_number": 196, "body": "def __len__(self):\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Returns the total number of items across all internal priority queues, providing a consolidated length measurement for the DownloaderAwarePriorityQueue."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pqueues.py", "function": "__contains__", "line_number": 199, "body": "def __contains__(self, slot):\n        return slot in self.pqueues", "is_method": true, "class_name": "DownloaderAwarePriorityQueue", "function_description": "Checks if a given slot exists in the priority queues managed by DownloaderAwarePriorityQueue. It enables quick membership testing within the queue system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "inspect_response", "line_number": 159, "body": "def inspect_response(response, spider):\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    Shell(spider.crawler).start(response=response, spider=spider)", "is_method": false, "function_description": "This function launches an interactive shell session to examine a given response within the context of a web spider, aiding debugging and inspection during web scraping processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_request_deferred", "line_number": 164, "body": "def _request_deferred(request):\n    \"\"\"Wrap a request inside a Deferred.\n\n    This function is harmful, do not use it until you know what you are doing.\n\n    This returns a Deferred whose first pair of callbacks are the request\n    callback and errback. The Deferred also triggers when the request\n    callback/errback is executed (i.e. when the request is downloaded)\n\n    WARNING: Do not call request.replace() until after the deferred is called.\n    \"\"\"\n    request_callback = request.callback\n    request_errback = request.errback\n\n    def _restore_callbacks(result):\n        request.callback = request_callback\n        request.errback = request_errback\n        return result\n\n    d = defer.Deferred()\n    d.addBoth(_restore_callbacks)\n    if request.callback:\n        d.addCallbacks(request.callback, request.errback)\n\n    request.callback, request.errback = d.callback, d.errback\n    return d", "is_method": false, "function_description": "Provides a Deferred wrapper for an asynchronous request, enabling callback chaining while preserving the original request callbacks. Intended for advanced use cases requiring controlled execution flow in async request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "start", "line_number": 39, "body": "def start(self, url=None, request=None, response=None, spider=None, redirect=True):\n        # disable accidental Ctrl-C key press from shutting down the engine\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if url:\n            self.fetch(url, spider, redirect=redirect)\n        elif request:\n            self.fetch(request, spider)\n        elif response:\n            request = response.request\n            self.populate_vars(response, request, spider)\n        else:\n            self.populate_vars()\n        if self.code:\n            print(eval(self.code, globals(), self.vars))\n        else:\n            \"\"\"\n            Detect interactive shell setting in scrapy.cfg\n            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n            [settings]\n            # shell can be one of ipython, bpython or python;\n            # to be used as the interactive python console, if available.\n            # (default is ipython, fallbacks in the order listed above)\n            shell = python\n            \"\"\"\n            cfg = get_config()\n            section, option = 'settings', 'shell'\n            env = os.environ.get('SCRAPY_PYTHON_SHELL')\n            shells = []\n            if env:\n                shells += env.strip().lower().split(',')\n            elif cfg.has_option(section, option):\n                shells += [cfg.get(section, option).strip().lower()]\n            else:  # try all by default\n                shells += DEFAULT_PYTHON_SHELLS.keys()\n            # always add standard shell as fallback\n            shells += ['python']\n            start_python_console(self.vars, shells=shells,\n                                 banner=self.vars.pop('banner', ''))", "is_method": true, "class_name": "Shell", "function_description": "Core method of the Shell class that initiates an interactive Python shell session, optionally preloading it with data from a URL, request, or response, supporting customizable shell environments and user-defined code execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_schedule", "line_number": 78, "body": "def _schedule(self, request, spider):\n        spider = self._open_spider(request, spider)\n        d = _request_deferred(request)\n        d.addCallback(lambda x: (x, spider))\n        self.crawler.engine.crawl(request, spider)\n        return d", "is_method": true, "class_name": "Shell", "function_description": "Internal method of the Shell class that schedules a request to be processed by a spider, returning a deferred object representing the asynchronous execution of that request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_open_spider", "line_number": 85, "body": "def _open_spider(self, request, spider):\n        if self.spider:\n            return self.spider\n\n        if spider is None:\n            spider = self.crawler.spider or self.crawler._create_spider()\n\n        self.crawler.spider = spider\n        self.crawler.engine.open_spider(spider, close_if_idle=False)\n        self.spider = spider\n        return spider", "is_method": true, "class_name": "Shell", "function_description": "Utility method in the Shell class that initializes and opens a spider instance for crawling, ensuring a single active spider is managed and started within the crawling engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "fetch", "line_number": 97, "body": "def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n        from twisted.internet import reactor\n        if isinstance(request_or_url, Request):\n            request = request_or_url\n        else:\n            url = any_to_uri(request_or_url)\n            request = Request(url, dont_filter=True, **kwargs)\n            if redirect:\n                request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n            else:\n                request.meta['handle_httpstatus_all'] = True\n        response = None\n        try:\n            response, spider = threads.blockingCallFromThread(\n                reactor, self._schedule, request, spider)\n        except IgnoreRequest:\n            pass\n        self.populate_vars(response, request, spider)", "is_method": true, "class_name": "Shell", "function_description": "Core method of the Shell class that sends HTTP requests from various input forms and handles their responses, supporting customizable redirection and integration with asynchronous event handling. It enables synchronous-like fetching in an asynchronous framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "populate_vars", "line_number": 116, "body": "def populate_vars(self, response=None, request=None, spider=None):\n        import scrapy\n\n        self.vars['scrapy'] = scrapy\n        self.vars['crawler'] = self.crawler\n        self.vars['item'] = self.item_class()\n        self.vars['settings'] = self.crawler.settings\n        self.vars['spider'] = spider\n        self.vars['request'] = request\n        self.vars['response'] = response\n        if self.inthread:\n            self.vars['fetch'] = self.fetch\n        self.vars['view'] = open_in_browser\n        self.vars['shelp'] = self.print_help\n        self.update_vars(self.vars)\n        if not self.code:\n            self.vars['banner'] = self.get_help()", "is_method": true, "class_name": "Shell", "function_description": "Core method of the Shell class that initializes the execution environment by populating variables with current Scrapy objects and utilities, facilitating interactive scripting or debugging within a Scrapy crawling context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "print_help", "line_number": 134, "body": "def print_help(self):\n        print(self.get_help())", "is_method": true, "class_name": "Shell", "function_description": "Provides a simple method to display the help information for the Shell class instance. It serves as a user-friendly way to access usage instructions or command descriptions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "get_help", "line_number": 137, "body": "def get_help(self):\n        b = []\n        b.append(\"Available Scrapy objects:\")\n        b.append(\"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\")\n        for k, v in sorted(self.vars.items()):\n            if self._is_relevant(v):\n                b.append(f\"  {k:<10} {v}\")\n        b.append(\"Useful shortcuts:\")\n        if self.inthread:\n            b.append(\"  fetch(url[, redirect=True]) \"\n                     \"Fetch URL and update local objects (by default, redirects are followed)\")\n            b.append(\"  fetch(req)                  \"\n                     \"Fetch a scrapy.Request and update local objects \")\n        b.append(\"  shelp()           Shell help (print this help)\")\n        b.append(\"  view(response)    View response in a browser\")\n\n        return \"\\n\".join(f\"[s] {line}\" for line in b)", "is_method": true, "class_name": "Shell", "function_description": "Provides a formatted help message listing available Scrapy objects, useful shortcuts, and commands within the Shell context, aiding users in understanding and interacting with the interactive scraping environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/shell.py", "function": "_is_relevant", "line_number": 155, "body": "def _is_relevant(self, value):\n        return isinstance(value, self.relevant_classes) or is_item(value)", "is_method": true, "class_name": "Shell", "function_description": "Utility method in Shell that checks if a value matches predefined relevant classes or meets a specific item condition, supporting type validation or filtering within the class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_configure", "line_number": 32, "body": "def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding = options.pop('encoding', None)\n        self.fields_to_export = options.pop('fields_to_export', None)\n        self.export_empty_fields = options.pop('export_empty_fields', False)\n        self.indent = options.pop('indent', None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Initial analysis shows this method configures export options for the BaseItemExporter by extracting known parameters from a dictionary and optionally enforcing strict option checking. It serves to initialize exporter settings with validation support, useful during instantiation or reconfiguration.\n\nOutput:\nInternal method that sets exporter configuration from given options, optionally enforcing strict validation to handle unexpected parameters during initialization or setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 47, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', lambda x: x)\n        return serializer(value)", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Utility method in BaseItemExporter that applies a specified serializer to a field's value, enabling customizable data serialization during export processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_get_serialized_fields", "line_number": 57, "body": "def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if field_name in item:\n                field_meta = item.get_field_meta(field_name)\n                value = self.serialize_field(field_meta, field_name, item[field_name])\n            else:\n                value = default_value\n\n            yield field_name, value", "is_method": true, "class_name": "BaseItemExporter", "function_description": "Core method of the BaseItemExporter class that generates an iterable of field names paired with their serialized values for an item, supporting optional inclusion of empty fields and custom default values. It facilitates flexible data export formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 95, "body": "def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + '\\n'\n        self.file.write(to_bytes(data, self.encoding))", "is_method": true, "class_name": "JsonLinesItemExporter", "function_description": "Utility method of JsonLinesItemExporter that serializes a given item to JSON Lines format and writes it to a file, supporting incremental export of individual data records."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_newline", "line_number": 115, "body": "def _beautify_newline(self):\n        if self.indent is not None:\n            self.file.write(b'\\n')", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Helper method in JsonItemExporter that writes a newline character to the output file when indentation is enabled, improving the readability of exported JSON data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 119, "body": "def start_exporting(self):\n        self.file.write(b\"[\")\n        self._beautify_newline()", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Begins the export process by writing the opening bracket of a JSON array to the file, preparing the exporter to output multiple JSON items. This sets the initial structure for exporting JSON data collections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 123, "body": "def finish_exporting(self):\n        self._beautify_newline()\n        self.file.write(b\"]\")", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Completes the JSON export process by finalizing formatting and closing the JSON array in the output file. Useful for ensuring well-formed JSON output at the end of data export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 127, "body": "def export_item(self, item):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b',')\n            self._beautify_newline()\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict)\n        self.file.write(to_bytes(data, self.encoding))", "is_method": true, "class_name": "JsonItemExporter", "function_description": "Method of JsonItemExporter that serializes and writes an item to a JSON file, managing commas and formatting to ensure valid JSON output during incremental export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_newline", "line_number": 148, "body": "def _beautify_newline(self, new_item=False):\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters('\\n')", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Private helper of XmlItemExporter that inserts newline characters to format XML output for better readability, especially before new items or when indentation is specified."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_beautify_indent", "line_number": 152, "body": "def _beautify_indent(self, depth=1):\n        if self.indent:\n            self.xg.characters(' ' * self.indent * depth)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Helper method within XmlItemExporter that adds indentation spaces for XML formatting based on the specified depth, improving the readability of the exported XML content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "start_exporting", "line_number": 156, "body": "def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})\n        self._beautify_newline(new_item=True)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Initializes the XML export process by starting the document and root element, preparing the exporter for subsequent item entries. This method sets up the XML structure before data export begins."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 161, "body": "def export_item(self, item):\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, {})\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=''):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Core method of XmlItemExporter that serializes and exports a single item as a formatted XML element, including its fields, supporting structured XML data output."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "finish_exporting", "line_number": 171, "body": "def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Finalizes the XML export process by closing the root element and ending the XML document, ensuring well-formed output. This method is essential for completing and properly closing XML exports."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_export_xml_field", "line_number": 175, "body": "def _export_xml_field(self, name, serialized_value, depth):\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, 'items'):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field('value', value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()", "is_method": true, "class_name": "XmlItemExporter", "function_description": "Recursive helper method of XmlItemExporter that formats and writes XML elements from complex nested data structures, supporting dictionaries, lists, and string values with proper indentation for readability."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 215, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._join_if_needed)\n        return serializer(value)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Core method of CsvItemExporter that converts a field's value into a serialized string using a specified or default serializer, facilitating custom formatting for CSV export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_join_if_needed", "line_number": 219, "body": "def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Utility method of CsvItemExporter that concatenates list or tuple values into a single string using a predefined separator, ensuring CSV-compatible output for multi-valued fields."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 227, "body": "def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value='',\n                                             include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Core method of CsvItemExporter that writes a single item's data as a CSV row, ensuring headers are written first. It serializes item fields and exports them, supporting consistent CSV output generation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_build_row", "line_number": 237, "body": "def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Core helper method of CsvItemExporter that converts each value in a sequence to a unicode string using the specified encoding, facilitating consistent CSV row formatting during export."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_write_headers_and_set_fields_to_export", "line_number": 244, "body": "def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            row = list(self._build_row(self.fields_to_export))\n            self.csv_writer.writerow(row)", "is_method": true, "class_name": "CsvItemExporter", "function_description": "Internal method of CsvItemExporter that writes CSV headers based on item fields and determines which fields to export, supporting dynamic field configuration before exporting data rows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 260, "body": "def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)", "is_method": true, "class_name": "PickleItemExporter", "function_description": "Core method of PickleItemExporter that serializes and writes an item's data to a file using pickle, enabling efficient storage or transfer of Python objects in a serialized format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 278, "body": "def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)", "is_method": true, "class_name": "MarshalItemExporter", "function_description": "Core method of the MarshalItemExporter class that serializes and exports an item's fields to a file using Python\u2019s marshal format, enabling efficient storage of Python object data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 288, "body": "def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + '\\n'))", "is_method": true, "class_name": "PprintItemExporter", "function_description": "Utility method in PprintItemExporter that formats and writes a serialized item as a prettified string to a file, aiding readable export of item data for logging or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_configure", "line_number": 303, "body": "def _configure(self, options, dont_fail=False):\n        self.binary = options.pop('binary', True)\n        super()._configure(options, dont_fail)\n        if self.binary:\n            warnings.warn(\n                \"PythonItemExporter will drop support for binary export in the future\",\n                ScrapyDeprecationWarning)\n        if not self.encoding:\n            self.encoding = 'utf-8'", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Configures the PythonItemExporter with specified options, including binary export settings and text encoding, while issuing a deprecation warning for future removal of binary support. It ensures proper setup before exporting items."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "serialize_field", "line_number": 313, "body": "def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._serialize_value)\n        return serializer(value)", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Core utility method of the PythonItemExporter class that serializes a given field's value using a custom or default serializer, facilitating flexible data export processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_serialize_value", "line_number": 317, "body": "def _serialize_value(self, value):\n        if isinstance(value, _BaseItem):\n            return self.export_item(value)\n        elif is_item(value):\n            return dict(self._serialize_item(value))\n        elif is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (str, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Core method of PythonItemExporter that converts various Python objects, including nested items and lists, into serialized forms suitable for exporting, handling encoding and recursive structure serialization automatically."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "_serialize_item", "line_number": 329, "body": "def _serialize_item(self, item):\n        for key, value in ItemAdapter(item).items():\n            key = to_bytes(key) if self.binary else key\n            yield key, self._serialize_value(value)", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Internal method of PythonItemExporter that converts each item's key-value pair into a serialized format, preparing them for output in either binary or text form depending on configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/exporters.py", "function": "export_item", "line_number": 334, "body": "def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        if self.binary:\n            result = dict(self._serialize_item(result))\n        return result", "is_method": true, "class_name": "PythonItemExporter", "function_description": "Utility method of PythonItemExporter that serializes and exports an item\u2019s fields, optionally converting them into a binary format for flexible data output or storage use cases."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/loader/common.py", "function": "wrap_loader_context", "line_number": 10, "body": "def wrap_loader_context(function, context):\n    \"\"\"Wrap functions that receive loader_context to contain the context\n    \"pre-loaded\" and expose a interface that receives only one argument\n    \"\"\"\n    warnings.warn(\n        \"scrapy.loader.common.wrap_loader_context has moved to a new library.\"\n        \"Please update your reference to itemloaders.common.wrap_loader_context\",\n        ScrapyDeprecationWarning,\n        stacklevel=2\n    )\n\n    return common.wrap_loader_context(function, context)", "is_method": false, "function_description": "This function wraps a given function by preloading its context, allowing it to be called with a single argument while handling loader context internally. It facilitates integration with loader context-aware functions and issues a deprecation warning about its relocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "add_request", "line_number": 35, "body": "def add_request(self, request):\n        self.inprogress.add(request)", "is_method": true, "class_name": "Slot", "function_description": "Adds a request to the Slot's set of ongoing requests, tracking active processes or operations associated with this slot instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "remove_request", "line_number": 38, "body": "def remove_request(self, request):\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()", "is_method": true, "class_name": "Slot", "function_description": "Removes a specified request from the Slot's active tasks and triggers any necessary closure events. This helps manage and update the Slot's current workload state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close", "line_number": 42, "body": "def close(self):\n        self.closing = defer.Deferred()\n        self._maybe_fire_closing()\n        return self.closing", "is_method": true, "class_name": "Slot", "function_description": "Core method of the Slot class that initiates the closing process and returns a Deferred object representing that asynchronous closing event. It enables other components to track or react to the closure status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_maybe_fire_closing", "line_number": 47, "body": "def _maybe_fire_closing(self):\n        if self.closing and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)", "is_method": true, "class_name": "Slot", "function_description": "Handles the finalization process of the Slot by canceling pending actions and stopping heartbeats when closing conditions are met, ensuring proper cleanup and callback invocation during shutdown."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "start", "line_number": 74, "body": "def start(self):\n        \"\"\"Start the execution engine\"\"\"\n        if self.running:\n            raise RuntimeError(\"Engine already running\")\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait = defer.Deferred()\n        yield self._closewait", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Starts the execution engine if not already running, initializes its state, sends a start signal, and waits for a shutdown event. Useful for managing the engine's lifecycle and coordinating startup procedures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "stop", "line_number": 84, "body": "def stop(self):\n        \"\"\"Stop the execution engine gracefully\"\"\"\n        if not self.running:\n            raise RuntimeError(\"Engine not running\")\n        self.running = False\n        dfd = self._close_all_spiders()\n        return dfd.addBoth(lambda _: self._finish_stopping_engine())", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Method in ExecutionEngine that stops the engine gracefully, ensuring all running spiders are closed before completing the shutdown process. It provides controlled termination functionality for the execution environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close", "line_number": 92, "body": "def close(self):\n        \"\"\"Close the execution engine gracefully.\n\n        If it has already been started, stop it. In all cases, close all spiders\n        and the downloader.\n        \"\"\"\n        if self.running:\n            # Will also close spiders and downloader\n            return self.stop()\n        elif self.open_spiders:\n            # Will also close downloader\n            return self._close_all_spiders()\n        else:\n            return defer.succeed(self.downloader.close())", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Service method of ExecutionEngine that gracefully stops running processes by shutting down active spiders and the downloader, ensuring proper resource cleanup regardless of the engine's current state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "pause", "line_number": 107, "body": "def pause(self):\n        \"\"\"Pause the execution engine\"\"\"\n        self.paused = True", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Utility method of the ExecutionEngine class that pauses its operation, allowing other components to temporarily halt processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "unpause", "line_number": 111, "body": "def unpause(self):\n        \"\"\"Resume the execution engine\"\"\"\n        self.paused = False", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Resumes the execution engine by clearing its paused state, allowing operations or processes to continue. Useful for controlling execution flow in managed runtime environments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_next_request", "line_number": 115, "body": "def _next_request(self, spider):\n        slot = self.slot\n        if not slot:\n            return\n\n        if self.paused:\n            return\n\n        while not self._needs_backout(spider):\n            if not self._next_request_from_scheduler(spider):\n                break\n\n        if slot.start_requests and not self._needs_backout(spider):\n            try:\n                request = next(slot.start_requests)\n            except StopIteration:\n                slot.start_requests = None\n            except Exception:\n                slot.start_requests = None\n                logger.error('Error while obtaining start requests',\n                             exc_info=True, extra={'spider': spider})\n            else:\n                self.crawl(request, spider)\n\n        if self.spider_is_idle(spider) and slot.close_if_idle:\n            self._spider_idle(spider)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Manages and schedules the next requests for a spider to crawl, handling start requests, scheduler polling, and spider idle state transitions within the execution engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_needs_backout", "line_number": 142, "body": "def _needs_backout(self, spider):\n        slot = self.slot\n        return (\n            not self.running\n            or slot.closing\n            or self.downloader.needs_backout()\n            or self.scraper.slot.needs_backout()\n        )", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Private method of ExecutionEngine that determines if the current crawling process should revert or halt based on engine state, slot activity, downloader, or scraper conditions. It helps manage and control crawl execution flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_next_request_from_scheduler", "line_number": 151, "body": "def _next_request_from_scheduler(self, spider):\n        slot = self.slot\n        request = slot.scheduler.next_request()\n        if not request:\n            return\n        d = self._download(request, spider)\n        d.addBoth(self._handle_downloader_output, request, spider)\n        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.remove_request(request))\n        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.nextcall.schedule())\n        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Internal method of ExecutionEngine that retrieves the next request from the scheduler, initiates its download, and manages callbacks for processing, error handling, and scheduling subsequent requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_handle_downloader_output", "line_number": 171, "body": "def _handle_downloader_output(self, response, request, spider):\n        if not isinstance(response, (Request, Response, Failure)):\n            raise TypeError(\n                \"Incorrect type: expected Request, Response or Failure, got \"\n                f\"{type(response)}: {response!r}\"\n            )\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(response, Request):\n            self.crawl(response, spider)\n            return\n        # response is a Response or Failure\n        d = self.scraper.enqueue_scrape(response, request, spider)\n        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Handles and processes the output from the downloader by either scheduling new requests or passing responses and failures to the scraper, facilitating the continuation of the crawling and scraping workflow within the ExecutionEngine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "spider_is_idle", "line_number": 188, "body": "def spider_is_idle(self, spider):\n        if not self.scraper.slot.is_idle():\n            # scraper is not idle\n            return False\n\n        if self.downloader.active:\n            # downloader has pending requests\n            return False\n\n        if self.slot.start_requests is not None:\n            # not all start requests are handled\n            return False\n\n        if self.slot.scheduler.has_pending_requests():\n            # scheduler has pending requests\n            return False\n\n        return True", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Determines whether a given spider is idle by checking if all associated scraping, downloading, and scheduling activities have completed. Useful for orchestrating or pausing spider operations based on their current activity status."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "open_spiders", "line_number": 208, "body": "def open_spiders(self):\n        return [self.spider] if self.spider else []", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Returns a list containing the currently assigned spider, or an empty list if none is assigned. This function provides a simple way to access active spiders within the ExecutionEngine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "has_capacity", "line_number": 211, "body": "def has_capacity(self):\n        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n        return not bool(self.slot)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Determines if the ExecutionEngine currently has the capacity to handle additional spiders by checking its internal slot availability. This helps manage workload distribution in concurrent spider executions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "crawl", "line_number": 215, "body": "def crawl(self, request, spider):\n        if spider not in self.open_spiders:\n            raise RuntimeError(f\"Spider {spider.name!r} not opened when crawling: {request}\")\n        self.schedule(request, spider)\n        self.slot.nextcall.schedule()", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Schedules a crawling request for an active spider within the execution engine, ensuring the spider is open before initiating the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "schedule", "line_number": 221, "body": "def schedule(self, request, spider):\n        self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)\n        if not self.slot.scheduler.enqueue_request(request):\n            self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Schedules a request for execution within the engine, signaling whether the request was accepted or dropped by the scheduler. This enables coordinated request handling and monitoring in scraping or task execution workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "download", "line_number": 226, "body": "def download(self, request, spider):\n        d = self._download(request, spider)\n        d.addBoth(self._downloaded, self.slot, request, spider)\n        return d", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Core method of ExecutionEngine that initiates a download request through the engine\u2019s pipeline, handling the response regardless of success or failure for further processing within the given spider context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_downloaded", "line_number": 231, "body": "def _downloaded(self, response, slot, request, spider):\n        slot.remove_request(request)\n        return self.download(response, spider) if isinstance(response, Request) else response", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Private method of ExecutionEngine that manages a downloaded response by removing its request from the processing slot and continuing the download if the response is a new request; otherwise, it returns the response as is."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_download", "line_number": 235, "body": "def _download(self, request, spider):\n        slot = self.slot\n        slot.add_request(request)\n\n        def _on_success(response):\n            if not isinstance(response, (Response, Request)):\n                raise TypeError(\n                    \"Incorrect type: expected Response or Request, got \"\n                    f\"{type(response)}: {response!r}\"\n                )\n            if isinstance(response, Response):\n                if response.request is None:\n                    response.request = request\n                logkws = self.logformatter.crawled(response.request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=response,\n                    request=response.request,\n                    spider=spider,\n                )\n            return response\n\n        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _\n\n        dwld = self.downloader.fetch(request, spider)\n        dwld.addCallbacks(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Performs the download of a request, handling success and completion callbacks while integrating with logging, signaling, and scheduling within a crawling slot. It manages the lifecycle of a web request in a scraping engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "open_spider", "line_number": 269, "body": "def open_spider(self, spider, start_requests=(), close_if_idle=True):\n        if not self.has_capacity():\n            raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n        logger.info(\"Spider opened\", extra={'spider': spider})\n        nextcall = CallLaterOnce(self._next_request, spider)\n        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.slot = slot\n        self.spider = spider\n        yield scheduler.open(spider)\n        yield self.scraper.open_spider(spider)\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        slot.nextcall.schedule()\n        slot.heartbeat.start(5)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Method of ExecutionEngine that initializes and starts a spider crawl, managing resource availability, scheduling, and signaling to coordinate the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_spider_idle", "line_number": 286, "body": "def _spider_idle(self, spider):\n        \"\"\"Called when a spider gets idle. This function is called when there\n        are no remaining pages to download or schedule. It can be called\n        multiple times. If some extension raises a DontCloseSpider exception\n        (in the spider_idle signal handler) the spider is not closed until the\n        next loop and this function is guaranteed to be called (at least) once\n        again for this spider.\n        \"\"\"\n        res = self.signals.send_catch_log(signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n            return\n\n        if self.spider_is_idle(spider):\n            self.close_spider(spider, reason='finished')", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Handles the event when a spider becomes idle by checking if it should remain active or be closed, coordinating spider lifecycle based on signal responses and idle state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "close_spider", "line_number": 301, "body": "def close_spider(self, spider, reason='cancelled'):\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n        slot = self.slot\n        if slot.closing:\n            return slot.closing\n        logger.info(\"Closing spider (%(reason)s)\",\n                    {'reason': reason},\n                    extra={'spider': spider})\n\n        dfd = slot.close()\n\n        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure('Downloader close failure'))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure('Scraper close failure'))\n\n        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n        dfd.addErrback(log_failure('Scheduler close failure'))\n\n        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n            signal=signals.spider_closed, spider=spider, reason=reason))\n        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n        dfd.addErrback(log_failure('Stats close failure'))\n\n        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                          {'reason': reason},\n                                          extra={'spider': spider}))\n\n        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n        dfd.addErrback(log_failure('Error while unassigning slot'))\n\n        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n        dfd.addErrback(log_failure('Error while unassigning spider'))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Service method of ExecutionEngine that cleanly shuts down a spider, cancels its requests, and closes related components while handling errors and signaling completion for resource cleanup and spider lifecycle management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_close_all_spiders", "line_number": 352, "body": "def _close_all_spiders(self):\n        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n        dlist = defer.DeferredList(dfds)\n        return dlist", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Internal method of the ExecutionEngine class that initiates the shutdown process for all currently open spiders and returns a combined deferred indicating when all closures complete."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_finish_stopping_engine", "line_number": 358, "body": "def _finish_stopping_engine(self):\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n        self._closewait.callback(None)", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Internal method of ExecutionEngine that signals engine stoppage completion and triggers associated callbacks to finalize shutdown procedures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_on_success", "line_number": 239, "body": "def _on_success(response):\n            if not isinstance(response, (Response, Request)):\n                raise TypeError(\n                    \"Incorrect type: expected Response or Request, got \"\n                    f\"{type(response)}: {response!r}\"\n                )\n            if isinstance(response, Response):\n                if response.request is None:\n                    response.request = request\n                logkws = self.logformatter.crawled(response.request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=response,\n                    request=response.request,\n                    spider=spider,\n                )\n            return response", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Internal callback method in ExecutionEngine that validates and processes successful HTTP responses or requests, logging relevant events and signaling response receipt to other components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "_on_complete", "line_number": 259, "body": "def _on_complete(_):\n            slot.nextcall.schedule()\n            return _", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Private callback that schedules the next execution step upon completion of a task within the ExecutionEngine. It ensures continuous task processing by triggering subsequent scheduled actions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "log_failure", "line_number": 313, "body": "def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Returns a callback function that logs an error message with failure details when called, facilitating standardized error handling and logging within the execution engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/engine.py", "function": "errback", "line_number": 314, "body": "def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )", "is_method": true, "class_name": "ExecutionEngine", "function_description": "Logs an error message with detailed failure information within the ExecutionEngine, supporting error tracking and debugging in asynchronous or event-driven workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "from_crawler", "line_number": 50, "body": "def from_crawler(cls, crawler):\n        settings = crawler.settings\n        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n        logunser = settings.getbool('SCHEDULER_DEBUG')\n        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n                   mqclass=mqclass, crawler=crawler)", "is_method": true, "class_name": "Scheduler", "function_description": "Factory method in the Scheduler class that initializes a Scheduler instance using configuration and components derived from a crawler object, enabling customized scheduling behavior within a web scraping context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "has_pending_requests", "line_number": 62, "body": "def has_pending_requests(self):\n        return len(self) > 0", "is_method": true, "class_name": "Scheduler", "function_description": "Checks if the Scheduler has any pending requests waiting to be processed, indicating workload presence. This helps in determining if the scheduler is currently managing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "open", "line_number": 65, "body": "def open(self, spider):\n        self.spider = spider\n        self.mqs = self._mq()\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()", "is_method": true, "class_name": "Scheduler", "function_description": "Initial inspection shows this method sets up internal components related to message and data queues, associates a spider object, and opens a data feed. It likely prepares the Scheduler instance to start processing or managing tasks for the given spider. This setup function returns the result of opening the data feed, implying it is used to initialize and enable communication channels and data flow before scheduling tasks.\n\nCore service: initializes the Scheduler with a spider context and prepares its messaging, data queues, and data feed for operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "close", "line_number": 71, "body": "def close(self, reason):\n        if self.dqs:\n            state = self.dqs.close()\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)", "is_method": true, "class_name": "Scheduler", "function_description": "Service method of the Scheduler class that closes the scheduler's resources, including a data queue system, and records its final state. It ensures proper shutdown and state persistence upon closure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "enqueue_request", "line_number": 77, "body": "def enqueue_request(self, request):\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        if dqok:\n            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n        return True", "is_method": true, "class_name": "Scheduler", "function_description": "Core method of the Scheduler class that adds requests to the queue with filtering and prioritization between disk and memory storage, while tracking enqueue statistics for efficient request management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "next_request", "line_number": 90, "body": "def next_request(self):\n        request = self.mqs.pop()\n        if request:\n            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request:\n                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n        if request:\n            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n        return request", "is_method": true, "class_name": "Scheduler", "function_description": "Scheduler method that fetches the next pending request from memory or disk queues, updating dequeue statistics accordingly to manage and track request processing flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "__len__", "line_number": 102, "body": "def __len__(self):\n        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)", "is_method": true, "class_name": "Scheduler", "function_description": "Returns the total number of scheduled tasks by summing two internal queues, reflecting the current workload managed by the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqpush", "line_number": 105, "body": "def _dqpush(self, request):\n        if self.dqs is None:\n            return\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\"Unable to serialize request: %(request)s - reason:\"\n                       \" %(reason)s - no more unserializable requests will be\"\n                       \" logged (stats being collected)\")\n                logger.warning(msg, {'request': request, 'reason': e},\n                               exc_info=True, extra={'spider': self.spider})\n                self.logunser = False\n            self.stats.inc_value('scheduler/unserializable',\n                                 spider=self.spider)\n            return\n        else:\n            return True", "is_method": true, "class_name": "Scheduler", "function_description": "Internal method of Scheduler that attempts to enqueue a request for deferred processing, handling serialization errors gracefully by logging and tracking such failures to maintain queue integrity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_mqpush", "line_number": 124, "body": "def _mqpush(self, request):\n        self.mqs.push(request)", "is_method": true, "class_name": "Scheduler", "function_description": "Enqueues a given request into the Scheduler's message queue for asynchronous processing or handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqpop", "line_number": 127, "body": "def _dqpop(self):\n        if self.dqs:\n            return self.dqs.pop()", "is_method": true, "class_name": "Scheduler", "function_description": "Private method in the Scheduler class that removes and returns the last element from a dequeue structure if it is not empty. It provides internal queue management functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_mq", "line_number": 131, "body": "def _mq(self):\n        \"\"\" Create a new priority queue instance, with in-memory storage \"\"\"\n        return create_instance(self.pqclass,\n                               settings=None,\n                               crawler=self.crawler,\n                               downstream_queue_cls=self.mqclass,\n                               key='')", "is_method": true, "class_name": "Scheduler", "function_description": "Creates and returns a new in-memory priority queue instance configured with scheduler settings, supporting task management within the Scheduler class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dq", "line_number": 139, "body": "def _dq(self):\n        \"\"\" Create a new priority queue instance, with disk storage \"\"\"\n        state = self._read_dqs_state(self.dqdir)\n        q = create_instance(self.pqclass,\n                            settings=None,\n                            crawler=self.crawler,\n                            downstream_queue_cls=self.dqclass,\n                            key=self.dqdir,\n                            startprios=state)\n        if q:\n            logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                        {'queuesize': len(q)}, extra={'spider': self.spider})\n        return q", "is_method": true, "class_name": "Scheduler", "function_description": "Creates and returns a new disk-backed priority queue initialized with saved state for resuming scheduled crawl requests, supporting efficient crawl resumption in the Scheduler."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_dqdir", "line_number": 153, "body": "def _dqdir(self, jobdir):\n        \"\"\" Return a folder name to keep disk queue state at \"\"\"\n        if jobdir:\n            dqdir = join(jobdir, 'requests.queue')\n            if not exists(dqdir):\n                os.makedirs(dqdir)\n            return dqdir", "is_method": true, "class_name": "Scheduler", "function_description": "Returns a directory path for storing disk queue state within a given job directory, creating it if it does not exist. This supports job scheduling by managing persistent queue data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_read_dqs_state", "line_number": 161, "body": "def _read_dqs_state(self, dqdir):\n        path = join(dqdir, 'active.json')\n        if not exists(path):\n            return ()\n        with open(path) as f:\n            return json.load(f)", "is_method": true, "class_name": "Scheduler", "function_description": "Internal method of the Scheduler class that reads and returns the state from an 'active.json' file within a specified directory, providing the current data quality service status if available."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scheduler.py", "function": "_write_dqs_state", "line_number": 168, "body": "def _write_dqs_state(self, dqdir, state):\n        with open(join(dqdir, 'active.json'), 'w') as f:\n            json.dump(state, f)", "is_method": true, "class_name": "Scheduler", "function_description": "Internal Scheduler method that saves the current state information to an \"active.json\" file within a specified directory for data quality state persistence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_isiterable", "line_number": 17, "body": "def _isiterable(possible_iterator):\n    return hasattr(possible_iterator, '__iter__')", "is_method": false, "function_description": "Helper function that checks if a given object is iterable, enabling functions to conditionally handle iterable inputs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_get_mwlist_from_settings", "line_number": 26, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Utility method of the SpiderMiddlewareManager class that extracts and constructs the list of spider middleware components configured in the settings, facilitating middleware management during scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_add_middleware", "line_number": 29, "body": "def _add_middleware(self, mw):\n        super()._add_middleware(mw)\n        if hasattr(mw, 'process_spider_input'):\n            self.methods['process_spider_input'].append(mw.process_spider_input)\n        if hasattr(mw, 'process_start_requests'):\n            self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n        process_spider_output = getattr(mw, 'process_spider_output', None)\n        self.methods['process_spider_output'].appendleft(process_spider_output)\n        process_spider_exception = getattr(mw, 'process_spider_exception', None)\n        self.methods['process_spider_exception'].appendleft(process_spider_exception)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Adds a middleware component and registers its spider-related processing methods to the manager's method routing lists for handling spider input, start requests, output, and exceptions. This enables dynamic extension of spider processing behavior in web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_input", "line_number": 40, "body": "def _process_spider_input(self, scrape_func, response, request, spider):\n        for method in self.methods['process_spider_input']:\n            try:\n                result = method(response=response, spider=spider)\n                if result is not None:\n                    msg = (f\"Middleware {method.__qualname__} must return None \"\n                           f\"or raise an exception, got {type(result)}\")\n                    raise _InvalidOutput(msg)\n            except _InvalidOutput:\n                raise\n            except Exception:\n                return scrape_func(Failure(), request, spider)\n        return scrape_func(response, request, spider)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Core method in SpiderMiddlewareManager that applies spider input processing hooks before spider execution, ensuring middleware compliance and enabling graceful error handling during web scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_evaluate_iterable", "line_number": 54, "body": "def _evaluate_iterable(self, response, spider, iterable, exception_processor_index, recover_to):\n        try:\n            for r in iterable:\n                yield r\n        except Exception as ex:\n            exception_result = self._process_spider_exception(response, spider, Failure(ex),\n                                                              exception_processor_index)\n            if isinstance(exception_result, Failure):\n                raise\n            recover_to.extend(exception_result)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Core method of SpiderMiddlewareManager that iterates over a sequence and handles exceptions by processing them through spider middleware, optionally recovering with alternative results to maintain iterative flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_exception", "line_number": 65, "body": "def _process_spider_exception(self, response, spider, _failure, start_index=0):\n        exception = _failure.value\n        # don't handle _InvalidOutput exception\n        if isinstance(exception, _InvalidOutput):\n            return _failure\n        method_list = islice(self.methods['process_spider_exception'], start_index, None)\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            result = method(response=response, exception=exception, spider=spider)\n            if _isiterable(result):\n                # stop exception handling by handing control over to the\n                # process_spider_output chain if an iterable has been returned\n                return self._process_spider_output(response, spider, result, method_index + 1)\n            elif result is None:\n                continue\n            else:\n                msg = (f\"Middleware {method.__qualname__} must return None \"\n                       f\"or an iterable, got {type(result)}\")\n                raise _InvalidOutput(msg)\n        return _failure", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Handles exceptions during spider processing by passing them through middleware methods, allowing custom responses or recovery actions; it enables flexible, chainable exception management within the SpiderMiddlewareManager."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_spider_output", "line_number": 87, "body": "def _process_spider_output(self, response, spider, result, start_index=0):\n        # items in this iterable do not need to go through the process_spider_output\n        # chain, they went through it already from the process_spider_exception method\n        recovered = MutableChain()\n\n        method_list = islice(self.methods['process_spider_output'], start_index, None)\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            try:\n                # might fail directly if the output value is not a generator\n                result = method(response=response, result=result, spider=spider)\n            except Exception as ex:\n                exception_result = self._process_spider_exception(response, spider, Failure(ex), method_index + 1)\n                if isinstance(exception_result, Failure):\n                    raise\n                return exception_result\n            if _isiterable(result):\n                result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)\n            else:\n                msg = (f\"Middleware {method.__qualname__} must return an \"\n                       f\"iterable, got {type(result)}\")\n                raise _InvalidOutput(msg)\n\n        return MutableChain(result, recovered)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Handles output processing through a chain of spider middleware methods, applying each method sequentially to transform or filter results while managing exceptions and ensuring returned values remain iterable. It enables modular post-processing of spider response data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "_process_callback_output", "line_number": 113, "body": "def _process_callback_output(self, response, spider, result):\n        recovered = MutableChain()\n        result = self._evaluate_iterable(response, spider, result, 0, recovered)\n        return MutableChain(self._process_spider_output(response, spider, result), recovered)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Internal method of SpiderMiddlewareManager that processes and evaluates the output from spider callbacks, transforming it into a structured chain for further handling within crawling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "scrape_response", "line_number": 118, "body": "def scrape_response(self, scrape_func, response, request, spider):\n        def process_callback_output(result):\n            return self._process_callback_output(response, spider, result)\n\n        def process_spider_exception(_failure):\n            return self._process_spider_exception(response, spider, _failure)\n\n        dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n        dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n        return dfd", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "Handles the execution of a spider's scraping function on a response, processing its output or exceptions asynchronously to integrate with middleware operations during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/spidermw.py", "function": "process_start_requests", "line_number": 129, "body": "def process_start_requests(self, start_requests, spider):\n        return self._process_chain('process_start_requests', start_requests, spider)", "is_method": true, "class_name": "SpiderMiddlewareManager", "function_description": "This method processes the initial requests of a spider by passing them through a middleware chain, enabling pre-processing or modification before crawling begins. It supports customization of start request handling in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "add_response_request", "line_number": 37, "body": "def add_response_request(self, response, request):\n        deferred = defer.Deferred()\n        self.queue.append((response, request, deferred))\n        if isinstance(response, Response):\n            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred", "is_method": true, "class_name": "Slot", "function_description": "Method of the Slot class that queues a response-request pair for processing and returns a Deferred object to track its asynchronous completion. It also updates the active load size based on response content to manage resource usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "next_response_request_deferred", "line_number": 46, "body": "def next_response_request_deferred(self):\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred", "is_method": true, "class_name": "Slot", "function_description": "Returns the next response, request, and deferred task from the queue while marking the request as active. This method manages sequential processing of queued items in the Slot class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "finish_response", "line_number": 51, "body": "def finish_response(self, response, request):\n        self.active.remove(request)\n        if isinstance(response, Response):\n            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE", "is_method": true, "class_name": "Slot", "function_description": "Method of the Slot class that finalizes a response by removing its request from the active set and adjusting the active size counter based on the response content length."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "is_idle", "line_number": 58, "body": "def is_idle(self):\n        return not (self.queue or self.active)", "is_method": true, "class_name": "Slot", "function_description": "Utility method of the Slot class that determines if the slot is currently idle by checking if it has no queued or active tasks. It helps manage task processing states efficiently."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "needs_backout", "line_number": 61, "body": "def needs_backout(self):\n        return self.active_size > self.max_active_size", "is_method": true, "class_name": "Slot", "function_description": "Determines if the Slot instance has exceeded its maximum allowed active size, indicating whether a rollback or backout operation is necessary. This helps manage resource limits or transactional states within the Slot class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "open_spider", "line_number": 78, "body": "def open_spider(self, spider):\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))\n        yield self.itemproc.open_spider(spider)", "is_method": true, "class_name": "Scraper", "function_description": "Initializes resources and opens the spider for scraping, setting up the environment needed for the spider's operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "close_spider", "line_number": 83, "body": "def close_spider(self, spider):\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        slot = self.slot\n        slot.closing = defer.Deferred()\n        slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider, slot)\n        return slot.closing", "is_method": true, "class_name": "Scraper", "function_description": "Service method in Scraper that finalizes scraping by closing a spider and releasing its associated resources, ensuring proper cleanup and allowing downstream processes to complete."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "is_idle", "line_number": 91, "body": "def is_idle(self):\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot", "is_method": true, "class_name": "Scraper", "function_description": "Utility method of the Scraper class that indicates whether there are no active spiders left to process, signaling the scraper's idle state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_check_if_closing", "line_number": 95, "body": "def _check_if_closing(self, spider, slot):\n        if slot.closing and slot.is_idle():\n            slot.closing.callback(spider)", "is_method": true, "class_name": "Scraper", "function_description": "Private method of the Scraper class that triggers a callback when a spider's slot is marked for closing and is idle, supporting controlled shutdown processes during web scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "enqueue_scrape", "line_number": 99, "body": "def enqueue_scrape(self, response, request, spider):\n        slot = self.slot\n        dfd = slot.add_response_request(response, request)\n\n        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _\n\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error('Scraper bug processing %(request)s',\n                                   {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n        self._scrape_next(spider, slot)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Handles scheduling and processing of a web scraping task by managing request-response lifecycle and triggering subsequent scraping actions while handling errors and tracking spider state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape_next", "line_number": 118, "body": "def _scrape_next(self, spider, slot):\n        while slot.queue:\n            response, request, deferred = slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)", "is_method": true, "class_name": "Scraper", "function_description": "Internal method of the Scraper class that processes queued requests by invoking the scraping logic for each response-request pair, managing asynchronous callbacks to handle results sequentially within a given slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape", "line_number": 123, "body": "def _scrape(self, result, request, spider):\n        \"\"\"\n        Handle the downloaded response or failure through the spider callback/errback\n        \"\"\"\n        if not isinstance(result, (Response, Failure)):\n            raise TypeError(f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\")\n        dfd = self._scrape2(result, request, spider)  # returns spider's processed output\n        dfd.addErrback(self.handle_spider_error, request, result, spider)\n        dfd.addCallback(self.handle_spider_output, request, result, spider)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Handles a downloaded web response or failure by invoking the appropriate spider callback or error handler, then processes and manages the spider's output asynchronously."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_scrape2", "line_number": 134, "body": "def _scrape2(self, result, request, spider):\n        \"\"\"\n        Handle the different cases of request's result been a Response or a Failure\n        \"\"\"\n        if isinstance(result, Response):\n            return self.spidermw.scrape_response(self.call_spider, result, request, spider)\n        else:  # result is a Failure\n            dfd = self.call_spider(result, request, spider)\n            return dfd.addErrback(self._log_download_errors, result, request, spider)", "is_method": true, "class_name": "Scraper", "function_description": "Core internal method of the Scraper class that processes a request's result, delegating response handling or error logging based on whether the result is a successful Response or a Failure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "call_spider", "line_number": 144, "body": "def call_spider(self, result, request, spider):\n        if isinstance(result, Response):\n            if getattr(result, \"request\", None) is None:\n                result.request = request\n            callback = result.request.callback or spider._parse\n            warn_on_generator_with_return_value(spider, callback)\n            dfd = defer_succeed(result)\n            dfd.addCallback(callback, **result.request.cb_kwargs)\n        else:  # result is a Failure\n            result.request = request\n            warn_on_generator_with_return_value(spider, request.errback)\n            dfd = defer_fail(result)\n            dfd.addErrback(request.errback)\n        return dfd.addCallback(iterate_spider_output)", "is_method": true, "class_name": "Scraper", "function_description": "Handles the processing of spider callbacks for HTTP responses or errors, managing deferreds to process results or failures during web scraping. It enables asynchronous handling of spider outputs within the Scraper class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "handle_spider_error", "line_number": 159, "body": "def handle_spider_error(self, _failure, request, response, spider):\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n            return\n        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n        logger.log(\n            *logformatter_adapter(logkws),\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure, response=response,\n            spider=spider\n        )\n        self.crawler.stats.inc_value(\n            f\"spider_exceptions/{_failure.value.__class__.__name__}\",\n            spider=spider\n        )", "is_method": true, "class_name": "Scraper", "function_description": "Handles errors encountered during spider crawling by logging the failure, updating error statistics, sending error signals, and closing the spider if a critical shutdown exception occurs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "handle_spider_output", "line_number": 180, "body": "def handle_spider_output(self, result, request, response, spider):\n        if not result:\n            return defer_succeed(None)\n        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n        dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,\n                       request, response, spider)\n        return dfd", "is_method": true, "class_name": "Scraper", "function_description": "Handles and processes the output of a web scraping spider by managing asynchronous item processing with concurrency limits and error handling, facilitating efficient data pipeline execution within the Scraper class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_process_spidermw_output", "line_number": 188, "body": "def _process_spidermw_output(self, output, request, response, spider):\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        if isinstance(output, Request):\n            self.crawler.engine.crawl(request=output, spider=spider)\n        elif is_item(output):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error(\n                'Spider must return request, item, or None, got %(typename)r in %(request)s',\n                {'request': request, 'typename': typename},\n                extra={'spider': spider},\n            )", "is_method": true, "class_name": "Scraper", "function_description": "Internal Scraper method that processes outputs from a spider, handling requests to be crawled, items to be processed, or logs errors for invalid outputs. It supports the scraping workflow by managing different spider-generated results accordingly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_log_download_errors", "line_number": 209, "body": "def _log_download_errors(self, spider_failure, download_failure, request, spider):\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here)\n        \"\"\"\n        if isinstance(download_failure, Failure) and not download_failure.check(IgnoreRequest):\n            if download_failure.frames:\n                logkws = self.logformatter.download_error(download_failure, request, spider)\n                logger.log(\n                    *logformatter_adapter(logkws),\n                    extra={'spider': spider},\n                    exc_info=failure_to_exc_info(download_failure),\n                )\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logkws = self.logformatter.download_error(\n                        download_failure, request, spider, errmsg)\n                    logger.log(\n                        *logformatter_adapter(logkws),\n                        extra={'spider': spider},\n                    )\n\n        if spider_failure is not download_failure:\n            return spider_failure", "is_method": true, "class_name": "Scraper", "function_description": "Handles and logs download-related errors during scraping, ensuring they are recorded without interrupting the scraping process. This method helps maintain robust spider operation by managing and silencing engine download failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "_itemproc_finished", "line_number": 234, "body": "def _itemproc_finished(self, output, item, response, spider):\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n        \"\"\"\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped, item=item, response=response,\n                    spider=spider, exception=output.value)\n            else:\n                logkws = self.logformatter.item_error(item, ex, response, spider)\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n                           exc_info=failure_to_exc_info(output))\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_error, item=item, response=response,\n                    spider=spider, failure=output)\n        else:\n            logkws = self.logformatter.scraped(output, response, spider)\n            if logkws is not None:\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_scraped, item=output, response=response,\n                spider=spider)", "is_method": true, "class_name": "Scraper", "function_description": "Handles completion of item processing by logging events and dispatching corresponding signals based on success, dropped items, or errors during scraping. It facilitates consistent post-processing communication within the Scraper workflow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/scraper.py", "function": "finish_scraping", "line_number": 103, "body": "def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _", "is_method": true, "class_name": "Scraper", "function_description": "Finalizes the current scraping response, checks if the scraping process should conclude, and initiates the next scraping action within the Scraper workflow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "_get_mwlist_from_settings", "line_number": 20, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(\n            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Helper method in DownloaderMiddlewareManager that extracts and builds the list of downloader middleware components from configuration settings for use in managing request/response processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "_add_middleware", "line_number": 24, "body": "def _add_middleware(self, mw):\n        if hasattr(mw, 'process_request'):\n            self.methods['process_request'].append(mw.process_request)\n        if hasattr(mw, 'process_response'):\n            self.methods['process_response'].appendleft(mw.process_response)\n        if hasattr(mw, 'process_exception'):\n            self.methods['process_exception'].appendleft(mw.process_exception)", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Internal method of DownloaderMiddlewareManager that registers middleware methods for request, response, and exception processing, organizing them for appropriate invocation during the download lifecycle."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "download", "line_number": 32, "body": "def download(self, download_func, request, spider):\n        @defer.inlineCallbacks\n        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield deferred_from_coro(method(request=request, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request=request, spider=spider))\n\n        @defer.inlineCallbacks\n        def process_response(response):\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods['process_response']:\n                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response\n\n        @defer.inlineCallbacks\n        def process_exception(failure):\n            exception = failure.value\n            for method in self.methods['process_exception']:\n                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure\n\n        deferred = mustbe_deferred(process_request, request)\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Core service of DownloaderMiddlewareManager that orchestrates request processing through middleware hooks for request, response, and exception handling, enabling customizable download workflows with support for asynchronous operations and error management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_request", "line_number": 34, "body": "def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield deferred_from_coro(method(request=request, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request=request, spider=spider))", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Manages request processing by invoking middleware methods sequentially, allowing modification or early return of responses before passing requests to the downloader. It ensures middleware outputs are valid and maintains the request lifecycle in web scraping pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_response", "line_number": 47, "body": "def process_response(response):\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods['process_response']:\n                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Processes and validates a response through configured middleware methods, ensuring each middleware returns either a Response or Request object before passing it along or returning a Request to redirect further handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/middleware.py", "function": "process_exception", "line_number": 65, "body": "def process_exception(failure):\n            exception = failure.value\n            for method in self.methods['process_exception']:\n                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure", "is_method": true, "class_name": "DownloaderMiddlewareManager", "function_description": "Handles exceptions raised during request processing by passing them through middleware methods, returning a valid response or propagating the failure if unhandled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_parsed_url_args", "line_number": 14, "body": "def _parsed_url_args(parsed):\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    path = to_bytes(path, encoding=\"ascii\")\n    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n    port = parsed.port\n    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n    if port is None:\n        port = 443 if scheme == b'https' else 80\n    return scheme, netloc, host, port, path", "is_method": false, "function_description": "Private helper function that extracts and encodes key URL components from a parsed URL object, providing standardized scheme, host, port, and path values for HTTP request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_parse", "line_number": 28, "body": "def _parse(url):\n    \"\"\" Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)", "is_method": false, "function_description": "Internal helper function that parses a URL string into its components (scheme, netloc, host, port, path), returning mostly byte-encoded parts with port as an integer, tailored for sanitized ASCII URLs from request objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "connectionMade", "line_number": 43, "body": "def connectionMade(self):\n        self.headers = Headers()  # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Initial analysis shows this method belongs to ScrapyHTTPPageGetter, indicating it relates to HTTP request handling in web scraping. The method connectionMade sets up HTTP request headers and sends the request with method, path, headers, and optional body. It primarily handles the initiation of an HTTP request when a connection is established. This makes it a crucial step in the HTTP request lifecycle, enabling subsequent response processing.\n\nDescription:  \nHandles the initiation of an HTTP request by sending the method, headers, and optional body once a connection is established in the ScrapyHTTPPageGetter."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "lineReceived", "line_number": 57, "body": "def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Method in ScrapyHTTPPageGetter that processes each received line from an HTTP response by stripping trailing whitespace before further handling. It facilitates clean and consistent line processing during HTTP page retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleHeader", "line_number": 60, "body": "def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Adds a header key-value pair to the HTTP request headers list, facilitating the customization of HTTP requests sent by ScrapyHTTPPageGetter."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleStatus", "line_number": 63, "body": "def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "This method forwards HTTP status information from the page getter to its factory, enabling status tracking or handling during web page retrieval operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleEndHeaders", "line_number": 66, "body": "def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Calls the factory's method to process HTTP headers after they are completely received. This function signals that header retrieval is finished, enabling further handling of the response in HTTP page fetching."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "connectionLost", "line_number": 69, "body": "def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Handles the event when the HTTP connection is lost, recording the reason and notifying the factory to manage the lost page accordingly. This method supports error handling and cleanup in HTTP page retrieval processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "handleResponse", "line_number": 74, "body": "def handleResponse(self, response):\n        if self.factory.method.upper() == b'HEAD':\n            self.factory.page(b'')\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Handles HTTP responses by passing page content or error signals to the factory based on request method and content length, then closes the connection. It supports page retrieval control in asynchronous HTTP operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "timeout", "line_number": 83, "body": "def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b'https'):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\n            defer.TimeoutError(f\"Getting {self.factory.url} took longer \"\n                               f\"than {self.factory.timeout} seconds.\"))", "is_method": true, "class_name": "ScrapyHTTPPageGetter", "function_description": "Method of ScrapyHTTPPageGetter that terminates an ongoing HTTP request if it exceeds the allowed timeout period, ensuring proper connection cleanup and notifying the factory of the timeout event."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_build_response", "line_number": 108, "body": "def _build_response(self, body, request):\n        request.meta['download_latency'] = self.headers_time - self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url)\n        return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Constructs and returns an HTTP response object from raw response details, attaching timing and status information for use within Scrapy's request handling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_set_connection_attributes", "line_number": 115, "body": "def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n        proxy = request.meta.get('proxy')\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Sets internal connection parameters based on the request's URL and proxy information, preparing the client for making HTTP connections accordingly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "__repr__", "line_number": 157, "body": "def __repr__(self):\n        return f\"<{self.__class__.__name__}: {self.url}>\"", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Returns a string representation of the ScrapyHTTPClientFactory instance showing its class name and URL. This aids in debugging and logging by providing concise object details."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "_cancelTimeout", "line_number": 160, "body": "def _cancelTimeout(self, result, timeoutCall):\n        if timeoutCall.active():\n            timeoutCall.cancel()\n        return result", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Utility method within ScrapyHTTPClientFactory that cancels an active timeout callback and returns the given result, helping manage timing control for asynchronous operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "buildProtocol", "line_number": 165, "body": "def buildProtocol(self, addr):\n        p = ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Constructs and configures a protocol instance for HTTP requests, setting redirect behavior, post-redirect actions, and optional timeout handling for network communication. It facilitates customized HTTP client protocol setup within ScrapyHTTPClientFactory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "gotHeaders", "line_number": 174, "body": "def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Sets the time when response headers were received and stores the headers for later access. It supports tracking and managing HTTP response metadata within the client."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "gotStatus", "line_number": 178, "body": "def gotStatus(self, version, status, message):\n        \"\"\"\n        Set the status of the request on us.\n        @param version: The HTTP version.\n        @type version: L{bytes}\n        @param status: The HTTP status code, an integer represented as a\n            bytestring.\n        @type status: L{bytes}\n        @param message: The HTTP status message.\n        @type message: L{bytes}\n        \"\"\"\n        self.version, self.status, self.message = version, status, message", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Method of ScrapyHTTPClientFactory that records the HTTP response's version, status code, and message for the current request, providing access to the response status details."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "page", "line_number": 191, "body": "def page(self, page):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.callback(page)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Handles the completion of a waiting asynchronous operation by invoking its callback with the given page, enabling continuation of processes dependent on page retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "noPage", "line_number": 196, "body": "def noPage(self, reason):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.errback(reason)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Handles the failure case when no page is retrieved by resetting waiting state and triggering an error callback with the given reason. Useful for managing asynchronous request errors in ScrapyHTTPClientFactory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/webclient.py", "function": "clientConnectionFailed", "line_number": 201, "body": "def clientConnectionFailed(self, _, reason):\n        \"\"\"\n        When a connection attempt fails, the request cannot be issued.  If no\n        result has yet been provided to the result Deferred, provide the\n        connection failure reason as an error result.\n        \"\"\"\n        if self.waiting:\n            self.waiting = 0\n            # If the connection attempt failed, there is nothing more to\n            # disconnect, so just fire that Deferred now.\n            self._disconnectedDeferred.callback(None)\n            self.deferred.errback(reason)", "is_method": true, "class_name": "ScrapyHTTPClientFactory", "function_description": "Handles connection failure by notifying waiting processes and triggering error callbacks, ensuring that pending requests are informed about the failure reason promptly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_concurrency_delay", "line_number": 58, "body": "def _get_concurrency_delay(concurrency, spider, settings):\n    delay = settings.getfloat('DOWNLOAD_DELAY')\n    if hasattr(spider, 'download_delay'):\n        delay = spider.download_delay\n\n    if hasattr(spider, 'max_concurrent_requests'):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay", "is_method": false, "function_description": "Utility function that determines the effective concurrency level and download delay for a spider, combining global settings with spider-specific overrides. It helps manage request rates in web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "free_transfer_slots", "line_number": 30, "body": "def free_transfer_slots(self):\n        return self.concurrency - len(self.transferring)", "is_method": true, "class_name": "Slot", "function_description": "Returns the number of available transfer slots by subtracting ongoing transfers from the total concurrency limit, helping manage parallel transfer operations within the Slot class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "download_delay", "line_number": 33, "body": "def download_delay(self):\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n        return self.delay", "is_method": true, "class_name": "Slot", "function_description": "Returns the delay duration before downloading, optionally randomized within a range around a base delay value. This helps manage timing variability in download operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "close", "line_number": 38, "body": "def close(self):\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()", "is_method": true, "class_name": "Slot", "function_description": "Utility method of the Slot class that cancels any scheduled future action if it is still active, helping manage asynchronous operations and resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "__repr__", "line_number": 42, "body": "def __repr__(self):\n        cls_name = self.__class__.__name__\n        return (f\"{cls_name}(concurrency={self.concurrency!r}, \"\n                f\"delay={self.delay:.2f}, \"\n                f\"randomize_delay={self.randomize_delay!r})\")", "is_method": true, "class_name": "Slot", "function_description": "Provides a readable string representation of a Slot instance showing its concurrency, delay (formatted to two decimals), and randomize_delay attributes for easier debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "__str__", "line_number": 48, "body": "def __str__(self):\n        return (\n            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n            f\"len(transferring)={len(self.transferring)} \"\n            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n        )", "is_method": true, "class_name": "Slot", "function_description": "Provides a readable string representation of a Slot instance, summarizing its concurrency, delays, and current task states for easier debugging and monitoring."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "fetch", "line_number": 87, "body": "def fetch(self, request, spider):\n        def _deactivate(response):\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd = self.middleware.download(self._enqueue_request, request, spider)\n        return dfd.addBoth(_deactivate)", "is_method": true, "class_name": "Downloader", "function_description": "Core method of the Downloader class that manages request lifecycle by adding requests to an active set during download and removing them upon completion, facilitating controlled asynchronous fetching with middleware processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "needs_backout", "line_number": 96, "body": "def needs_backout(self):\n        return len(self.active) >= self.total_concurrency", "is_method": true, "class_name": "Downloader", "function_description": "Checks if the number of active downloads has reached or exceeded the allowed concurrency limit, indicating whether new downloads should be deferred or backed out."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_slot", "line_number": 99, "body": "def _get_slot(self, request, spider):\n        key = self._get_slot_key(request, spider)\n        if key not in self.slots:\n            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n\n        return key, self.slots[key]", "is_method": true, "class_name": "Downloader", "function_description": "Internal method of the Downloader class that manages and retrieves a concurrency slot for a given request and spider, ensuring proper handling of concurrent downloads with respect to domain or IP limits."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_get_slot_key", "line_number": 108, "body": "def _get_slot_key(self, request, spider):\n        if self.DOWNLOAD_SLOT in request.meta:\n            return request.meta[self.DOWNLOAD_SLOT]\n\n        key = urlparse_cached(request).hostname or ''\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key", "is_method": true, "class_name": "Downloader", "function_description": "Private method of the Downloader class that determines a unique key for a download slot based on request metadata or hostname, supporting IP-based concurrency control for efficient management of download slots."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_enqueue_request", "line_number": 118, "body": "def _enqueue_request(self, request, spider):\n        key, slot = self._get_slot(request, spider)\n        request.meta[self.DOWNLOAD_SLOT] = key\n\n        def _deactivate(response):\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n                                    request=request,\n                                    spider=spider)\n        deferred = defer.Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred", "is_method": true, "class_name": "Downloader", "function_description": "Internal method of the Downloader class that schedules a download request into a processing queue, manages its active state, and signals its initiation to control and track download concurrency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_process_queue", "line_number": 135, "body": "def _process_queue(self, spider, slot):\n        from twisted.internet import reactor\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break", "is_method": true, "class_name": "Downloader", "function_description": "Internal method of Downloader that manages request scheduling and execution for a given slot, enforcing download delays and queue order to control request concurrency and timing during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_download", "line_number": 160, "body": "def _download(self, slot, request, spider):\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives, remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(signal=signals.request_left_downloader,\n                                        request=request,\n                                        spider=spider)\n            return _\n\n        return dfd.addBoth(finish_transferring)", "is_method": true, "class_name": "Downloader", "function_description": "Internal method of the Downloader class that manages the lifecycle of downloading a request, including signaling download completion, tracking active transfers, and processing the download queue to maintain flow control."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "close", "line_number": 192, "body": "def close(self):\n        self._slot_gc_loop.stop()\n        for slot in self.slots.values():\n            slot.close()", "is_method": true, "class_name": "Downloader", "function_description": "Service method in Downloader that stops internal cleanup operations and closes all active download slots, ensuring proper resource release when download activities complete."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "_slot_gc", "line_number": 197, "body": "def _slot_gc(self, age=60):\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()", "is_method": true, "class_name": "Downloader", "function_description": "Internal method of the Downloader class that cleans up inactive download slots inactive beyond a specified age, freeing resources by closing and removing them from management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/__init__.py", "function": "finish_transferring", "line_number": 182, "body": "def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(signal=signals.request_left_downloader,\n                                        request=request,\n                                        spider=spider)\n            return _", "is_method": true, "class_name": "Downloader", "function_description": "Finalizes a completed download request by updating the transfer queue, processing subsequent requests, and signaling that the request has left the downloader."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/tls.py", "function": "_identityVerifyingInfoCallback", "line_number": 45, "body": "def _identityVerifyingInfoCallback(self, connection, where, ret):\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                             self._hostnameASCII,\n                             connection.get_protocol_version_name(),\n                             connection.get_cipher_name(),\n                             )\n                server_cert = connection.get_peer_certificate()\n                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                             x509name_to_string(server_cert.get_issuer()),\n                             x509name_to_string(server_cert.get_subject()),\n                             )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug('SSL temp key: %s', key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n                        self._hostnameASCII, e))\n\n            except ValueError as e:\n                logger.warning(\n                    'Ignoring error while verifying certificate '\n                    'from host \"{}\" (exception: {})'.format(\n                        self._hostnameASCII, repr(e)))", "is_method": true, "class_name": "ScrapyClientTLSOptions", "function_description": "Method of ScrapyClientTLSOptions that manages TLS handshake events by setting the server hostname, optionally logging connection details, and verifying the server's SSL certificate matches the expected hostname for secure communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "load_context_factory_from_settings", "line_number": 120, "body": "def load_context_factory_from_settings(settings, crawler):\n    ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n    context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n    # try method-aware context factory\n    try:\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n        )\n        msg = \"\"\"\n            '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n            e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n            Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n            settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n        warnings.warn(msg)\n\n    return context_factory", "is_method": false, "function_description": "Function that creates and returns a TLS/SSL context factory instance based on provided settings and crawler, supporting configurable SSL methods while maintaining backward compatibility with older context factory classes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "from_settings", "line_number": 37, "body": "def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Factory method of ScrapyClientContextFactory that creates an instance configured with TLS settings extracted from given settings, facilitating customized SSL/TLS client contexts for secure web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "getCertificateOptions", "line_number": 42, "body": "def getCertificateOptions(self):\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super().__init__\n        return CertificateOptions(\n            verify=False,\n            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Provides SSL/TLS certificate options configured to disable verification, support legacy method settings, and apply custom ciphers for client context establishment in Scrapy networking operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "getContext", "line_number": 63, "body": "def getContext(self, hostname=None, port=None):\n        return self.getCertificateOptions().getContext()", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Utility method in ScrapyClientContextFactory that returns an SSL context for secure connections, optionally using specific hostname and port settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 66, "body": "def creatorForNetloc(self, hostname, port):\n        return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n                                      verbose_logging=self.tls_verbose_logging)", "is_method": true, "class_name": "ScrapyClientContextFactory", "function_description": "Creates and returns TLS options configured for a specified hostname and port, facilitating secure client connections within the ScrapyClientContextFactory context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 90, "body": "def creatorForNetloc(self, hostname, port):\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={'method': self._ssl_method},\n        )", "is_method": true, "class_name": "BrowserLikeContextFactory", "function_description": "Provides TLS configuration options for a client based on a hostname and port, using platform-trusted root certificates and a specified SSL method. Enables secure network connections tailored for the given server endpoint."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/contextfactory.py", "function": "creatorForNetloc", "line_number": 114, "body": "def creatorForNetloc(self, hostname, port):\n        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options", "is_method": true, "class_name": "AcceptableProtocolsContextFactory", "function_description": "Creates and returns a network context configured for a specific hostname and port with allowable protocols set. It provides customized connection parameters enforcing protocol restrictions during network communication setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/datauri.py", "function": "download_request", "line_number": 12, "body": "def download_request(self, request, spider):\n        uri = parse_data_uri(request.url)\n        respcls = responsetypes.from_mimetype(uri.media_type)\n\n        resp_kwargs = {}\n        if (issubclass(respcls, TextResponse)\n                and uri.media_type.split('/')[0] == 'text'):\n            charset = uri.media_type_parameters.get('charset')\n            resp_kwargs['encoding'] = charset\n\n        return respcls(url=request.url, body=uri.data, **resp_kwargs)", "is_method": true, "class_name": "DataURIDownloadHandler", "function_description": "Provides a response object for a data URI download request by parsing the URI, determining the appropriate response type, and extracting the data with proper encoding if it's a text media type. It enables handling inline data resources within web crawlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "tunnel_request_data", "line_number": 165, "body": "def tunnel_request_data(host, port, proxy_auth_header=None):\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_unicode as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))\n    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\\r\\n'\n    tunnel_req += b'Host: ' + host_value + b'\\r\\n'\n    if proxy_auth_header:\n        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\\r\\n'\n    tunnel_req += b'\\r\\n'\n    return tunnel_req", "is_method": false, "function_description": "Function that constructs the raw binary data for an HTTP CONNECT request to establish a tunnel through a proxy, optionally including proxy authorization headers. This enables other functions to initiate proxy tunneling connections with proper request formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "from_crawler", "line_number": 52, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Creates an instance of HTTP11DownloadHandler using configuration settings and the crawler instance from the scraping framework. This enables customized handler initialization within the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "download_request", "line_number": 55, "body": "def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request)", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Provides an asynchronous HTTP download service by delegating requests to a configured ScrapyAgent, supporting customizable size limits and error handling tailored for web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "close", "line_number": 67, "body": "def close(self):\n        from twisted.internet import reactor\n        d = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Closes cached HTTP/1.1 connections managed by the handler, ensuring the operation completes or times out to avoid indefinite hanging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "requestTunnel", "line_number": 113, "body": "def requestTunnel(self, protocol):\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse\n        self._protocol = protocol\n        return protocol", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Establishes a proxy tunnel by requesting it from the proxy server and redirects incoming data handling to process the proxy's response, enabling tunneled communication through the proxy endpoint."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "processProxyResponse", "line_number": 122, "body": "def processProxyResponse(self, rcvd_bytes):\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        self._connectBuffer += rcvd_bytes\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b'\\r\\n\\r\\n' not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group('status')) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            if respm:\n                extra = {'status': int(respm.group('status')),\n                         'reason': respm.group('reason').strip()}\n            else:\n                extra = rcvd_bytes[:32]\n            self._tunnelReadyDeferred.errback(\n                TunnelError('Could not open CONNECT tunnel with proxy '\n                            f'{self._host}:{self._port} [{extra!r}]')\n            )", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Handles the proxy's response to establish a CONNECT tunnel, signaling readiness on success or raising a TunnelError on failure. It enables secure tunneling through a proxy by transitioning to TLS once the tunnel is confirmed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectFailed", "line_number": 153, "body": "def connectFailed(self, reason):\n        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n        self._tunnelReadyDeferred.errback(reason)", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Handles connection failure by notifying the associated deferred object, allowing calling code to respond to the error during the tunneling TCP client connection process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connect", "line_number": 157, "body": "def connect(self, protocolFactory):\n        self._protocolFactory = protocolFactory\n        connectDeferred = super().connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred", "is_method": true, "class_name": "TunnelingTCP4ClientEndpoint", "function_description": "Method of TunnelingTCP4ClientEndpoint that initiates a TCP connection using a protocol factory and manages tunnel setup asynchronously, providing a deferred that signals when the tunnel is ready or connection fails."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_getEndpoint", "line_number": 200, "body": "def _getEndpoint(self, uri):\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )", "is_method": true, "class_name": "TunnelingAgent", "function_description": "Private method of TunnelingAgent that creates a tunneling TCP client endpoint for connecting to a specified URI through proxy settings, handling connection context and timeouts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_requestWithEndpoint", "line_number": 211, "body": "def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key = key + self._proxyConf\n        return super()._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )", "is_method": true, "class_name": "TunnelingAgent", "function_description": "Utility method in TunnelingAgent that modifies connection keys with proxy configuration to ensure correct handling of tunneled HTTP requests through a proxy, enabling proper reuse and caching of connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "request", "line_number": 238, "body": "def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )", "is_method": true, "class_name": "ScrapyProxyAgent", "function_description": "Provides a method to send HTTP requests through a configured proxy server, enabling network communication to target URIs via the proxy within the ScrapyProxyAgent context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_get_agent", "line_number": 273, "body": "def _get_agent(self, request, timeout):\n        from twisted.internet import reactor\n        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost = to_unicode(proxyHost)\n            omitConnectTunnel = b'noconnect' in proxyParams\n            if omitConnectTunnel:\n                warnings.warn(\n                    \"Using HTTPS proxies in the noconnect mode is deprecated. \"\n                    \"If you use Zyte Smart Proxy Manager (formerly Crawlera), \"\n                    \"it doesn't require this mode anymore, so you should \"\n                    \"update scrapy-crawlera to 1.3.0+ and remove '?noconnect' \"\n                    \"from the Zyte Smart Proxy Manager URL.\",\n                    ScrapyDeprecationWarning,\n                )\n            if scheme == b'https' and not omitConnectTunnel:\n                proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                proxyConf = (proxyHost, proxyPort, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            else:\n                return self._ProxyAgent(\n                    reactor=reactor,\n                    proxyURI=to_bytes(proxy, encoding='ascii'),\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )", "is_method": true, "class_name": "ScrapyAgent", "function_description": "This internal method of ScrapyAgent selects and returns the appropriate HTTP agent for a request, handling proxy configuration, tunneling, and connection parameters based on request metadata and URL scheme. It supports flexible networking for web scraping with proxy and timeout management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "download_request", "line_number": 319, "body": "def download_request(self, request):\n        from twisted.internet import reactor\n        timeout = request.meta.get('download_timeout') or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b'Proxy-Authorization')\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        else:\n            bodyproducer = None\n        start_time = time()\n        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d.addCallback(self._cb_bodyready, request)\n        d.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, url, timeout)\n        return d", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Provides an asynchronous mechanism to perform HTTP requests with timeout handling, header management, and response processing within the ScrapyAgent class for efficient web scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_timeout", "line_number": 346, "body": "def _cb_timeout(self, result, request, url, timeout):\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Internal callback in ScrapyAgent that handles request timeouts by cancelling active timers and raising a TimeoutError if the response takes too long, ensuring timely failure detection in web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_latency", "line_number": 357, "body": "def _cb_latency(self, result, request, start_time):\n        request.meta['download_latency'] = time() - start_time\n        return result", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Internal callback method of ScrapyAgent that records the download latency of a request by calculating elapsed time since its start, facilitating performance monitoring of web scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_headers_from_twisted_response", "line_number": 362, "body": "def _headers_from_twisted_response(response):\n        headers = Headers()\n        if response.length != UNKNOWN_LENGTH:\n            headers[b'Content-Length'] = str(response.length).encode()\n        headers.update(response.headers.getAllRawHeaders())\n        return headers", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Private method of ScrapyAgent that constructs HTTP headers from a Twisted response, including content length if known, supporting downstream processing requiring standardized header objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_bodyready", "line_number": 369, "body": "def _cb_bodyready(self, txresponse, request):\n        headers_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.headers_received,\n            headers=self._headers_from_twisted_response(txresponse),\n            body_length=txresponse.length,\n            request=request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in headers_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                             {\"request\": request, \"handler\": handler.__qualname__})\n                txresponse._transport.stopProducing()\n                with suppress(AttributeError):\n                    txresponse._transport._producer.loseConnection()\n                return {\n                    \"txresponse\": txresponse,\n                    \"body\": b\"\",\n                    \"flags\": [\"download_stopped\"],\n                    \"certificate\": None,\n                    \"ip_address\": None,\n                    \"failure\": result if result.value.fail else None,\n                }\n\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return {\n                \"txresponse\": txresponse,\n                \"body\": b\"\",\n                \"flags\": None,\n                \"certificate\": None,\n                \"ip_address\": None,\n            }\n\n        maxsize = request.meta.get('download_maxsize', self._maxsize)\n        warnsize = request.meta.get('download_warnsize', self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n\n        if maxsize and expected_size > maxsize:\n            warning_msg = (\"Cancelling download of %(url)s: expected response \"\n                           \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n            warning_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n\n            logger.warning(warning_msg, warning_args)\n\n            txresponse._transport._producer.loseConnection()\n            raise defer.CancelledError(warning_msg % warning_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\"Expected response size (%(size)s) larger than \"\n                           \"download warn size (%(warnsize)s) in request %(request)s.\",\n                           {'size': expected_size, 'warnsize': warnsize, 'request': request})\n\n        def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d = defer.Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(\n                finished=d,\n                txresponse=txresponse,\n                request=request,\n                maxsize=maxsize,\n                warnsize=warnsize,\n                fail_on_dataloss=fail_on_dataloss,\n                crawler=self._crawler,\n            )\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Handles response body retrieval for HTTP requests in Scrapy, enforcing download size limits and allowing premature termination via signals. It manages response streaming, cancellation, and warning mechanisms to control the data received from web servers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cb_bodydone", "line_number": 445, "body": "def _cb_bodydone(self, result, request, url):\n        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n        try:\n            version = result[\"txresponse\"].version\n            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n        except (AttributeError, TypeError, IndexError):\n            protocol = None\n        response = respcls(\n            url=url,\n            status=int(result[\"txresponse\"].code),\n            headers=headers,\n            body=result[\"body\"],\n            flags=result[\"flags\"],\n            certificate=result[\"certificate\"],\n            ip_address=result[\"ip_address\"],\n            protocol=protocol,\n        )\n        if result.get(\"failure\"):\n            result[\"failure\"].value.response = response\n            return result[\"failure\"]\n        return response", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Processes the completion of an HTTP request in Scrapy, constructing a Response object from the raw result data for further handling or error propagation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "startProducing", "line_number": 476, "body": "def startProducing(self, consumer):\n        consumer.write(self.body)\n        return defer.succeed(None)", "is_method": true, "class_name": "_RequestBodyProducer", "function_description": "Provides the capability to start producing data by writing the stored request body to a consumer. This method facilitates asynchronous data transmission in network or I/O operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_finish_response", "line_number": 504, "body": "def _finish_response(self, flags=None, failure=None):\n        self._finished.callback({\n            \"txresponse\": self._txresponse,\n            \"body\": self._bodybuf.getvalue(),\n            \"flags\": flags,\n            \"certificate\": self._certificate,\n            \"ip_address\": self._ip_address,\n            \"failure\": failure,\n        })", "is_method": true, "class_name": "_ResponseReader", "function_description": "Finalizes the response by triggering a callback with response data, status flags, connection details, and any failure information to notify listeners or handlers of completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectionMade", "line_number": 514, "body": "def connectionMade(self):\n        if self._certificate is None:\n            with suppress(AttributeError):\n                self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())\n\n        if self._ip_address is None:\n            self._ip_address = ipaddress.ip_address(self.transport._producer.getPeer().host)", "is_method": true, "class_name": "_ResponseReader", "function_description": "Method that initializes SSL certificate and IP address attributes when a connection is established, enabling secure and identifiable communication tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "dataReceived", "line_number": 522, "body": "def dataReceived(self, bodyBytes):\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        bytes_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.bytes_received,\n            data=bodyBytes,\n            request=self._request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in bytes_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                             {\"request\": self._request, \"handler\": handler.__qualname__})\n                self.transport.stopProducing()\n                self.transport._producer.loseConnection()\n                failure = result if result.value.fail else None\n                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.warning(\"Received (%(bytes)s) bytes larger than download \"\n                           \"max size (%(maxsize)s) in request %(request)s.\",\n                           {'bytes': self._bytes_received,\n                            'maxsize': self._maxsize,\n                            'request': self._request})\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n            self._reached_warnsize = True\n            logger.warning(\"Received more bytes than download \"\n                           \"warn size (%(warnsize)s) in request %(request)s.\",\n                           {'warnsize': self._warnsize,\n                            'request': self._request})", "is_method": true, "class_name": "_ResponseReader", "function_description": "Handles incoming data chunks during a response, buffering bytes, enforcing size limits, sending byte-received signals, and stopping the download if requested or size limits are exceeded. Useful for controlled and monitored network data reception."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "connectionLost", "line_number": 562, "body": "def connectionLost(self, reason):\n        if self._finished.called:\n            return\n\n        if reason.check(ResponseDone):\n            self._finish_response()\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finish_response(flags=[\"partial\"])\n            return\n\n        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n            if not self._fail_on_dataloss:\n                self._finish_response(flags=[\"dataloss\"])\n                return\n\n            elif not self._fail_on_dataloss_warned:\n                logger.warning(\"Got data loss in %s. If you want to process broken \"\n                               \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                               \" -- This message won't be shown in further requests\",\n                               self._txresponse.request.absoluteURI.decode())\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)", "is_method": true, "class_name": "_ResponseReader", "function_description": "Handles connection loss events by determining response completion status and processing data loss conditions, optionally logging warnings or marking responses as partial or failed based on configured tolerance settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "cancel_delayed_call", "line_number": 80, "body": "def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result", "is_method": true, "class_name": "HTTP11DownloadHandler", "function_description": "Cancels an active delayed call if it exists and returns the provided result. This function is useful for safely terminating scheduled actions in asynchronous operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http11.py", "function": "_cancel", "line_number": 423, "body": "def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()", "is_method": true, "class_name": "ScrapyAgent", "function_description": "Terminates the current network connection immediately without processing any further data, allowing rapid cancellation of ongoing operations within ScrapyAgent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/file.py", "function": "download_request", "line_number": 11, "body": "def download_request(self, request, spider):\n        filepath = file_uri_to_path(request.url)\n        with open(filepath, 'rb') as fo:\n            body = fo.read()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)", "is_method": true, "class_name": "FileDownloadHandler", "function_description": "Provides a response object containing file data for a download request by reading the file from a local path derived from the request URL. Enables simulated file downloads within a web scraping context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_get_handler", "line_number": 32, "body": "def _get_handler(self, scheme):\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = 'no handler available for that scheme'\n            return None\n\n        return self._load_handler(scheme)", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Internal method of the DownloadHandlers class that retrieves and caches the appropriate download handler for a given URL scheme, enabling efficient reuse and lazy initialization of handlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_load_handler", "line_number": 46, "body": "def _load_handler(self, scheme, skip_lazy=False):\n        path = self._schemes[scheme]\n        try:\n            dhcls = load_object(path)\n            if skip_lazy and getattr(dhcls, 'lazy', True):\n                return None\n            dh = create_instance(\n                objcls=dhcls,\n                settings=self._crawler.settings,\n                crawler=self._crawler,\n            )\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                         {\"clspath\": path, \"scheme\": scheme},\n                         exc_info=True, extra={'crawler': self._crawler})\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n            return dh", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Core method of DownloadHandlers that loads and initializes a handler for a given scheme, managing lazy loading and error handling to ensure appropriate handler availability or error recording."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "download_request", "line_number": 70, "body": "def download_request(self, request, spider):\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\")\n        return handler.download_request(request, spider)", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Core method of DownloadHandlers that delegates processing of a download request to the appropriate handler based on the URL scheme, raising an error if the scheme is unsupported."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/__init__.py", "function": "_close", "line_number": 78, "body": "def _close(self, *_a, **_kw):\n        for dh in self._handlers.values():\n            if hasattr(dh, 'close'):\n                yield dh.close()", "is_method": true, "class_name": "DownloadHandlers", "function_description": "Private generator method in DownloadHandlers that yields the result of closing all handlers which support a close operation, facilitating resource cleanup across multiple download handlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "dataReceived", "line_number": 50, "body": "def dataReceived(self, data):\n        self.body.write(data)\n        self.size += len(data)", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Method of ReceivedDataProtocol that appends incoming data chunks to an internal buffer and updates the total received data size. It supports incremental data reception in streaming or network communication contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "filename", "line_number": 55, "body": "def filename(self):\n        return self.__filename", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Returns the stored filename associated with the ReceivedDataProtocol instance. This accessor method provides read-only access to the filename attribute for use by other components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "close", "line_number": 58, "body": "def close(self):\n        self.body.close() if self.filename else self.body.seek(0)", "is_method": true, "class_name": "ReceivedDataProtocol", "function_description": "Closes the data stream if a file is associated; otherwise, resets the stream position to the beginning. This ensures proper resource handling or prepares the data for rereading."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "from_crawler", "line_number": 79, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Factory method for FTPDownloadHandler that creates an instance using settings obtained from a given crawler object."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "download_request", "line_number": 82, "body": "def download_request(self, request, spider):\n        from twisted.internet import reactor\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n                                                  self.passive_mode)) else 0\n        creator = ClientCreator(reactor, FTPClient, user, password, passive=passive_mode)\n        dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Provides FTP download capability by establishing an FTP client connection using request-specific credentials and modes, then initiating the file retrieval process for a given URL and spider context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "gotClient", "line_number": 93, "body": "def gotClient(self, client, request, filepath):\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        return client.retrieveFile(filepath, protocol).addCallbacks(\n            callback=self._build_response,\n            callbackArgs=(request, protocol),\n            errback=self._failed,\n            errbackArgs=(request,),\n        )", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Handles incoming FTP client connections to initiate file retrieval and manages callbacks for processing the response or handling errors during the download."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "_build_response", "line_number": 103, "body": "def _build_response(self, result, request, protocol):\n        self.result = result\n        respcls = responsetypes.from_args(url=request.url)\n        protocol.close()\n        body = protocol.filename or protocol.body.read()\n        headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Constructs and returns a standardized response object based on the FTP download result, including metadata like filename and size, and ensures protocol cleanup after completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/ftp.py", "function": "_failed", "line_number": 111, "body": "def _failed(self, result, request):\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(url=request.url, status=httpcode, body=to_bytes(message))\n        raise result.type(result.value)", "is_method": true, "class_name": "FTPDownloadHandler", "function_description": "Handles FTP command failure by translating FTP error codes into corresponding HTTP responses, enabling consistent error reporting for FTP download requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "from_crawler", "line_number": 33, "body": "def from_crawler(cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler) -> H2DownloadHandlerOrSubclass:\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "Class method that initializes an H2DownloadHandler instance using settings from a given crawler, facilitating integration with crawler-based configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "download_request", "line_number": 36, "body": "def download_request(self, request: Request, spider: Spider) -> Deferred:\n        agent = ScrapyH2Agent(\n            context_factory=self._context_factory,\n            pool=self._pool,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request, spider)", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "Method of H2DownloadHandler that initiates an HTTP/2 download for a given request using a specialized agent, facilitating asynchronous web scraping with protocol-specific handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "close", "line_number": 44, "body": "def close(self) -> None:\n        self._pool.close_connections()", "is_method": true, "class_name": "H2DownloadHandler", "function_description": "Closes all active connections managed by the H2DownloadHandler's connection pool, ensuring proper resource cleanup and preventing further network activity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_get_agent", "line_number": 65, "body": "def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n        from twisted.internet import reactor\n        bind_address = request.meta.get('bindaddress') or self._bind_address\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxy_host = proxy_host.decode()\n            omit_connect_tunnel = b'noconnect' in proxy_params\n            if omit_connect_tunnel:\n                warnings.warn(\"Using HTTPS proxies in the noconnect mode is not supported by the \"\n                              \"downloader handler. If you use Crawlera, it doesn't require this \"\n                              \"mode anymore, so you should update scrapy-crawlera to 1.3.0+ \"\n                              \"and remove '?noconnect' from the Crawlera URL.\")\n\n            if scheme == b'https' and not omit_connect_tunnel:\n                # ToDo\n                raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')\n            return self._ProxyAgent(\n                reactor=reactor,\n                context_factory=self._context_factory,\n                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),\n                connect_timeout=timeout,\n                bind_address=bind_address,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            context_factory=self._context_factory,\n            connect_timeout=timeout,\n            bind_address=bind_address,\n            pool=self._pool,\n        )", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Provides an appropriate HTTP/2 agent for a given request, handling proxy configurations and connection settings transparently. It supports direct or proxy connections but does not yet support HTTPS tunneling via HTTP/2."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "download_request", "line_number": 100, "body": "def download_request(self, request: Request, spider: Spider) -> Deferred:\n        from twisted.internet import reactor\n        timeout = request.meta.get('download_timeout') or self._connect_timeout\n        agent = self._get_agent(request, timeout)\n\n        start_time = time()\n        d = agent.request(request, spider)\n        d.addCallback(self._cb_latency, request, start_time)\n\n        timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n        return d", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Provides an asynchronous HTTP request mechanism with timeout handling, enabling efficient and cancellable downloads within Scrapy spiders. It integrates latency tracking and robust timeout cancellation for managed network operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_cb_latency", "line_number": 114, "body": "def _cb_latency(response: Response, request: Request, start_time: float) -> Response:\n        request.meta['download_latency'] = time() - start_time\n        return response", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Internal callback method that records and attaches the download latency of a Scrapy HTTP request to its metadata for performance tracking and monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http2.py", "function": "_cb_timeout", "line_number": 119, "body": "def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl: DelayedCall) -> Response:\n        if timeout_cl.active():\n            timeout_cl.cancel()\n            return response\n\n        url = urldefrag(request.url)[0]\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")", "is_method": true, "class_name": "ScrapyH2Agent", "function_description": "Internal callback used to handle response timeouts, raising an error if the response takes longer than the specified timeout duration. It ensures timely processing by canceling the timeout if the response arrives on time."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/s3.py", "function": "from_crawler", "line_number": 51, "body": "def from_crawler(cls, crawler, **kwargs):\n        return cls(crawler.settings, crawler=crawler, **kwargs)", "is_method": true, "class_name": "S3DownloadHandler", "function_description": "Factory method that creates an instance of S3DownloadHandler using settings and context provided by a crawler for integration within a crawl process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/s3.py", "function": "download_request", "line_number": 54, "body": "def download_request(self, request, spider):\n        p = urlparse_cached(request)\n        scheme = 'https' if request.meta.get('is_secure') else 'http'\n        bucket = p.hostname\n        path = p.path + '?' + p.query if p.query else p.path\n        url = f'{scheme}://{bucket}.s3.amazonaws.com{path}'\n        if self.anon:\n            request = request.replace(url=url)\n        elif self._signer is not None:\n            import botocore.awsrequest\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url=f'{scheme}://s3.amazonaws.com/{bucket}{path}',\n                headers=request.headers.to_unicode_dict(),\n                data=request.body)\n            self._signer.add_auth(awsrequest)\n            request = request.replace(\n                url=url, headers=awsrequest.headers.items())\n        else:\n            signed_headers = self.conn.make_request(\n                method=request.method,\n                bucket=bucket,\n                key=unquote(p.path),\n                query_args=unquote(p.query),\n                headers=request.headers,\n                data=request.body,\n            )\n            request = request.replace(url=url, headers=signed_headers)\n        return self._download_http(request, spider)", "is_method": true, "class_name": "S3DownloadHandler", "function_description": "Handles and prepares S3 download requests by constructing the correct URL and applying authentication (anonymous, signed, or via connection), then initiates the HTTP download process. It enables secure and flexible retrieval of S3 resources within a crawling context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "from_crawler", "line_number": 17, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings, crawler)", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "Class method that initializes an HTTP10DownloadHandler instance using settings and context from a crawler, facilitating integration with the crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "download_request", "line_number": 20, "body": "def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "Core method of HTTP10DownloadHandler that initiates an HTTP download for a given request and spider, returning a deferred object representing the asynchronous download operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/downloader/handlers/http10.py", "function": "_connect", "line_number": 26, "body": "def _connect(self, factory):\n        from twisted.internet import reactor\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b'https':\n            client_context_factory = create_instance(\n                objcls=self.ClientContextFactory,\n                settings=self._settings,\n                crawler=self._crawler,\n            )\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        else:\n            return reactor.connectTCP(host, port, factory)", "is_method": true, "class_name": "HTTP10DownloadHandler", "function_description": "Core method of HTTP10DownloadHandler establishing a network connection via TCP or SSL depending on the URL scheme, facilitating secure or plain HTTP downloads within an asynchronous event-driven framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 44, "body": "def __str__(self) -> str:\n        return (f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\")", "is_method": true, "class_name": "InvalidNegotiatedProtocol", "function_description": "Returns a string representation indicating the expected protocol name versus the actually negotiated protocol, useful for error reporting in protocol negotiation scenarios."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 57, "body": "def __str__(self) -> str:\n        return f'Received GOAWAY frame from {self.remote_ip_address!r}'", "is_method": true, "class_name": "RemoteTerminatedConnection", "function_description": "Returns a string representation indicating that a GOAWAY frame was received from a specific remote IP address, useful for logging or debugging connection termination events."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "__str__", "line_number": 65, "body": "def __str__(self) -> str:\n        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"", "is_method": true, "class_name": "MethodNotAllowed405", "function_description": "Provides a custom string representation of the MethodNotAllowed405 error, including the remote IP address involved in the HTTP 405 response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "h2_connected", "line_number": 136, "body": "def h2_connected(self) -> bool:\n        \"\"\"Boolean to keep track of the connection status.\n        This is used while initiating pending streams to make sure\n        that we initiate stream only during active HTTP/2 Connection\n        \"\"\"\n        return bool(self.transport.connected) and self.metadata['settings_acknowledged']", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Method of H2ClientProtocol that indicates whether an active HTTP/2 connection exists and settings have been acknowledged, ensuring streams are initiated only when the connection is fully established."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "allowed_max_concurrent_streams", "line_number": 144, "body": "def allowed_max_concurrent_streams(self) -> int:\n        \"\"\"We keep total two streams for client (sending data) and\n        server side (receiving data) for a single request. To be safe\n        we choose the minimum. Since this value can change in event\n        RemoteSettingsChanged we make variable a property.\n        \"\"\"\n        return min(\n            self.conn.local_settings.max_concurrent_streams,\n            self.conn.remote_settings.max_concurrent_streams\n        )", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Provides the maximum number of concurrent streams allowed by the client-server connection, ensuring safe limits based on both local and remote settings. Useful for managing stream concurrency in HTTP/2 communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_send_pending_requests", "line_number": 155, "body": "def _send_pending_requests(self) -> None:\n        \"\"\"Initiate all pending requests from the deque following FIFO\n        We make sure that at any time {allowed_max_concurrent_streams}\n        streams are active.\n        \"\"\"\n        while (\n            self._pending_request_stream_pool\n            and self.metadata['active_streams'] < self.allowed_max_concurrent_streams\n            and self.h2_connected\n        ):\n            self.metadata['active_streams'] += 1\n            stream = self._pending_request_stream_pool.popleft()\n            stream.initiate_request()\n            self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Manages and initiates pending HTTP/2 requests up to the allowed concurrency limit, ensuring active streams do not exceed the maximum while the connection is established."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "pop_stream", "line_number": 170, "body": "def pop_stream(self, stream_id: int) -> Stream:\n        \"\"\"Perform cleanup when a stream is closed\n        \"\"\"\n        stream = self.streams.pop(stream_id)\n        self.metadata['active_streams'] -= 1\n        self._send_pending_requests()\n        return stream", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Cleans up and removes a closed stream by ID from active streams, updates metadata, triggers pending request processing, and returns the removed stream object for further handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_new_stream", "line_number": 178, "body": "def _new_stream(self, request: Request, spider: Spider) -> Stream:\n        \"\"\"Instantiates a new Stream object\n        \"\"\"\n        stream = Stream(\n            stream_id=next(self._stream_id_generator),\n            request=request,\n            protocol=self,\n            download_maxsize=getattr(spider, 'download_maxsize', self.metadata['default_download_maxsize']),\n            download_warnsize=getattr(spider, 'download_warnsize', self.metadata['default_download_warnsize']),\n        )\n        self.streams[stream.stream_id] = stream\n        return stream", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Creates and registers a new Stream object for handling an HTTP/2 request, configuring it with download size limits from the spider or defaults. This method manages stream lifecycle within the protocol for network communication tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_write_to_transport", "line_number": 191, "body": "def _write_to_transport(self) -> None:\n        \"\"\" Write data to the underlying transport connection\n        from the HTTP2 connection instance if any\n        \"\"\"\n        # Reset the idle timeout as connection is still actively sending data\n        self.resetTimeout()\n\n        data = self.conn.data_to_send()\n        self.transport.write(data)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Internal method of H2ClientProtocol that sends any pending HTTP/2 data through the transport layer, ensuring the connection remains active by resetting its idle timeout."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "request", "line_number": 201, "body": "def request(self, request: Request, spider: Spider) -> Deferred:\n        if not isinstance(request, Request):\n            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n\n        stream = self._new_stream(request, spider)\n        d = stream.get_response()\n\n        # Add the stream to the request pool\n        self._pending_request_stream_pool.append(stream)\n\n        # If we receive a request when connection is idle\n        # We need to initiate pending requests\n        self._send_pending_requests()\n        return d", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Core method of H2ClientProtocol that initiates an HTTP/2 request, manages its lifecycle within a request pool, and returns a Deferred representing the asynchronous response retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connectionMade", "line_number": 216, "body": "def connectionMade(self) -> None:\n        \"\"\"Called by Twisted when the connection is established. We can start\n        sending some data now: we should open with the connection preamble.\n        \"\"\"\n        # Initialize the timeout\n        self.setTimeout(self.IDLE_TIMEOUT)\n\n        destination = self.transport.getPeer()\n        self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n\n        # Initiate H2 Connection\n        self.conn.initiate_connection()\n        self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Initializes and prepares the connection upon establishment by setting a timeout, recording the peer's IP address, and starting the HTTP/2 protocol handshake for data transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_lose_connection_with_error", "line_number": 230, "body": "def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n        \"\"\"Helper function to lose the connection with the error sent as a\n        reason\"\"\"\n        self._conn_lost_errors += errors\n        self.transport.loseConnection()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Internal helper of H2ClientProtocol that records connection errors and then terminates the transport connection to signal the loss."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "handshakeCompleted", "line_number": 236, "body": "def handshakeCompleted(self) -> None:\n        \"\"\"\n        Close the connection if it's not made via the expected protocol\n        \"\"\"\n        if self.transport.negotiatedProtocol is not None and self.transport.negotiatedProtocol != PROTOCOL_NAME:\n            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n            self._lose_connection_with_error([InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Method of H2ClientProtocol that closes the connection if the negotiated protocol does not match the expected protocol, ensuring protocol compliance during the handshake phase."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_check_received_data", "line_number": 244, "body": "def _check_received_data(self, data: bytes) -> None:\n        \"\"\"Checks for edge cases where the connection to remote fails\n        without raising an appropriate H2Error\n\n        Arguments:\n            data -- Data received from the remote\n        \"\"\"\n        if data.startswith(b'HTTP/2.0 405 Method Not Allowed'):\n            raise MethodNotAllowed405(self.metadata['ip_address'])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Internal method of H2ClientProtocol that validates received data to detect specific remote connection failures and raises an error if an unsupported HTTP/2 method response is encountered."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "dataReceived", "line_number": 254, "body": "def dataReceived(self, data: bytes) -> None:\n        # Reset the idle timeout as connection is still actively receiving data\n        self.resetTimeout()\n\n        try:\n            self._check_received_data(data)\n            events = self.conn.receive_data(data)\n            self._handle_events(events)\n        except H2Error as e:\n            if isinstance(e, FrameTooLargeError):\n                # hyper-h2 does not drop the connection in this scenario, we\n                # need to abort the connection manually.\n                self._conn_lost_errors += [e]\n                self.transport.abortConnection()\n                return\n\n            # Save this error as ultimately the connection will be dropped\n            # internally by hyper-h2. Saved error will be passed to all the streams\n            # closed with the connection.\n            self._lose_connection_with_error([e])\n        finally:\n            self._write_to_transport()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Processes incoming data on an HTTP/2 connection, handling protocol events, errors, and connection state updates to maintain communication integrity. It ensures timely connection management by resetting timeouts and writing responses as needed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "timeoutConnection", "line_number": 277, "body": "def timeoutConnection(self) -> None:\n        \"\"\"Called when the connection times out.\n        We lose the connection with TimeoutError\"\"\"\n\n        # Check whether there are open streams. If there are, we're going to\n        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n        # NO_ERROR.\n        if (\n            self.conn.open_outbound_streams > 0\n            or self.conn.open_inbound_streams > 0\n            or self.metadata['active_streams'] > 0\n        ):\n            error_code = ErrorCodes.PROTOCOL_ERROR\n        else:\n            error_code = ErrorCodes.NO_ERROR\n        self.conn.close_connection(error_code=error_code)\n        self._write_to_transport()\n\n        self._lose_connection_with_error([\n            TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")\n        ])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles connection timeout by closing the HTTP/2 connection with an appropriate error code based on active streams, then triggers connection loss with a timeout error notification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connectionLost", "line_number": 299, "body": "def connectionLost(self, reason: Failure = connectionDone) -> None:\n        \"\"\"Called by Twisted when the transport connection is lost.\n        No need to write anything to transport here.\n        \"\"\"\n        # Cancel the timeout if not done yet\n        self.setTimeout(None)\n\n        # Notify the connection pool instance such that no new requests are\n        # sent over current connection\n        if not reason.check(connectionDone):\n            self._conn_lost_errors.append(reason)\n\n        self._conn_lost_deferred.callback(self._conn_lost_errors)\n\n        for stream in self.streams.values():\n            if stream.metadata['request_sent']:\n                close_reason = StreamCloseReason.CONNECTION_LOST\n            else:\n                close_reason = StreamCloseReason.INACTIVE\n            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n\n        self.metadata['active_streams'] -= len(self.streams)\n        self.streams.clear()\n        self._pending_request_stream_pool.clear()\n        self.conn.close_connection()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles cleanup and notifies relevant components when the network connection is lost, ensuring active streams are closed and no new requests are sent on this H2ClientProtocol connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "_handle_events", "line_number": 325, "body": "def _handle_events(self, events: List[Event]) -> None:\n        \"\"\"Private method which acts as a bridge between the events\n        received from the HTTP/2 data and IH2EventsHandler\n\n        Arguments:\n            events -- A list of events that the remote peer triggered by sending data\n        \"\"\"\n        for event in events:\n            if isinstance(event, ConnectionTerminated):\n                self.connection_terminated(event)\n            elif isinstance(event, DataReceived):\n                self.data_received(event)\n            elif isinstance(event, ResponseReceived):\n                self.response_received(event)\n            elif isinstance(event, StreamEnded):\n                self.stream_ended(event)\n            elif isinstance(event, StreamReset):\n                self.stream_reset(event)\n            elif isinstance(event, WindowUpdated):\n                self.window_updated(event)\n            elif isinstance(event, SettingsAcknowledged):\n                self.settings_acknowledged(event)\n            elif isinstance(event, UnknownFrameReceived):\n                logger.warning('Unknown frame received: %s', event.frame)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Private method in H2ClientProtocol that processes a list of HTTP/2 events by dispatching each to its corresponding handler, facilitating event-driven communication between the protocol and its event handlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "connection_terminated", "line_number": 351, "body": "def connection_terminated(self, event: ConnectionTerminated) -> None:\n        self._lose_connection_with_error([\n            RemoteTerminatedConnection(self.metadata['ip_address'], event)\n        ])", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles termination of a connection by triggering appropriate cleanup and error signaling when a remote endpoint ends the connection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "data_received", "line_number": 356, "body": "def data_received(self, event: DataReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_data(event.data, event.flow_controlled_length)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles incoming data events by passing the received data to the corresponding stream within the H2ClientProtocol, enabling proper processing of HTTP/2 stream data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "response_received", "line_number": 364, "body": "def response_received(self, event: ResponseReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_headers(event.headers)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles incoming response events by routing headers to the corresponding stream, enabling proper processing of server responses in the HTTP/2 client protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "settings_acknowledged", "line_number": 372, "body": "def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n        self.metadata['settings_acknowledged'] = True\n\n        # Send off all the pending requests as now we have\n        # established a proper HTTP/2 connection\n        self._send_pending_requests()\n\n        # Update certificate when our HTTP/2 connection is established\n        self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Marks the HTTP/2 connection as established by acknowledging settings, then sends pending requests and updates the connection's certificate metadata accordingly. This method ensures readiness for communication after successful protocol setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "stream_ended", "line_number": 382, "body": "def stream_ended(self, event: StreamEnded) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.ENDED, from_protocol=True)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles the termination of a stream by removing it from tracking and closing it properly, ensuring resources are released when a stream ends in the protocol context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "stream_reset", "line_number": 390, "body": "def stream_reset(self, event: StreamReset) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.RESET, from_protocol=True)", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles a stream reset event by closing the associated stream if it exists, ensuring proper cleanup within the H2ClientProtocol. It manages server-initiated stream resets gracefully without raising errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "window_updated", "line_number": 398, "body": "def window_updated(self, event: WindowUpdated) -> None:\n        if event.stream_id != 0:\n            self.streams[event.stream_id].receive_window_update()\n        else:\n            # Send leftover data for all the streams\n            for stream in self.streams.values():\n                stream.receive_window_update()", "is_method": true, "class_name": "H2ClientProtocol", "function_description": "Handles window update events by notifying the appropriate stream(s) to adjust their flow control windows, ensuring proper data transmission management in the HTTP/2 client protocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "buildProtocol", "line_number": 414, "body": "def buildProtocol(self, addr) -> H2ClientProtocol:\n        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)", "is_method": true, "class_name": "H2ClientFactory", "function_description": "Creates and returns a new H2ClientProtocol instance configured with the factory's URI, settings, and connection lost handler. This method facilitates establishing HTTP/2 client protocol connections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/protocol.py", "function": "acceptableProtocols", "line_number": 417, "body": "def acceptableProtocols(self) -> List[bytes]:\n        return [PROTOCOL_NAME]", "is_method": true, "class_name": "H2ClientFactory", "function_description": "Returns a list of protocols that the H2ClientFactory supports for establishing connections. This helps other components identify compatible communication protocols."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 34, "body": "def __str__(self) -> str:\n        return f'InactiveStreamClosed: Connection was closed without sending the request {self.request!r}'", "is_method": true, "class_name": "InactiveStreamClosed", "function_description": "String representation method of the InactiveStreamClosed class that describes the connection closure event when no request was sent. It helps in clear logging or error messages for debugging stream issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 45, "body": "def __str__(self) -> str:\n        return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'", "is_method": true, "class_name": "InvalidHostname", "function_description": "Returns a formatted string describing the InvalidHostname error, including the expected hostname or netloc and the actual request details."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "__str__", "line_number": 154, "body": "def __str__(self) -> str:\n        return f'Stream(id={self.stream_id!r})'", "is_method": true, "class_name": "Stream", "function_description": "Returns a string representation of the Stream object showing its identifier, useful for readable display or debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_log_warnsize", "line_number": 160, "body": "def _log_warnsize(self) -> bool:\n        \"\"\"Checks if we have received data which exceeds the download warnsize\n        and whether we have not already logged about it.\n\n        Returns:\n            True if both the above conditions hold true\n            False if any of the conditions is false\n        \"\"\"\n        content_length_header = int(self._response['headers'].get(b'Content-Length', -1))\n        return (\n            self._download_warnsize\n            and (\n                self._response['flow_controlled_size'] > self._download_warnsize\n                or content_length_header > self._download_warnsize\n            )\n            and not self.metadata['reached_warnsize']\n        )", "is_method": true, "class_name": "Stream", "function_description": "Utility method of the Stream class that determines if received data exceeds a predefined warning size and if a warning about this has not been logged yet. It helps prevent repeated warnings for large downloads."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "get_response", "line_number": 178, "body": "def get_response(self) -> Deferred:\n        \"\"\"Simply return a Deferred which fires when response\n        from the asynchronous request is available\n        \"\"\"\n        return self._deferred_response", "is_method": true, "class_name": "Stream", "function_description": "Returns a Deferred object that signals when the asynchronous response for this Stream instance becomes available, enabling asynchronous handling of the response data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "check_request_url", "line_number": 184, "body": "def check_request_url(self) -> bool:\n        # Make sure that we are sending the request to the correct URL\n        url = urlparse(self._request.url)\n        return (\n            url.netloc == str(self._protocol.metadata['uri'].host, 'utf-8')\n            or url.netloc == str(self._protocol.metadata['uri'].netloc, 'utf-8')\n            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n        )", "is_method": true, "class_name": "Stream", "function_description": "Utility method of the Stream class that verifies if the request URL matches the expected host or IP address and port, ensuring requests are sent to the correct endpoint."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_get_request_headers", "line_number": 193, "body": "def _get_request_headers(self) -> List[Tuple[str, str]]:\n        url = urlparse(self._request.url)\n\n        path = url.path\n        if url.query:\n            path += '?' + url.query\n\n        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n        # MUST include a value of '/'. The exception to this rule is an\n        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n        # a path component; these MUST include a \":path\" pseudo-header field\n        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n        if not path:\n            path = '*' if self._request.method == 'OPTIONS' else '/'\n\n        # Make sure pseudo-headers comes before all the other headers\n        headers = [\n            (':method', self._request.method),\n            (':authority', url.netloc),\n        ]\n\n        # The \":scheme\" and \":path\" pseudo-header fields MUST\n        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n        if self._request.method != 'CONNECT':\n            headers += [\n                (':scheme', self._protocol.metadata['uri'].scheme),\n                (':path', path),\n            ]\n\n        content_length = str(len(self._request.body))\n        headers.append(('Content-Length', content_length))\n\n        content_length_name = self._request.headers.normkey(b'Content-Length')\n        for name, values in self._request.headers.items():\n            for value in values:\n                value = str(value, 'utf-8')\n                if name == content_length_name:\n                    if value != content_length:\n                        logger.warning(\n                            'Ignoring bad Content-Length header %r of request %r, '\n                            'sending %r instead',\n                            value,\n                            self._request,\n                            content_length,\n                        )\n                    continue\n                headers.append((str(name, 'utf-8'), value))\n\n        return headers", "is_method": true, "class_name": "Stream", "function_description": "Generates HTTP/2-compliant request headers from the Stream's request object, ensuring correct pseudo-header ordering and consistent Content-Length. This enables proper formatting for HTTP/2 request transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "initiate_request", "line_number": 244, "body": "def initiate_request(self) -> None:\n        if self.check_request_url():\n            headers = self._get_request_headers()\n            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n            self.metadata['request_sent'] = True\n            self.send_data()\n        else:\n            # Close this stream calling the response errback\n            # Note that we have not sent any headers\n            self.close(StreamCloseReason.INVALID_HOSTNAME)", "is_method": true, "class_name": "Stream", "function_description": "Initiates a network request by sending headers if the destination URL is valid; otherwise, it closes the stream due to an invalid hostname. This enables controlled request handling within the Stream class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "send_data", "line_number": 255, "body": "def send_data(self) -> None:\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n         data as part of the request.\n\n         If the content length is 0 initially then we end the stream immediately and\n         wait for response data.\n\n         Warning: Only call this method when stream not closed from client side\n            and has initiated request already by sending HEADER frame. If not then\n            stream will raise ProtocolError (raise by h2 state machine).\n         \"\"\"\n        if self.metadata['stream_closed_local']:\n            raise StreamClosedError(self.stream_id)\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._protocol.conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send_size = min(window_size, self.metadata['remaining_content_length'])\n\n        # We now need to send a number of data frames.\n        while bytes_to_send_size > 0:\n            chunk_size = min(bytes_to_send_size, max_frame_size)\n\n            data_chunk_start_id = self.metadata['request_content_length'] - self.metadata['remaining_content_length']\n            data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]\n\n            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n\n            bytes_to_send_size = bytes_to_send_size - chunk_size\n            self.metadata['remaining_content_length'] = self.metadata['remaining_content_length'] - chunk_size\n\n        self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])\n\n        # End the stream if no more data needs to be send\n        if self.metadata['remaining_content_length'] == 0:\n            self._protocol.conn.end_stream(self.stream_id)", "is_method": true, "class_name": "Stream", "function_description": "Utility method in the Stream class that sends request body data over the network respecting flow control and frame size limits, ending the stream when all data has been transmitted. It is crucial for managing data transfer after headers are sent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_window_update", "line_number": 300, "body": "def receive_window_update(self) -> None:\n        \"\"\"Flow control window size was changed.\n        Send data that earlier could not be sent as we were\n        blocked behind the flow control.\n        \"\"\"\n        if (\n            self.metadata['remaining_content_length']\n            and not self.metadata['stream_closed_server']\n            and self.metadata['request_sent']\n        ):\n            self.send_data()", "is_method": true, "class_name": "Stream", "function_description": "Handles flow control updates by sending previously blocked data when window size increases, ensuring smooth data transmission in the Stream class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_data", "line_number": 312, "body": "def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n        self._response['body'].write(data)\n        self._response['flow_controlled_size'] += flow_controlled_length\n\n        # We check maxsize here in case the Content-Length header was not received\n        if self._download_maxsize and self._response['flow_controlled_size'] > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata['reached_warnsize'] = True\n            warning_msg = (\n                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n                f'warn size ({self._download_warnsize}) in request {self._request}'\n            )\n            logger.warning(warning_msg)\n\n        # Acknowledge the data received\n        self._protocol.conn.acknowledge_received_data(\n            self._response['flow_controlled_size'],\n            self.stream_id\n        )", "is_method": true, "class_name": "Stream", "function_description": "Method of the Stream class that processes incoming data chunks by appending to the response body, enforcing download size limits, logging size warnings, and managing flow control acknowledgments for data reception."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "receive_headers", "line_number": 335, "body": "def receive_headers(self, headers: List[HeaderTuple]) -> None:\n        for name, value in headers:\n            self._response['headers'][name] = value\n\n        # Check if we exceed the allowed max data size which can be received\n        expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n        if self._download_maxsize and expected_size > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata['reached_warnsize'] = True\n            warning_msg = (\n                f'Expected response size ({expected_size}) larger than '\n                f'download warn size ({self._download_warnsize}) in request {self._request}'\n            )\n            logger.warning(warning_msg)", "is_method": true, "class_name": "Stream", "function_description": "Method of the Stream class that processes HTTP headers, updating response metadata and enforcing size limits to prevent excessive data downloads. It also logs warnings if the expected size exceeds a warning threshold."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "reset_stream", "line_number": 353, "body": "def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n        \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n        if self.metadata['stream_closed_local']:\n            raise StreamClosedError(self.stream_id)\n\n        # Clear buffer earlier to avoid keeping data in memory for a long time\n        self._response['body'].truncate(0)\n\n        self.metadata['stream_closed_local'] = True\n        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n        self.close(reason)", "is_method": true, "class_name": "Stream", "function_description": "Closes the stream locally by sending a reset signal to the remote peer, clearing buffered data and marking the stream as closed to prevent further use. This is useful for forcibly terminating streams in protocols handling bidirectional communication."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "close", "line_number": 365, "body": "def close(\n        self,\n        reason: StreamCloseReason,\n        errors: Optional[List[BaseException]] = None,\n        from_protocol: bool = False,\n    ) -> None:\n        \"\"\"Based on the reason sent we will handle each case.\n        \"\"\"\n        if self.metadata['stream_closed_server']:\n            raise StreamClosedError(self.stream_id)\n\n        if not isinstance(reason, StreamCloseReason):\n            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')\n\n        # Have default value of errors as an empty list as\n        # some cases can add a list of exceptions\n        errors = errors or []\n\n        if not from_protocol:\n            self._protocol.pop_stream(self.stream_id)\n\n        self.metadata['stream_closed_server'] = True\n\n        # We do not check for Content-Length or Transfer-Encoding in response headers\n        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n\n        # NOTE: Order of handling the events is important here\n        # As we immediately cancel the request when maxsize is exceeded while\n        # receiving DATA_FRAME's when we have received the headers (not\n        # having Content-Length)\n        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n            expected_size = int(self._response['headers'].get(\n                b'Content-Length',\n                self._response['flow_controlled_size'])\n            )\n            error_msg = (\n                f'Cancelling download of {self._request.url}: received response '\n                f'size ({expected_size}) larger than download max size ({self._download_maxsize})'\n            )\n            logger.error(error_msg)\n            self._deferred_response.errback(CancelledError(error_msg))\n\n        elif reason is StreamCloseReason.ENDED:\n            self._fire_response_deferred()\n\n        # Stream was abruptly ended here\n        elif reason is StreamCloseReason.CANCELLED:\n            # Client has cancelled the request. Remove all the data\n            # received and fire the response deferred with no flags set\n\n            # NOTE: The data is already flushed in Stream.reset_stream() called\n            # immediately when the stream needs to be cancelled\n\n            # There maybe no :status in headers, we make\n            # HTTP Status Code: 499 - Client Closed Request\n            self._response['headers'][':status'] = '499'\n            self._fire_response_deferred()\n\n        elif reason is StreamCloseReason.RESET:\n            self._deferred_response.errback(ResponseFailed([\n                Failure(\n                    f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n                    ProtocolError\n                )\n            ]))\n\n        elif reason is StreamCloseReason.CONNECTION_LOST:\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INACTIVE:\n            errors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n            self._deferred_response.errback(InvalidHostname(\n                self._request,\n                str(self._protocol.metadata['uri'].host, 'utf-8'),\n                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n            ))", "is_method": true, "class_name": "Stream", "function_description": "Method of the Stream class that closes the stream based on a specific reason, handling various closure scenarios like max size exceeded, cancellation, reset, or connection loss, and triggering appropriate response callbacks or errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_fire_response_deferred", "line_number": 447, "body": "def _fire_response_deferred(self) -> None:\n        \"\"\"Builds response from the self._response dict\n        and fires the response deferred callback with the\n        generated response instance\"\"\"\n\n        body = self._response['body'].getvalue()\n        response_cls = responsetypes.from_args(\n            headers=self._response['headers'],\n            url=self._request.url,\n            body=body,\n        )\n\n        response = response_cls(\n            url=self._request.url,\n            status=int(self._response['headers'][':status']),\n            headers=self._response['headers'],\n            body=body,\n            request=self._request,\n            certificate=self._protocol.metadata['certificate'],\n            ip_address=self._protocol.metadata['ip_address'],\n            protocol='h2',\n        )\n\n        self._deferred_response.callback(response)", "is_method": true, "class_name": "Stream", "function_description": "Constructs a complete response object from internal data and triggers the associated deferred callback with this response, facilitating asynchronous response handling in the Stream class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/stream.py", "function": "_cancel", "line_number": 143, "body": "def _cancel(_) -> None:\n            # Close this stream as gracefully as possible\n            # If the associated request is initiated we reset this stream\n            # else we directly call close() method\n            if self.metadata['request_sent']:\n                self.reset_stream(StreamCloseReason.CANCELLED)\n            else:\n                self.close(StreamCloseReason.CANCELLED)", "is_method": true, "class_name": "Stream", "function_description": "Private method of the Stream class that cancels the stream gracefully by resetting it if a request was sent or closing it otherwise. It manages stream termination to handle cancellation scenarios properly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_connection", "line_number": 31, "body": "def get_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n        if key in self._pending_requests:\n            # Received a request while connecting to remote\n            # Create a deferred which will fire with the H2ClientProtocol\n            # instance\n            d = Deferred()\n            self._pending_requests[key].append(d)\n            return d\n\n        # Check if we already have a connection to the remote\n        conn = self._connections.get(key, None)\n        if conn:\n            # Return this connection instance wrapped inside a deferred\n            return defer.succeed(conn)\n\n        # No connection is established for the given URI\n        return self._new_connection(key, uri, endpoint)", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Provides asynchronous access to an HTTP/2 connection for a given key and endpoint, queuing requests during connection setup and reusing existing connections to optimize resource management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "_new_connection", "line_number": 49, "body": "def _new_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n        self._pending_requests[key] = deque()\n\n        conn_lost_deferred = Deferred()\n        conn_lost_deferred.addCallback(self._remove_connection, key)\n\n        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n        conn_d = endpoint.connect(factory)\n        conn_d.addCallback(self.put_connection, key)\n\n        d = Deferred()\n        self._pending_requests[key].append(d)\n        return d", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Creates a new HTTP/2 connection for a given key and URI, managing connection lifecycle and pending request queues asynchronously. This supports efficient connection reuse and request handling within the H2ConnectionPool."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "put_connection", "line_number": 63, "body": "def put_connection(self, conn: H2ClientProtocol, key: Tuple) -> H2ClientProtocol:\n        self._connections[key] = conn\n\n        # Now as we have established a proper HTTP/2 connection\n        # we fire all the deferred's with the connection instance\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.callback(conn)\n\n        return conn", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Manages HTTP/2 connections by storing a new connection with a key and fulfilling any pending requests waiting for that connection, enabling efficient reuse within the connection pool."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "_remove_connection", "line_number": 75, "body": "def _remove_connection(self, errors: List[BaseException], key: Tuple) -> None:\n        self._connections.pop(key)\n\n        # Call the errback of all the pending requests for this connection\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.errback(errors)", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Internal method of H2ConnectionPool that removes a specific connection and triggers error callbacks for all its pending requests, ensuring proper cleanup and notification of connection-related failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "close_connections", "line_number": 84, "body": "def close_connections(self) -> None:\n        \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n        Returns:\n            Deferred that fires when all connections have been closed\n        \"\"\"\n        for conn in self._connections.values():\n            conn.transport.abortConnection()", "is_method": true, "class_name": "H2ConnectionPool", "function_description": "Closes and aborts all active HTTP/2 connections managed by the pool, ensuring they are terminated and removed for proper resource cleanup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_endpoint", "line_number": 110, "body": "def get_endpoint(self, uri: URI):\n        return self.endpoint_factory.endpointForURI(uri)", "is_method": true, "class_name": "H2Agent", "function_description": "Utility method in H2Agent that returns the service endpoint corresponding to a given URI, enabling interaction with the appropriate resource or API address."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_key", "line_number": 113, "body": "def get_key(self, uri: URI) -> Tuple:\n        \"\"\"\n        Arguments:\n            uri - URI obtained directly from request URL\n        \"\"\"\n        return uri.scheme, uri.host, uri.port", "is_method": true, "class_name": "H2Agent", "function_description": "This method extracts and returns the scheme, host, and port components from a given URI. It provides a standardized way to identify network endpoints within the H2Agent context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "request", "line_number": 120, "body": "def request(self, request: Request, spider: Spider) -> Deferred:\n        uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))\n        try:\n            endpoint = self.get_endpoint(uri)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n\n        key = self.get_key(uri)\n        d = self._pool.get_connection(key, uri, endpoint)\n        d.addCallback(lambda conn: conn.request(request, spider))\n        return d", "is_method": true, "class_name": "H2Agent", "function_description": "Core method of H2Agent that initiates an HTTP/2 request by obtaining or creating a connection and delegating the request execution, providing asynchronous request handling with support for connection pooling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_endpoint", "line_number": 152, "body": "def get_endpoint(self, uri: URI):\n        return self.endpoint_factory.endpointForURI(self._proxy_uri)", "is_method": true, "class_name": "ScrapyProxyH2Agent", "function_description": "Returns the proxy endpoint associated with the agent's configured proxy URI, enabling requests to be routed through the designated proxy server."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/core/http2/agent.py", "function": "get_key", "line_number": 155, "body": "def get_key(self, uri: URI) -> Tuple:\n        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port", "is_method": true, "class_name": "ScrapyProxyH2Agent", "function_description": "Returns a tuple key identifying the HTTP proxy by its host and port, used to distinguish proxy connections instead of direct request URIs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "get_engine_status", "line_number": 7, "body": "def get_engine_status(engine):\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"engine.has_capacity()\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle(engine.spider)\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]\n        except Exception as e:\n            checks += [(test, f\"{type(e).__name__} (exception)\")]\n\n    return checks", "is_method": false, "function_description": "Function that generates a comprehensive status report of various runtime aspects of an engine, aiding in monitoring and debugging its activity and resource utilization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "format_engine_status", "line_number": 37, "body": "def format_engine_status(engine=None):\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += f\"{test:<47} : {result}\\n\"\n    s += \"\\n\"\n\n    return s", "is_method": false, "function_description": "Utility function that formats and returns a readable status report of an execution engine's diagnostic checks, useful for monitoring or debugging engine health."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/engine.py", "function": "print_engine_status", "line_number": 47, "body": "def print_engine_status(engine):\n    print(format_engine_status(engine))", "is_method": false, "function_description": "Prints a formatted status report of the given engine. Useful for quickly displaying engine state information in readable form."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "send_catch_log", "line_number": 23, "body": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = (named.pop('dont_log', _IgnoredException), StopDownload)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender, *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses", "is_method": false, "function_description": "Provides a robust signal-sending mechanism that catches and logs handler errors, returning Failure objects instead of raising exceptions. Useful for safely dispatching signals without interrupting program flow due to handler failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "send_catch_log_deferred", "line_number": 49, "body": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender, *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d", "is_method": false, "function_description": "Function that sends a signal to multiple handlers supporting asynchronous (deferred) responses, returning a deferred that resolves once all handlers complete. It also logs errors from handlers unless explicitly suppressed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "disconnect_all", "line_number": 75, "body": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)", "is_method": false, "function_description": "Utility function that removes all registered handlers for a given signal and sender, facilitating cleanup tasks such as resetting state after tests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/signal.py", "function": "logerror", "line_number": 54, "body": "def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure", "is_method": false, "function_description": "Utility function that logs error details from a failure event unless the failure type matches specified exceptions to ignore, aiding in error tracking during signal handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_fingerprint", "line_number": 23, "body": "def request_fingerprint(\n    request: Request,\n    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n    keep_fragments: bool = False,\n):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (i.e. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    Also, servers usually ignore fragments in urls when handling requests,\n    so they are also ignored by default when calculating the fingerprint.\n    If you want to include them, set the keep_fragments argument to True\n    (for instance when handling requests with a headless browser).\n\n    \"\"\"\n    headers: Optional[Tuple[bytes, ...]] = None\n    if include_headers:\n        headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n    cache = _fingerprint_cache.setdefault(request, {})\n    cache_key = (headers, keep_fragments)\n    if cache_key not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n        fp.update(request.body or b'')\n        if headers:\n            for hdr in headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[cache_key] = fp.hexdigest()\n    return cache[cache_key]", "is_method": false, "function_description": "Generates a unique hash fingerprint for an HTTP request that identifies its target resource, ignoring irrelevant variations like parameter order or session cookies. Useful for caching, deduplication, or request comparison in web scraping or HTTP client contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_authenticate", "line_number": 79, "body": "def request_authenticate(request: Request, username: str, password: str) -> None:\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)", "is_method": false, "function_description": "Function that applies HTTP Basic Authentication to a request by setting its Authorization header using the provided username and password. It enables secure user authentication for HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "request_httprepr", "line_number": 86, "body": "def request_httprepr(request: Request) -> bytes:\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s", "is_method": false, "function_description": "This function generates a raw HTTP byte-string representation of a given request object for reference purposes, helpful for debugging or logging HTTP request details before transmission."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/request.py", "function": "referer_str", "line_number": 103, "body": "def referer_str(request: Request) -> Optional[str]:\n    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n    referrer = request.headers.get('Referer')\n    if referrer is None:\n        return referrer\n    return to_unicode(referrer, errors='replace')", "is_method": false, "function_description": "This function extracts and returns the 'Referer' HTTP header from a request, converting it to a safe Unicode string for logging purposes. It provides a reliable way to capture referral information from HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "_getarg", "line_number": 30, "body": "def _getarg(request, name, default=None, type=str):\n    return type(request.args[name][0]) if name in request.args else default", "is_method": false, "function_description": "Helper function that extracts a named argument from a request's query parameters, applying a specified type conversion and providing a default if the argument is absent. Useful for safely retrieving and casting request parameters in web handlers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "getChild", "line_number": 12, "body": "def getChild(self, name, request):\n        return self", "is_method": true, "class_name": "Root", "function_description": "Returns the current object regardless of input, potentially serving as a placeholder or default child retrieval method in the Root class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/benchserver.py", "function": "render", "line_number": 15, "body": "def render(self, request):\n        total = _getarg(request, b'total', 100, int)\n        show = _getarg(request, b'show', 10, int)\n        nlist = [random.randint(1, total) for _ in range(show)]\n        request.write(b\"<html><head></head><body>\")\n        args = request.args.copy()\n        for nl in nlist:\n            args['n'] = nl\n            argstr = urlencode(args, doseq=True)\n            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n                          .encode('utf8'))\n        request.write(b\"</body></html>\")\n        return b''", "is_method": true, "class_name": "Root", "function_description": "Generates an HTML page with a specified number of random \"follow\" links, each linking to a URL with query parameters based on the request. This supports dynamic creation of navigable links for web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_is_from_any_domain", "line_number": 19, "body": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith(f'.{d}')) for d in domains)", "is_method": false, "function_description": "Function that checks if a given URL belongs to any domain in a specified list, supporting exact and subdomain matches. Useful for domain-based filtering or validation of URLs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_is_from_spider", "line_number": 28, "body": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url, [spider.name] + list(getattr(spider, 'allowed_domains', [])))", "is_method": false, "function_description": "Checks if a URL belongs to a given spider by verifying if it matches the spider's name or allowed domains. Useful for filtering URLs specific to a web crawler's scope."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "url_has_any_extension", "line_number": 33, "body": "def url_has_any_extension(url, extensions):\n    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions", "is_method": false, "function_description": "Function that checks if a URL's path ends with any of the specified file extensions, useful for filtering or validating URLs based on their file types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "parse_url", "line_number": 37, "body": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_unicode(url, encoding))", "is_method": false, "function_description": "Utility function that ensures a URL is parsed into a standardized URL object, gracefully handling already parsed inputs or raw URL strings with optional encoding support."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "escape_ajax", "line_number": 46, "body": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])", "is_method": false, "function_description": "Function that converts AJAX crawlable URLs into a format compatible with Google's AJAX crawling scheme, enabling web crawlers to access dynamic content by replacing hashbang fragments with `_escaped_fragment_` query parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "add_http_if_no_scheme", "line_number": 75, "body": "def add_http_if_no_scheme(url):\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url", "is_method": false, "function_description": "Function that ensures a URL string includes a scheme by adding \"http://\" if none is present, standardizing URLs for consistent handling in web-related operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_posix_path", "line_number": 86, "body": "def _is_posix_path(string):\n    return bool(\n        re.match(\n            r'''\n            ^                   # start with...\n            (\n                \\.              # ...a single dot,\n                (\n                    \\. | [^/\\.]+  # optionally followed by\n                )?                # either a second dot or some characters\n                |\n                ~   # $HOME\n            )?      # optional match of \".\", \"..\" or \".blabla\"\n            /       # at least one \"/\" for a file path,\n            .       # and something after the \"/\"\n            ''',\n            string,\n            flags=re.VERBOSE,\n        )\n    )", "is_method": false, "function_description": "Internal utility function that checks whether a given string matches POSIX-style file path patterns, including relative paths and home directory shorthand. It helps determine if a string represents a valid POSIX path structure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_windows_path", "line_number": 108, "body": "def _is_windows_path(string):\n    return bool(\n        re.match(\n            r'''\n            ^\n            (\n                [a-z]:\\\\\n                | \\\\\\\\\n            )\n            ''',\n            string,\n            flags=re.IGNORECASE | re.VERBOSE,\n        )\n    )", "is_method": false, "function_description": "Determines if a given string represents a Windows-style file path by checking for typical drive letter or UNC path patterns. Useful for functions needing to validate or handle Windows paths specifically."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "_is_filesystem_path", "line_number": 124, "body": "def _is_filesystem_path(string):\n    return _is_posix_path(string) or _is_windows_path(string)", "is_method": false, "function_description": "Utility function that determines if a given string represents a filesystem path in either POSIX or Windows format. It supports file path validation across different operating systems."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "guess_scheme", "line_number": 128, "body": "def guess_scheme(url):\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n    http:// otherwise.\"\"\"\n    if _is_filesystem_path(url):\n        return any_to_uri(url)\n    return add_http_if_no_scheme(url)", "is_method": false, "function_description": "Function that ensures a URL includes a scheme by adding 'file://' for filesystem paths or 'http://' if missing, facilitating consistent URL formatting for downstream processing or network requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/url.py", "function": "strip_url", "line_number": 136, "body": "def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):\n\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):\n        netloc = netloc.split('@')[-1]\n    if strip_default_port and parsed_url.port:\n        if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n                                                    ('https', 443),\n                                                    ('ftp', 21)):\n            netloc = netloc.replace(f':{parsed_url.port}', '')\n    return urlunparse((\n        parsed_url.scheme,\n        netloc,\n        '/' if origin_only else parsed_url.path,\n        '' if origin_only else parsed_url.params,\n        '' if origin_only else parsed_url.query,\n        '' if strip_fragment else parsed_url.fragment\n    ))", "is_method": false, "function_description": "Utility function that cleans and simplifies URLs by removing credentials, default ports, fragments, or entire path/query components to provide a normalized or origin-only URL representation for easier comparison or processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "get_base_url", "line_number": 22, "body": "def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url, response.encoding)\n    return _baseurl_cache[response]", "is_method": false, "function_description": "Utility function that returns the base URL for a given HTTP response, caching results to optimize repeated access within web scraping or crawling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "get_meta_refresh", "line_number": 33, "body": "def get_meta_refresh(\n    response: \"scrapy.http.response.text.TextResponse\",\n    ignore_tags: Optional[Iterable[str]] = ('script', 'noscript'),\n) -> Union[Tuple[None, None], Tuple[float, str]]:\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(\n            text, response.url, response.encoding, ignore_tags=ignore_tags)\n    return _metaref_cache[response]", "is_method": false, "function_description": "Function parses and caches the meta refresh directive from an HTTP response, returning the delay time and target URL. It helps detect and handle automatic page redirects in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "response_status_message", "line_number": 45, "body": "def response_status_message(status: Union[bytes, float, int, str]) -> str:\n    \"\"\"Return status code plus status text descriptive message\n    \"\"\"\n    status_int = int(status)\n    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n    return f'{status_int} {to_unicode(message)}'", "is_method": false, "function_description": "Function that converts an HTTP status code to a human-readable string combining the code and its standard descriptive message. Useful for generating clear status responses in web applications or APIs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "response_httprepr", "line_number": 53, "body": "def response_httprepr(response: Response) -> bytes:\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    values = [\n        b\"HTTP/1.1 \",\n        to_bytes(str(response.status)),\n        b\" \",\n        to_bytes(http.RESPONSES.get(response.status, b'')),\n        b\"\\r\\n\",\n    ]\n    if response.headers:\n        values.extend([response.headers.to_string(), b\"\\r\\n\"])\n    values.extend([b\"\\r\\n\", response.body])\n    return b\"\".join(values)", "is_method": false, "function_description": "This function converts an HTTP response object into its raw byte string representation, including status line, headers, and body, useful for inspecting or logging HTTP responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/response.py", "function": "open_in_browser", "line_number": 71, "body": "def open_in_browser(\n    response: Union[\"scrapy.http.response.html.HtmlResponse\", \"scrapy.http.response.text.TextResponse\"],\n    _openfunc: Callable[[str], Any] = webbrowser.open,\n) -> Any:\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = f'<head><base href=\"{response.url}\">'\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: \"\n                        f\"{response.__class__.__name__}\")\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(f\"file://{fname}\")", "is_method": false, "function_description": "Utility function that opens an HTTP response's content in a local web browser, ensuring external links work by adjusting the HTML base tag when needed. It supports both HTML and plain text responses for convenient content preview."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/httpobj.py", "function": "urlparse_cached", "line_number": 13, "body": "def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]", "is_method": false, "function_description": "Function caches and returns the parsed URL from a request or response object, optimizing repeated URL parsing to improve efficiency in HTTP processing contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "iterate_spider_output", "line_number": 13, "body": "def iterate_spider_output(result):\n    if inspect.isasyncgen(result):\n        d = deferred_from_coro(collect_asyncgen(result))\n        d.addCallback(iterate_spider_output)\n        return d\n    elif inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(result)", "is_method": false, "function_description": "Function providing a unified way to iterate over various asynchronous or synchronous generator outputs, converting them into a consistent iterable form for further processing in asynchronous workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "iter_spider_classes", "line_number": 25, "body": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (i.e. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in vars(module).values():\n        if (\n            inspect.isclass(obj)\n            and issubclass(obj, Spider)\n            and obj.__module__ == module.__name__\n            and getattr(obj, 'name', None)\n        ):\n            yield obj", "is_method": false, "function_description": "Function that produces an iterator over all instantiable spider classes defined within a given module, facilitating dynamic discovery of spider implementations for web crawling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/spider.py", "function": "spidercls_for_request", "line_number": 43, "body": "def spidercls_for_request(spider_loader, request, default_spidercls=None,\n                          log_none=False, log_multiple=False):\n    \"\"\"Return a spider class that handles the given Request.\n\n    This will look for the spiders that can handle the given request (using\n    the spider loader) and return a Spider class if (and only if) there is\n    only one Spider able to handle the Request.\n\n    If multiple spiders (or no spider) are found, it will return the\n    default_spidercls passed. It can optionally log if multiple or no spiders\n    are found.\n    \"\"\"\n    snames = spider_loader.find_by_request(request)\n    if len(snames) == 1:\n        return spider_loader.load(snames[0])\n\n    if len(snames) > 1 and log_multiple:\n        logger.error('More than one spider can handle: %(request)s - %(snames)s',\n                     {'request': request, 'snames': ', '.join(snames)})\n\n    if len(snames) == 0 and log_none:\n        logger.error('Unable to find spider that handles: %(request)s',\n                     {'request': request})\n\n    return default_spidercls", "is_method": false, "function_description": "Utility function that determines the single appropriate spider class to handle a given request, returning a default or logging errors when no or multiple suitable spiders exist. It aids in routing requests to the correct spider for processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/template.py", "function": "render_templatefile", "line_number": 8, "body": "def render_templatefile(path, **kwargs):\n    with open(path, 'rb') as fp:\n        raw = fp.read().decode('utf8')\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n\n    if path.endswith('.tmpl'):\n        os.rename(path, render_path)\n\n    with open(render_path, 'wb') as fp:\n        fp.write(content.encode('utf8'))", "is_method": false, "function_description": "Function that processes a template file by substituting variables, optionally renaming the file to remove a `.tmpl` suffix, and saving the rendered content back, enabling dynamic file content generation from templates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/template.py", "function": "string_camelcase", "line_number": 26, "body": "def string_camelcase(string):\n    \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n\n    >>> string_camelcase('lost-pound')\n    'LostPound'\n\n    >>> string_camelcase('missing_images')\n    'MissingImages'\n\n    \"\"\"\n    return CAMELCASE_INVALID_CHARS.sub('', string.title())", "is_method": false, "function_description": "Function that converts a string into CamelCase format by capitalizing words and removing invalid characters, useful for standardizing identifiers or names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__getitem__", "line_number": 22, "body": "def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Overrides dictionary item access to retrieve values using a case-insensitive key lookup by normalizing the key before retrieval. This enables case-insensitive access to dictionary entries in CaselessDict."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 25, "body": "def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))", "is_method": true, "class_name": "CaselessDict", "function_description": "Overrides item assignment to store keys in a normalized form, ensuring case-insensitive key handling within the dictionary. This enables consistent setting of values regardless of key letter casing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__delitem__", "line_number": 28, "body": "def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Provides a case-insensitive way to delete a key-value pair from the dictionary by normalizing the key before deletion. Enables consistent key management without case-sensitivity issues in CaselessDict instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__contains__", "line_number": 31, "body": "def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))", "is_method": true, "class_name": "CaselessDict", "function_description": "Overrides membership test to check if a key exists in the dictionary regardless of case, enabling case-insensitive key lookup in CaselessDict instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__copy__", "line_number": 35, "body": "def __copy__(self):\n        return self.__class__(self)", "is_method": true, "class_name": "CaselessDict", "function_description": "Creates and returns a shallow copy of the CaselessDict instance, preserving its case-insensitive key behavior. This enables duplication of the dictionary without modifying the original."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "normkey", "line_number": 39, "body": "def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()", "is_method": true, "class_name": "CaselessDict", "function_description": "Normalizes dictionary keys to lowercase, enabling case-insensitive key access throughout the CaselessDict class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "normvalue", "line_number": 43, "body": "def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value", "is_method": true, "class_name": "CaselessDict", "function_description": "Method in CaselessDict that returns values unchanged when setting dictionary entries, potentially serving as a placeholder for value normalization before insertion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "get", "line_number": 47, "body": "def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))", "is_method": true, "class_name": "CaselessDict", "function_description": "Provides a case-insensitive dictionary lookup that normalizes keys and default values, returning the associated value or a specified default if the key is absent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "setdefault", "line_number": 50, "body": "def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))", "is_method": true, "class_name": "CaselessDict", "function_description": "Overrides the dictionary setdefault method to handle keys case-insensitively by normalizing keys and default values, ensuring consistent storage and retrieval in a case-insensitive dictionary context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "update", "line_number": 53, "body": "def update(self, seq):\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super().update(iseq)", "is_method": true, "class_name": "CaselessDict", "function_description": "Core method of CaselessDict that updates the dictionary with key-value pairs, normalizing keys and values to enforce case-insensitive behavior during updates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "fromkeys", "line_number": 59, "body": "def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)", "is_method": true, "class_name": "CaselessDict", "function_description": "Class method of CaselessDict that creates a new CaselessDict instance with specified keys all assigned the same given value. Useful for initializing dictionaries where keys should be handled case-insensitively."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "pop", "line_number": 62, "body": "def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)", "is_method": true, "class_name": "CaselessDict", "function_description": "Overrides the pop method to remove and return a value by a case-insensitive key from the CaselessDict, supporting default returns if the key is absent."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 76, "body": "def __setitem__(self, key, value):\n        if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)\n        super().__setitem__(key, value)", "is_method": true, "class_name": "LocalCache", "function_description": "Method of the LocalCache class that stores a key-value pair while enforcing a size limit by removing the oldest items as needed. It ensures the cache does not exceed its predefined capacity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__setitem__", "line_number": 99, "body": "def __setitem__(self, key, value):\n        try:\n            super().__setitem__(key, value)\n        except TypeError:\n            pass", "is_method": true, "class_name": "LocalWeakReferencedCache", "function_description": "Overrides setting an item in the cache to handle TypeError exceptions silently, ensuring stability when storing keys that may not be compatible. This allows the cache to safely ignore unsupported key types without interrupting execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__getitem__", "line_number": 105, "body": "def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except (TypeError, KeyError):\n            return None", "is_method": true, "class_name": "LocalWeakReferencedCache", "function_description": "Overridden dictionary access method in LocalWeakReferencedCache that returns None instead of raising an error when a key is missing or invalid, enabling safer and more convenient retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/datatypes.py", "function": "__contains__", "line_number": 118, "body": "def __contains__(self, item):\n        return item not in self.seq", "is_method": true, "class_name": "SequenceExclude", "function_description": "This method checks if an item is absent from the sequence held by the SequenceExclude class, effectively reversing the typical containment logic for exclusion purposes. It enables quick checks for non-membership within the sequence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_enable_windows_terminal_processing", "line_number": 12, "body": "def _enable_windows_terminal_processing():\n    # https://stackoverflow.com/a/36760881\n    kernel32 = ctypes.windll.kernel32\n    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))", "is_method": false, "function_description": "Enables advanced processing modes for the Windows terminal to support features like ANSI escape sequences, enhancing console output capabilities on Windows systems."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_tty_supports_color", "line_number": 18, "body": "def _tty_supports_color():\n    if sys.platform != \"win32\":\n        return True\n\n    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n        return True\n\n    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n    # processing is enabled.\n    return _enable_windows_terminal_processing()", "is_method": false, "function_description": "Function that checks if the current terminal supports color output, considering platform type and Windows version to determine ANSI escape code compatibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "_colorize", "line_number": 30, "body": "def _colorize(text, colorize=True):\n    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n        return text\n    try:\n        from pygments import highlight\n    except ImportError:\n        return text\n    else:\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n        return highlight(text, PythonLexer(), TerminalFormatter())", "is_method": false, "function_description": "Utility function that conditionally applies syntax highlighting to Python code when outputting to a compatible terminal, enhancing readability if color support is available and requested."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "pformat", "line_number": 43, "body": "def pformat(obj, *args, **kwargs):\n    return _colorize(pformat_(obj), kwargs.pop('colorize', True))", "is_method": false, "function_description": "Utility function that returns a formatted string representation of an object, optionally applying syntax coloring for improved readability and visual distinction."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/display.py", "function": "pprint", "line_number": 47, "body": "def pprint(obj, *args, **kwargs):\n    print(pformat(obj, *args, **kwargs))", "is_method": false, "function_description": "Utility function that pretty-prints Python objects using formatted string representation for improved readability. It simplifies displaying complex data structures in an organized manner."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ossignal.py", "function": "install_shutdown_handlers", "line_number": 12, "body": "def install_shutdown_handlers(function, override_sigint=True):\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n    SIGINT handler won't be install if there is already a handler in place\n    (e.g.  Pdb)\n    \"\"\"\n    from twisted.internet import reactor\n    reactor._handleSignals()\n    signal.signal(signal.SIGTERM, function)\n    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, 'SIGBREAK'):\n        signal.signal(signal.SIGBREAK, function)", "is_method": false, "function_description": "Utility function that sets a specified function as the handler for common process shutdown signals, optionally preserving existing SIGINT handlers, to enable graceful termination across platforms."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "format_live_refs", "line_number": 34, "body": "def format_live_refs(ignore=NoneType):\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(),\n                             key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s", "is_method": false, "function_description": "Function that generates a formatted summary table of currently tracked live object references by class, including their count and age, optionally ignoring certain classes. Useful for monitoring object lifetimes and debugging memory usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "print_live_refs", "line_number": 49, "body": "def print_live_refs(*a, **kw):\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))", "is_method": false, "function_description": "Utility function that prints information about currently tracked live objects, aiding in debugging memory usage or reference management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "get_oldest", "line_number": 54, "body": "def get_oldest(class_name):\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]", "is_method": false, "function_description": "Function that returns the oldest live object instance for a given class name by inspecting current references. It helps track or manage object lifetimes across different class types dynamically."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "iter_all", "line_number": 63, "body": "def iter_all(class_name):\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()", "is_method": false, "function_description": "Function that returns all live object instances by a given class name, enabling iteration over currently referenced objects of that class in the runtime environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/trackref.py", "function": "__new__", "line_number": 28, "body": "def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj", "is_method": true, "class_name": "object_ref", "function_description": "Specialized constructor for the object_ref class that creates a new instance and tracks its creation time for live reference management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "_parse_headers_and_cookies", "line_number": 37, "body": "def _parse_headers_and_cookies(parsed_args):\n    headers = []\n    cookies = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(':', 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == 'Cookie':\n            for name, morsel in SimpleCookie(val).items():\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(':', 1)\n        headers.append(('Authorization', basic_auth_header(user, password)))\n\n    return headers, cookies", "is_method": false, "function_description": "Parses command-line arguments to extract HTTP headers and cookies, including authorization credentials, returning them in structured formats for HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "curl_to_request_kwargs", "line_number": 57, "body": "def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n                                        cURL options are unknown. Otherwise\n                                        raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != 'curl':\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = f'Unrecognized options: {\", \".join(argv)}'\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = 'http://' + url\n\n    method = parsed_args.method or 'GET'\n\n    result = {'method': method.upper(), 'url': url}\n\n    headers, cookies = _parse_headers_and_cookies(parsed_args)\n\n    if headers:\n        result['headers'] = headers\n    if cookies:\n        result['cookies'] = cookies\n    if parsed_args.data:\n        result['body'] = parsed_args.data\n        if not parsed_args.method:\n            # if the \"data\" is specified but the \"method\" is not specified,\n            # the default method is 'POST'\n            result['method'] = 'POST'\n\n    return result", "is_method": false, "function_description": "Function that converts a cURL command string into a dictionary of keyword arguments suitable for making HTTP requests, parsing method, URL, headers, cookies, and body data for easy integration with request libraries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/curl.py", "function": "error", "line_number": 11, "body": "def error(self, message):\n        error_msg = f'There was an error parsing the curl command: {message}'\n        raise ValueError(error_msg)", "is_method": true, "class_name": "CurlParser", "function_description": "Raises a ValueError with a formatted message indicating an error occurred while parsing a curl command. It provides a standardized way to report parsing errors within CurlParser."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_fail", "line_number": 15, "body": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d", "is_method": false, "function_description": "Function that returns a Deferred which triggers a failure callback after a short delay, allowing the event reactor to process I/O before handling the error. Useful for deferring error handling to the next iteration in asynchronous event loops."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_succeed", "line_number": 28, "body": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d", "is_method": false, "function_description": "Utility function that returns a Deferred which callbacks with the given result after a 100ms delay, allowing the event loop to process other events before invoking the callback."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "defer_result", "line_number": 41, "body": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)", "is_method": false, "function_description": "This function standardizes different result types into a Deferred object, ensuring consistent asynchronous handling of success or failure outcomes in Twisted-based code. It simplifies integrating various results with asynchronous workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "mustbe_deferred", "line_number": 50, "body": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except Exception:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)", "is_method": false, "function_description": "Provides a deferred execution wrapper that calls a function and schedules its callback or errback for the next event loop iteration, facilitating asynchronous task handling with delayed response invocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "parallel", "line_number": 67, "body": "def parallel(iterable, count, callable, *args, **named):\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = task.Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])", "is_method": false, "function_description": "Function that concurrently executes a given callable over items in an iterable with a limit on simultaneous calls, enabling controlled parallel processing for asynchronous tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_chain", "line_number": 78, "body": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d", "is_method": false, "function_description": "Function that creates and executes a deferred pipeline by chaining multiple callback functions sequentially, passing the initial input through each. Useful for asynchronous workflows requiring ordered processing steps."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_chain_both", "line_number": 87, "body": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(\n            callback=cb, errback=eb,\n            callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw,\n        )\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d", "is_method": false, "function_description": "Utility function that creates a Twisted Deferred by chaining paired callback and errback functions, then triggers it with the given input or error to manage asynchronous processing workflows effectively."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "process_parallel", "line_number": 103, "body": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d", "is_method": false, "function_description": "Utility function that concurrently executes multiple callback functions on the same input, returning a Deferred that resolves with the results of all successful callbacks or fails immediately on any error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "iter_errback", "line_number": 113, "body": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)", "is_method": false, "function_description": "Utility function that iterates over a sequence and invokes a specified error-handling callback if any exception occurs during iteration, allowing graceful error management while processing elements."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "deferred_from_coro", "line_number": 127, "body": "def deferred_from_coro(o):\n    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n    if isinstance(o, defer.Deferred):\n        return o\n    if asyncio.isfuture(o) or inspect.isawaitable(o):\n        if not is_asyncio_reactor_installed():\n            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n            return defer.ensureDeferred(o)\n        else:\n            # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n            return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n    return o", "is_method": false, "function_description": "Utility function that converts coroutines or awaitable objects into Twisted Deferreds, enabling interoperability between asyncio and Twisted asynchronous frameworks. It ensures proper wrapping based on the reactor environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "deferred_f_from_coro_f", "line_number": 142, "body": "def deferred_f_from_coro_f(coro_f):\n    \"\"\" Converts a coroutine function into a function that returns a Deferred.\n\n    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n    This is useful for callback chains, as callback functions are called with the previous callback result.\n    \"\"\"\n    @wraps(coro_f)\n    def f(*coro_args, **coro_kwargs):\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n    return f", "is_method": false, "function_description": "Utility function that wraps a coroutine function to return a Deferred object, enabling seamless integration of async coroutines in callback chains or synchronous-style deferred workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/defer.py", "function": "maybeDeferred_coro", "line_number": 154, "body": "def maybeDeferred_coro(f, *args, **kw):\n    \"\"\" Copy of defer.maybeDeferred that also converts coroutines to Deferreds. \"\"\"\n    try:\n        result = f(*args, **kw)\n    except:  # noqa: E722\n        return defer.fail(failure.Failure(captureVars=defer.Deferred.debug))\n\n    if isinstance(result, defer.Deferred):\n        return result\n    elif asyncio.isfuture(result) or inspect.isawaitable(result):\n        return deferred_from_coro(result)\n    elif isinstance(result, failure.Failure):\n        return defer.fail(result)\n    else:\n        return defer.succeed(result)", "is_method": false, "function_description": "Utility function that executes a callable and converts its result\u2014whether a Deferred, coroutine, Future, or exception\u2014into a Twisted Deferred, enabling seamless asynchronous handling across different async types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "xmliter", "line_number": 14, "body": "def xmliter(obj, nodename):\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    nodename_patt = re.escape(nodename)\n\n    DOCUMENT_HEADER_RE = re.compile(r'<\\?xml[^>]+>\\s*', re.S)\n    HEADER_END_RE = re.compile(fr'<\\s*/{nodename_patt}\\s*>', re.S)\n    END_TAG_RE = re.compile(r'<\\s*/([^\\s>]+)\\s*>', re.S)\n    NAMESPACE_RE = re.compile(r'((xmlns[:A-Za-z]*)=[^>\\s]+)', re.S)\n    text = _body_or_str(obj)\n\n    document_header = re.search(DOCUMENT_HEADER_RE, text)\n    document_header = document_header.group().strip() if document_header else ''\n    header_end_idx = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end_idx[1]:].strip() if header_end_idx else ''\n    namespaces = {}\n    if header_end:\n        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n            tag = re.search(fr'<\\s*{tagname}.*?xmlns[:=][^>]*>', text[:header_end_idx[1]], re.S)\n            if tag:\n                namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))\n\n    r = re.compile(fr'<{nodename_patt}[\\s>].*?</{nodename_patt}>', re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = (\n            document_header\n            + match.group().replace(\n                nodename,\n                f'{nodename} {\" \".join(namespaces.values())}',\n                1\n            )\n            + header_end\n        )\n        yield Selector(text=nodetext, type='xml')", "is_method": false, "function_description": "Utility function that iterates over all XML nodes with a given name from various input types, yielding XML selectors for each node to facilitate parsing and processing of XML feeds or documents."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "xmliter_lxml", "line_number": 56, "body": "def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n    from lxml import etree\n    reader = _StreamReader(obj)\n    tag = f'{{{namespace}}}{nodename}' if namespace else nodename\n    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n    selxpath = '//' + (f'{prefix}:{nodename}' if namespace else nodename)\n    for _, node in iterable:\n        nodetext = etree.tostring(node, encoding='unicode')\n        node.clear()\n        xs = Selector(text=nodetext, type='xml')\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]", "is_method": false, "function_description": "Utility function that streams and parses XML elements by tag name (optionally with namespace) from a file-like object, yielding individual elements as XPath-selectable XML nodes for efficient incremental processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "csviter", "line_number": 96, "body": "def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n    \"\"\" Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n\n    def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]\n\n    lines = StringIO(_body_or_str(obj, unicode=True))\n\n    kwargs = {}\n    if delimiter:\n        kwargs[\"delimiter\"] = delimiter\n    if quotechar:\n        kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            row = next(csv_r)\n        except StopIteration:\n            return\n        headers = row_to_unicode(row)\n\n    for row in csv_r:\n        row = row_to_unicode(row)\n        if len(row) != len(headers):\n            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                           \"should be: %(csvheader)d)\",\n                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n                            'csvheader': len(headers)})\n            continue\n        else:\n            yield dict(zip(headers, row))", "is_method": false, "function_description": "Function that parses various CSV input types and yields each row as a dictionary keyed by headers, supporting custom delimiters, quote characters, and encoding to facilitate flexible CSV data processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_body_or_str", "line_number": 145, "body": "def _body_or_str(obj, unicode=True):\n    expected_types = (Response, str, bytes)\n    if not isinstance(obj, expected_types):\n        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n        raise TypeError(\n            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n        )\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        elif isinstance(obj, TextResponse):\n            return obj.text\n        else:\n            return obj.body.decode('utf-8')\n    elif isinstance(obj, str):\n        return obj if unicode else obj.encode('utf-8')\n    else:\n        return obj.decode('utf-8') if unicode else obj", "is_method": false, "function_description": "Utility function that extracts the textual content or raw body from Response, str, or bytes objects, returning it as a unicode string or bytes based on preference. It standardizes content retrieval across these types for downstream processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "read", "line_number": 81, "body": "def read(self, n=65535):\n        self.read = self._read_unicode if self._is_unicode else self._read_string\n        return self.read(n).lstrip()", "is_method": true, "class_name": "_StreamReader", "function_description": "Core method of _StreamReader that reads a specified number of characters or bytes from the stream, automatically handling Unicode or byte strings, and returns the result with leading whitespace removed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_read_string", "line_number": 85, "body": "def _read_string(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e]", "is_method": true, "class_name": "_StreamReader", "function_description": "Private method in _StreamReader that returns a substring of specified length from the current pointer, advancing the pointer accordingly for sequential text reading."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "_read_unicode", "line_number": 90, "body": "def _read_unicode(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e].encode('utf-8')", "is_method": true, "class_name": "_StreamReader", "function_description": "Internal method of _StreamReader that extracts and returns a UTF-8 encoded substring of specified length from the current read position in the text stream."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/iterators.py", "function": "row_to_unicode", "line_number": 114, "body": "def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]", "is_method": false, "function_description": "Converts all elements in a row to Unicode strings using a specified encoding, ensuring consistent text representation for data processing or display tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "build_component_list", "line_number": 15, "body": "def build_component_list(compdict, custom=None, convert=update_classpath):\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n                             'please update your settings')\n\n    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n                                     'convert to the same '\n                                     'object, please update your settings'\n                                     )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(f'Invalid value {value} for component {name}, '\n                                 'please provide a real number or None instead')\n\n    # BEGIN Backward compatibility for old (base, custom) call signature\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    if custom is not None:\n        compdict.update(custom)\n    # END Backward compatibility\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]", "is_method": false, "function_description": "Function that constructs a sorted list of component class paths from a dictionary, ensuring uniqueness and valid priority values. It supports integration with custom components and maintains backward compatibility with older call signatures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "arglist_to_dict", "line_number": 61, "body": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)", "is_method": false, "function_description": "This function converts a list of \"key=value\" strings into a dictionary mapping keys to values. It is useful for parsing argument lists into structured dictionary form for easier data access and manipulation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "closest_scrapy_cfg", "line_number": 68, "body": "def closest_scrapy_cfg(path='.', prevpath=None):\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if path == prevpath:\n        return ''\n    path = os.path.abspath(path)\n    cfgfile = os.path.join(path, 'scrapy.cfg')\n    if os.path.exists(cfgfile):\n        return cfgfile\n    return closest_scrapy_cfg(os.path.dirname(path), path)", "is_method": false, "function_description": "Function that locates the nearest scrapy.cfg configuration file by searching the current directory and ascending parent directories. It helps other functions identify the base Scrapy project configuration location."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "init_env", "line_number": 81, "body": "def init_env(project='default', set_syspath=True):\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option('settings', project):\n        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = os.path.dirname(closest)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)", "is_method": false, "function_description": "Utility function that sets up the environment for running a Scrapy project by configuring settings and optionally adjusting the Python path for module accessibility within the project directory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "get_config", "line_number": 96, "body": "def get_config(use_closest=True):\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg", "is_method": false, "function_description": "Function that retrieves Scrapy configuration files and loads them into a ConfigParser object, enabling access to consolidated configuration settings with optional source prioritization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "get_sources", "line_number": 104, "body": "def get_sources(use_closest=True):\n    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n    sources = [\n        '/etc/scrapy.cfg',\n        r'c:\\scrapy\\scrapy.cfg',\n        xdg_config_home + '/scrapy.cfg',\n        os.path.expanduser('~/.scrapy.cfg'),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources", "is_method": false, "function_description": "Function that generates a list of potential configuration file paths for a Scrapy project, optionally including the closest config file. Useful for locating Scrapy settings across standard and user-specific directories."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "feed_complete_default_values_from_settings", "line_number": 117, "body": "def feed_complete_default_values_from_settings(feed, settings):\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT'))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", dict())\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out", "is_method": false, "function_description": "Function that populates a feed configuration dictionary with default export-related settings, ensuring all necessary parameters are set for consistent feed exporting behavior."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "feed_process_params_from_cli", "line_number": 132, "body": "def feed_process_params_from_cli(settings, output, output_format=None,\n                                 overwrite_output=None):\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats = without_none_values(\n        settings.getwithbase('FEED_EXPORTERS')\n    ).keys()\n\n    def check_valid_format(output_format):\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    if output_format:\n        if len(output) == 1:\n            check_valid_format(output_format)\n            message = (\n                'The -t command line option is deprecated in favor of '\n                'specifying the output format within the output URI. See the '\n                'documentation of the -o and -O options for more information.',\n            )\n            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n            return {output[0]: {'format': output_format}}\n        else:\n            raise UsageError(\n                'The -t command-line option cannot be used if multiple output '\n                'URIs are specified'\n            )\n\n    result = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(':', 1)\n        except ValueError:\n            feed_uri = element\n            feed_format = os.path.splitext(element)[1].replace('.', '')\n        else:\n            if feed_uri == '-':\n                feed_uri = 'stdout:'\n        check_valid_format(feed_format)\n        result[feed_uri] = {'format': feed_format}\n        if overwrite:\n            result[feed_uri]['overwrite'] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict('FEEDS'))\n\n    return result", "is_method": false, "function_description": "Function that processes and validates command-line feed export parameters, returning a dictionary formatted for feed settings with support for output format specification, output overwriting, and error handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_check_components", "line_number": 18, "body": "def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n                             'please update your settings')", "is_method": false, "function_description": "Utility function that validates a list of components by ensuring all elements convert to unique objects, raising an error if any duplicates exist after conversion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_map_keys", "line_number": 23, "body": "def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n                                     'convert to the same '\n                                     'object, please update your settings'\n                                     )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in compdict.items()}", "is_method": false, "function_description": "Transforms keys in a dictionary or BaseSettings instance using a conversion function, ensuring no key conflicts and preserving priority metadata. Useful for normalizing configuration keys while validating against duplicates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "_validate_values", "line_number": 40, "body": "def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(f'Invalid value {value} for component {name}, '\n                                 'please provide a real number or None instead')", "is_method": false, "function_description": "Utility function that checks whether all values in a dictionary are real numbers or None, raising an error if any invalid value is found. It ensures data integrity for components requiring numeric or null values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/conf.py", "function": "check_valid_format", "line_number": 143, "body": "def check_valid_format(output_format):\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )", "is_method": false, "function_description": "Utility function that validates whether a given output format is supported, raising an error with usage instructions if the format is unrecognized."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_ipython_shell", "line_number": 5, "body": "def _embed_ipython_shell(namespace={}, banner=''):\n    \"\"\"Start an IPython Shell\"\"\"\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed\n        from IPython.terminal.ipapp import load_default_config\n    except ImportError:\n        from IPython.frontend.terminal.embed import InteractiveShellEmbed\n        from IPython.frontend.terminal.ipapp import load_default_config\n\n    @wraps(_embed_ipython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        config = load_default_config()\n        # Always use .instace() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config)\n        shell()\n    return wrapper", "is_method": false, "function_description": "Provides a function factory that creates and starts an IPython interactive shell with a customizable namespace and banner, enabling dynamic code execution and exploration within a given environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_bpython_shell", "line_number": 28, "body": "def _embed_bpython_shell(namespace={}, banner=''):\n    \"\"\"Start a bpython shell\"\"\"\n    import bpython\n\n    @wraps(_embed_bpython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        bpython.embed(locals_=namespace, banner=banner)\n    return wrapper", "is_method": false, "function_description": "Utility function that creates and returns a callable to start an interactive bpython shell with a given namespace and optional banner message."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_ptpython_shell", "line_number": 38, "body": "def _embed_ptpython_shell(namespace={}, banner=''):\n    \"\"\"Start a ptpython shell\"\"\"\n    import ptpython.repl\n\n    @wraps(_embed_ptpython_shell)\n    def wrapper(namespace=namespace, banner=''):\n        print(banner)\n        ptpython.repl.embed(locals=namespace)\n    return wrapper", "is_method": false, "function_description": "Returns a function that starts an interactive ptpython shell with a given namespace and banner, allowing users to execute Python code dynamically within that context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "_embed_standard_shell", "line_number": 49, "body": "def _embed_standard_shell(namespace={}, banner=''):\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n    try:  # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter  # noqa: F401\n        readline.parse_and_bind(\"tab:complete\")\n\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)\n    return wrapper", "is_method": false, "function_description": "Provides a callable that launches an interactive Python shell with optional namespace and banner, supporting tab-completion on Unix systems for convenient code exploration and testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "get_shell_embed_func", "line_number": 74, "body": "def get_shell_embed_func(shells=None, known_shells=None):\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None:  # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None:  # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue", "is_method": false, "function_description": "Returns the first available shell embedding function from a prioritized list, enabling dynamic selection of an embeddable shell environment that supports interactive Python execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "start_python_console", "line_number": 92, "body": "def start_python_console(namespace=None, banner='', shells=None):\n    \"\"\"Start Python console bound to the given namespace.\n    Readline support and tab completion will be used on Unix, if available.\n    \"\"\"\n    if namespace is None:\n        namespace = {}\n\n    try:\n        shell = get_shell_embed_func(shells)\n        if shell is not None:\n            shell(namespace=namespace, banner=banner)\n    except SystemExit:  # raised when using exit() in python code.interact\n        pass", "is_method": false, "function_description": "Function that launches an interactive Python console with access to a specified namespace, supporting enhanced features like tab completion and readline where available. It enables users to execute Python code dynamically within a customized environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 15, "body": "def wrapper(namespace=namespace, banner=''):\n        config = load_default_config()\n        # Always use .instace() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config)\n        shell()", "is_method": false, "function_description": "Function that initializes and launches a fresh interactive IPython shell with a customizable namespace and banner, ensuring a clean environment for user interaction and code exploration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 33, "body": "def wrapper(namespace=namespace, banner=''):\n        bpython.embed(locals_=namespace, banner=banner)", "is_method": false, "function_description": "Starts an interactive bpython session using a specified namespace and optional banner message, enabling live code experimentation within that context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 43, "body": "def wrapper(namespace=namespace, banner=''):\n        print(banner)\n        ptpython.repl.embed(locals=namespace)", "is_method": false, "function_description": "Displays an interactive Python REPL session with a customizable banner, using a provided namespace for variable context and evaluation. This enables dynamic code exploration and debugging within a specified environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/console.py", "function": "wrapper", "line_number": 61, "body": "def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)", "is_method": false, "function_description": "This function launches an interactive Python shell with a custom local namespace and optional banner, enabling dynamic code execution and debugging within the specified context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "failure_to_exc_info", "line_number": 18, "body": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())", "is_method": false, "function_description": "Function that extracts standard exception information tuples from Failure instances, facilitating compatibility with Python\u2019s error-handling mechanisms."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "configure_logging", "line_number": 62, "body": "def configure_logging(settings=None, install_root_handler=True):\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver('twisted')\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool('LOG_STDOUT'):\n        sys.stdout = StreamLogger(logging.getLogger('stdout'))\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)", "is_method": false, "function_description": "Function that initializes and configures logging behavior for Scrapy and Twisted frameworks, including warning routing, log level settings, and optional root handler installation, supporting customization via provided settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "install_scrapy_root_handler", "line_number": 105, "body": "def install_scrapy_root_handler(settings):\n    global _scrapy_root_handler\n\n    if (_scrapy_root_handler is not None\n            and _scrapy_root_handler in logging.root.handlers):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)", "is_method": false, "function_description": "This function configures the root logging handler based on given settings, ensuring that Scrapy's logging setup replaces any existing root handler. It enables consistent logging behavior across the application."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "get_scrapy_root_handler", "line_number": 116, "body": "def get_scrapy_root_handler():\n    return _scrapy_root_handler", "is_method": false, "function_description": "Returns the current root handler used by Scrapy framework, allowing other components to access or interact with the main request handling mechanism."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "_get_handler", "line_number": 123, "body": "def _get_handler(settings):\n    \"\"\" Return a log handler object according to settings \"\"\"\n    filename = settings.get('LOG_FILE')\n    if filename:\n        encoding = settings.get('LOG_ENCODING')\n        handler = logging.FileHandler(filename, encoding=encoding)\n    elif settings.getbool('LOG_ENABLED'):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get('LOG_FORMAT'),\n        datefmt=settings.get('LOG_DATEFORMAT')\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get('LOG_LEVEL'))\n    if settings.getbool('LOG_SHORT_NAMES'):\n        handler.addFilter(TopLevelFormatter(['scrapy']))\n    return handler", "is_method": false, "function_description": "Utility function that returns a configured logging handler based on given settings, supporting file, stream, or null handlers with custom formatting and filtering options. It enables flexible logging setup tailored to user preferences."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "log_scrapy_info", "line_number": 145, "body": "def log_scrapy_info(settings):\n    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n    versions = [\n        f\"{name} {version}\"\n        for name, version in scrapy_components_versions()\n        if name != \"Scrapy\"\n    ]\n    logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n    from twisted.internet import reactor\n    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n    from twisted.internet import asyncioreactor\n    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n        logger.debug(\n            \"Using asyncio event loop: %s.%s\",\n            reactor._asyncioEventloop.__module__,\n            reactor._asyncioEventloop.__class__.__name__,\n        )", "is_method": false, "function_description": "Utility function that logs Scrapy framework and component versions along with details about the Twisted reactor and asyncio event loop in use, assisting in debugging and environment inspection during Scrapy execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "logformatter_adapter", "line_number": 197, "body": "def logformatter_adapter(logkws):\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {'level', 'msg', 'args'} <= set(logkws):\n        warnings.warn('Missing keys in LogFormatter method',\n                      ScrapyDeprecationWarning)\n\n    if 'format' in logkws:\n        warnings.warn('`format` key in LogFormatter methods has been '\n                      'deprecated, use `msg` instead',\n                      ScrapyDeprecationWarning)\n\n    level = logkws.get('level', logging.INFO)\n    message = logkws.get('format', logkws.get('msg'))\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = logkws if not logkws.get('args') else logkws['args']\n\n    return (level, message, args)", "is_method": false, "function_description": "Utility function that converts dictionary outputs from LogFormatter methods into positional arguments for logger calls, ensuring compatibility with older logging interfaces and handling deprecation warnings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "filter", "line_number": 39, "body": "def filter(self, record):\n        if any(record.name.startswith(logger + '.') for logger in self.loggers):\n            record.name = record.name.split('.', 1)[0]\n        return True", "is_method": true, "class_name": "TopLevelFormatter", "function_description": "Filters log records by adjusting their logger name prefix if it starts with any specified logger names, enabling consistent naming for log processing or formatting purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "write", "line_number": 176, "body": "def write(self, buf):\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())", "is_method": true, "class_name": "StreamLogger", "function_description": "This method writes each line from an input buffer to a configured logger at a specified log level, enabling stream-like logging of multiline text inputs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "flush", "line_number": 180, "body": "def flush(self):\n        for h in self.logger.handlers:\n            h.flush()", "is_method": true, "class_name": "StreamLogger", "function_description": "Flushes all handlers of the logger to ensure that any buffered log output is written out immediately. This helps maintain up-to-date log records especially in streaming or real-time logging scenarios."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/log.py", "function": "emit", "line_number": 192, "body": "def emit(self, record):\n        sname = f'log_count/{record.levelname}'\n        self.crawler.stats.inc_value(sname)", "is_method": true, "class_name": "LogCounterHandler", "function_description": "Count and record the occurrences of log messages by their severity level, updating the crawler's statistics accordingly for monitoring or analysis purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "test_site", "line_number": 29, "body": "def test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)", "is_method": false, "function_description": "Constructs and returns a test web server site with predefined static resources and redirects to facilitate testing of HTTP responses and redirects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "setUp", "line_number": 8, "body": "def setUp(self):\n        from twisted.internet import reactor\n        super().setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"", "is_method": true, "class_name": "SiteTest", "function_description": "Set up a local test server listening on a random TCP port for the SiteTest class, enabling test cases to interact with it via a base URL."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "tearDown", "line_number": 14, "body": "def tearDown(self):\n        super().tearDown()\n        self.site.stopListening()", "is_method": true, "class_name": "SiteTest", "function_description": "Cleans up test resources by stopping site listening after each test case, ensuring proper shutdown and freeing resources in SiteTest test workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "url", "line_number": 18, "body": "def url(self, path):\n        return urljoin(self.baseurl, path)", "is_method": true, "class_name": "SiteTest", "function_description": "Utility method of the SiteTest class that constructs a full URL by combining the base URL with a given path, facilitating consistent URL generation for site testing or web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testsite.py", "function": "render", "line_number": 23, "body": "def render(self, request):\n        content = util.Redirect.render(self, request)\n        return content.replace(b'http-equiv=\\\"refresh\\\"',\n            b'http-no-equiv=\\\"do-not-refresh-me\\\"')", "is_method": true, "class_name": "NoMetaRefreshRedirect", "function_description": "Replaces HTTP meta refresh directives with a non-refresh attribute in redirect responses to prevent automatic page refresh behavior. This method modifies redirection content to control client-side refresh actions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "deprecated", "line_number": 9, "body": "def deprecated(use_instead=None):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped\n\n    if callable(use_instead):\n        deco = deco(use_instead)\n        use_instead = None\n    return deco", "is_method": false, "function_description": "Utility function providing a decorator to mark other functions as deprecated, issuing a warning on usage and optionally suggesting an alternative function for improved code maintenance and clarity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "defers", "line_number": 30, "body": "def defers(func):\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)\n    return wrapped", "is_method": false, "function_description": "Decorator that ensures a function's result is wrapped in a deferred, enabling consistent asynchronous handling regardless of the original function's return type. Useful for integrating synchronous functions into asynchronous workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "inthread", "line_number": 38, "body": "def inthread(func):\n    \"\"\"Decorator to call a function in a thread and return a deferred with the\n    result\n    \"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return threads.deferToThread(func, *a, **kw)\n    return wrapped", "is_method": false, "function_description": "Decorator that runs a function asynchronously in a separate thread and returns a deferred result, enabling non-blocking execution in concurrent applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "deco", "line_number": 14, "body": "def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped", "is_method": false, "function_description": "Decorator function that marks another function as deprecated by issuing a warning when called, optionally suggesting an alternative function to use instead. It helps in signaling deprecated features during runtime."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/decorators.py", "function": "wrapped", "line_number": 43, "body": "def wrapped(*a, **kw):\n        return threads.deferToThread(func, *a, **kw)", "is_method": false, "function_description": "Wraps a function to run asynchronously in a separate thread, enabling non-blocking execution in concurrent applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/job.py", "function": "job_dir", "line_number": 4, "body": "def job_dir(settings):\n    path = settings['JOBDIR']\n    if path and not os.path.exists(path):\n        os.makedirs(path)\n    return path", "is_method": false, "function_description": "Function that ensures a job directory exists based on configuration settings and returns its path, providing a reliable way for other functions to access or create a workspace directory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ftp.py", "function": "ftp_makedirs_cwd", "line_number": 7, "body": "def ftp_makedirs_cwd(ftp, path, first_call=True):\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)", "is_method": false, "function_description": "Utility function that sets the current directory on an FTP connection, creating all intermediate directories as needed to ensure the specified path exists. It facilitates automated remote directory setup during FTP operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ftp.py", "function": "ftp_store_file", "line_number": 21, "body": "def ftp_store_file(\n        *, path, file, host, port,\n        username, password, use_active_mode=False, overwrite=True):\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        command = 'STOR' if overwrite else 'APPE'\n        ftp.storbinary(f'{command} {filename}', file)\n        file.close()", "is_method": false, "function_description": "Utility function to upload a file to a specified path on an FTP server using provided credentials, supporting active/passive mode and optional overwriting. It manages connection, directory navigation, and file transfer seamlessly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "assert_gcs_environ", "line_number": 16, "body": "def assert_gcs_environ():\n    if 'GCS_PROJECT_ID' not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")", "is_method": false, "function_description": "Function that verifies the presence of the GCS_PROJECT_ID environment variable, raising an exception to skip tests if it's missing, ensuring proper configuration for Google Cloud Storage operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "skip_if_no_boto", "line_number": 21, "body": "def skip_if_no_boto():\n    if not is_botocore_available():\n        raise SkipTest('missing botocore library')", "is_method": false, "function_description": "This function checks for the availability of the botocore library and raises an exception to skip a test if the library is missing. It is useful for conditionally bypassing tests that depend on botocore."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_gcs_content_and_delete", "line_number": 26, "body": "def get_gcs_content_and_delete(bucket, path):\n    from google.cloud import storage\n    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob", "is_method": false, "function_description": "Function that fetches the content and access control list of a Google Cloud Storage object, then deletes the object from the bucket. Useful for retrieving and removing cloud-stored data atomically."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_ftp_content_and_delete", "line_number": 37, "body": "def get_ftp_content_and_delete(\n        path, host, port, username,\n        password, use_active_mode=False):\n    from ftplib import FTP\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data = []\n\n    def buffer_data(data):\n        ftp_data.append(data)\n    ftp.retrbinary(f'RETR {path}', buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return \"\".join(ftp_data)", "is_method": false, "function_description": "Function that connects to an FTP server to retrieve the contents of a specified file and then deletes that file from the server. Useful for automated file processing workflows requiring file read-and-remove operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_crawler", "line_number": 57, "body": "def get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(settings_dict)\n    return runner.create_crawler(spidercls or Spider)", "is_method": false, "function_description": "Provides an unconfigured Scrapy Crawler instance optionally configured with custom settings and a specified spider class, facilitating flexible web crawling setup within scraping projects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_pythonpath", "line_number": 69, "body": "def get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')", "is_method": false, "function_description": "Returns a PYTHONPATH string configured to include the current Scrapy installation path, ensuring spawned processes can locate this Scrapy environment correctly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_testenv", "line_number": 76, "body": "def get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env", "is_method": false, "function_description": "Returns an environment dictionary configured to ensure spawned processes use the current Scrapy installation instead of a system-wide one, facilitating consistent module imports across forked processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "assert_samelines", "line_number": 85, "body": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)", "is_method": false, "function_description": "Utility function that asserts two texts have identical lines regardless of platform-specific line ending differences, supporting consistent multiline text comparison in test cases."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "get_from_asyncio_queue", "line_number": 92, "body": "def get_from_asyncio_queue(value):\n    q = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter", "is_method": false, "function_description": "Creates an asyncio Queue, enqueues the provided value, and returns a coroutine that retrieves this value from the queue. Useful for asynchronous retrieval of a single item in asyncio workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/test.py", "function": "mock_google_cloud_storage", "line_number": 99, "body": "def mock_google_cloud_storage():\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Client, Bucket, Blob\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)", "is_method": false, "function_description": "Creates and returns autospec mocks for Google Cloud Storage's Client, Bucket, and Blob classes with preset return values for use in testing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "listen_tcp", "line_number": 9, "body": "def listen_tcp(portrange, host, factory):\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    from twisted.internet import reactor\n    if len(portrange) > 2:\n        raise ValueError(f\"invalid portrange: {portrange}\")\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if not hasattr(portrange, '__iter__'):\n        return reactor.listenTCP(portrange, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1] + 1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise", "is_method": false, "function_description": "Function providing TCP listener setup that tries binding to ports within a specified range on a given host, simplifying the process of finding an available port for network services using Twisted's reactor."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "install_reactor", "line_number": 53, "body": "def install_reactor(reactor_path, event_loop_path=None):\n    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n    import path. Also installs the asyncio event loop with the specified import\n    path if the asyncio reactor is enabled\"\"\"\n    reactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        with suppress(error.ReactorAlreadyInstalledError):\n            if event_loop_path is not None:\n                event_loop_class = load_object(event_loop_path)\n                event_loop = event_loop_class()\n                asyncio.set_event_loop(event_loop)\n            else:\n                event_loop = asyncio.get_event_loop()\n            asyncioreactor.install(eventloop=event_loop)\n    else:\n        *module, _ = reactor_path.split(\".\")\n        installer_path = module + [\"install\"]\n        installer = load_object(\".\".join(installer_path))\n        with suppress(error.ReactorAlreadyInstalledError):\n            installer()", "is_method": false, "function_description": "Function to install a specified Twisted reactor and optionally configure the asyncio event loop, enabling integration of different event-driven frameworks in asynchronous Python applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "verify_installed_reactor", "line_number": 75, "body": "def verify_installed_reactor(reactor_path):\n    \"\"\"Raises :exc:`Exception` if the installed\n    :mod:`~twisted.internet.reactor` does not match the specified import\n    path.\"\"\"\n    from twisted.internet import reactor\n    reactor_class = load_object(reactor_path)\n    if not isinstance(reactor, reactor_class):\n        msg = (\"The installed reactor \"\n               f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n               f\"match the requested one ({reactor_path})\")\n        raise Exception(msg)", "is_method": false, "function_description": "Function that checks if the currently installed Twisted reactor matches a specified reactor class and raises an exception if they differ, ensuring the correct reactor is in use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "is_asyncio_reactor_installed", "line_number": 88, "body": "def is_asyncio_reactor_installed():\n    from twisted.internet import reactor\n    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)", "is_method": false, "function_description": "Utility function that checks if the Twisted reactor is using the asyncio event loop integration, enabling conditional logic based on the reactor's asynchronous backend."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "schedule", "line_number": 39, "body": "def schedule(self, delay=0):\n        from twisted.internet import reactor\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)", "is_method": true, "class_name": "CallLaterOnce", "function_description": "Schedules the wrapped callable to run once after a specified delay, ensuring it is not scheduled multiple times concurrently. Useful for deferring execution while avoiding duplicate scheduling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "cancel", "line_number": 44, "body": "def cancel(self):\n        if self._call:\n            self._call.cancel()", "is_method": true, "class_name": "CallLaterOnce", "function_description": "Method of the CallLaterOnce class that cancels a scheduled callback if it exists, preventing the deferred execution from occurring."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reactor.py", "function": "__call__", "line_number": 48, "body": "def __call__(self):\n        self._call = None\n        return self._func(*self._a, **self._kw)", "is_method": true, "class_name": "CallLaterOnce", "function_description": "Calls the stored function once with preset arguments and keyword arguments, then clears the reference to prevent subsequent calls. Useful for deferred or one-time execution scenarios."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "attribute", "line_number": 8, "body": "def attribute(obj, oldattr, newattr, version='0.12'):\n    cname = obj.__class__.__name__\n    warnings.warn(\n        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n        ScrapyDeprecationWarning,\n        stacklevel=3)", "is_method": false, "function_description": "Function that issues a deprecation warning indicating that an object's attribute is obsolete and suggests the replacement attribute, helping developers manage attribute updates across versions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "create_deprecated_class", "line_number": 17, "body": "def create_deprecated_class(\n    name,\n    new_class,\n    clsdict=None,\n    warn_category=ScrapyDeprecationWarning,\n    warn_once=True,\n    old_class_path=None,\n    new_class_path=None,\n    subclass_warn_message=\"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\"\n):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super().__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(f\"Error detecting parent module: {e!r}\")\n\n    return deprecated_cls", "is_method": false, "function_description": "Utility function that creates a deprecated class wrapper to warn when the old class is inherited or instantiated, facilitating safe renaming of base classes while preserving subclass and instance checks for compatibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "_clspath", "line_number": 123, "body": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return f'{cls.__module__}.{cls.__name__}'", "is_method": false, "function_description": "Utility function that returns the full class path as a string, using the module and class name by default or a provided override value. It facilitates identification or serialization of classes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "update_classpath", "line_number": 134, "body": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if isinstance(path, str) and path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path", "is_method": false, "function_description": "This function updates deprecated classpath strings to their new locations, issuing a deprecation warning when replacements occur. It helps maintain compatibility by mapping old paths to current ones."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "method_is_overridden", "line_number": 145, "body": "def method_is_overridden(subclass, base_class, method_name):\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base:\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__", "is_method": false, "function_description": "Utility function that checks if a specific method in a base class has been overridden in a given subclass, helping to determine customized behavior in class hierarchies. It supports introspection for method override detection in inheritance structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__new__", "line_number": 58, "body": "def __new__(metacls, name, bases, clsdict_):\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls", "is_method": true, "class_name": "DeprecatedClass", "function_description": "Core method for DeprecatedClass's metaclass that ensures only the first created class is stored as the deprecated_class reference, facilitating tracking of the initially deprecated class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__instancecheck__", "line_number": 80, "body": "def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})", "is_method": true, "class_name": "DeprecatedClass", "function_description": "Overrides instance checking to determine if an object is an instance of a class or its subclasses, enhancing type-checking behavior for the DeprecatedClass."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__subclasscheck__", "line_number": 84, "body": "def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)", "is_method": true, "class_name": "DeprecatedClass", "function_description": "Customizes subclass checking behavior specifically when comparing with a deprecated class, ensuring that only direct checks against the deprecated class use this logic while preserving standard behavior otherwise."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/deprecate.py", "function": "__call__", "line_number": 98, "body": "def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)", "is_method": true, "class_name": "DeprecatedClass", "function_description": "This method intercepts instantiation calls to the DeprecatedClass, issuing a warning to inform users about class deprecation while allowing normal object creation to proceed. It helps manage deprecated class usage by notifying developers during runtime."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "inside_project", "line_number": 16, "body": "def inside_project():\n    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n    if scrapy_module is not None:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(f\"Cannot import scrapy settings module {scrapy_module}: {exc}\")\n        else:\n            return True\n    return bool(closest_scrapy_cfg())", "is_method": false, "function_description": "Determines if the current environment is within a Scrapy project by checking for Scrapy settings or configuration files, supporting context-aware operations within Scrapy-based applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "project_data_dir", "line_number": 28, "body": "def project_data_dir(project='default'):\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = cfg.get(DATADIR_CFG_SECTION, project)\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n    if not exists(d):\n        os.makedirs(d)\n    return d", "is_method": false, "function_description": "Returns the data directory path for a given project, creating it if missing. It ensures the directory exists within the current project context for consistent data storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "data_path", "line_number": 45, "body": "def data_path(path, createdir=False):\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    if not isabs(path):\n        if inside_project():\n            path = join(project_data_dir(), path)\n        else:\n            path = join('.scrapy', path)\n    if createdir and not exists(path):\n        os.makedirs(path)\n    return path", "is_method": false, "function_description": "Utility function to resolve a file path relative to the project\u2019s .scrapy data directory, optionally creating the directory if it does not exist. It supports absolute paths by returning them unchanged."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/project.py", "function": "get_project_settings", "line_number": 60, "body": "def get_project_settings():\n    if ENVVAR not in os.environ:\n        project = os.environ.get('SCRAPY_PROJECT', 'default')\n        init_env(project)\n\n    settings = Settings()\n    settings_module_path = os.environ.get(ENVVAR)\n    if settings_module_path:\n        settings.setmodule(settings_module_path, priority='project')\n\n    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n                      k.startswith('SCRAPY_')}\n    valid_envvars = {\n        'CHECK',\n        'PROJECT',\n        'PYTHON_SHELL',\n        'SETTINGS_MODULE',\n    }\n    setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}\n    if setting_envvars:\n        setting_envvar_list = ', '.join(sorted(setting_envvars))\n        warnings.warn(\n            'Use of environment variables prefixed with SCRAPY_ to override '\n            'settings is deprecated. The following environment variables are '\n            f'currently defined: {setting_envvar_list}',\n            ScrapyDeprecationWarning\n        )\n    settings.setdict(scrapy_envvars, priority='project')\n\n    return settings", "is_method": false, "function_description": "Function that assembles and returns a configured project settings object by initializing environment variables and applying overrides, supporting backward compatibility and deprecation warnings for Scrapy project configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/sitemap.py", "function": "sitemap_urls_from_robots", "line_number": 40, "body": "def sitemap_urls_from_robots(robots_text, base_url=None):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith('sitemap:'):\n            url = line.split(':', 1)[1].strip()\n            yield urljoin(base_url, url)", "is_method": false, "function_description": "Function that extracts and iterates over all sitemap URLs declared in a robots.txt file, optionally resolving relative URLs against a base URL for consistent access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/sitemap.py", "function": "__iter__", "line_number": 23, "body": "def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d", "is_method": true, "class_name": "Sitemap", "function_description": "Provides an iterator that yields dictionaries representing sitemap entries, extracting location URLs and alternate links for each element. Useful for iterating over parsed sitemap data in a structured format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "ffi_buf_to_string", "line_number": 12, "body": "def ffi_buf_to_string(buf):\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))", "is_method": false, "function_description": "Converts a low-level FFI buffer into a Unicode string. This function facilitates handling raw OpenSSL data by providing a readable text format for other components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "x509name_to_string", "line_number": 16, "body": "def x509name_to_string(x509name):\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n\n    return ffi_buf_to_string(result_buffer)", "is_method": false, "function_description": "Converts an X509Name object into a readable string representation, facilitating easy display or logging of certificate names in security-related applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "get_temp_key_info", "line_number": 24, "body": "def get_temp_key_info(ssl_object):\n    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n        return None\n\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append('RSA')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append('DH')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append('ECDH')\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')\n    return ', '.join(key_info)", "is_method": false, "function_description": "Utility function that extracts and returns the type and size of the temporary cryptographic key used in an SSL connection, supporting RSA, DH, and ECDH keys for security diagnostics or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/ssl.py", "function": "get_openssl_version", "line_number": 57, "body": "def get_openssl_version():\n    system_openssl = OpenSSL.SSL.SSLeay_version(\n        OpenSSL.SSL.SSLEAY_VERSION\n    ).decode('ascii', errors='replace')\n    return f'{OpenSSL.version.__version__} ({system_openssl})'", "is_method": false, "function_description": "Function that returns the combined version information of the OpenSSL Python package and the underlying system OpenSSL library. This helps verify compatibility between Python's OpenSSL wrapper and the actual SSL implementation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "arg_to_iter", "line_number": 24, "body": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]", "is_method": false, "function_description": "Utility function that ensures any input (None, single value, or iterable) is returned as an iterable, treating dictionaries as single elements to simplify consistent iteration over diverse inputs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "load_object", "line_number": 38, "body": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        else:\n            raise TypeError(\"Unexpected argument type, expected string \"\n                            \"or object, got: %s\" % type(path))\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj", "is_method": false, "function_description": "Utility function that loads and returns a Python object (class, function, variable, or instance) specified by its full import path, or returns the callable directly. It enables dynamic object retrieval for flexible module and attribute access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "walk_modules", "line_number": 71, "body": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods", "is_method": false, "function_description": "Function that recursively loads a specified module and all its submodules, returning a list of these imported modules while propagating import exceptions. Useful for dynamic module discovery and bulk loading within a package hierarchy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "extract_regex", "line_number": 93, "body": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n    warnings.warn(\n        \"scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2\n    )\n\n    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except Exception:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, str):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]", "is_method": false, "function_description": "Function that extracts text matches from input using a regex with support for named groups, multiple groups, or full matches, returning a cleaned list of Unicode strings suitable for flexible pattern-based text retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "md5sum", "line_number": 122, "body": "def md5sum(file):\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()", "is_method": false, "function_description": "Function that computes the MD5 checksum of a file-like object efficiently by processing it in chunks, enabling integrity verification without loading the entire file into memory."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "rel_has_nofollow", "line_number": 139, "body": "def rel_has_nofollow(rel):\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and 'nofollow' in rel.split()", "is_method": false, "function_description": "Utility function that checks if a link's rel attribute includes the 'nofollow' directive, useful for processing or filtering hyperlinks based on SEO or crawling preferences."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "create_instance", "line_number": 144, "body": "def create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, 'from_crawler'):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = 'from_crawler'\n    elif hasattr(objcls, 'from_settings'):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = 'from_settings'\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = '__new__'\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance", "is_method": false, "function_description": "Utility function that instantiates a class using either `from_crawler` or `from_settings` constructors when available, facilitating flexible object creation within frameworks relying on crawler or settings contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "set_environ", "line_number": 180, "body": "def set_environ(**kwargs):\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v", "is_method": false, "function_description": "Function providing a context manager that temporarily sets environment variables and restores their original values upon exit. Useful for safely managing environment changes during specific code execution blocks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "walk_callable", "line_number": 197, "body": "def walk_callable(node):\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node", "is_method": false, "function_description": "Function that iterates over the AST nodes within a function body, skipping any nested functions to allow analysis focused on the outer function scope only. It enables targeted traversal of code blocks without descending into nested function definitions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "is_generator_with_return_value", "line_number": 216, "body": "def is_generator_with_return_value(callable):\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return _generator_callbacks_cache[callable]\n\n    def returns_none(return_node):\n        value = return_node.value\n        return value is None or isinstance(value, ast.NameConstant) and value.value is None\n\n    if inspect.isgeneratorfunction(callable):\n        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return _generator_callbacks_cache[callable]\n\n    _generator_callbacks_cache[callable] = False\n    return _generator_callbacks_cache[callable]", "is_method": false, "function_description": "Determines whether a given callable is a generator function that explicitly returns a non-None value, which can be useful for identifying generators with meaningful return results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "warn_on_generator_with_return_value", "line_number": 240, "body": "def warn_on_generator_with_return_value(spider, callable):\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                'different than None. This could lead to unexpected behaviour. Please see '\n                'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            'This will not prevent your code from working, but it prevents Scrapy from detecting '\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            'Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), '\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )", "is_method": false, "function_description": "Utility function that warns if a spider method is a generator that improperly returns a non-None value, helping to detect potential generator usage issues in web scraping code."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/misc.py", "function": "returns_none", "line_number": 224, "body": "def returns_none(return_node):\n        value = return_node.value\n        return value is None or isinstance(value, ast.NameConstant) and value.value is None", "is_method": false, "function_description": "Utility function that checks if a return statement explicitly returns None or a None constant, useful for analyzing function exit points in abstract syntax trees."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "read1", "line_number": 13, "body": "def read1(gzf, size=-1):\n    return gzf.read1(size)", "is_method": false, "function_description": "This function provides a simple wrapper to read up to a specified number of bytes from a given gzip file object, facilitating controlled data retrieval from compressed files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "gunzip", "line_number": 17, "body": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_list = []\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = f.read1(8196)\n            output_list.append(chunk)\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_list is empty and f.extrabuf\n            # contains the whole page content\n            if output_list or getattr(f, 'extrabuf', None):\n                try:\n                    output_list.append(f.extrabuf[-f.extrasize:])\n                finally:\n                    break\n            else:\n                raise\n    return b''.join(output_list)", "is_method": false, "function_description": "Function that decompresses gzip-compressed data, recovering as much content as possible even if checksum errors occur, providing robust decompression for potentially corrupted gzip inputs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/gz.py", "function": "gzip_magic_number", "line_number": 44, "body": "def gzip_magic_number(response):\n    return response.body[:3] == b'\\x1f\\x8b\\x08'", "is_method": false, "function_description": "Function that checks if a response body starts with the gzip file signature, indicating gzip compression. Useful for detecting whether response content is gzip-encoded."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/serialize.py", "function": "default", "line_number": 16, "body": "def default(self, o):\n        if isinstance(o, set):\n            return list(o)\n        elif isinstance(o, datetime.datetime):\n            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif is_item(o):\n            return ItemAdapter(o).asdict()\n        elif isinstance(o, Request):\n            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n        elif isinstance(o, Response):\n            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n        else:\n            return super().default(o)", "is_method": true, "class_name": "ScrapyJSONEncoder", "function_description": "Custom JSON encoder method that converts various complex Scrapy-related and Python types into JSON-serializable formats for seamless data encoding during scraping or serialization tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "request_to_dict", "line_number": 11, "body": "def request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n        'flags': request.flags,\n        'cb_kwargs': request.cb_kwargs,\n    }\n    if type(request) is not Request:\n        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n    return d", "is_method": false, "function_description": "Utility function that converts a Request object into a dictionary, optionally resolving callback and errback methods by spider name. It enables serialization or inspection of request details for handling or debugging in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "request_from_dict", "line_number": 43, "body": "def request_from_dict(d, spider=None):\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    cb = d['callback']\n    if cb and spider:\n        cb = _get_method(spider, cb)\n    eb = d['errback']\n    if eb and spider:\n        eb = _get_method(spider, eb)\n    request_cls = load_object(d['_class']) if '_class' in d else Request\n    return request_cls(\n        url=to_unicode(d['url']),\n        callback=cb,\n        errback=eb,\n        method=d['method'],\n        headers=d['headers'],\n        body=d['body'],\n        cookies=d['cookies'],\n        meta=d['meta'],\n        encoding=d['_encoding'],\n        priority=d['priority'],\n        dont_filter=d['dont_filter'],\n        flags=d.get('flags'),\n        cb_kwargs=d.get('cb_kwargs'),\n    )", "is_method": false, "function_description": "Function that constructs a Request object from a dictionary, optionally resolving callback and errback methods from a given spider. It supports customizable request parameters for web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "_find_method", "line_number": 73, "body": "def _find_method(obj, func):\n    # Only instance methods contain ``__func__``\n    if obj and hasattr(func, '__func__'):\n        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n        for name, obj_func in members:\n            # We need to use __func__ to access the original\n            # function object because instance method objects\n            # are generated each time attribute is retrieved from\n            # instance.\n            #\n            # Reference: The standard type hierarchy\n            # https://docs.python.org/3/reference/datamodel.html\n            if obj_func.__func__ is func.__func__:\n                return name\n    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")", "is_method": false, "function_description": "Utility function that identifies the name of an instance method within an object by matching the underlying function, facilitating reflection or introspection of methods bound to specific instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/reqser.py", "function": "_get_method", "line_number": 90, "body": "def _get_method(obj, name):\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(f\"Method {name!r} not found in: {obj}\")", "is_method": false, "function_description": "Utility function that fetches a named method from an object, raising an error if the method does not exist. It ensures access to object methods by name with explicit failure feedback."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/boto.py", "function": "is_botocore", "line_number": 7, "body": "def is_botocore():\n    \"\"\" Returns True if botocore is available, otherwise raises NotConfigured. Never returns False.\n\n    Previously, when boto was supported in addition to botocore, this returned False if boto was available\n    but botocore wasn't.\n    \"\"\"\n    message = (\n        'is_botocore() is deprecated and always returns True or raises an Exception, '\n        'so it cannot be used for checking if boto is available instead of botocore. '\n        'You can use scrapy.utils.boto.is_botocore_available() to check if botocore '\n        'is available.'\n    )\n    warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n    try:\n        import botocore  # noqa: F401\n        return True\n    except ImportError:\n        raise NotConfigured('missing botocore library')", "is_method": false, "function_description": "Checks for the presence of the botocore library, returning True if available; otherwise, it raises a configuration error. This deprecated function verifies botocore dependency for AWS-related operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/boto.py", "function": "is_botocore_available", "line_number": 27, "body": "def is_botocore_available():\n    try:\n        import botocore  # noqa: F401\n        return True\n    except ImportError:\n        return False", "is_method": false, "function_description": "Utility function that checks if the botocore library is installed and available for use in the current environment."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/versions.py", "function": "scrapy_components_versions", "line_number": 15, "body": "def scrapy_components_versions():\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\", platform.platform()),\n    ]", "is_method": false, "function_description": "Function that gathers and returns version and platform information for Scrapy and its key dependencies, useful for diagnostics and environment verification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "flatten", "line_number": 18, "body": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))", "is_method": false, "function_description": "Function that recursively converts nested sequences into a single flat list containing all elements, simplifying complex iterables into a linear structure for easier processing or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "iflatten", "line_number": 38, "body": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in iflatten(el):\n                yield el_\n        else:\n            yield el", "is_method": false, "function_description": "Utility function that iteratively flattens deeply nested lists or sequences, yielding elements one by one as an iterator for efficient, lazy traversal of nested structures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "is_listlike", "line_number": 50, "body": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))", "is_method": false, "function_description": "Utility function that determines if a variable behaves like a list or iterable container, excluding strings and bytes. It helps identify if an object supports iteration suitable for list-like processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "unique", "line_number": 74, "body": "def unique(list_, key=lambda x: x):\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result", "is_method": false, "function_description": "Function that returns a list of unique items from the input while preserving their original order. It supports custom uniqueness criteria via an optional key function."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_unicode", "line_number": 87, "body": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError('to_unicode must receive a bytes or str '\n                        f'object, got {type(text).__name__}')\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)", "is_method": false, "function_description": "Function that ensures a given input is returned as a Unicode string, decoding bytes using a specified encoding if necessary. It standardizes text inputs for consistent Unicode handling in text processing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_bytes", "line_number": 100, "body": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError('to_bytes must receive a str or bytes '\n                        f'object, got {type(text).__name__}')\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)", "is_method": false, "function_description": "Function that converts a string to its binary (bytes) representation using a specified encoding, or returns the input unchanged if already in bytes. It ensures consistent byte output for text processing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "to_native_str", "line_number": 114, "body": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of ``text``. \"\"\"\n    return to_unicode(text, encoding, errors)", "is_method": false, "function_description": "Converts input text to a native string format, handling encoding and error options. Useful for consistent string representation across different Python versions or text types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "re_rsearch", "line_number": 119, "body": "def re_rsearch(pattern, text, chunk_size=1024):\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = [match for match in pattern.finditer(chunk)]\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None", "is_method": false, "function_description": "Function that performs a reverse regular expression search on large text by scanning chunks from the end, returning the last match's position or None if not found. Useful for efficiently locating patterns near text's end without scanning entire content forward."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "memoizemethod_noargs", "line_number": 153, "body": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method", "is_method": false, "function_description": "Decorator that caches a no-argument method's result per instance using weak references to avoid redundant computations while allowing object cleanup. Useful for optimizing repeated calls to expensive instance methods without arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "binary_is_text", "line_number": 172, "body": "def binary_is_text(data):\n    \"\"\" Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)", "is_method": false, "function_description": "Function that determines whether a bytes object represents text by checking for the absence of unprintable control characters. Useful for distinguishing textual data from binary content in data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "_getargspec_py23", "line_number": 181, "body": "def _getargspec_py23(func):\n    \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n                                                        defaults)\n\n    Was identical to inspect.getargspec() in python2, but uses\n    inspect.getfullargspec() for python3 behind the scenes to avoid\n    DeprecationWarning.\n\n    >>> def f(a, b=2, *ar, **kw):\n    ...     pass\n\n    >>> _getargspec_py23(f)\n    ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n    \"\"\"\n    return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])", "is_method": false, "function_description": "Utility function that retrieves a function\u2019s argument specification compatible across Python 2 and 3, providing parameter names, variable arguments, keyword arguments, and default values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "get_func_args", "line_number": 198, "body": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        spec = inspect.getfullargspec(func)\n        func_args = spec.args + spec.kwonlyargs\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError(f'{type(func)} is not callable')\n    if stripself:\n        func_args.pop(0)\n    return func_args", "is_method": false, "function_description": "Utility function that extracts and returns the list of argument names from any callable object, supporting functions, methods, classes, partials, and callable instances. It optionally excludes the 'self' parameter for methods to aid in introspection or dynamic invocation scenarios."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "get_spec", "line_number": 226, "body": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = _getargspec_py23(func)\n    elif hasattr(func, '__call__'):\n        spec = _getargspec_py23(func.__call__)\n    else:\n        raise TypeError(f'{type(func)} is not callable')\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs", "is_method": false, "function_description": "Utility function that extracts a callable's argument names and default keyword argument values as separate lists, supporting functions, methods, and callable objects for introspection or dynamic invocation purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "equal_attributes", "line_number": 263, "body": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True", "is_method": false, "function_description": "Function that checks if two objects have equal values for a specified list of attributes or derived keys, supporting both attribute names and callable keys for flexible comparison. It is useful for validating object equivalence based on selected properties."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "retry_on_eintr", "line_number": 295, "body": "def retry_on_eintr(function, *args, **kw):\n    \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n    while True:\n        try:\n            return function(*args, **kw)\n        except IOError as e:\n            if e.errno != errno.EINTR:\n                raise", "is_method": false, "function_description": "Utility function that repeatedly executes a given function, automatically retrying it if interrupted by EINTR errors, ensuring robust operation against system call interruptions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "without_none_values", "line_number": 305, "body": "def without_none_values(iterable):\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    try:\n        return {k: v for k, v in iterable.items() if v is not None}\n    except AttributeError:\n        return type(iterable)((v for v in iterable if v is not None))", "is_method": false, "function_description": "Function that returns a copy of an iterable with all None values removed. It supports both mappings and sequences, providing a cleaned version without None entries for downstream processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "global_object_name", "line_number": 317, "body": "def global_object_name(obj):\n    \"\"\"\n    Return full name of a global object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__name__}\"", "is_method": false, "function_description": "Utility function that returns the full import path of a global object, enabling identification or reference of the object by its module and name."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__getitem__", "line_number": 288, "body": "def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]", "is_method": true, "class_name": "WeakKeyCache", "function_description": "Provides dictionary-like access to cached values keyed by weak references; it returns existing entries or creates and stores default values for missing keys using a factory function. This supports memory-efficient caching with automatic cleanup of unused keys."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "garbage_collect", "line_number": 329, "body": "def garbage_collect():\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()", "is_method": false, "function_description": "Force garbage collection by running the process twice to ensure complete cleanup of unused objects. This function is useful for managing memory explicitly, especially in environments like PyPy where collection may require multiple passes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "garbage_collect", "line_number": 334, "body": "def garbage_collect():\n        gc.collect()", "is_method": false, "function_description": "Calls the Python garbage collector to free up unused memory by cleaning up unreferenced objects. This function helps manage memory usage during program execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "extend", "line_number": 346, "body": "def extend(self, *iterables):\n        self.data = chain(self.data, chain.from_iterable(iterables))", "is_method": true, "class_name": "MutableChain", "function_description": "Core method of MutableChain that extends its data sequence by appending multiple iterable inputs, enabling dynamic and flexible concatenation of elements from various sources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__iter__", "line_number": 349, "body": "def __iter__(self):\n        return self", "is_method": true, "class_name": "MutableChain", "function_description": "Provides iterator functionality to the MutableChain class, allowing the object itself to be used directly in iteration contexts such as loops."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "__next__", "line_number": 352, "body": "def __next__(self):\n        return next(self.data)", "is_method": true, "class_name": "MutableChain", "function_description": "Core iterator method of the MutableChain class that returns the next item from its underlying data sequence, enabling the class to be used in iteration contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/python.py", "function": "next", "line_number": 356, "body": "def next(self):\n        return self.__next__()", "is_method": true, "class_name": "MutableChain", "function_description": "This method provides an alias for the iterator's __next__ method, enabling the MutableChain instance to be advanced to its next item. It facilitates iteration control in contexts where next() is preferred over __next__()."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "execute", "line_number": 13, "body": "def execute(self, args, check_code=True, settings=None):\n        from twisted.internet import reactor\n        env = os.environ.copy()\n        if settings is not None:\n            env['SCRAPY_SETTINGS_MODULE'] = settings\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred", "is_method": true, "class_name": "ProcessTest", "function_description": "Core method of the ProcessTest class that asynchronously runs a subprocess with customized environment and arguments, returning a deferred that resolves upon process completion with optional exit code verification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "_process_finished", "line_number": 24, "body": "def _process_finished(self, pp, cmd, check_code):\n        if pp.exitcode and check_code:\n            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n            msg += f\"\\n>>> stdout <<<\\n{pp.out}\"\n            msg += \"\\n\"\n            msg += f\"\\n>>> stderr <<<\\n{pp.err}\"\n            raise RuntimeError(msg)\n        return pp.exitcode, pp.out, pp.err", "is_method": true, "class_name": "ProcessTest", "function_description": "Internal method of ProcessTest that checks a process's exit code and outputs, raising an error if the process failed and error checking is enabled; otherwise returns the exit code, stdout, and stderr."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "outReceived", "line_number": 42, "body": "def outReceived(self, data):\n        self.out += data", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "Handles incoming data by appending it to an internal buffer, facilitating accumulation of output during a process's execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "errReceived", "line_number": 45, "body": "def errReceived(self, data):\n        self.err += data", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "Accumulates error data received during a process, enabling the TestProcessProtocol class to collect and manage error output for later inspection or handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/utils/testproc.py", "function": "processEnded", "line_number": 48, "body": "def processEnded(self, status):\n        self.exitcode = status.value.exitCode\n        self.deferred.callback(self)", "is_method": true, "class_name": "TestProcessProtocol", "function_description": "Handles the termination of a process by capturing its exit code and triggering a callback to notify that the process has finished. This enables asynchronous tracking and response to process completion within the TestProcessProtocol."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "sanitize_module_name", "line_number": 14, "body": "def sanitize_module_name(module_name):\n    \"\"\"Sanitize the given module name, by replacing dashes and points\n    with underscores and prefixing it with a letter if it doesn't start\n    with one\n    \"\"\"\n    module_name = module_name.replace('-', '_').replace('.', '_')\n    if module_name[0] not in string.ascii_letters:\n        module_name = \"a\" + module_name\n    return module_name", "is_method": false, "function_description": "Function that standardizes module names by replacing dashes and periods with underscores and ensuring they start with a letter, supporting consistent and valid Python module naming conventions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "syntax", "line_number": 30, "body": "def syntax(self):\n        return \"[options] <name> <domain>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command's syntax string indicating required and optional arguments for usage guidance or help display."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "short_desc", "line_number": 33, "body": "def short_desc(self):\n        return \"Generate new spider using pre-defined templates\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command class\u2019s functionality, which is to generate new spiders from predefined templates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "add_options", "line_number": 36, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                          help=\"List available templates\")\n        parser.add_option(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n                          help=\"Edit spider after creating it\")\n        parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n                          help=\"Dump template to standard output\")\n        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n                          help=\"Uses a custom template.\")\n        parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n                          help=\"If the spider already exists, overwrite it with the template\")", "is_method": true, "class_name": "Command", "function_description": "Adds command-line options for managing spider templates, enabling listing, editing, dumping, selecting, and force-overwriting templates in the command interface."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "run", "line_number": 49, "body": "def run(self, args, opts):\n        if opts.list:\n            self._list_templates()\n            return\n        if opts.dump:\n            template_file = self._find_template(opts.dump)\n            if template_file:\n                with open(template_file, \"r\") as f:\n                    print(f.read())\n            return\n        if len(args) != 2:\n            raise UsageError()\n\n        name, domain = args[0:2]\n        module = sanitize_module_name(name)\n\n        if self.settings.get('BOT_NAME') == module:\n            print(\"Cannot create a spider with the same name as your project\")\n            return\n\n        if not opts.force and self._spider_exists(name):\n            return\n\n        template_file = self._find_template(opts.template)\n        if template_file:\n            self._genspider(module, name, domain, opts.template, template_file)\n            if opts.edit:\n                self.exitcode = os.system(f'scrapy edit \"{name}\"')", "is_method": true, "class_name": "Command", "function_description": "Provides command execution for managing spider templates by listing, displaying, or generating spiders based on user options, enforcing naming rules and supporting edit-launch after creation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_genspider", "line_number": 78, "body": "def _genspider(self, module, name, domain, template_name, template_file):\n        \"\"\"Generate the spider module, based on the given template\"\"\"\n        capitalized_module = ''.join(s.capitalize() for s in module.split('_'))\n        tvars = {\n            'project_name': self.settings.get('BOT_NAME'),\n            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),\n            'module': module,\n            'name': name,\n            'domain': domain,\n            'classname': f'{capitalized_module}Spider'\n        }\n        if self.settings.get('NEWSPIDER_MODULE'):\n            spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n            spiders_dir = abspath(dirname(spiders_module.__file__))\n        else:\n            spiders_module = None\n            spiders_dir = \".\"\n        spider_file = f\"{join(spiders_dir, module)}.py\"\n        shutil.copyfile(template_file, spider_file)\n        render_templatefile(spider_file, **tvars)\n        print(f\"Created spider {name!r} using template {template_name!r} \",\n              end=('' if spiders_module else '\\n'))\n        if spiders_module:\n            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")", "is_method": true, "class_name": "Command", "function_description": "Generates a spider Python module file from a template by filling in project-specific details and placing it in the appropriate directory. This automates creating spider components within a scraping project."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_find_template", "line_number": 103, "body": "def _find_template(self, template):\n        template_file = join(self.templates_dir, f'{template}.tmpl')\n        if exists(template_file):\n            return template_file\n        print(f\"Unable to find template: {template}\\n\")\n        print('Use \"scrapy genspider --list\" to see all available templates.')", "is_method": true, "class_name": "Command", "function_description": "Utility method in the Command class that locates a template file by name within the templates directory, aiding in command template management and providing user guidance if the template is not found."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_list_templates", "line_number": 110, "body": "def _list_templates(self):\n        print(\"Available templates:\")\n        for filename in sorted(os.listdir(self.templates_dir)):\n            if filename.endswith('.tmpl'):\n                print(f\"  {splitext(filename)[0]}\")", "is_method": true, "class_name": "Command", "function_description": "Utility method in the Command class that lists all available template names by printing filenames with a '.tmpl' extension from a specific directory. It helps users see which templates they can use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "_spider_exists", "line_number": 116, "body": "def _spider_exists(self, name):\n        if not self.settings.get('NEWSPIDER_MODULE'):\n            # if run as a standalone command and file with same filename already exists\n            if exists(name + \".py\"):\n                print(f\"{abspath(name + '.py')} already exists\")\n                return True\n            return False\n\n        try:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n        else:\n            # if spider with same name exists\n            print(f\"Spider {name!r} already exists in module:\")\n            print(f\"  {spidercls.__module__}\")\n            return True\n\n        # a file with the same name exists in the target directory\n        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n        spiders_dir = dirname(spiders_module.__file__)\n        spiders_dir_abs = abspath(spiders_dir)\n        if exists(join(spiders_dir_abs, name + \".py\")):\n            print(f\"{join(spiders_dir_abs, (name + '.py'))} already exists\")\n            return True\n\n        return False", "is_method": true, "class_name": "Command", "function_description": "Checks whether a spider with the given name already exists as a file or loaded class, preventing duplicate spider creation in the context of a web crawling framework. Useful for validating spider names before generating new spider files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/genspider.py", "function": "templates_dir", "line_number": 145, "body": "def templates_dir(self):\n        return join(\n            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n            'spiders'\n        )", "is_method": true, "class_name": "Command", "function_description": "Returns the file system path to the directory containing spider templates, using a configured setting or default package location. This path is useful for accessing or managing spider template files within the Command context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "syntax", "line_number": 9, "body": "def syntax(self):\n        return \"[options] <spider>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command syntax string illustrating the required format and arguments for invoking the command. This helps users understand how to structure command input."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "short_desc", "line_number": 12, "body": "def short_desc(self):\n        return \"Run a spider\"", "is_method": true, "class_name": "Command", "function_description": "Provides a brief description of the command's action, indicating it runs a spider process. Useful for identifying or summarizing the command's purpose in interfaces or logs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/crawl.py", "function": "run", "line_number": 15, "body": "def run(self, args, opts):\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\"running 'scrapy crawl' with more than one spider is not supported\")\n        spname = args[0]\n\n        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n        if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):\n            self.exitcode = 1\n        else:\n            self.crawler_process.start()\n\n            if (\n                self.crawler_process.bootstrap_failed\n                or hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception\n            ):\n                self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Core method of the Command class that runs a web crawling process using specified arguments and options, managing execution flow and exit codes based on success or failure conditions. It validates input and controls the lifecycle of the crawl operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/list.py", "function": "short_desc", "line_number": 9, "body": "def short_desc(self):\n        return \"List available spiders\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command, indicating it lists available spiders. This helps quickly identify the Command's purpose in spider management contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/list.py", "function": "run", "line_number": 12, "body": "def run(self, args, opts):\n        for s in sorted(self.crawler_process.spider_loader.list()):\n            print(s)", "is_method": true, "class_name": "Command", "function_description": "Lists and prints the names of all available spiders in the crawler process, aiding users in identifying which spiders can be run or managed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "short_desc", "line_number": 7, "body": "def short_desc(self):\n        return \"Open URL in browser, as seen by Scrapy\"", "is_method": true, "class_name": "Command", "function_description": "Provides a brief description of the Command class functionality, indicating it opens a URL in a browser context used by Scrapy."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "long_desc", "line_number": 10, "body": "def long_desc(self):\n        return \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the command's main function, which is to fetch a URL via Scrapy and display its contents in a browser."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "add_options", "line_number": 13, "body": "def add_options(self, parser):\n        super().add_options(parser)\n        parser.remove_option(\"--headers\")", "is_method": true, "class_name": "Command", "function_description": "Overrides the add_options method to customize command-line parser options by removing the \"--headers\" option after inheriting the base options. It adapts argument parsing behavior for this specific Command subclass."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/view.py", "function": "_print_response", "line_number": 17, "body": "def _print_response(self, response, opts):\n        open_in_browser(response)", "is_method": true, "class_name": "Command", "function_description": "Internal method of the Command class that opens a given response in a web browser, facilitating immediate viewing or interaction based on provided options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "printSummary", "line_number": 12, "body": "def printSummary(self, start, stop):\n        write = self.stream.write\n        writeln = self.stream.writeln\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")", "is_method": true, "class_name": "TextTestResult", "function_description": "Provides a summary report of test results including total tests run, duration, and failure or error counts. Useful for giving quick, human-readable test execution feedback in testing frameworks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "syntax", "line_number": 44, "body": "def syntax(self):\n        return \"[options] <spider>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command syntax indicating required and optional arguments, guiding users on how to invoke the command properly."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "short_desc", "line_number": 47, "body": "def short_desc(self):\n        return \"Check spider contracts\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command instance, summarizing its purpose as checking spider contracts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "add_options", "line_number": 50, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                          help=\"only list contracts, without checking them\")\n        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n                          help=\"print contract tests for all spiders\")", "is_method": true, "class_name": "Command", "function_description": "Adds specific command-line options to enable listing contracts or verbose output, enhancing the command's configurability for contract testing in spider management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/check.py", "function": "run", "line_number": 57, "body": "def run(self, args, opts):\n        # load contracts\n        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        spider_loader = self.crawler_process.spider_loader\n\n        with set_environ(SCRAPY_CHECK='true'):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start_requests = lambda s: conman.from_spider(s, result)\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f'  * {method}')\n            else:\n                start = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start, stop)\n                self.exitcode = int(not result.wasSuccessful())", "is_method": true, "class_name": "Command", "function_description": "Runs contract-based tests on spiders by loading contracts, configuring spiders to execute contract checks, and either listing tested methods or performing the tests with results summarized on completion. It supports verbose output and sets the process exit code based on test success."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "_import_file", "line_number": 10, "body": "def _import_file(filepath):\n    abspath = os.path.abspath(filepath)\n    dirname, file = os.path.split(abspath)\n    fname, fext = os.path.splitext(file)\n    if fext not in ('.py', '.pyw'):\n        raise ValueError(f\"Not a Python source file: {abspath}\")\n    if dirname:\n        sys.path = [dirname] + sys.path\n    try:\n        module = import_module(fname)\n    finally:\n        if dirname:\n            sys.path.pop(0)\n    return module", "is_method": false, "function_description": "This function dynamically imports a Python source file as a module given its file path, handling temporary path adjustments to enable import by filename. It facilitates runtime loading of Python code outside normal package contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "syntax", "line_number": 31, "body": "def syntax(self):\n        return \"[options] <spider_file>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the expected command-line syntax, specifying the required spider file argument and optional flags for the command-line interface."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "short_desc", "line_number": 34, "body": "def short_desc(self):\n        return \"Run a self-contained spider (without creating a project)\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command class functionality, specifically indicating it runs a self-contained spider without project creation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "long_desc", "line_number": 37, "body": "def long_desc(self):\n        return \"Run the spider defined in the given file\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the command\u2019s purpose, specifically to run the spider defined in a specified file. This method provides a concise summary for user interfaces or help systems."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/runspider.py", "function": "run", "line_number": 40, "body": "def run(self, args, opts):\n        if len(args) != 1:\n            raise UsageError()\n        filename = args[0]\n        if not os.path.exists(filename):\n            raise UsageError(f\"File not found: {filename}\\n\")\n        try:\n            module = _import_file(filename)\n        except (ImportError, ValueError) as e:\n            raise UsageError(f\"Unable to load {filename!r}: {e}\\n\")\n        spclasses = list(iter_spider_classes(module))\n        if not spclasses:\n            raise UsageError(f\"No spider found in file: {filename}\\n\")\n        spidercls = spclasses.pop()\n\n        self.crawler_process.crawl(spidercls, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Executes a spider command by loading a spider class from a specified file and running it with given options, managing the crawling process lifecycle and error handling during setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "set_crawler", "line_number": 27, "body": "def set_crawler(self, crawler):\n        if hasattr(self, '_crawler'):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler = crawler", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Sets the crawler instance for the ScrapyCommand, ensuring it is assigned only once to prevent reassignment errors."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "long_desc", "line_number": 44, "body": "def long_desc(self):\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines, since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Provides a long description for the command, defaulting to the short description if no longer one is available. It ensures the description is concise and suitable for command-line formatting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "help", "line_number": 51, "body": "def help(self):\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines, since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Provides detailed help information for the command by returning its comprehensive description, supporting multi-line content for thorough user guidance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "add_options", "line_number": 58, "body": "def add_options(self, parser):\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        group = OptionGroup(parser, \"Global Options\")\n        group.add_option(\"--logfile\", metavar=\"FILE\",\n                         help=\"log file. if omitted stderr will be used\")\n        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n                         help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n        group.add_option(\"--nolog\", action=\"store_true\",\n                         help=\"disable logging completely\")\n        group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n                         help=\"write python cProfile stats to FILE\")\n        group.add_option(\"--pidfile\", metavar=\"FILE\",\n                         help=\"write process ID to FILE\")\n        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                         help=\"set/override setting (may be repeated)\")\n        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n        parser.add_option_group(group)", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Provides the ScrapyCommand class with command-line options to configure logging, profiling, process ID handling, settings overrides, and debugger activation for flexible spider execution control."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "process_options", "line_number": 79, "body": "def process_options(self, args, opts):\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set),\n                                  priority='cmdline')\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n            self.settings.set('LOG_FILE', opts.logfile, priority='cmdline')\n\n        if opts.loglevel:\n            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n            self.settings.set('LOG_LEVEL', opts.loglevel, priority='cmdline')\n\n        if opts.nolog:\n            self.settings.set('LOG_ENABLED', False, priority='cmdline')\n\n        if opts.pidfile:\n            with open(opts.pidfile, \"w\") as f:\n                f.write(str(os.getpid()) + os.linesep)\n\n        if opts.pdb:\n            failure.startDebugMode()", "is_method": true, "class_name": "ScrapyCommand", "function_description": "Method of ScrapyCommand that processes and applies command-line options to configure settings, logging, process management, and debugging behavior for a Scrapy command execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "add_options", "line_number": 115, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                          help=\"set spider argument (may be repeated)\")\n        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                          help=\"append scraped items to the end of FILE (use - for stdout)\")\n        parser.add_option(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n                          help=\"dump scraped items into FILE, overwriting any existing file\")\n        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                          help=\"format to use for dumping items\")", "is_method": true, "class_name": "BaseRunSpiderCommand", "function_description": "Adds command-line options to configure spider arguments, output files, and output formats for running spiders, enabling customizable scraping behavior through the command interface."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/__init__.py", "function": "process_options", "line_number": 126, "body": "def process_options(self, args, opts):\n        ScrapyCommand.process_options(self, args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                opts.output_format,\n                opts.overwrite_output,\n            )\n            self.settings.set('FEEDS', feeds, priority='cmdline')", "is_method": true, "class_name": "BaseRunSpiderCommand", "function_description": "Preprocesses and validates command-line options for a spider run, converting arguments into structured parameters and configuring output feed settings accordingly. It ensures proper input formatting and updates spider settings based on user-specified output options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "syntax", "line_number": 15, "body": "def syntax(self):\n        return \"[options] <url>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command's expected syntax format, indicating it accepts options followed by a URL argument. This helps other functions display or validate the command usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "short_desc", "line_number": 18, "body": "def short_desc(self):\n        return \"Fetch a URL using the Scrapy downloader\"", "is_method": true, "class_name": "Command", "function_description": "Provides a brief description of the Command class's functionality, indicating it fetches a URL using the Scrapy downloader."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "long_desc", "line_number": 21, "body": "def long_desc(self):\n        return (\n            \"Fetch a URL using the Scrapy downloader and print its content\"\n            \" to stdout. You may want to use --nolog to disable logging\"\n        )", "is_method": true, "class_name": "Command", "function_description": "Returns a descriptive string explaining that the command fetches a URL via Scrapy and outputs its content, suggesting the use of a flag to disable logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "add_options", "line_number": 27, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n                          help=\"print response HTTP headers instead of body\")\n        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                          help=\"do not handle HTTP 3xx status codes and print response as-is\")", "is_method": true, "class_name": "Command", "function_description": "Adds command-line options specific to the Command class for selecting a spider, toggling HTTP header display, and controlling HTTP redirect behavior during request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_headers", "line_number": 35, "body": "def _print_headers(self, headers, prefix):\n        for key, values in headers.items():\n            for value in values:\n                self._print_bytes(prefix + b' ' + key + b': ' + value)", "is_method": true, "class_name": "Command", "function_description": "Utility method of the Command class that prints HTTP headers with a specified prefix, formatting each header key-value pair for output. It supports displaying multiple values per header."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_response", "line_number": 40, "body": "def _print_response(self, response, opts):\n        if opts.headers:\n            self._print_headers(response.request.headers, b'>')\n            print('>')\n            self._print_headers(response.headers, b'<')\n        else:\n            self._print_bytes(response.body)", "is_method": true, "class_name": "Command", "function_description": "Internal method of the Command class that displays HTTP response headers when requested, otherwise prints the response body, supporting flexible output formatting for HTTP responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "_print_bytes", "line_number": 48, "body": "def _print_bytes(self, bytes_):\n        sys.stdout.buffer.write(bytes_ + b'\\n')", "is_method": true, "class_name": "Command", "function_description": "Private method of the Command class that writes raw byte sequences followed by a newline directly to the standard output. It facilitates low-level byte output operations typically for command-line interfaces."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/fetch.py", "function": "run", "line_number": 51, "body": "def run(self, args, opts):\n        if len(args) != 1 or not is_url(args[0]):\n            raise UsageError()\n        request = Request(args[0], callback=self._print_response,\n                          cb_kwargs={\"opts\": opts}, dont_filter=True)\n        # by default, let the framework handle redirects,\n        # i.e. command handles all codes expect 3xx\n        if not opts.no_redirect:\n            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n\n        spidercls = DefaultSpider\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        else:\n            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n        self.crawler_process.start()", "is_method": true, "class_name": "Command", "function_description": "Executes a web crawling command that processes a single URL with optional spider selection and redirect handling, then runs the crawl using a specified or default spider within the crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "syntax", "line_number": 27, "body": "def syntax(self):\n        return \"[options] <url>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command's expected syntax format, indicating optional flags and a required URL argument for correct usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "short_desc", "line_number": 30, "body": "def short_desc(self):\n        return \"Parse URL (using its spider) and print the results\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command class's function to parse a URL with its spider and output the results. This aids in quickly understanding the command's primary action."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_options", "line_number": 33, "body": "def add_options(self, parser):\n        BaseRunSpiderCommand.add_options(self, parser)\n        parser.add_option(\"--spider\", dest=\"spider\", default=None,\n                          help=\"use this spider without looking for one\")\n        parser.add_option(\"--pipelines\", action=\"store_true\",\n                          help=\"process items through pipelines\")\n        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n                          help=\"don't show links to follow (extracted requests)\")\n        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n                          help=\"don't show scraped items\")\n        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n                          help=\"avoid using pygments to colorize the output\")\n        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n                          help=\"use CrawlSpider rules to discover the callback\")\n        parser.add_option(\"-c\", \"--callback\", dest=\"callback\",\n                          help=\"use this callback for parsing, instead looking for a callback\")\n        parser.add_option(\"-m\", \"--meta\", dest=\"meta\",\n                          help=\"inject extra meta into the Request, it must be a valid raw json string\")\n        parser.add_option(\"--cbkwargs\", dest=\"cbkwargs\",\n                          help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n                          help=\"maximum depth for parsing requests [default: %default]\")\n        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                          help=\"print each depth level one by one\")", "is_method": true, "class_name": "Command", "function_description": "Service method of the Command class that extends a command-line parser by adding spider-specific options for controlling crawling behavior, output display, request metadata, and callback handling in web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "max_level", "line_number": 59, "body": "def max_level(self):\n        max_items, max_requests = 0, 0\n        if self.items:\n            max_items = max(self.items)\n        if self.requests:\n            max_requests = max(self.requests)\n        return max(max_items, max_requests)", "is_method": true, "class_name": "Command", "function_description": "Core method of the Command class that returns the highest value found in either its items or requests lists, useful for determining peak levels in command-related data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_items", "line_number": 67, "body": "def add_items(self, lvl, new_items):\n        old_items = self.items.get(lvl, [])\n        self.items[lvl] = old_items + new_items", "is_method": true, "class_name": "Command", "function_description": "Adds new items to a specified level within the Command's items collection, appending them to any existing items at that level. This enables dynamic extension of items grouped by levels."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "add_requests", "line_number": 71, "body": "def add_requests(self, lvl, new_reqs):\n        old_reqs = self.requests.get(lvl, [])\n        self.requests[lvl] = old_reqs + new_reqs", "is_method": true, "class_name": "Command", "function_description": "Adds new requests to a specified level within the Command's request registry, enabling organized management of multiple request groups by their priority or category."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_items", "line_number": 75, "body": "def print_items(self, lvl=None, colour=True):\n        if lvl is None:\n            items = [item for lst in self.items.values() for item in lst]\n        else:\n            items = self.items.get(lvl, [])\n\n        print(\"# Scraped Items \", \"-\" * 60)\n        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)", "is_method": true, "class_name": "Command", "function_description": "Provides a method to display collected items, optionally filtered by level, with formatted and colorized output for easy inspection of scraped data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_requests", "line_number": 84, "body": "def print_requests(self, lvl=None, colour=True):\n        if lvl is None:\n            if self.requests:\n                requests = self.requests[max(self.requests)]\n            else:\n                requests = []\n        else:\n            requests = self.requests.get(lvl, [])\n\n        print(\"# Requests \", \"-\" * 65)\n        display.pprint(requests, colorize=colour)", "is_method": true, "class_name": "Command", "function_description": "Prints and displays stored requests at a specified level or the highest level by default, optionally with colorized formatting. This aids in inspecting request data managed by the Command class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "print_results", "line_number": 96, "body": "def print_results(self, opts):\n        colour = not opts.nocolour\n\n        if opts.verbose:\n            for level in range(1, self.max_level + 1):\n                print(f'\\n>>> DEPTH LEVEL: {level} <<<')\n                if not opts.noitems:\n                    self.print_items(level, colour)\n                if not opts.nolinks:\n                    self.print_requests(level, colour)\n        else:\n            print(f'\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<')\n            if not opts.noitems:\n                self.print_items(colour=colour)\n            if not opts.nolinks:\n                self.print_requests(colour=colour)", "is_method": true, "class_name": "Command", "function_description": "Provides formatted output of command execution results, optionally showing detailed item and request information by depth level with configurable color and verbosity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "run_callback", "line_number": 113, "body": "def run_callback(self, response, callback, cb_kwargs=None):\n        cb_kwargs = cb_kwargs or {}\n        items, requests = [], []\n\n        for x in iterate_spider_output(callback(response, **cb_kwargs)):\n            if is_item(x):\n                items.append(x)\n            elif isinstance(x, Request):\n                requests.append(x)\n        return items, requests", "is_method": true, "class_name": "Command", "function_description": "Core method of the Command class that executes a callback with a response, separating and returning the resulting data items and requests for further processing or crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "get_callback_from_rules", "line_number": 124, "body": "def get_callback_from_rules(self, spider, response):\n        if getattr(spider, 'rules', None):\n            for rule in spider.rules:\n                if rule.link_extractor.matches(response.url):\n                    return rule.callback or \"parse\"\n        else:\n            logger.error('No CrawlSpider rules found in spider %(spider)r, '\n                         'please specify a callback to use for parsing',\n                         {'spider': spider.name})", "is_method": true, "class_name": "Command", "function_description": "Provides the appropriate callback method for a given response based on the spider's matching crawl rules, facilitating automated request handling in web crawlers. Returns a default parse callback if no specific one is set in the matched rule."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "set_spidercls", "line_number": 134, "body": "def set_spidercls(self, url, opts):\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            try:\n                self.spidercls = spider_loader.load(opts.spider)\n            except KeyError:\n                logger.error('Unable to find spider: %(spider)s',\n                             {'spider': opts.spider})\n        else:\n            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n            if not self.spidercls:\n                logger.error('Unable to find spider for: %(url)s', {'url': url})\n\n        def _start_requests(spider):\n            yield self.prepare_request(spider, Request(url), opts)\n        self.spidercls.start_requests = _start_requests", "is_method": true, "class_name": "Command", "function_description": "Sets the spider class to be used for crawling based on provided options or URL, preparing it to generate initial requests accordingly. This enables dynamic selection and setup of spiders for web scraping commands."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "start_parsing", "line_number": 151, "body": "def start_parsing(self, url, opts):\n        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n        self.pcrawler = list(self.crawler_process.crawlers)[0]\n        self.crawler_process.start()\n\n        if not self.first_response:\n            logger.error('No response downloaded for: %(url)s',\n                         {'url': url})", "is_method": true, "class_name": "Command", "function_description": "Starts a web crawling process using specified options and a spider class, managing crawler execution and logging an error if no response is obtained for the given URL."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "prepare_request", "line_number": 160, "body": "def prepare_request(self, spider, request, opts):\n        def callback(response, **cb_kwargs):\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            # determine real callback\n            cb = response.meta['_callback']\n            if not cb:\n                if opts.callback:\n                    cb = opts.callback\n                elif opts.rules and self.first_response == response:\n                    cb = self.get_callback_from_rules(spider, response)\n\n                    if not cb:\n                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n                                     {'url': response.url, 'spider': spider.name})\n                        return\n                else:\n                    cb = 'parse'\n\n            if not callable(cb):\n                cb_method = getattr(spider, cb, None)\n                if callable(cb_method):\n                    cb = cb_method\n                else:\n                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n                                 {'callback': cb, 'spider': spider.name})\n                    return\n\n            # parse items and requests\n            depth = response.meta['_depth']\n\n            items, requests = self.run_callback(response, cb, cb_kwargs)\n            if opts.pipelines:\n                itemproc = self.pcrawler.engine.scraper.itemproc\n                for item in items:\n                    itemproc.process_item(item, spider)\n            self.add_items(depth, items)\n            self.add_requests(depth, requests)\n\n            scraped_data = items if opts.output else []\n            if depth < opts.depth:\n                for req in requests:\n                    req.meta['_depth'] = depth + 1\n                    req.meta['_callback'] = req.callback\n                    req.callback = callback\n                scraped_data += requests\n\n            return scraped_data\n\n        # update request meta if any extra meta was passed through the --meta/-m opts.\n        if opts.meta:\n            request.meta.update(opts.meta)\n\n        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.\n        if opts.cbkwargs:\n            request.cb_kwargs.update(opts.cbkwargs)\n\n        request.meta['_depth'] = 1\n        request.meta['_callback'] = request.callback\n        request.callback = callback\n        return request", "is_method": true, "class_name": "Command", "function_description": "Sets up a web scraping request with a custom callback that manages response handling, item processing, and recursive request scheduling based on crawl depth and spider rules. It enables dynamic control over request flow and data extraction during crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_options", "line_number": 224, "body": "def process_options(self, args, opts):\n        BaseRunSpiderCommand.process_options(self, args, opts)\n\n        self.process_request_meta(opts)\n        self.process_request_cb_kwargs(opts)", "is_method": true, "class_name": "Command", "function_description": "Runs initial option processing from the base command, then handles request metadata and callback parameter processing specific to the Command class. It sets up request-related options for further use in command execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_request_meta", "line_number": 230, "body": "def process_request_meta(self, opts):\n        if opts.meta:\n            try:\n                opts.meta = json.loads(opts.meta)\n            except ValueError:\n                raise UsageError(\"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                                 \"Example: --meta='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)", "is_method": true, "class_name": "Command", "function_description": "Processes the meta option by parsing its JSON string value, validating input for command requests. It ensures the meta parameter contains well-formed JSON or raises an error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "process_request_cb_kwargs", "line_number": 238, "body": "def process_request_cb_kwargs(self, opts):\n        if opts.cbkwargs:\n            try:\n                opts.cbkwargs = json.loads(opts.cbkwargs)\n            except ValueError:\n                raise UsageError(\"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n                                 \"Example: --cbkwargs='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)", "is_method": true, "class_name": "Command", "function_description": "Processes and validates the command options to ensure callback keyword arguments are correctly parsed from JSON, raising an error for invalid inputs. It enables commands to accept structured callback parameters via JSON strings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "run", "line_number": 246, "body": "def run(self, args, opts):\n        # parse arguments\n        if not len(args) == 1 or not is_url(args[0]):\n            raise UsageError()\n        else:\n            url = args[0]\n\n        # prepare spidercls\n        self.set_spidercls(url, opts)\n\n        if self.spidercls and opts.depth > 0:\n            self.start_parsing(url, opts)\n            self.print_results(opts)", "is_method": true, "class_name": "Command", "function_description": "Runs a command that validates a URL argument, initializes a spider class accordingly, and triggers a web parsing process up to a specified depth, then outputs the parsing results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "_start_requests", "line_number": 147, "body": "def _start_requests(spider):\n            yield self.prepare_request(spider, Request(url), opts)", "is_method": true, "class_name": "Command", "function_description": "Core utility method of the Command class that initiates request generation for a spider by preparing and yielding the initial request with specified options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/parse.py", "function": "callback", "line_number": 161, "body": "def callback(response, **cb_kwargs):\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            # determine real callback\n            cb = response.meta['_callback']\n            if not cb:\n                if opts.callback:\n                    cb = opts.callback\n                elif opts.rules and self.first_response == response:\n                    cb = self.get_callback_from_rules(spider, response)\n\n                    if not cb:\n                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n                                     {'url': response.url, 'spider': spider.name})\n                        return\n                else:\n                    cb = 'parse'\n\n            if not callable(cb):\n                cb_method = getattr(spider, cb, None)\n                if callable(cb_method):\n                    cb = cb_method\n                else:\n                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n                                 {'callback': cb, 'spider': spider.name})\n                    return\n\n            # parse items and requests\n            depth = response.meta['_depth']\n\n            items, requests = self.run_callback(response, cb, cb_kwargs)\n            if opts.pipelines:\n                itemproc = self.pcrawler.engine.scraper.itemproc\n                for item in items:\n                    itemproc.process_item(item, spider)\n            self.add_items(depth, items)\n            self.add_requests(depth, requests)\n\n            scraped_data = items if opts.output else []\n            if depth < opts.depth:\n                for req in requests:\n                    req.meta['_depth'] = depth + 1\n                    req.meta['_callback'] = req.callback\n                    req.callback = callback\n                scraped_data += requests\n\n            return scraped_data", "is_method": true, "class_name": "Command", "function_description": "Handles a response by determining and executing the appropriate callback, processing resulting items and requests, and managing recursion depth to control request flow in a web scraping command context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "syntax", "line_number": 13, "body": "def syntax(self):\n        return \"[options]\"", "is_method": true, "class_name": "Command", "function_description": "Returns the command's syntax string representing its expected options format, useful for generating help messages or usage instructions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "short_desc", "line_number": 16, "body": "def short_desc(self):\n        return \"Get settings values\"", "is_method": true, "class_name": "Command", "function_description": "Utility method of the Command class that provides a brief description indicating the function retrieves settings values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "add_options", "line_number": 19, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\",\n                          help=\"print raw setting value\")\n        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a boolean\")\n        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as an integer\")\n        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a float\")\n        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n                          help=\"print setting value, interpreted as a list\")", "is_method": true, "class_name": "Command", "function_description": "Adds command-line options to a parser that allow retrieval of configuration settings in various data types, enabling flexible inspection of settings for commands in the Command class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/settings.py", "function": "run", "line_number": 32, "body": "def run(self, args, opts):\n        settings = self.crawler_process.settings\n        if opts.get:\n            s = settings.get(opts.get)\n            if isinstance(s, BaseSettings):\n                print(json.dumps(s.copy_to_dict()))\n            else:\n                print(s)\n        elif opts.getbool:\n            print(settings.getbool(opts.getbool))\n        elif opts.getint:\n            print(settings.getint(opts.getint))\n        elif opts.getfloat:\n            print(settings.getfloat(opts.getfloat))\n        elif opts.getlist:\n            print(settings.getlist(opts.getlist))", "is_method": true, "class_name": "Command", "function_description": "Provides command-line access to retrieve and print various types of configuration settings from a crawler's settings, supporting raw, boolean, integer, float, and list formats."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "syntax", "line_number": 11, "body": "def syntax(self):\n        return \"[-v]\"", "is_method": true, "class_name": "Command", "function_description": "Returns the syntax string representing the command's usage options. This provides a concise usage format for displaying or parsing command arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "short_desc", "line_number": 14, "body": "def short_desc(self):\n        return \"Print Scrapy version\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the command's function, specifically indicating that it prints the Scrapy version. This method provides a quick summary for command identification or help displays."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "add_options", "line_number": 17, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n                          help=\"also display twisted/python/platform info (useful for bug reports)\")", "is_method": true, "class_name": "Command", "function_description": "Adds command-line options to enable verbose output, including detailed environment information, enhancing command flexibility and aiding in debugging and bug reporting."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/version.py", "function": "run", "line_number": 22, "body": "def run(self, args, opts):\n        if opts.verbose:\n            versions = scrapy_components_versions()\n            width = max(len(n) for (n, _) in versions)\n            for name, version in versions:\n                print(f\"{name:<{width}} : {version}\")\n        else:\n            print(f\"Scrapy {scrapy.__version__}\")", "is_method": true, "class_name": "Command", "function_description": "Method of the Command class that displays Scrapy and its components' versions, either detailed when verbose is enabled or just the main Scrapy version otherwise. Useful for diagnostics and environment verification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_make_writable", "line_number": 26, "body": "def _make_writable(path):\n    current_permissions = os.stat(path).st_mode\n    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)", "is_method": false, "function_description": "Utility function that modifies a file's permissions to ensure the owner has write access, facilitating subsequent file write operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "syntax", "line_number": 37, "body": "def syntax(self):\n        return \"<project_name> [project_dir]\"", "is_method": true, "class_name": "Command", "function_description": "Returns the syntax format for using the Command, indicating required and optional arguments for correct command invocation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "short_desc", "line_number": 40, "body": "def short_desc(self):\n        return \"Create new project\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the command's primary action. Useful for displaying concise summaries in command listings or help interfaces."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_is_valid_name", "line_number": 43, "body": "def _is_valid_name(self, project_name):\n        def _module_exists(module_name):\n            try:\n                import_module(module_name)\n                return True\n            except ImportError:\n                return False\n\n        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n            print('Error: Project names must begin with a letter and contain'\n                  ' only\\nletters, numbers and underscores')\n        elif _module_exists(project_name):\n            print(f'Error: Module {project_name!r} already exists')\n        else:\n            return True\n        return False", "is_method": true, "class_name": "Command", "function_description": "Checks if a given project name is valid by ensuring it follows naming rules and does not clash with existing module names. This helps prevent naming conflicts and enforces proper project identifiers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_copytree", "line_number": 60, "body": "def _copytree(self, src, dst):\n        \"\"\"\n        Since the original function always creates the directory, to resolve\n        the issue a new function had to be created. It's a simple copy and\n        was reduced for this case.\n\n        More info at:\n        https://github.com/scrapy/scrapy/pull/2005\n        \"\"\"\n        ignore = IGNORE\n        names = os.listdir(src)\n        ignored_names = ignore(src, names)\n\n        if not os.path.exists(dst):\n            os.makedirs(dst)\n\n        for name in names:\n            if name in ignored_names:\n                continue\n\n            srcname = os.path.join(src, name)\n            dstname = os.path.join(dst, name)\n            if os.path.isdir(srcname):\n                self._copytree(srcname, dstname)\n            else:\n                copy2(srcname, dstname)\n                _make_writable(dstname)\n\n        copystat(src, dst)\n        _make_writable(dst)", "is_method": true, "class_name": "Command", "function_description": "Private method in the Command class that recursively copies a directory tree from source to destination, preserving file metadata and handling writable permissions while ignoring specified files. It enables controlled directory duplication with selective file inclusion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "run", "line_number": 91, "body": "def run(self, args, opts):\n        if len(args) not in (1, 2):\n            raise UsageError()\n\n        project_name = args[0]\n        project_dir = args[0]\n\n        if len(args) == 2:\n            project_dir = args[1]\n\n        if exists(join(project_dir, 'scrapy.cfg')):\n            self.exitcode = 1\n            print(f'Error: scrapy.cfg already exists in {abspath(project_dir)}')\n            return\n\n        if not self._is_valid_name(project_name):\n            self.exitcode = 1\n            return\n\n        self._copytree(self.templates_dir, abspath(project_dir))\n        move(join(project_dir, 'module'), join(project_dir, project_name))\n        for paths in TEMPLATES_TO_RENDER:\n            path = join(*paths)\n            tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))\n            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n        print(f\"New Scrapy project '{project_name}', using template directory \"\n              f\"'{self.templates_dir}', created in:\")\n        print(f\"    {abspath(project_dir)}\\n\")\n        print(\"You can start your first spider with:\")\n        print(f\"    cd {project_dir}\")\n        print(\"    scrapy genspider example example.com\")", "is_method": true, "class_name": "Command", "function_description": "Creates a new Scrapy project in a specified directory using template files, validates project naming, and initializes project structure while preventing overwriting existing projects. It guides users through the initial setup with usage instructions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "templates_dir", "line_number": 124, "body": "def templates_dir(self):\n        return join(\n            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n            'project'\n        )", "is_method": true, "class_name": "Command", "function_description": "Returns the directory path for project-specific templates, defaulting to a base templates folder if none is configured. This helps locate template files used in the Command class operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/startproject.py", "function": "_module_exists", "line_number": 44, "body": "def _module_exists(module_name):\n            try:\n                import_module(module_name)\n                return True\n            except ImportError:\n                return False", "is_method": true, "class_name": "Command", "function_description": "Utility function in the Command class that checks if a specified Python module can be imported, indicating its availability in the environment. It helps conditionally handle optional dependencies or features."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "syntax", "line_number": 24, "body": "def syntax(self):\n        return \"[url|file]\"", "is_method": true, "class_name": "Command", "function_description": "Returns the expected syntax string for a Command indicating it accepts a URL or file input format. This helps other components understand the command's input structure."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "short_desc", "line_number": 27, "body": "def short_desc(self):\n        return \"Interactive scraping console\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command class, identifying it as an interactive scraping console."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "long_desc", "line_number": 30, "body": "def long_desc(self):\n        return (\"Interactive console for scraping the given url or file. \"\n                \"Use ./file.html syntax or full path for local file.\")", "is_method": true, "class_name": "Command", "function_description": "Returns a descriptive summary of the interactive console's capability to scrape content from URLs or local files using specified file path syntax."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "add_options", "line_number": 34, "body": "def add_options(self, parser):\n        ScrapyCommand.add_options(self, parser)\n        parser.add_option(\"-c\", dest=\"code\",\n                          help=\"evaluate the code in the shell, print the result and exit\")\n        parser.add_option(\"--spider\", dest=\"spider\",\n                          help=\"use this spider\")\n        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                          help=\"do not handle HTTP 3xx status codes and print response as-is\")", "is_method": true, "class_name": "Command", "function_description": "Provides command-line options to customize the shell command's behavior, including code evaluation, spider selection, and HTTP redirect handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "run", "line_number": 49, "body": "def run(self, args, opts):\n        url = args[0] if args else None\n        if url:\n            # first argument may be a local file\n            url = guess_scheme(url)\n\n        spider_loader = self.crawler_process.spider_loader\n\n        spidercls = DefaultSpider\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        elif url:\n            spidercls = spidercls_for_request(spider_loader, Request(url),\n                                              spidercls, log_multiple=True)\n\n        # The crawler is created this way since the Shell manually handles the\n        # crawling engine, so the set up in the crawl method won't work\n        crawler = self.crawler_process._create_crawler(spidercls)\n        # The Shell class needs a persistent engine in the crawler\n        crawler.engine = crawler._create_engine()\n        crawler.engine.start()\n\n        self._start_crawler_thread()\n\n        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n        shell.start(url=url, redirect=not opts.no_redirect)", "is_method": true, "class_name": "Command", "function_description": "Core method of the Command class that configures and launches a web crawling process with a specified spider, then starts an interactive shell session to control or inspect the crawl in real time."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/shell.py", "function": "_start_crawler_thread", "line_number": 76, "body": "def _start_crawler_thread(self):\n        t = Thread(target=self.crawler_process.start,\n                   kwargs={'stop_after_crawl': False})\n        t.daemon = True\n        t.start()", "is_method": true, "class_name": "Command", "function_description": "Starts and runs the crawler process in a background daemon thread, enabling asynchronous web crawling without blocking the main program flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "syntax", "line_number": 13, "body": "def syntax(self):\n        return \"<spider>\"", "is_method": true, "class_name": "Command", "function_description": "Returns the fixed syntax string \"<spider>\" that represents the command's expected format or keyword. This can be used to identify or display the command's syntax in user interfaces or parsers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "short_desc", "line_number": 16, "body": "def short_desc(self):\n        return \"Edit spider\"", "is_method": true, "class_name": "Command", "function_description": "Returns a brief description of the Command instance, indicating it is related to editing a spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "long_desc", "line_number": 19, "body": "def long_desc(self):\n        return (\"Edit a spider using the editor defined in the EDITOR environment\"\n                \" variable or else the EDITOR setting\")", "is_method": true, "class_name": "Command", "function_description": "Returns a descriptive string explaining that a spider can be edited using the system's configured editor from environment variables or settings. It provides user guidance on the edit command's functionality within the Command class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "_err", "line_number": 23, "body": "def _err(self, msg):\n        sys.stderr.write(msg + os.linesep)\n        self.exitcode = 1", "is_method": true, "class_name": "Command", "function_description": "Private method of the Command class that reports an error message to standard error and sets the command's exit code to indicate failure. It supports signaling error conditions during command execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/edit.py", "function": "run", "line_number": 27, "body": "def run(self, args, opts):\n        if len(args) != 1:\n            raise UsageError()\n\n        editor = self.settings['EDITOR']\n        try:\n            spidercls = self.crawler_process.spider_loader.load(args[0])\n        except KeyError:\n            return self._err(f\"Spider not found: {args[0]}\")\n\n        sfile = sys.modules[spidercls.__module__].__file__\n        sfile = sfile.replace('.pyc', '.py')\n        self.exitcode = os.system(f'{editor} \"{sfile}\"')", "is_method": true, "class_name": "Command", "function_description": "Provides a command to open a specified spider's source file in the configured editor, facilitating quick code access and editing within a crawling framework. It validates the spider name and handles errors for missing spiders."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "short_desc", "line_number": 19, "body": "def short_desc(self):\n        return \"Run quick benchmark test\"", "is_method": true, "class_name": "Command", "function_description": "Provides a brief description of the Command instance's purpose, indicating it runs a quick benchmark test. Useful for summarizing the command's function in interfaces or logs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "run", "line_number": 22, "body": "def run(self, args, opts):\n        with _BenchServer():\n            self.crawler_process.crawl(_BenchSpider, total=100000)\n            self.crawler_process.start()", "is_method": true, "class_name": "Command", "function_description": "Starts and runs a web crawling process using a benchmark spider to collect a fixed number of items, encapsulated within a benchmarking server context to measure performance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "__enter__", "line_number": 30, "body": "def __enter__(self):\n        from scrapy.utils.test import get_testenv\n        pargs = [sys.executable, '-u', '-m', 'scrapy.utils.benchserver']\n        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE,\n                                     env=get_testenv())\n        self.proc.stdout.readline()", "is_method": true, "class_name": "_BenchServer", "function_description": "Context: _BenchServer appears to manage a benchmarking server process for Scrapy utilities. The __enter__ method initializes and starts this subprocess, preparing the environment and waiting for it to become ready. This enables usage within a context manager to ensure proper setup and teardown of the benchmark server.\n\nDescription:  \nStarts and prepares a subprocess running the Scrapy benchmark server, enabling context-managed execution of benchmarking tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "__exit__", "line_number": 37, "body": "def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.wait()\n        time.sleep(0.2)", "is_method": true, "class_name": "_BenchServer", "function_description": "Ensures the _BenchServer process is terminated and properly cleaned up when exiting a context, providing reliable resource management during benchmarking sessions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "start_requests", "line_number": 51, "body": "def start_requests(self):\n        qargs = {'total': self.total, 'show': self.show}\n        url = f'{self.baseurl}?{urlencode(qargs, doseq=True)}'\n        return [scrapy.Request(url, dont_filter=True)]", "is_method": true, "class_name": "_BenchSpider", "function_description": "Generates and returns an initial web request with query parameters to begin crawling from a specified base URL. It initiates the spider's scraping process by constructing the start URL with pagination or filtering options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/commands/bench.py", "function": "parse", "line_number": 56, "body": "def parse(self, response):\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)", "is_method": true, "class_name": "_BenchSpider", "function_description": "Core method of the _BenchSpider class that recursively extracts and follows links from a web response, enabling comprehensive crawling of linked web pages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/selector/unified.py", "function": "_st", "line_number": 14, "body": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    return st", "is_method": false, "function_description": "Helper function that determines the response format type, defaulting to 'xml' or 'html' based on the response object when no type is explicitly provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/selector/unified.py", "function": "_response_from_text", "line_number": 20, "body": "def _response_from_text(text, st):\n    rt = XmlResponse if st == 'xml' else HtmlResponse\n    return rt(url='about:blank', encoding='utf-8',\n              body=to_bytes(text, 'utf-8'))", "is_method": false, "function_description": "Utility function that generates an HTTP response object with the given text content, choosing XML or HTML format based on the specified response type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "potential_domain_matches", "line_number": 90, "body": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]", "is_method": false, "function_description": "Function that generates possible domain variations for cookie matching, including base and subdomains with optional leading dots, useful for evaluating cookie applicability across related domains."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "extract_cookies", "line_number": 22, "body": "def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method of the CookieJar class that extracts cookies from an HTTP response using the associated request, facilitating cookie management in HTTP interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "add_cookie_header", "line_number": 27, "body": "def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()", "is_method": true, "class_name": "CookieJar", "function_description": "Adds appropriate cookie headers to an HTTP request based on the request's domain, managing cookies and periodically clearing expired ones to maintain valid session state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "_cookies", "line_number": 60, "body": "def _cookies(self):\n        return self.jar._cookies", "is_method": true, "class_name": "CookieJar", "function_description": "Private method of the CookieJar class that accesses and returns the internal cookie storage, providing direct access to the underlying cookies maintained by the jar."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "clear_session_cookies", "line_number": 63, "body": "def clear_session_cookies(self, *args, **kwargs):\n        return self.jar.clear_session_cookies(*args, **kwargs)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method in CookieJar that clears all session cookies from the internal cookie store, facilitating management of temporary cookies typically used during a browsing session."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "clear", "line_number": 66, "body": "def clear(self, domain=None, path=None, name=None):\n        return self.jar.clear(domain, path, name)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method in CookieJar to remove stored cookies selectively or entirely, supporting domain, path, and name filtering for precise cookie management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "__iter__", "line_number": 69, "body": "def __iter__(self):\n        return iter(self.jar)", "is_method": true, "class_name": "CookieJar", "function_description": "Enables iteration over all stored cookies in the CookieJar, allowing other functions to easily access each cookie sequentially."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "__len__", "line_number": 72, "body": "def __len__(self):\n        return len(self.jar)", "is_method": true, "class_name": "CookieJar", "function_description": "Returns the number of cookies currently stored in the CookieJar instance, providing a quick way to check its size."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_policy", "line_number": 75, "body": "def set_policy(self, pol):\n        return self.jar.set_policy(pol)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method in CookieJar that sets the cookie handling policy, allowing customization of how cookies are managed during HTTP interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "make_cookies", "line_number": 78, "body": "def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method in CookieJar that creates cookie objects from HTTP response and request data, facilitating cookie management in HTTP interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_cookie", "line_number": 83, "body": "def set_cookie(self, cookie):\n        self.jar.set_cookie(cookie)", "is_method": true, "class_name": "CookieJar", "function_description": "Utility method of the CookieJar class that adds a cookie to its internal cookie storage for managing HTTP cookie state during requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "set_cookie_if_ok", "line_number": 86, "body": "def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))", "is_method": true, "class_name": "CookieJar", "function_description": "Sets a cookie in the jar if it meets acceptance criteria for the given request, facilitating controlled cookie management within HTTP interactions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_full_url", "line_number": 126, "body": "def get_full_url(self):\n        return self.request.url", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the complete URL of the wrapped request. This function provides easy access to the original request's URL for processing or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_host", "line_number": 129, "body": "def get_host(self):\n        return urlparse_cached(self.request).netloc", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the network location (host) part of the wrapped request's URL, enabling easy access to the target host for request processing or routing purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_type", "line_number": 132, "body": "def get_type(self):\n        return urlparse_cached(self.request).scheme", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the URL scheme (e.g., \"http\" or \"https\") of the wrapped request, enabling identification of the request's protocol type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "is_unverifiable", "line_number": 135, "body": "def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)", "is_method": true, "class_name": "WrappedRequest", "function_description": "Determines if a request is unverifiable per RFC 2965, indicating whether the user had the option to approve it, useful for controlling automatic resource loading or privacy-sensitive operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_origin_req_host", "line_number": 145, "body": "def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the hostname (origin) of the wrapped HTTP request's URL. This method helps identify the source host of the incoming request for processing or routing purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "full_url", "line_number": 150, "body": "def full_url(self):\n        return self.get_full_url()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the complete URL of the current wrapped request, providing easy access to the full resource address within the WrappedRequest context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "host", "line_number": 154, "body": "def host(self):\n        return self.get_host()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the host information associated with the WrappedRequest by invoking an internal method. It serves as a straightforward accessor for obtaining the request's host data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "type", "line_number": 158, "body": "def type(self):\n        return self.get_type()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the type of the wrapped request, providing a convenient way to access the request's classification or category."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "unverifiable", "line_number": 162, "body": "def unverifiable(self):\n        return self.is_unverifiable()", "is_method": true, "class_name": "WrappedRequest", "function_description": "This method provides a direct check to determine if a WrappedRequest is unverifiable by delegating to its internal verification logic. It serves as a simple accessor for unverifiability status in request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "origin_req_host", "line_number": 166, "body": "def origin_req_host(self):\n        return self.get_origin_req_host()", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the original request host of the WrappedRequest instance, providing access to the initial target host information of the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "has_header", "line_number": 169, "body": "def has_header(self, name):\n        return name in self.request.headers", "is_method": true, "class_name": "WrappedRequest", "function_description": "Checks if a specified header exists in the wrapped HTTP request. Useful for quickly verifying presence of headers before accessing their values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_header", "line_number": 172, "body": "def get_header(self, name, default=None):\n        return to_unicode(self.request.headers.get(name, default),\n                          errors='replace')", "is_method": true, "class_name": "WrappedRequest", "function_description": "Returns the specified HTTP request header's value as a Unicode string, providing a default if the header is not present. This method helps safely access headers within WrappedRequest instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "header_items", "line_number": 176, "body": "def header_items(self):\n        return [\n            (to_unicode(k, errors='replace'),\n             [to_unicode(x, errors='replace') for x in v])\n            for k, v in self.request.headers.items()\n        ]", "is_method": true, "class_name": "WrappedRequest", "function_description": "Provides a list of HTTP request headers with their names and values converted to unicode, ensuring consistent and safe string handling for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "add_unredirected_header", "line_number": 183, "body": "def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)", "is_method": true, "class_name": "WrappedRequest", "function_description": "Adds a header with the specified name and value to the request without following HTTP redirects. This enables manipulation of request headers directly for WrappedRequest instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "info", "line_number": 192, "body": "def info(self):\n        return self", "is_method": true, "class_name": "WrappedResponse", "function_description": "Returns the WrappedResponse instance itself, providing direct access to its own data or methods. This can be used to retrieve or inspect the response object contained within the class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/cookies.py", "function": "get_all", "line_number": 195, "body": "def get_all(self, name, default=None):\n        return [to_unicode(v, errors='replace')\n                for v in self.response.headers.getlist(name)]", "is_method": true, "class_name": "WrappedResponse", "function_description": "Provides a list of all header values matching a given name from an HTTP response, decoding them as Unicode. Useful for accessing multiple headers with the same name in web responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "normkey", "line_number": 13, "body": "def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())", "is_method": true, "class_name": "Headers", "function_description": "Normalizes header keys by converting them to title case and byte format, ensuring consistent byte-encoded keys for header management or comparison."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "normvalue", "line_number": 17, "body": "def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (str, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]", "is_method": true, "class_name": "Headers", "function_description": "Utility method of the Headers class that normalizes input into a list of byte strings, ensuring consistent byte-type representation for header values regardless of their initial format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "_tobytes", "line_number": 28, "body": "def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, str):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return str(x).encode(self.encoding)\n        else:\n            raise TypeError(f'Unsupported value type: {type(x)}')", "is_method": true, "class_name": "Headers", "function_description": "Internal utility of the Headers class that converts values to bytes using the specified encoding, supporting bytes, strings, and integers. It ensures consistent byte representation for header processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "__getitem__", "line_number": 38, "body": "def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)[-1]\n        except IndexError:\n            return None", "is_method": true, "class_name": "Headers", "function_description": "Overrides item access in Headers to return the last value associated with a given key, or None if the key is absent. This enables convenient retrieval of header values in a multi-valued header collection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "get", "line_number": 44, "body": "def get(self, key, def_val=None):\n        try:\n            return super().get(key, def_val)[-1]\n        except IndexError:\n            return None", "is_method": true, "class_name": "Headers", "function_description": "Method of the Headers class that retrieves the last value associated with a given key, returning a default or None if the key has no values. Useful for accessing the most recent header entry in multi-valued headers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "getlist", "line_number": 50, "body": "def getlist(self, key, def_val=None):\n        try:\n            return super().__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []", "is_method": true, "class_name": "Headers", "function_description": "Method of the Headers class that returns the list of values for a given header key, or a normalized default value if the key is missing. It simplifies safe access to header values with fallback support."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "setlist", "line_number": 58, "body": "def setlist(self, key, list_):\n        self[key] = list_", "is_method": true, "class_name": "Headers", "function_description": "Utility method in the Headers class that assigns a list to a specified key, enabling straightforward setting of header values stored as lists."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "setlistdefault", "line_number": 61, "body": "def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)", "is_method": true, "class_name": "Headers", "function_description": "Utility method in the Headers class that sets a default list value for a given key if it is not already present, facilitating consistent handling of header entries with default empty or specified lists."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "appendlist", "line_number": 64, "body": "def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst", "is_method": true, "class_name": "Headers", "function_description": "Utility method of the Headers class that appends one or multiple normalized values to the list of values under a specified key, maintaining header consistency and facilitating multi-valued header management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "items", "line_number": 69, "body": "def items(self):\n        return ((k, self.getlist(k)) for k in self.keys())", "is_method": true, "class_name": "Headers", "function_description": "Returns an iterator of header keys paired with their full list of values, providing access to all header entries including those with multiple values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "values", "line_number": 72, "body": "def values(self):\n        return [self[k] for k in self.keys()]", "is_method": true, "class_name": "Headers", "function_description": "Returns a list of all header values contained in the Headers instance, providing easy access to the stored header data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "to_string", "line_number": 75, "body": "def to_string(self):\n        return headers_dict_to_raw(self)", "is_method": true, "class_name": "Headers", "function_description": "Converts the Headers object into its raw string representation, typically for use in HTTP communication or logging. This enables easy transformation of header data into a standard textual format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "to_unicode_dict", "line_number": 78, "body": "def to_unicode_dict(self):\n        \"\"\" Return headers as a CaselessDict with unicode keys\n        and unicode values. Multiple values are joined with ','.\n        \"\"\"\n        return CaselessDict(\n            (to_unicode(key, encoding=self.encoding),\n             to_unicode(b','.join(value), encoding=self.encoding))\n            for key, value in self.items())", "is_method": true, "class_name": "Headers", "function_description": "Converts header keys and values to unicode strings, aggregating multiple values with commas, and returns them in a case-insensitive dictionary to ensure consistent and uniform header representation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/headers.py", "function": "__copy__", "line_number": 87, "body": "def __copy__(self):\n        return self.__class__(self)", "is_method": true, "class_name": "Headers", "function_description": "Creates and returns a shallow copy of the current Headers instance, preserving its existing data for independent use or modification without affecting the original."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/common.py", "function": "obsolete_setter", "line_number": 1, "body": "def obsolete_setter(setter, attrname):\n    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n        raise AttributeError(msg)\n    return newsetter", "is_method": false, "function_description": "Utility function that creates a setter which raises an error indicating an attribute is immutable and directs users to use a replacement method instead. Useful for enforcing immutability and guiding proper attribute modification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/json_request.py", "function": "replace", "line_number": 39, "body": "def replace(self, *args, **kwargs):\n        body_passed = kwargs.get('body', None) is not None\n        data = kwargs.pop('data', None)\n        data_passed = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn('Both body and data passed. data will be ignored')\n\n        elif not body_passed and data_passed:\n            kwargs['body'] = self._dumps(data)\n\n        return super().replace(*args, **kwargs)", "is_method": true, "class_name": "JsonRequest", "function_description": "Method of JsonRequest that creates a modified copy of the request, allowing replacement of the JSON body using either 'body' or 'data' parameters, prioritizing 'body' if both are provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/json_request.py", "function": "_dumps", "line_number": 52, "body": "def _dumps(self, data):\n        \"\"\"Convert to JSON \"\"\"\n        return json.dumps(data, **self._dumps_kwargs)", "is_method": true, "class_name": "JsonRequest", "function_description": "Utility method in JsonRequest that serializes Python data structures into JSON format using preset serialization options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_form_url", "line_number": 61, "body": "def _get_form_url(form, url):\n    if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)", "is_method": false, "function_description": "Utility function that constructs and returns the absolute URL for a web form's submission, resolving relative paths from the form's base URL and handling default action attributes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_urlencode", "line_number": 70, "body": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=True)", "is_method": false, "function_description": "Utility function that converts a sequence of key-value pairs into a URL-encoded query string, handling multiple values per key with proper encoding."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_form", "line_number": 77, "body": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(f\"No <form> element found in {response}\")\n\n    if formname is not None:\n        f = root.xpath(f'//form[@name=\"{formname}\"]')\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath(f'//form[@id=\"{formid}\"]')\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError(f'No <form> element found with {formxpath}')\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(f\"Form number {formnumber} not found in {response}\")\n        else:\n            return form", "is_method": false, "function_description": "Utility function that locates and returns a specific HTML form element from a web response using its name, ID, XPath, or index; it enables precise form extraction for web scraping or interaction tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_inputs", "line_number": 119, "body": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    if not formdata:\n        formdata = ()\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[not(@type) or @type['\n                        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n                        ' and (../@checked or'\n                        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n                        namespaces={\n                            \"re\": \"http://exslt.org/regular-expressions\"})\n    values = [(k, '' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata_keys]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    if isinstance(formdata, dict):\n        formdata = formdata.items()\n\n    values.extend((k, v) for k, v in formdata if v is not None)\n    return values", "is_method": false, "function_description": "Utility function that extracts form input names and values, merging existing form data and click event data to prepare a complete set of inputs for form submission or processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_value", "line_number": 151, "body": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v", "is_method": false, "function_description": "Helper function returning the name and value of a form element, handling special cases for 'select' elements. It supports form data extraction by normalizing different element types."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_select_value", "line_number": 159, "body": "def _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags without options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath('.//option[@selected]')\n        v = [(o.get('value') or o.text or '').strip() for o in selected_options]\n    return n, v", "is_method": false, "function_description": "Utility function that determines and returns the appropriate selected value(s) from an HTML select element, handling both single and multiple selection cases including specific edge conditions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "_get_clickable", "line_number": 174, "body": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = list(form.xpath(\n        'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n        '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n        namespaces={\"re\": \"http://exslt.org/regular-expressions\"}\n    ))\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get('name'), el.get('value') or '')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get('name'), el.get('value') or '')\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = './/*' + ''.join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get('name'), el[0].get('value') or '')\n    elif len(el) > 1:\n        raise ValueError(f\"Multiple elements found ({el!r}) matching the \"\n                         f\"criteria in clickdata: {clickdata!r}\")\n    else:\n        raise ValueError(f'No clickable element matching clickdata: {clickdata!r}')", "is_method": false, "function_description": "Utility function that identifies and returns the name and value of a clickable form element based on given click data or defaults to the first clickable element if no data is provided. It supports precise element selection by index or attribute matching."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/form.py", "function": "from_response", "line_number": 39, "body": "def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n\n        kwargs.setdefault('encoding', response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n\n        method = kwargs.pop('method', form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = 'GET'\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)", "is_method": true, "class_name": "FormRequest", "function_description": "Creates a FormRequest instance by extracting form data and attributes from an HTTP response, supporting form identification via name, id, CSS, or XPath selectors. Useful for automating web form submissions with customizable parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "cb_kwargs", "line_number": 47, "body": "def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs", "is_method": true, "class_name": "Request", "function_description": "Utility method of the Request class that provides access to a dictionary of callback keyword arguments, initializing it if not already set. It supports managing callback parameters within request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "meta", "line_number": 53, "body": "def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta", "is_method": true, "class_name": "Request", "function_description": "Provides access to the Request object's metadata dictionary, initializing it if unset. This supports storing and retrieving arbitrary metadata related to the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_get_url", "line_number": 58, "body": "def _get_url(self):\n        return self._url", "is_method": true, "class_name": "Request", "function_description": "Private method of the Request class that returns the current URL associated with the request. It provides access to the stored URL value for internal use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_set_url", "line_number": 61, "body": "def _set_url(self, url):\n        if not isinstance(url, str):\n            raise TypeError(f'Request url must be str or unicode, got {type(url).__name__}')\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if (\n            '://' not in self._url\n            and not self._url.startswith('about:')\n            and not self._url.startswith('data:')\n        ):\n            raise ValueError(f'Missing scheme in request url: {self._url}')", "is_method": true, "class_name": "Request", "function_description": "Sets and validates the request URL, ensuring it is a properly encoded string with a valid scheme for HTTP requests. This method helps maintain URL integrity in the Request class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_get_body", "line_number": 77, "body": "def _get_body(self):\n        return self._body", "is_method": true, "class_name": "Request", "function_description": "Returns the stored body content of the Request instance. This method provides access to the raw data payload associated with the request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "_set_body", "line_number": 80, "body": "def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)", "is_method": true, "class_name": "Request", "function_description": "Private method in the Request class that sets the request body, encoding it to bytes or assigning an empty byte string if no body is provided."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "encoding", "line_number": 89, "body": "def encoding(self):\n        return self._encoding", "is_method": true, "class_name": "Request", "function_description": "Returns the encoding format used in the Request, providing access to the character encoding of the request data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "__str__", "line_number": 92, "body": "def __str__(self):\n        return f\"<{self.method} {self.url}>\"", "is_method": true, "class_name": "Request", "function_description": "Returns a concise string representation of the Request instance showing its HTTP method and URL. This aids in debugging and logging HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "copy", "line_number": 97, "body": "def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()", "is_method": true, "class_name": "Request", "function_description": "Returns a duplicate of the current Request object, providing an identical but separate instance for independent use or modification."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "replace", "line_number": 101, "body": "def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)", "is_method": true, "class_name": "Request", "function_description": "Creates a new Request object by copying the current one's attributes with optional overrides, enabling modification of specific fields while preserving others for flexible request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/request/__init__.py", "function": "from_curl", "line_number": 112, "body": "def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)", "is_method": true, "class_name": "Request", "function_description": "Creates a Request object by parsing a cURL command string, extracting HTTP method, URL, headers, cookies, and body, while allowing overrides and optional error handling for unknown options. Ideal for converting cURL commands into Request instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "cb_kwargs", "line_number": 43, "body": "def cb_kwargs(self):\n        try:\n            return self.request.cb_kwargs\n        except AttributeError:\n            raise AttributeError(\n                \"Response.cb_kwargs not available, this response \"\n                \"is not tied to any request\"\n            )", "is_method": true, "class_name": "Response", "function_description": "Provides access to callback keyword arguments associated with the response's request; raises an error if the response is not linked to any request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "meta", "line_number": 53, "body": "def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )", "is_method": true, "class_name": "Response", "function_description": "Returns the metadata associated with the request tied to this Response object, raising an error if no such request exists. This method provides access to request-specific information stored within a Response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_get_url", "line_number": 62, "body": "def _get_url(self):\n        return self._url", "is_method": true, "class_name": "Response", "function_description": "Private method of the Response class that returns the stored URL associated with the response instance. It provides access to the response's source URL for internal or debugging use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_set_url", "line_number": 65, "body": "def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError(f'{type(self).__name__} url must be str, '\n                            f'got {type(url).__name__}')", "is_method": true, "class_name": "Response", "function_description": "Sets the URL attribute of the Response object, enforcing that the provided URL is a string. Raises an error if the input type is invalid to ensure data consistency."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_get_body", "line_number": 74, "body": "def _get_body(self):\n        return self._body", "is_method": true, "class_name": "Response", "function_description": "Returns the body content stored within the Response object. This method provides direct access to the response's main data payload."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "_set_body", "line_number": 77, "body": "def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body", "is_method": true, "class_name": "Response", "function_description": "Internal method of the Response class that sets the response body ensuring it is bytes, raising an error otherwise, and defaulting to an empty byte string when None is provided. It enforces correct body type for HTTP responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "__str__", "line_number": 90, "body": "def __str__(self):\n        return f\"<{self.status} {self.url}>\"", "is_method": true, "class_name": "Response", "function_description": "String representation method of the Response class that returns a concise summary of the response's status and URL. It provides a human-readable identifier for easy debugging and logging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "copy", "line_number": 95, "body": "def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()", "is_method": true, "class_name": "Response", "function_description": "Method of the Response class that returns a duplicate of the current Response object, facilitating reuse or modification without altering the original instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "replace", "line_number": 99, "body": "def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in [\n            \"url\", \"status\", \"headers\", \"body\", \"request\", \"flags\", \"certificate\", \"ip_address\", \"protocol\",\n        ]:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)", "is_method": true, "class_name": "Response", "function_description": "Creates a new Response object copying all attributes from the current one, except for any explicitly provided to be replaced. Useful for modifying specific parts of a response while preserving others."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "urljoin", "line_number": 110, "body": "def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)", "is_method": true, "class_name": "Response", "function_description": "Method of the Response class that combines the instance's base URL with a relative URL to produce an absolute URL, facilitating consistent URL resolution in HTTP responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "text", "line_number": 116, "body": "def text(self):\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as str\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "This method signals that the response content is not text by raising an error, serving as a placeholder for subclasses to override when returning text content. It enforces text content handling in response subclasses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "css", "line_number": 122, "body": "def css(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "Raises an error for non-text responses, indicating that CSS manipulation is unsupported. It enforces that only text-based Response subclasses provide CSS-related functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "xpath", "line_number": 128, "body": "def xpath(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")", "is_method": true, "class_name": "Response", "function_description": "This method indicates that XPath queries are unsupported for non-text response content by raising an error, serving as a placeholder to restrict such operations in the Response class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "follow", "line_number": 134, "body": "def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding='utf-8', priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n\n        .. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n\n        return Request(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "Response", "function_description": "Creates and returns a new Request object to follow a given URL, supporting relative URLs and Link objects. It facilitates generating follow-up requests in web crawling workflows with customizable request parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/__init__.py", "function": "follow_all", "line_number": 173, "body": "def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n                   cookies=None, meta=None, encoding='utf-8', priority=0,\n                   dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Generator[Request, None, None]\n        \"\"\"\n        .. versionadded:: 2.0\n\n        Return an iterable of :class:`~.Request` instances to follow all links\n        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n        not only absolute URLs.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if not hasattr(urls, '__iter__'):\n            raise TypeError(\"'urls' argument must be an iterable\")\n        return (\n            self.follow(\n                url=url,\n                callback=callback,\n                method=method,\n                headers=headers,\n                body=body,\n                cookies=cookies,\n                meta=meta,\n                encoding=encoding,\n                priority=priority,\n                dont_filter=dont_filter,\n                errback=errback,\n                cb_kwargs=cb_kwargs,\n                flags=flags,\n            )\n            for url in urls\n        )", "is_method": true, "class_name": "Response", "function_description": "Service method of the Response class that generates Request objects to follow multiple URLs at once, supporting various request options and flexible URL input types for efficient web crawling or scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_url_from_selector", "line_number": 252, "body": "def _url_from_selector(sel):\n    # type: (parsel.Selector) -> str\n    if isinstance(sel.root, str):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, 'tag'):\n        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n    if sel.root.tag not in ('a', 'link'):\n        raise _InvalidSelector(\"Only <a> and <link> elements are supported; \"\n                               f\"got <{sel.root.tag}>\")\n    href = sel.root.get('href')\n    if href is None:\n        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n    return strip_html5_whitespace(href)", "is_method": false, "function_description": "Utility function that extracts and returns the cleaned href URL from an HTML <a> or <link> element within a selector, ensuring valid element types and attributes. It supports URL retrieval in web-scraping or parsing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_set_url", "line_number": 40, "body": "def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = to_unicode(url, self.encoding)\n        else:\n            super()._set_url(url)", "is_method": true, "class_name": "TextResponse", "function_description": "Sets the URL for the TextResponse object, ensuring it is stored as a Unicode string if given as a regular string, otherwise delegating to the parent class method. This supports consistent URL handling within the response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_set_body", "line_number": 46, "body": "def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, str):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - '\n                                f'{type(self).__name__} has no encoding')\n            self._body = body.encode(self._encoding)\n        else:\n            super()._set_body(body)", "is_method": true, "class_name": "TextResponse", "function_description": "Sets the internal body content of a TextResponse, encoding string input to bytes using the specified encoding. It ensures the body is stored in a consistent byte format for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "replace", "line_number": 56, "body": "def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)", "is_method": true, "class_name": "TextResponse", "function_description": "Utility method in TextResponse that substitutes the default encoding into replacement operations, ensuring consistent text encoding when creating modified response objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "encoding", "line_number": 61, "body": "def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()", "is_method": true, "class_name": "TextResponse", "function_description": "Returns the character encoding used for the text response, choosing a declared encoding if available, otherwise inferring it from the response body content. This method helps determine how to correctly decode the response data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_declared_encoding", "line_number": 64, "body": "def _declared_encoding(self):\n        return (\n            self._encoding\n            or self._headers_encoding()\n            or self._body_declared_encoding()\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Internal method of TextResponse that determines the text encoding by checking explicit settings, response headers, or declared body encoding to accurately interpret response content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "body_as_unicode", "line_number": 71, "body": "def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        warnings.warn('Response.body_as_unicode() is deprecated, '\n                      'please use Response.text instead.',\n                      ScrapyDeprecationWarning, stacklevel=2)\n        return self.text", "is_method": true, "class_name": "TextResponse", "function_description": "Returns the response body decoded as a Unicode string, but is deprecated in favor of accessing the `.text` property directly. It provides backward compatibility for text retrieval from a response."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "json", "line_number": 78, "body": "def json(self):\n        \"\"\"\n        .. versionadded:: 2.2\n\n        Deserialize a JSON document to a Python object.\n        \"\"\"\n        if self._cached_decoded_json is _NONE:\n            self._cached_decoded_json = json.loads(self.text)\n        return self._cached_decoded_json", "is_method": true, "class_name": "TextResponse", "function_description": "Provides a cached deserialization of the text content into a Python object, enabling efficient repeated access to the JSON representation within the TextResponse class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "text", "line_number": 89, "body": "def text(self):\n        \"\"\" Body as unicode \"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = f'charset={benc}'\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody", "is_method": true, "class_name": "TextResponse", "function_description": "Returns the body content of a TextResponse as a Unicode string, ensuring correct character encoding is applied and caching the result for efficient repeated access."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "urljoin", "line_number": 99, "body": "def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)", "is_method": true, "class_name": "TextResponse", "function_description": "Utility method of TextResponse that combines its base URL with a relative URL to produce a complete absolute URL for consistent URL resolution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_headers_encoding", "line_number": 105, "body": "def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_unicode(content_type))", "is_method": true, "class_name": "TextResponse", "function_description": "Private method in TextResponse that determines the character encoding from the response's Content-Type header, facilitating correct text decoding based on header information."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_body_inferred_encoding", "line_number": 109, "body": "def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_unicode(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                                          auto_detect_fun=self._auto_detect_fun,\n                                          default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc", "is_method": true, "class_name": "TextResponse", "function_description": "Provides the inferred character encoding of the response body, caching the result for efficient repeated access. This helps in accurately interpreting the body content based on headers or auto-detection."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_auto_detect_fun", "line_number": 119, "body": "def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)", "is_method": true, "class_name": "TextResponse", "function_description": "Private method in TextResponse that attempts to identify the correct encoding of given text by testing multiple encodings and resolving the first one that succeeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "_body_declared_encoding", "line_number": 128, "body": "def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)", "is_method": true, "class_name": "TextResponse", "function_description": "Returns the character encoding declared in the HTML body of the text response, facilitating correct interpretation of the response content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "selector", "line_number": 132, "body": "def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector", "is_method": true, "class_name": "TextResponse", "function_description": "Provides a cached Scrapy Selector instance for the TextResponse, enabling efficient parsing and querying of the response content without repeated reinitialization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "xpath", "line_number": 138, "body": "def xpath(self, query, **kwargs):\n        return self.selector.xpath(query, **kwargs)", "is_method": true, "class_name": "TextResponse", "function_description": "Utility method of the TextResponse class that performs an XPath query on the underlying selector to extract data from structured text documents like HTML or XML."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "css", "line_number": 141, "body": "def css(self, query):\n        return self.selector.css(query)", "is_method": true, "class_name": "TextResponse", "function_description": "Utility method in TextResponse that applies a CSS selector query to extract elements from the response content. It facilitates easy access to specific parts of the text using CSS syntax."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "follow", "line_number": 144, "body": "def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding=None, priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be not only an absolute URL, but also\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super().follow(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Creates and returns a Request object to follow a link from diverse URL representations, supporting relative URLs, selectors, and link objects. It facilitates web crawling by enabling seamless navigation from a response to subsequent requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/http/response/text.py", "function": "follow_all", "line_number": 185, "body": "def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n                   cookies=None, meta=None, encoding=None, priority=0,\n                   dont_filter=False, errback=None, cb_kwargs=None, flags=None,\n                   css=None, xpath=None):\n        # type: (...) -> Generator[Request, None, None]\n        \"\"\"\n        A generator that produces :class:`~.Request` instances to follow all\n        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n        ``__init__`` method, except that each ``urls`` element does not need to be\n        an absolute URL, it can be any of the following:\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n\n        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n        selectors from which links cannot be obtained (for instance, anchor tags without an\n        ``href`` attribute)\n        \"\"\"\n        arguments = [x for x in (urls, css, xpath) if x is not None]\n        if len(arguments) != 1:\n            raise ValueError(\n                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n            )\n        if not urls:\n            if css:\n                urls = self.css(css)\n            if xpath:\n                urls = self.xpath(xpath)\n        if isinstance(urls, parsel.SelectorList):\n            selectors = urls\n            urls = []\n            for sel in selectors:\n                with suppress(_InvalidSelector):\n                    urls.append(_url_from_selector(sel))\n        return super().follow_all(\n            urls=urls,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )", "is_method": true, "class_name": "TextResponse", "function_description": "Utility method of TextResponse that generates requests to follow all given links, accepting various link formats or selectors, enabling flexible multi-link navigation in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_matches", "line_number": 50, "body": "def _matches(url, regexs):\n    return any(r.search(url) for r in regexs)", "is_method": false, "function_description": "Utility function that checks if a given URL matches any pattern from a list of regular expressions, enabling quick pattern-based URL filtering or validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_is_valid_url", "line_number": 54, "body": "def _is_valid_url(url):\n    return url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}", "is_method": false, "function_description": "Utility function that checks if a URL uses one of the accepted schemes (http, https, file, ftp), enabling validation of URLs before further processing or network operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "__new__", "line_number": 62, "body": "def __new__(cls, *args, **kwargs):\n        from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n        if issubclass(cls, FilteringLinkExtractor) and not issubclass(cls, LxmlLinkExtractor):\n            warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                 'please use scrapy.linkextractors.LinkExtractor instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super().__new__(cls)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Constructor override that issues a deprecation warning when FilteringLinkExtractor is instantiated improperly, guiding users to use the updated LinkExtractor class instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_link_allowed", "line_number": 94, "body": "def _link_allowed(self, link):\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n            return False\n        if self.restrict_text and not _matches(link.text, self.restrict_text):\n            return False\n        return True", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Determines if a given link meets multiple filtering criteria like URL validity, allowed or denied URL patterns, domains, file extensions, and link text. It helps ensure only links matching specified rules are accepted for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "matches", "line_number": 112, "body": "def matches(self, url):\n\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Method of FilteringLinkExtractor that determines whether a given URL is allowed based on domain and regex inclusion or exclusion rules, supporting precise filtering in web crawling or link extraction tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_process_links", "line_number": 123, "body": "def _process_links(self, links):\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        links = self.link_extractor._process_links(links)\n        return links", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Internal method of FilteringLinkExtractor that filters and optionally canonicalizes URLs before further processing, ensuring only allowed links are handled for downstream extraction tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/__init__.py", "function": "_extract_links", "line_number": 131, "body": "def _extract_links(self, *args, **kwargs):\n        return self.link_extractor._extract_links(*args, **kwargs)", "is_method": true, "class_name": "FilteringLinkExtractor", "function_description": "Delegates link extraction tasks to an internal link extractor, serving as a pass-through method to retrieve links based on provided arguments within the FilteringLinkExtractor class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_nons", "line_number": 25, "body": "def _nons(tag):\n    if isinstance(tag, str):\n        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n            return tag.split('}')[-1]\n    return tag", "is_method": false, "function_description": "Returns the local name of an XML tag by stripping the XHTML namespace if present, enabling simpler tag processing in XML or HTML parsing tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_identity", "line_number": 32, "body": "def _identity(x):\n    return x", "is_method": false, "function_description": "Returns the input argument unchanged. This utility function serves as a default or placeholder transformation in data processing pipelines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_canonicalize_link_url", "line_number": 36, "body": "def _canonicalize_link_url(link):\n    return canonicalize_url(link.url, keep_fragments=True)", "is_method": false, "function_description": "This function standardizes a link's URL while preserving its fragment component, ensuring consistent URL formatting for tasks like link comparison or processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_iter_links", "line_number": 51, "body": "def _iter_links(self, document):\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield (el, attrib, attribs[attrib])", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Internal method of LxmlParserLinkExtractor that iterates through document elements, yielding those whose tags and attributes pass specific scanning criteria for link extraction purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_extract_links", "line_number": 61, "body": "def _extract_links(self, selector, response_url, response_encoding, base_url):\n        links = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            url = safe_url_string(url, encoding=response_encoding)\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(url, _collect_string_content(el) or '',\n                        nofollow=rel_has_nofollow(el.get('rel')))\n            links.append(link)\n        return self._deduplicate_if_needed(links)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Extracts and returns processed, absolute links from a given HTML selector, normalizing URLs and handling nofollow attributes. It supports link cleaning and deduplication for reliable link collection in web scraping contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "extract_links", "line_number": 84, "body": "def extract_links(self, response):\n        base_url = get_base_url(response)\n        return self._extract_links(response.selector, response.url, response.encoding, base_url)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Utility method of LxmlParserLinkExtractor that extracts all links from a response using its selector and URL context, facilitating link retrieval for web scraping or crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_process_links", "line_number": 88, "body": "def _process_links(self, links):\n        \"\"\" Normalize and filter extracted links\n\n        The subclass should override it if neccessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Internal method of LxmlParserLinkExtractor that normalizes and filters extracted links by deduplication, intended to be overridden for customized link processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "_deduplicate_if_needed", "line_number": 95, "body": "def _deduplicate_if_needed(self, links):\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links", "is_method": true, "class_name": "LxmlParserLinkExtractor", "function_description": "Helper method in LxmlParserLinkExtractor that returns a deduplicated list of links if uniqueness is required; otherwise, it returns the original list unchanged."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/linkextractors/lxmlhtml.py", "function": "extract_links", "line_number": 142, "body": "def extract_links(self, response):\n        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n        specified :class:`response <scrapy.http.Response>`.\n\n        Only links that match the settings passed to the ``__init__`` method of\n        the link extractor are returned.\n\n        Duplicate links are omitted.\n        \"\"\"\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [\n                subdoc\n                for x in self.restrict_xpaths\n                for subdoc in response.xpath(x)\n            ]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        return unique_list(all_links)", "is_method": true, "class_name": "LxmlLinkExtractor", "function_description": "Extracts and returns a deduplicated list of links from a web response that meet specified filtering criteria, facilitating targeted link extraction for web crawling or scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_identity", "line_number": 17, "body": "def _identity(x):\n    return x", "is_method": false, "function_description": "Simple function that returns its input unchanged, often used as a default or placeholder operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_get_method", "line_number": 25, "body": "def _get_method(method, spider):\n    if callable(method):\n        return method\n    elif isinstance(method, str):\n        return getattr(spider, method, None)", "is_method": false, "function_description": "Utility function that resolves a callable method from either a direct function reference or the name of a method within a spider object, facilitating dynamic method retrieval."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_compile", "line_number": 55, "body": "def _compile(self, spider):\n        self.callback = _get_method(self.callback, spider)\n        self.errback = _get_method(self.errback, spider)\n        self.process_links = _get_method(self.process_links, spider)\n        self.process_request = _get_method(self.process_request, spider)", "is_method": true, "class_name": "Rule", "function_description": "Private method of the Rule class that resolves and binds its callback-related attributes to methods from a given spider, preparing the rule for execution within that spider's context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_parse", "line_number": 70, "body": "def _parse(self, response, **kwargs):\n        return self._parse_response(\n            response=response,\n            callback=self.parse_start_url,\n            cb_kwargs=kwargs,\n            follow=True,\n        )", "is_method": true, "class_name": "CrawlSpider", "function_description": "Internal helper method of CrawlSpider that initiates parsing of a response by delegating to a callback, starting the spider's crawl from the initial URL and handling follow-up requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "process_results", "line_number": 81, "body": "def process_results(self, response, results):\n        return results", "is_method": true, "class_name": "CrawlSpider", "function_description": "Returns the given crawling results as-is without modification. This method serves as a placeholder or pass-through for processing scraped data in the CrawlSpider context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_build_request", "line_number": 84, "body": "def _build_request(self, rule_index, link):\n        return Request(\n            url=link.url,\n            callback=self._callback,\n            errback=self._errback,\n            meta=dict(rule=rule_index, link_text=link.text),\n        )", "is_method": true, "class_name": "CrawlSpider", "function_description": "Helper method in CrawlSpider that constructs a Request object for a given link, associating it with a specific crawling rule and providing callbacks for handling responses or errors during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_requests_to_follow", "line_number": 92, "body": "def _requests_to_follow(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        seen = set()\n        for rule_index, rule in enumerate(self._rules):\n            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                     if lnk not in seen]\n            for link in rule.process_links(links):\n                seen.add(link)\n                request = self._build_request(rule_index, link)\n                yield rule.process_request(request, response)", "is_method": true, "class_name": "CrawlSpider", "function_description": "Core method of CrawlSpider that generates follow-up requests by extracting and processing links from an HTML response according to defined crawling rules, enabling automated and rule-based web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_callback", "line_number": 104, "body": "def _callback(self, response):\n        rule = self._rules[response.meta['rule']]\n        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)", "is_method": true, "class_name": "CrawlSpider", "function_description": "Internal handler in CrawlSpider that processes responses using the associated rule's callback and parameters, facilitating rule-based navigation and parsing during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_errback", "line_number": 108, "body": "def _errback(self, failure):\n        rule = self._rules[failure.request.meta['rule']]\n        return self._handle_failure(failure, rule.errback)", "is_method": true, "class_name": "CrawlSpider", "function_description": "Handles request failures by invoking the specific error callback defined for the crawling rule associated with the failed request."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_parse_response", "line_number": 112, "body": "def _parse_response(self, response, callback, cb_kwargs, follow=True):\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            cb_res = self.process_results(response, cb_res)\n            for request_or_item in iterate_spider_output(cb_res):\n                yield request_or_item\n\n        if follow and self._follow_links:\n            for request_or_item in self._requests_to_follow(response):\n                yield request_or_item", "is_method": true, "class_name": "CrawlSpider", "function_description": "Internal method of CrawlSpider that processes a response by invoking a callback and yielding results, and optionally generates follow-up requests based on extracted links to enable recursive crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_handle_failure", "line_number": 123, "body": "def _handle_failure(self, failure, errback):\n        if errback:\n            results = errback(failure) or ()\n            for request_or_item in iterate_spider_output(results):\n                yield request_or_item", "is_method": true, "class_name": "CrawlSpider", "function_description": "Internal method of CrawlSpider that processes failure cases by invoking an error callback and yielding its results, enabling customized error handling and continuation of crawling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "_compile_rules", "line_number": 129, "body": "def _compile_rules(self):\n        self._rules = []\n        for rule in self.rules:\n            self._rules.append(copy.copy(rule))\n            self._rules[-1]._compile(self)", "is_method": true, "class_name": "CrawlSpider", "function_description": "Internal method of the CrawlSpider class that duplicates and compiles its crawling rules for internal use, preparing them to control spider behavior during the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/crawl.py", "function": "from_crawler", "line_number": 136, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n        return spider", "is_method": true, "class_name": "CrawlSpider", "function_description": "Creates and returns a CrawlSpider instance configured to follow links based on crawler settings, enabling customized crawling behavior in web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "process_results", "line_number": 28, "body": "def process_results(self, response, results):\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (items or requests).\n        \"\"\"\n        return results", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Core method in XMLFeedSpider that processes results generated from a response, allowing post-processing (like setting identifiers) before passing them to the framework. It supports customizing how extracted items or requests are finalized."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "adapt_response", "line_number": 38, "body": "def adapt_response(self, response):\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Allows customization of the feed response before parsing by enabling modifications or preprocessing steps. It serves as a hook for altering the input data within XMLFeedSpider workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_node", "line_number": 45, "body": "def parse_node(self, response, selector):\n        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n        if hasattr(self, 'parse_item'):  # backward compatibility\n            return self.parse_item(response, selector)\n        raise NotImplementedError", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "This placeholder method in XMLFeedSpider defines the interface for parsing nodes from a response and requires custom implementation in subclasses for specific extraction logic."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_nodes", "line_number": 51, "body": "def parse_nodes(self, response, nodes):\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either an item, a request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            for result_item in self.process_results(response, ret):\n                yield result_item", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Core method of XMLFeedSpider that processes matched XML nodes by parsing each node and yielding extracted items or requests, serving as a customizable entry point for handling XML feed data during crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_parse", "line_number": 64, "body": "def _parse(self, response, **kwargs):\n        if not hasattr(self, 'parse_node'):\n            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n\n        response = self.adapt_response(response)\n        if self.iterator == 'iternodes':\n            nodes = self._iternodes(response)\n        elif self.iterator == 'xml':\n            selector = Selector(response, type='xml')\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f'//{self.itertag}')\n        elif self.iterator == 'html':\n            selector = Selector(response, type='html')\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f'//{self.itertag}')\n        else:\n            raise NotSupported('Unsupported node iterator')\n\n        return self.parse_nodes(response, nodes)", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Core method of the XMLFeedSpider class that processes an XML or HTML response to extract nodes based on a configured iterator type, then passes these nodes for further parsing. It enables flexible scraping of structured feed data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_iternodes", "line_number": 84, "body": "def _iternodes(self, response):\n        for node in xmliter(response, self.itertag):\n            self._register_namespaces(node)\n            yield node", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Yields XML nodes matching a specified tag from a response, while registering their namespaces. It supports iterative processing of XML feeds within the XMLFeedSpider class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_register_namespaces", "line_number": 89, "body": "def _register_namespaces(self, selector):\n        for (prefix, uri) in self.namespaces:\n            selector.register_namespace(prefix, uri)", "is_method": true, "class_name": "XMLFeedSpider", "function_description": "Registers all predefined XML namespaces with a given selector to enable accurate XML parsing within the XMLFeedSpider class. This facilitates proper handling of XML elements and attributes using namespace prefixes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "process_results", "line_number": 107, "body": "def process_results(self, response, results):\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return results", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "This method returns the given results without modification and serves as a placeholder to maintain interface consistency with XMLFeedSpider in CSVFeedSpider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "adapt_response", "line_number": 111, "body": "def adapt_response(self, response):\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return response", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "The method returns the response unchanged, serving as a direct passthrough likely intended for compatibility or override purposes in CSVFeedSpider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_row", "line_number": 115, "body": "def parse_row(self, response, row):\n        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n        raise NotImplementedError", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "This placeholder method in CSVFeedSpider must be overridden to define custom parsing logic for each CSV row during crawling. It serves as the primary extension point for processing row data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "parse_rows", "line_number": 119, "body": "def parse_rows(self, response):\n        \"\"\"Receives a response and a dict (representing each row) with a key for\n        each provided (or detected) header of the CSV file.  This spider also\n        gives the opportunity to override adapt_response and\n        process_results methods for pre and post-processing purposes.\n        \"\"\"\n\n        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n            ret = iterate_spider_output(self.parse_row(response, row))\n            for result_item in self.process_results(response, ret):\n                yield result_item", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "Processes a CSV response by iterating over each row and yielding processed results. It supports customizable pre- and post-processing steps for flexible CSV data extraction within the CSVFeedSpider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/feed.py", "function": "_parse", "line_number": 131, "body": "def _parse(self, response, **kwargs):\n        if not hasattr(self, 'parse_row'):\n            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n        response = self.adapt_response(response)\n        return self.parse_rows(response)", "is_method": true, "class_name": "CSVFeedSpider", "function_description": "Core method of CSVFeedSpider that adapts a response and invokes row parsing, enforcing the presence of a user-defined parse_row method for processing CSV feed data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "logger", "line_number": 35, "body": "def logger(self):\n        logger = logging.getLogger(self.name)\n        return logging.LoggerAdapter(logger, {'spider': self})", "is_method": true, "class_name": "Spider", "function_description": "Provides a configured logger adapter for the Spider instance that tags logs with the spider's identity, facilitating contextualized logging within spider operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "log", "line_number": 39, "body": "def log(self, message, level=logging.DEBUG, **kw):\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)", "is_method": true, "class_name": "Spider", "function_description": "Utility method in Spider that logs messages at specified levels using the spider's internal logger, facilitating consistent and configurable logging throughout the spider's operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "from_crawler", "line_number": 49, "body": "def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider", "is_method": true, "class_name": "Spider", "function_description": "Factory method that creates and initializes a Spider instance with a given crawler, linking the spider to the crawler's context for managing web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "_set_crawler", "line_number": 54, "body": "def _set_crawler(self, crawler):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)", "is_method": true, "class_name": "Spider", "function_description": "Private method of the Spider class that assigns a crawler instance to the spider and connects its closure handler to the spider_closed signal for cleanup or final processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "start_requests", "line_number": 59, "body": "def start_requests(self):\n        cls = self.__class__\n        if not self.start_urls and hasattr(self, 'start_url'):\n            raise AttributeError(\n                \"Crawling could not start: 'start_urls' not found \"\n                \"or empty (but found 'start_url' attribute instead, \"\n                \"did you miss an 's'?)\")\n        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n            warnings.warn(\n                \"Spider.make_requests_from_url method is deprecated; it \"\n                \"won't be called in future Scrapy releases. Please \"\n                \"override Spider.start_requests method instead \"\n                f\"(see {cls.__module__}.{cls.__name__}).\",\n            )\n            for url in self.start_urls:\n                yield self.make_requests_from_url(url)\n        else:\n            for url in self.start_urls:\n                yield Request(url, dont_filter=True)", "is_method": true, "class_name": "Spider", "function_description": "Initiates the crawling process by generating start requests from a spider's configured URLs, handling compatibility with deprecated methods and ensuring proper URL setups before yielding requests to the scraping engine."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "make_requests_from_url", "line_number": 79, "body": "def make_requests_from_url(self, url):\n        \"\"\" This method is deprecated. \"\"\"\n        warnings.warn(\n            \"Spider.make_requests_from_url method is deprecated: \"\n            \"it will be removed and not be called by the default \"\n            \"Spider.start_requests method in future Scrapy releases. \"\n            \"Please override Spider.start_requests method instead.\"\n        )\n        return Request(url, dont_filter=True)", "is_method": true, "class_name": "Spider", "function_description": "Returns a web request for the given URL but is deprecated and should be replaced by overriding the start_requests method instead."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "_parse", "line_number": 89, "body": "def _parse(self, response, **kwargs):\n        return self.parse(response, **kwargs)", "is_method": true, "class_name": "Spider", "function_description": "This internal Spider method delegates response processing to the public parse method, enabling flexible response handling in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "update_settings", "line_number": 96, "body": "def update_settings(cls, settings):\n        settings.setdict(cls.custom_settings or {}, priority='spider')", "is_method": true, "class_name": "Spider", "function_description": "Updates the provided settings with the spider's custom configurations, ensuring that spider-specific priorities override existing ones. This supports flexible and prioritized configuration management within spider instances."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "handles_request", "line_number": 100, "body": "def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)", "is_method": true, "class_name": "Spider", "function_description": "Determines if a given request's URL falls under the Spider class's domain or scope. This method helps route requests to the appropriate spider handler based on URL matching."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "close", "line_number": 104, "body": "def close(spider, reason):\n        closed = getattr(spider, 'closed', None)\n        if callable(closed):\n            return closed(reason)", "is_method": true, "class_name": "Spider", "function_description": "Method of the Spider class that triggers the spider's custom close callback with a given reason, enabling cleanup or finalization actions upon spider shutdown."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/__init__.py", "function": "__str__", "line_number": 109, "body": "def __str__(self):\n        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"", "is_method": true, "class_name": "Spider", "function_description": "Returns a readable string representation of the Spider instance, including its class name, assigned name, and memory address. This aids in debugging and logging by providing clear identification of Spider objects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "regex", "line_number": 87, "body": "def regex(x):\n    if isinstance(x, str):\n        return re.compile(x)\n    return x", "is_method": false, "function_description": "This function ensures the input is a compiled regular expression, compiling it if given as a string; it standardizes inputs for regex processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "iterloc", "line_number": 93, "body": "def iterloc(it, alt=False):\n    for d in it:\n        yield d['loc']\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and 'alternate' in d:\n            yield from d['alternate']", "is_method": false, "function_description": "Utility function that iterates over a collection of dictionaries, yielding the 'loc' values and, optionally, any alternate URLs present, facilitating extraction of primary and alternate locations from structured link data."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "start_requests", "line_number": 29, "body": "def start_requests(self):\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)", "is_method": true, "class_name": "SitemapSpider", "function_description": "Generates initial HTTP requests for each sitemap URL, initiating the sitemap crawling process. This method enables the spider to begin retrieving and parsing sitemap data for web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "sitemap_filter", "line_number": 33, "body": "def sitemap_filter(self, entries):\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        for entry in entries:\n            yield entry", "is_method": true, "class_name": "SitemapSpider", "function_description": "Method of SitemapSpider that provides a mechanism to filter sitemap entries based on their attributes, enabling selective processing of URLs, such as filtering by modification date."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "_parse_sitemap", "line_number": 41, "body": "def _parse_sitemap(self, response):\n        if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n                               {'response': response}, extra={'spider': self})\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == 'sitemapindex':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == 'urlset':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break", "is_method": true, "class_name": "SitemapSpider", "function_description": "Parses sitemap or robots.txt responses to recursively follow and extract URLs based on defined filters and callbacks. It enables automated traversal and processing of sitemap links for web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/sitemap.py", "function": "_get_sitemap_body", "line_number": 66, "body": "def _get_sitemap_body(self, response):\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        elif gzip_magic_number(response):\n            return gunzip(response.body)\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n            return response.body", "is_method": true, "class_name": "SitemapSpider", "function_description": "Extracts and returns the raw sitemap XML content from a web response, handling decompression if the sitemap is gzip-compressed. Useful for sitemap parsing workflows in web crawling and scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "start_requests", "line_number": 8, "body": "def start_requests(self):\n        self._postinit_reqs = super().start_requests()\n        return iterate_spider_output(self.init_request())", "is_method": true, "class_name": "InitSpider", "function_description": "Initiates the spider's request process by calling the parent start_requests method and handling the initial request through a custom output iterator. It sets up the initial crawling requests for the spider to begin scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "initialized", "line_number": 12, "body": "def initialized(self, response=None):\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop('_postinit_reqs')", "is_method": true, "class_name": "InitSpider", "function_description": "Returns and removes the list of post-initialization requests stored during the spider's setup phase, facilitating the continuation of the crawling process after initial setup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spiders/init.py", "function": "init_request", "line_number": 18, "body": "def init_request(self):\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()", "is_method": true, "class_name": "InitSpider", "function_description": "Provides an initialization request for the InitSpider, enabling setup steps before the spider starts; override this to perform custom initialization requests that conclude by calling the spider's initialized method."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "from_crawler", "line_number": 13, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n            raise NotConfigured\n        return cls(crawler.stats)", "is_method": true, "class_name": "DownloaderStats", "function_description": "Utility method in DownloaderStats that initializes an instance from a crawler if downloader statistics are enabled; it raises an error if the feature is disabled."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_request", "line_number": 18, "body": "def process_request(self, request, spider):\n        self.stats.inc_value('downloader/request_count', spider=spider)\n        self.stats.inc_value(f'downloader/request_method_count/{request.method}', spider=spider)\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)", "is_method": true, "class_name": "DownloaderStats", "function_description": "Utility method of DownloaderStats that updates statistics for each download request, tracking request counts, methods used, and total byte size to monitor downloader activity per spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_response", "line_number": 24, "body": "def process_response(self, request, response, spider):\n        self.stats.inc_value('downloader/response_count', spider=spider)\n        self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)\n        reslen = len(response_httprepr(response))\n        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n        return response", "is_method": true, "class_name": "DownloaderStats", "function_description": "Tracks and updates download statistics such as total responses, responses by status code, and total response byte size for each spider. This function helps monitor downloader performance during web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/stats.py", "function": "process_exception", "line_number": 31, "body": "def process_exception(self, request, exception, spider):\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value('downloader/exception_count', spider=spider)\n        self.stats.inc_value(f'downloader/exception_type_count/{ex_class}', spider=spider)", "is_method": true, "class_name": "DownloaderStats", "function_description": "Tracks and increments counts of download exceptions overall and by exception type to monitor downloader error occurrences during spider execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "Factory method that initializes DownloadTimeoutMiddleware using the crawler's DOWNLOAD_TIMEOUT setting and connects its spider_opened handler to the spider_opened signal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "spider_opened", "line_number": 21, "body": "def spider_opened(self, spider):\n        self._timeout = getattr(spider, 'download_timeout', self._timeout)", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "Sets the middleware's download timeout value based on the spider's specified timeout when the spider is opened. This allows customization of timeout behavior per spider instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/downloadtimeout.py", "function": "process_request", "line_number": 24, "body": "def process_request(self, request, spider):\n        if self._timeout:\n            request.meta.setdefault('download_timeout', self._timeout)", "is_method": true, "class_name": "DownloadTimeoutMiddleware", "function_description": "Middleware method that ensures an outgoing request has a download timeout set, enforcing a maximum wait time for responses during crawling to prevent indefinite hangs."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "from_crawler", "line_number": 24, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n            raise NotConfigured\n        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n        return cls(auth_encoding)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Factory method of HttpProxyMiddleware that initializes an instance based on crawler settings, ensuring the middleware is only created if HTTP proxy functionality is enabled. It configures authentication encoding for proxy use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_basic_auth_header", "line_number": 30, "body": "def _basic_auth_header(self, username, password):\n        user_pass = to_bytes(\n            f'{unquote(username)}:{unquote(password)}',\n            encoding=self.auth_encoding)\n        return base64.b64encode(user_pass)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Generates a base64-encoded Basic Authentication header value from a username and password, supporting inclusion in HTTP proxy requests for authentication purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_get_proxy", "line_number": 36, "body": "def _get_proxy(self, url, orig_type):\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Utility method of HttpProxyMiddleware that parses a proxy URL to extract authentication credentials and reconstructs a clean proxy URL for use in HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "process_request", "line_number": 47, "body": "def process_request(self, request, spider):\n        # ignore if proxy is already set\n        if 'proxy' in request.meta:\n            if request.meta['proxy'] is None:\n                return\n            # extract credentials if present\n            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n            request.meta['proxy'] = proxy_url\n            if creds and not request.headers.get('Proxy-Authorization'):\n                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n            return\n        elif not self.proxies:\n            return\n\n        parsed = urlparse_cached(request)\n        scheme = parsed.scheme\n\n        # 'no_proxy' is only supported by http schemes\n        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n            return\n\n        if scheme in self.proxies:\n            self._set_proxy(request, scheme)", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Middleware method in HttpProxyMiddleware that assigns appropriate proxy settings and authentication headers to outgoing HTTP requests, ensuring requests are routed via proxies unless already set or bypassed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpproxy.py", "function": "_set_proxy", "line_number": 71, "body": "def _set_proxy(self, request, scheme):\n        creds, proxy = self.proxies[scheme]\n        request.meta['proxy'] = proxy\n        if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds", "is_method": true, "class_name": "HttpProxyMiddleware", "function_description": "Sets proxy settings and authorization headers on an HTTP request based on the specified scheme, enabling requests to be routed through authenticated proxies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "from_crawler", "line_number": 26, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Static factory method for BaseRedirectMiddleware that creates an instance using settings sourced from the given crawler, facilitating middleware initialization within a web scraping framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "_redirect", "line_number": 29, "body": "def _redirect(self, redirected, request, spider, reason):\n        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n        redirects = request.meta.get('redirect_times', 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta['redirect_times'] = redirects\n            redirected.meta['redirect_ttl'] = ttl - 1\n            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + [request.url]\n            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + [reason]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                         {'reason': reason, 'redirected': redirected, 'request': request},\n                         extra={'spider': spider})\n            return redirected\n        else:\n            logger.debug(\"Discarding %(request)s: max redirections reached\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest(\"max redirections reached\")", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Handles HTTP request redirection by updating metadata and retry count, enforcing a maximum redirect limit to prevent infinite loops. It returns a modified redirected request or raises an exception if the limit is exceeded."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "_redirect_request_using_get", "line_number": 49, "body": "def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected", "is_method": true, "class_name": "BaseRedirectMiddleware", "function_description": "Utility method in BaseRedirectMiddleware that creates a new GET request redirecting to a specified URL, removing body content and related headers to ensure proper HTTP redirection behavior."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "process_response", "line_number": 62, "body": "def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n            or response.status in request.meta.get('handle_httpstatus_list', [])\n            or request.meta.get('handle_httpstatus_all', False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)", "is_method": true, "class_name": "RedirectMiddleware", "function_description": "Handles HTTP redirection in web scraping by processing redirect responses and returning appropriately modified requests to follow the redirect, respecting spider and request-specific rules. It ensures compliant URL redirection for various HTTP redirect status codes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/redirect.py", "function": "process_response", "line_number": 99, "body": "def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or request.method == 'HEAD'\n            or not isinstance(response, HtmlResponse)\n        ):\n            return response\n\n        interval, url = get_meta_refresh(response,\n                                         ignore_tags=self._ignore_tags)\n        if url and interval < self._maxdelay:\n            redirected = self._redirect_request_using_get(request, url)\n            return self._redirect(redirected, request, spider, 'meta refresh')\n\n        return response", "is_method": true, "class_name": "MetaRefreshMiddleware", "function_description": "Handles HTTP responses by detecting and following meta refresh redirects within a specified delay, enabling spiders to transparently process redirected HTML pages."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "from_crawler", "line_number": 13, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.settings['USER_AGENT'])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Factory method of UserAgentMiddleware that initializes an instance using crawler settings and connects it to the spider_opened signal for lifecycle awareness."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "spider_opened", "line_number": 18, "body": "def spider_opened(self, spider):\n        self.user_agent = getattr(spider, 'user_agent', self.user_agent)", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Updates the middleware's user agent string based on the spider's user agent attribute when the spider starts, enabling dynamic user agent configuration per spider instance."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/useragent.py", "function": "process_request", "line_number": 21, "body": "def process_request(self, request, spider):\n        if self.user_agent:\n            request.headers.setdefault(b'User-Agent', self.user_agent)", "is_method": true, "class_name": "UserAgentMiddleware", "function_description": "Core utility method of the UserAgentMiddleware class that sets a specific User-Agent header in HTTP requests, allowing customization of client identity for web crawling or scraping purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "get_retry_request", "line_number": 38, "body": "def get_retry_request(\n    request: Request,\n    *,\n    spider: Spider,\n    reason: Union[str, Exception] = 'unspecified',\n    max_retry_times: Optional[int] = None,\n    priority_adjust: Optional[int] = None,\n    logger: Logger = retry_logger,\n    stats_base_key: str = 'retry',\n):\n    \"\"\"\n    Returns a new :class:`~scrapy.Request` object to retry the specified\n    request, or ``None`` if retries of the specified request have been\n    exhausted.\n\n    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n    follows::\n\n        def parse(self, response):\n            if not response.text:\n                new_request_or_none = get_retry_request(\n                    response.request,\n                    spider=self,\n                    reason='empty',\n                )\n                return new_request_or_none\n\n    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n    retry request. It is used to access the :ref:`settings <topics-settings>`\n    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n    :func:`logging.debug`).\n\n    *reason* is a string or an :class:`Exception` object that indicates the\n    reason why the request needs to be retried. It is used to name retry stats.\n\n    *max_retry_times* is a number that determines the maximum number of times\n    that *request* can be retried. If not specified or ``None``, the number is\n    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n    is read from the :setting:`RETRY_TIMES` setting.\n\n    *priority_adjust* is a number that determines how the priority of the new\n    request changes in relation to *request*. If not specified, the number is\n    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n\n    *logger* is the logging.Logger object to be used when logging messages\n\n    *stats_base_key* is a string to be used as the base key for the\n    retry-related job stats\n    \"\"\"\n    settings = spider.crawler.settings\n    stats = spider.crawler.stats\n    retry_times = request.meta.get('retry_times', 0) + 1\n    if max_retry_times is None:\n        max_retry_times = request.meta.get('max_retry_times')\n        if max_retry_times is None:\n            max_retry_times = settings.getint('RETRY_TIMES')\n    if retry_times <= max_retry_times:\n        logger.debug(\n            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n            {'request': request, 'retry_times': retry_times, 'reason': reason},\n            extra={'spider': spider}\n        )\n        new_request = request.copy()\n        new_request.meta['retry_times'] = retry_times\n        new_request.dont_filter = True\n        if priority_adjust is None:\n            priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n        new_request.priority = request.priority + priority_adjust\n\n        if callable(reason):\n            reason = reason()\n        if isinstance(reason, Exception):\n            reason = global_object_name(reason.__class__)\n\n        stats.inc_value(f'{stats_base_key}/count')\n        stats.inc_value(f'{stats_base_key}/reason_count/{reason}')\n        return new_request\n    else:\n        stats.inc_value(f'{stats_base_key}/max_reached')\n        logger.error(\n            \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n            \"%(reason)s\",\n            {'request': request, 'retry_times': retry_times, 'reason': reason},\n            extra={'spider': spider},\n        )\n        return None", "is_method": false, "function_description": "Utility function that generates a new Scrapy Request to retry a given request up to a configurable limit, adjusting its priority and logging retry statistics; returns None when retry limits are exceeded."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "from_crawler", "line_number": 144, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Factory method of RetryMiddleware that creates an instance using settings obtained from the given crawler, facilitating middleware initialization within a web scraping framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "process_response", "line_number": 147, "body": "def process_response(self, request, response, spider):\n        if request.meta.get('dont_retry', False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Core function of RetryMiddleware that inspects HTTP responses and conditionally retries requests based on specified status codes, supporting robust handling of transient network errors during web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "process_exception", "line_number": 155, "body": "def process_exception(self, request, exception, spider):\n        if (\n            isinstance(exception, self.EXCEPTIONS_TO_RETRY)\n            and not request.meta.get('dont_retry', False)\n        ):\n            return self._retry(request, exception, spider)", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Utility method in RetryMiddleware that handles retriable exceptions during request processing, triggering a retry unless disabled via request metadata."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/retry.py", "function": "_retry", "line_number": 162, "body": "def _retry(self, request, reason, spider):\n        max_retry_times = request.meta.get('max_retry_times', self.max_retry_times)\n        priority_adjust = request.meta.get('priority_adjust', self.priority_adjust)\n        return get_retry_request(\n            request,\n            reason=reason,\n            spider=spider,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )", "is_method": true, "class_name": "RetryMiddleware", "function_description": "Core method of RetryMiddleware that constructs a retry request with customizable retry limits and priority adjustments, facilitating controlled request retries for robust web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_tar", "line_number": 31, "body": "def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Method of DecompressionMiddleware that detects if a response body is a tar archive and, if so, extracts its first file, returning a new response with the extracted content and appropriate response class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_zip", "line_number": 42, "body": "def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Utility method in DecompressionMiddleware that detects if a response body is a ZIP archive and extracts its first file, returning a modified response with the uncompressed content for downstream processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_gzip", "line_number": 54, "body": "def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Performs gzip decompression on a response's body if it is gzipped, returning a new response with the decompressed content to facilitate handling of compressed HTTP responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "_is_bzip2", "line_number": 64, "body": "def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Internal method of DecompressionMiddleware that attempts to decompress a response body using bzip2 and returns a new response with the decompressed content if successful."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/decompression.py", "function": "process_response", "line_number": 73, "body": "def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in self._formats.items():\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response", "is_method": true, "class_name": "DecompressionMiddleware", "function_description": "Middleware method that processes HTTP responses by decompressing their bodies using registered formats, enabling spiders to handle compressed content transparently."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COOKIES_ENABLED'):\n            raise NotConfigured\n        return cls(crawler.settings.getbool('COOKIES_DEBUG'))", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Factory method of CookiesMiddleware that creates an instance if cookies are enabled in settings; otherwise, it prevents middleware use by raising a configuration error."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "process_request", "line_number": 26, "body": "def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        for cookie in self._get_request_cookies(jar, request):\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Middleware method that manages and attaches appropriate cookies from a specific cookie jar to outgoing requests, ensuring correct cookie handling based on request metadata and spider context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "process_response", "line_number": 40, "body": "def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Core method of CookiesMiddleware that updates and manages cookies from HTTP responses, ensuring valid cookies are extracted and stored per request context while honoring flags to skip merging cookies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_debug_cookie", "line_number": 52, "body": "def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = [to_unicode(c, errors='replace')\n                  for c in request.headers.getlist('Cookie')]\n            if cl:\n                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                logger.debug(msg, extra={'spider': spider})", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Provides debugging support by logging the cookies sent with a web request when debugging is enabled, aiding in tracking and diagnosing cookie-related behavior in Crawling operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_debug_set_cookie", "line_number": 61, "body": "def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = [to_unicode(c, errors='replace')\n                  for c in response.headers.getlist('Set-Cookie')]\n            if cl:\n                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                logger.debug(msg, extra={'spider': spider})", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Provides a debugging utility within CookiesMiddleware to log all 'Set-Cookie' headers received in HTTP responses, aiding in monitoring and troubleshooting cookie handling during web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_format_cookie", "line_number": 70, "body": "def _format_cookie(self, cookie, request):\n        \"\"\"\n        Given a dict consisting of cookie components, return its string representation.\n        Decode from bytes if necessary.\n        \"\"\"\n        decoded = {}\n        for key in (\"name\", \"value\", \"path\", \"domain\"):\n            if cookie.get(key) is None:\n                if key in (\"name\", \"value\"):\n                    msg = \"Invalid cookie found in request {}: {} ('{}' is missing)\"\n                    logger.warning(msg.format(request, cookie, key))\n                    return\n                continue\n            if isinstance(cookie[key], str):\n                decoded[key] = cookie[key]\n            else:\n                try:\n                    decoded[key] = cookie[key].decode(\"utf8\")\n                except UnicodeDecodeError:\n                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n                                   request, cookie)\n                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n\n        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n        for key, value in decoded.items():  # path, domain\n            cookie_str += f\"; {key.capitalize()}={value}\"\n        return cookie_str", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Formats a cookie dictionary into its standard string representation for HTTP headers, handling decoding of byte values and logging warnings for missing or improperly encoded fields. This supports consistent cookie processing in HTTP request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/cookies.py", "function": "_get_request_cookies", "line_number": 98, "body": "def _get_request_cookies(self, jar, request):\n        \"\"\"\n        Extract cookies from the Request.cookies attribute\n        \"\"\"\n        if not request.cookies:\n            return []\n        elif isinstance(request.cookies, dict):\n            cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n        else:\n            cookies = request.cookies\n        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n        response = Response(request.url, headers={\"Set-Cookie\": formatted})\n        return jar.make_cookies(response, request)", "is_method": true, "class_name": "CookiesMiddleware", "function_description": "Extracts and formats cookies from a request object into cookie jar entries suitable for HTTP transactions, supporting both dict and list cookie formats. This function enables consistent cookie handling within web request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "from_crawler", "line_number": 35, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Factory method for RobotsTxtMiddleware that initializes an instance using crawler settings, enabling integration with the crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "process_request", "line_number": 38, "body": "def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        d = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Middleware method that manages robot.txt compliance by asynchronously invoking a parser to determine if a web request should proceed or be blocked based on crawling rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "process_request_2", "line_number": 45, "body": "def process_request_2(self, rp, request, spider):\n        if rp is None:\n            return\n\n        useragent = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b'User-Agent', self._default_useragent)\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            self.crawler.stats.inc_value('robotstxt/forbidden')\n            raise IgnoreRequest(\"Forbidden by robots.txt\")", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "This method enforces robots.txt rules by checking if a request is allowed for a given user-agent and raising an exception to block disallowed requests. It helps ensure web crawlers respect site access restrictions during scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "robot_parser", "line_number": 58, "body": "def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc, spider)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n            self.crawler.stats.inc_value('robotstxt/request_count')\n\n        if isinstance(self._parsers[netloc], Deferred):\n            d = Deferred()\n\n            def cb(result):\n                d.callback(result)\n                return result\n            self._parsers[netloc].addCallback(cb)\n            return d\n        else:\n            return self._parsers[netloc]", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Core method of RobotsTxtMiddleware that manages asynchronous retrieval and parsing of robots.txt files per domain, ensuring compliance checks for web crawling requests. It caches parsers to optimize repeated access and handles download errors gracefully."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_logerror", "line_number": 87, "body": "def _logerror(self, failure, request, spider):\n        if failure.type is not IgnoreRequest:\n            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                         {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Internal method of RobotsTxtMiddleware that logs download errors except those caused by ignored requests, aiding in debugging and monitoring spider request failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_parse_robots", "line_number": 95, "body": "def _parse_robots(self, response, netloc, spider):\n        self.crawler.stats.inc_value('robotstxt/response_count')\n        self.crawler.stats.inc_value(f'robotstxt/response_status_count/{response.status}')\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Internal method of RobotsTxtMiddleware that processes a robots.txt HTTP response, updates parsing state per host, and tracks response statistics for managing crawler access rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "_robots_error", "line_number": 103, "body": "def _robots_error(self, failure, netloc):\n        if failure.type is not IgnoreRequest:\n            key = f'robotstxt/exception_count/{failure.type}'\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Handles errors encountered during robots.txt retrieval by updating error statistics and signaling failure in the parsing process, ensuring the crawler reacts appropriately to inaccessible or problematic robots.txt files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/robotstxt.py", "function": "cb", "line_number": 79, "body": "def cb(result):\n                d.callback(result)\n                return result", "is_method": true, "class_name": "RobotsTxtMiddleware", "function_description": "Callback function that processes the result by triggering a deferred callback and returning the same result, facilitating asynchronous handling within RobotsTxtMiddleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "from_crawler", "line_number": 34, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n            raise NotConfigured\n        try:\n            return cls(stats=crawler.stats)\n        except TypeError:\n            warnings.warn(\n                \"HttpCompressionMiddleware subclasses must either modify \"\n                \"their '__init__' method to support a 'stats' parameter or \"\n                \"reimplement the 'from_crawler' method.\",\n                ScrapyDeprecationWarning,\n            )\n            result = cls()\n            result.stats = crawler.stats\n            return result", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Initial inspection shows this method is a class method (`from_crawler`) designed to instantiate HttpCompressionMiddleware based on crawler settings. It checks if compression is enabled and either raises an error or creates an instance accordingly, managing backward compatibility. Its service is to configure and provide a middleware instance with statistics tracking when applicable.\n\nCore utility class method of HttpCompressionMiddleware that instantiates the middleware if compression is enabled, managing compatibility with varying constructor signatures and integrating crawler statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "process_request", "line_number": 50, "body": "def process_request(self, request, spider):\n        request.headers.setdefault('Accept-Encoding',\n                                   b\", \".join(ACCEPTED_ENCODINGS))", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Adds accepted compression encodings to HTTP request headers, enabling servers to send compressed responses for improved network efficiency. This supports automated handling of content encoding in web scraping or crawling processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "process_response", "line_number": 54, "body": "def process_response(self, request, response, spider):\n\n        if request.method == 'HEAD':\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist('Content-Encoding')\n            if content_encoding:\n                encoding = content_encoding.pop()\n                decoded_body = self._decode(response.body, encoding.lower())\n                if self.stats:\n                    self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n                    self.stats.inc_value('httpcompression/response_count', spider=spider)\n                respcls = responsetypes.from_args(\n                    headers=response.headers, url=response.url, body=decoded_body\n                )\n                kwargs = dict(cls=respcls, body=decoded_body)\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs['encoding'] = None\n                response = response.replace(**kwargs)\n                if not content_encoding:\n                    del response.headers['Content-Encoding']\n\n        return response", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Core method of HttpCompressionMiddleware that decompresses encoded HTTP responses, updates response metadata and statistics, and returns a decoded response for further processing in a web scraping context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcompression.py", "function": "_decode", "line_number": 80, "body": "def _decode(self, body, encoding):\n        if encoding == b'gzip' or encoding == b'x-gzip':\n            body = gunzip(body)\n\n        if encoding == b'deflate':\n            try:\n                body = zlib.decompress(body)\n            except zlib.error:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                body = zlib.decompress(body, -15)\n        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n            body = brotli.decompress(body)\n        if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:\n            # Using its streaming API since its simple API could handle only cases\n            # where there is content size data embedded in the frame\n            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n            body = reader.read()\n        return body", "is_method": true, "class_name": "HttpCompressionMiddleware", "function_description": "Decodes compressed HTTP response bodies using supported encodings like gzip, deflate, Brotli, and Zstandard, enabling middleware to transparently handle compressed content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/defaultheaders.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n        return cls(headers.items())", "is_method": true, "class_name": "DefaultHeadersMiddleware", "function_description": "Factory method for DefaultHeadersMiddleware that initializes an instance using default request headers from a crawler's settings, excluding any headers with None values."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/defaultheaders.py", "function": "process_request", "line_number": 20, "body": "def process_request(self, request, spider):\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)", "is_method": true, "class_name": "DefaultHeadersMiddleware", "function_description": "Adds default headers to each outgoing request if they are not already set, ensuring consistent header inclusion for all requests processed by the middleware."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "from_crawler", "line_number": 46, "body": "def from_crawler(cls: Type[HttpCacheMiddlewareTV], crawler: Crawler) -> HttpCacheMiddlewareTV:\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Factory method that initializes HttpCacheMiddleware from a Crawler instance and connects its lifecycle event handlers for spider open and close signals."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "spider_opened", "line_number": 52, "body": "def spider_opened(self, spider: Spider) -> None:\n        self.storage.open_spider(spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Initializes or prepares the HTTP cache storage when a web crawling spider starts operating."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "spider_closed", "line_number": 55, "body": "def spider_closed(self, spider: Spider) -> None:\n        self.storage.close_spider(spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Handles cleanup and resource release when a web scraping spider finishes, ensuring proper closure of stored data related to that spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_request", "line_number": 58, "body": "def process_request(self, request: Request, spider: Spider) -> Optional[Response]:\n        if request.meta.get('dont_cache', False):\n            return None\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta['_dont_cache'] = True  # flag as uncacheable\n            return None\n\n        # Look for cached response and check if expired\n        cachedresponse = self.storage.retrieve_response(spider, request)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/miss', spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value('httpcache/ignore', spider=spider)\n                raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n            return None  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append('cached')\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value('httpcache/hit', spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta['cached_response'] = cachedresponse\n\n        return None", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Middleware method that intercepts HTTP requests to return cached responses when available and fresh, reducing redundant network calls and improving scraping efficiency by applying caching policies and statistics tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_response", "line_number": 88, "body": "def process_response(self, request: Request, response: Response, spider: Spider) -> Response:\n        if request.meta.get('dont_cache', False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if 'cached' in response.flags or '_dont_cache' in request.meta:\n            request.meta.pop('_dont_cache', None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if 'Date' not in response.headers:\n            response.headers['Date'] = formatdate(usegmt=True)\n\n        # Do not validate first-hand responses\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/firsthand', spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value('httpcache/revalidate', spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value('httpcache/invalidate', spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Middleware method that manages HTTP response caching by deciding whether to use, update, or bypass cached responses based on request metadata and cache validation policies, optimizing web scraping efficiency and bandwidth usage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "process_exception", "line_number": 117, "body": "def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Optional[Response]:\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n            return cachedresponse\n        return None", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Handles exceptions during HTTP requests by returning a cached response if available and the exception matches retryable errors, enabling resilient web scraping through efficient error recovery."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpcache.py", "function": "_cache_response", "line_number": 126, "body": "def _cache_response(\n        self, spider: Spider, response: Response, request: Request, cachedresponse: Optional[Response]\n    ) -> None:\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value('httpcache/store', spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value('httpcache/uncacheable', spider=spider)", "is_method": true, "class_name": "HttpCacheMiddleware", "function_description": "Internal method of the HttpCacheMiddleware class that conditionally stores HTTP responses in cache based on policy evaluation, while tracking cache storage statistics per spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "_has_ajaxcrawlable_meta", "line_number": 70, "body": "def _has_ajaxcrawlable_meta(text):\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if 'fragment' not in text:\n        return False\n    if 'content' not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None", "is_method": false, "function_description": "Utility function that checks if HTML text contains an AJAX crawlable meta tag indicating support for crawler-friendly content rendering. It helps determine if a page uses AJAX crawlable metadata for SEO or crawling purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "from_crawler", "line_number": 30, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "Static factory method that creates an AjaxCrawlMiddleware instance using crawler settings, enabling middleware initialization within a crawling framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "process_response", "line_number": 33, "body": "def process_response(self, request, response, spider):\n\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != 'GET':\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if 'ajax_crawlable' in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url + '#!')\n        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n                     extra={'spider': spider})\n\n        ajax_crawl_request.meta['ajax_crawlable'] = True\n        return ajax_crawl_request", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "Handles HTTP GET HTML responses to detect and transform AJAX crawlable pages by generating modified requests, enabling spiders to correctly crawl dynamic content using the AJAX crawling scheme."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py", "function": "_has_ajax_crawlable_variant", "line_number": 57, "body": "def _has_ajax_crawlable_variant(self, response):\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[:self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)", "is_method": true, "class_name": "AjaxCrawlMiddleware", "function_description": "Method of AjaxCrawlMiddleware that checks if a webpage response supports AJAX crawling by verifying specific meta tags, facilitating proper indexing of dynamic content by search engines."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Factory method for HttpAuthMiddleware that creates an instance and connects its spider_opened handler to the crawler's spider_opened signal for initialization during spider startup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "spider_opened", "line_number": 26, "body": "def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            if not hasattr(spider, 'http_auth_domain'):\n                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n                              'problems if the spider makes requests to several different domains. http_auth_domain '\n                              'will be set to the domain of the first request, please set it to the correct value '\n                              'explicitly.',\n                              category=ScrapyDeprecationWarning)\n                self.domain_unset = True\n            else:\n                self.domain = spider.http_auth_domain\n                self.domain_unset = False", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Initializes HTTP basic authentication for a spider on start, setting credentials and warning if the authentication domain is unspecified to prevent security issues."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/downloadermiddlewares/httpauth.py", "function": "process_request", "line_number": 42, "body": "def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            domain = urlparse_cached(request).hostname\n            if self.domain_unset:\n                self.domain = domain\n                self.domain_unset = False\n            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                request.headers[b'Authorization'] = auth", "is_method": true, "class_name": "HttpAuthMiddleware", "function_description": "Middleware method that adds an Authorization header to HTTP requests targeting a specific domain, enabling authenticated requests within a web scraping or crawling context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "get_settings_priority", "line_number": 19, "body": "def get_settings_priority(priority):\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, str):\n        return SETTINGS_PRIORITIES[priority]\n    else:\n        return priority", "is_method": false, "function_description": "Helper function that converts a string-based priority name into its corresponding numerical value or returns the numerical priority as is, facilitating uniform priority handling in settings management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "iter_default_settings", "line_number": 453, "body": "def iter_default_settings():\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)", "is_method": false, "function_description": "Utility function that iterates over and yields default configuration settings as (name, value) pairs, enabling easy access or inspection of preset constants."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "overridden_settings", "line_number": 460, "body": "def overridden_settings(settings):\n    \"\"\"Return a dict of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value", "is_method": false, "function_description": "Utility function that yields key-value pairs of settings that differ from their default values, enabling identification of overridden configuration options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "set", "line_number": 46, "body": "def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority", "is_method": true, "class_name": "SettingsAttribute", "function_description": "Sets the attribute's value only if the provided priority is greater than or equal to the current one, ensuring controlled updates based on priority levels."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__str__", "line_number": 54, "body": "def __str__(self):\n        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"", "is_method": true, "class_name": "SettingsAttribute", "function_description": "Returns a string representation of the SettingsAttribute instance, showing its value and priority for easier inspection and debugging."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__getitem__", "line_number": 88, "body": "def __getitem__(self, opt_name):\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method in BaseSettings that retrieves the value of a specified option if it exists, returning None otherwise. It enables dictionary-like access to configuration attributes by their names."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__contains__", "line_number": 93, "body": "def __contains__(self, name):\n        return name in self.attributes", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides membership checking for BaseSettings, allowing users to verify if a given attribute name exists within the settings. This supports intuitive attribute presence validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "get", "line_number": 96, "body": "def get(self, name, default=None):\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return self[name] if self[name] is not None else default", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method in BaseSettings that retrieves a setting value by name, returning a default if the setting is not found, without altering the setting's original type."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getbool", "line_number": 108, "body": "def getbool(self, name, default=False):\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\"Supported values for boolean settings \"\n                             \"are 0/1, True/False, '0'/'1', \"\n                             \"'True'/'False' and 'true'/'false'\")", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method of the BaseSettings class that retrieves a setting value and interprets it as a boolean, supporting common string and numeric representations. It ensures consistent boolean parsing from environment variables or configuration sources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getint", "line_number": 136, "body": "def getint(self, name, default=0):\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return int(self.get(name, default))", "is_method": true, "class_name": "BaseSettings", "function_description": "Method of BaseSettings that retrieves a configuration value by name and returns it as an integer, providing a default if the setting is missing. Useful for accessing numeric settings with fallback handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getfloat", "line_number": 148, "body": "def getfloat(self, name, default=0.0):\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return float(self.get(name, default))", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method of the BaseSettings class that retrieves a setting value by name and returns it as a float, providing a default if the setting is not found. This supports flexible configuration handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getlist", "line_number": 160, "body": "def getlist(self, name, default=None):\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, str):\n            value = value.split(',')\n        return list(value)", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method in BaseSettings that retrieves a setting as a list, converting comma-separated strings into lists and returning copies of existing lists, facilitating consistent list handling from various configuration sources."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getdict", "line_number": 179, "body": "def getdict(self, name, default=None):\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, str):\n            value = json.loads(value)\n        return dict(value)", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method of the BaseSettings class that retrieves a setting by name and returns its value as a dictionary, supporting conversion from JSON strings or nested settings instances for consistent dictionary representation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getwithbase", "line_number": 200, "body": "def getwithbase(self, name):\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: str\n        \"\"\"\n        compbs = BaseSettings()\n        compbs.update(self[name + '_BASE'])\n        compbs.update(self[name])\n        return compbs", "is_method": true, "class_name": "BaseSettings", "function_description": "Core method of BaseSettings that merges a dictionary setting with its associated \"_BASE\" version, producing a combined configuration useful for layered or inherited settings management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "getpriority", "line_number": 212, "body": "def getpriority(self, name):\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: str\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method of BaseSettings that returns the numerical priority of a given setting name, or None if the setting does not exist. It helps determine the precedence of configuration options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "maxpriority", "line_number": 224, "body": "def maxpriority(self):\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(self.getpriority(name) for name in self)\n        else:\n            return get_settings_priority('default')", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides the highest numerical priority among all stored settings or returns the default priority if no settings exist, enabling priority-based configuration management within the BaseSettings context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__setitem__", "line_number": 236, "body": "def __setitem__(self, name, value):\n        self.set(name, value)", "is_method": true, "class_name": "BaseSettings", "function_description": "Provides dictionary-like item assignment to set configuration values within the BaseSettings object for convenient and consistent settings management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "set", "line_number": 239, "body": "def set(self, name, value, priority='project'):\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: str\n\n        :param value: the value to associate with the setting\n        :type value: object\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Sets or updates a configuration setting with an associated priority, enabling layered and controlled management of application settings before initialization. This supports flexible customization of the crawler\u2019s behavior in the BaseSettings context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "setdict", "line_number": 267, "body": "def setdict(self, values, priority='project'):\n        self.update(values, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method in BaseSettings that updates configuration settings from a dictionary with a specified priority level, facilitating flexible and prioritized configuration management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "setmodule", "line_number": 270, "body": "def setmodule(self, module, priority='project'):\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: types.ModuleType or str\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, str):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Core method of the BaseSettings class that loads and applies configuration settings from a specified module, assigning them a defined priority to manage conflicting values. This enables modular and prioritized configuration management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "update", "line_number": 292, "body": "def update(self, values, priority='project'):\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, str):\n            values = json.loads(values)\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in values.items():\n                    self.set(name, value, values.getpriority(name))\n            else:\n                for name, value in values.items():\n                    self.set(name, value, priority)", "is_method": true, "class_name": "BaseSettings", "function_description": "Service method in BaseSettings that updates multiple settings at once from a dict, JSON string, or another BaseSettings object, applying specified or inherited priorities to each key/value pair. It simplifies bulk configuration changes with priority management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "delete", "line_number": 325, "body": "def delete(self, name, priority='project'):\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= self.getpriority(name):\n            del self.attributes[name]", "is_method": true, "class_name": "BaseSettings", "function_description": "Core method of BaseSettings that deletes a setting by name if the specified priority level is sufficiently high, enabling controlled removal of configuration attributes based on priority rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__delitem__", "line_number": 331, "body": "def __delitem__(self, name):\n        self._assert_mutability()\n        del self.attributes[name]", "is_method": true, "class_name": "BaseSettings", "function_description": "Method of BaseSettings that deletes a specified attribute, enforcing mutability rules before removal. It allows dynamic modification of settings by removing existing configuration entries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_assert_mutability", "line_number": 335, "body": "def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")", "is_method": true, "class_name": "BaseSettings", "function_description": "Private method of BaseSettings that enforces immutability by raising an error if modifications are attempted when the settings object is frozen."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "copy", "line_number": 339, "body": "def copy(self):\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method of BaseSettings that creates an independent deep copy of the current settings instance, allowing safe modifications without affecting the original configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "freeze", "line_number": 351, "body": "def freeze(self):\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True", "is_method": true, "class_name": "BaseSettings", "function_description": "Core method of BaseSettings that makes the current settings immutable, preventing any further modifications or changes after it is invoked."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "frozencopy", "line_number": 361, "body": "def frozencopy(self):\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy", "is_method": true, "class_name": "BaseSettings", "function_description": "Returns an immutable copy of the current settings, ensuring the copy cannot be modified. This enables safe sharing or usage of configuration states without risk of unintended changes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__iter__", "line_number": 371, "body": "def __iter__(self):\n        return iter(self.attributes)", "is_method": true, "class_name": "BaseSettings", "function_description": "Enables iteration over the attributes of a BaseSettings instance, allowing users to loop through its stored settings easily. This supports convenient access and manipulation of configuration parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__len__", "line_number": 374, "body": "def __len__(self):\n        return len(self.attributes)", "is_method": true, "class_name": "BaseSettings", "function_description": "Returns the number of attributes stored in the BaseSettings instance, providing a quick way to determine its size or count of configuration entries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_to_dict", "line_number": 377, "body": "def _to_dict(self):\n        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n                for k, v in self.items()}", "is_method": true, "class_name": "BaseSettings", "function_description": "Converts the BaseSettings instance and any nested BaseSettings objects into a dictionary representation. This allows other components to easily access the settings as standard Python dictionaries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "copy_to_dict", "line_number": 381, "body": "def copy_to_dict(self):\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()", "is_method": true, "class_name": "BaseSettings", "function_description": "Returns an independent dictionary copy of the current settings with their priorities, allowing safe inspection or modification without affecting the original configuration state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "_repr_pretty_", "line_number": 397, "body": "def _repr_pretty_(self, p, cycle):\n        if cycle:\n            p.text(repr(self))\n        else:\n            p.text(pformat(self.copy_to_dict()))", "is_method": true, "class_name": "BaseSettings", "function_description": "Utility method in BaseSettings providing a detailed, human-readable representation of the object's settings for use in interactive environments or pretty-printers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__len__", "line_number": 411, "body": "def __len__(self):\n        return len(self.o)", "is_method": true, "class_name": "_DictProxy", "function_description": "Returns the number of items in the underlying dictionary. This method provides the length functionality for the _DictProxy class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__getitem__", "line_number": 414, "body": "def __getitem__(self, k):\n        return self.o[k]", "is_method": true, "class_name": "_DictProxy", "function_description": "Provides dictionary-like access to underlying data by returning the value associated with a given key. Enables seamless retrieval from the encapsulated dictionary within the _DictProxy class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__setitem__", "line_number": 417, "body": "def __setitem__(self, k, v):\n        self.settings.set(k, v, priority=self.priority)\n        self.o[k] = v", "is_method": true, "class_name": "_DictProxy", "function_description": "Sets a key-value pair in the proxy\u2019s underlying settings with a given priority and updates the local dictionary. This enables controlled and prioritized modification of configuration settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/settings/__init__.py", "function": "__delitem__", "line_number": 421, "body": "def __delitem__(self, k):\n        del self.o[k]", "is_method": true, "class_name": "_DictProxy", "function_description": "Core method of the _DictProxy class that deletes a specified key-value pair from the underlying dictionary. It enables controlled removal of dictionary entries."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "build_storage", "line_number": 33, "body": "def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n    argument_names = get_func_args(builder)\n    if 'feed_options' in argument_names:\n        kwargs['feed_options'] = feed_options\n    else:\n        warnings.warn(\n            \"{} does not support the 'feed_options' keyword argument. Add a \"\n            \"'feed_options' parameter to its signature to remove this \"\n            \"warning. This parameter will become mandatory in a future \"\n            \"version of Scrapy.\"\n            .format(builder.__qualname__),\n            category=ScrapyDeprecationWarning\n        )\n    return builder(*preargs, uri, *args, **kwargs)", "is_method": false, "function_description": "Utility function that calls a builder callable to create a storage object, conditionally passing a feed_options parameter and warning if unsupported, facilitating flexible storage construction with backward compatibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 56, "body": "def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"", "is_method": true, "class_name": "IFeedStorage", "function_description": "Method of IFeedStorage that opens and prepares storage for a specified spider, returning a file-like object used by exporters to write data during scraping processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 60, "body": "def store(file):\n        \"\"\"Store the given file stream\"\"\"", "is_method": true, "class_name": "IFeedStorage", "function_description": "Core method of the IFeedStorage class that saves a provided file stream for persistent storage. It enables other components to store files within the feed storage system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 67, "body": "def open(self, spider):\n        path = spider.crawler.settings['FEED_TEMPDIR']\n        if path and not os.path.isdir(path):\n            raise OSError('Not a Directory: ' + str(path))\n\n        return NamedTemporaryFile(prefix='feed-', dir=path)", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Creates and opens a temporary file for storing spider feed data, ensuring the specified directory exists. This supports safe, temporary storage during web scraping operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 74, "body": "def store(self, file):\n        return threads.deferToThread(self._store_in_thread, file)", "is_method": true, "class_name": "BlockingFeedStorage", "function_description": "Utility method of BlockingFeedStorage that asynchronously stores a file using a separate thread to avoid blocking the main execution flow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 94, "body": "def open(self, spider):\n        return self._stdout", "is_method": true, "class_name": "StdoutFeedStorage", "function_description": "Returns a writable stream connected to standard output for the given spider, enabling direct data output during scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open", "line_number": 109, "body": "def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, self.write_mode)", "is_method": true, "class_name": "FileFeedStorage", "function_description": "Utility method of FileFeedStorage that opens a file for reading or writing, ensuring the file\u2019s directory exists by creating it if necessary."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "store", "line_number": 115, "body": "def store(self, file):\n        file.close()", "is_method": true, "class_name": "FileFeedStorage", "function_description": "Closes the given file object, finalizing any ongoing operations. It provides a utility to ensure files are properly closed after use, preventing resource leaks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 142, "body": "def from_crawler(cls, crawler, uri, *, feed_options=None):\n        return build_storage(\n            cls,\n            uri,\n            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n            feed_options=feed_options,\n        )", "is_method": true, "class_name": "S3FeedStorage", "function_description": "Factory method of S3FeedStorage that creates a storage instance using AWS credentials and settings extracted from a crawler, facilitating S3 feed storage configuration based on crawler environment parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 152, "body": "def _store_in_thread(self, file):\n        file.seek(0)\n        kwargs = {'ACL': self.acl} if self.acl else {}\n        self.s3_client.put_object(\n            Bucket=self.bucketname, Key=self.keyname, Body=file,\n            **kwargs)\n        file.close()", "is_method": true, "class_name": "S3FeedStorage", "function_description": "Internal method of S3FeedStorage that uploads a given file to an S3 bucket using predefined bucket and key names, applying access control settings if specified. It handles file positioning and closing after upload."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 171, "body": "def from_crawler(cls, crawler, uri):\n        return cls(\n            uri,\n            crawler.settings['GCS_PROJECT_ID'],\n            crawler.settings['FEED_STORAGE_GCS_ACL'] or None\n        )", "is_method": true, "class_name": "GCSFeedStorage", "function_description": "Factory method of GCSFeedStorage that creates an instance using settings from a crawler and a given URI, facilitating configuration-based initialization of cloud storage feed handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 178, "body": "def _store_in_thread(self, file):\n        file.seek(0)\n        from google.cloud.storage import Client\n        client = Client(project=self.project_id)\n        bucket = client.get_bucket(self.bucket_name)\n        blob = bucket.blob(self.blob_name)\n        blob.upload_from_file(file, predefined_acl=self.acl)", "is_method": true, "class_name": "GCSFeedStorage", "function_description": "Internal method of GCSFeedStorage that uploads a given file object to a specified Google Cloud Storage bucket using pre-configured project, bucket, blob name, and access control settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 200, "body": "def from_crawler(cls, crawler, uri, *, feed_options=None):\n        return build_storage(\n            cls,\n            uri,\n            crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE'),\n            feed_options=feed_options,\n        )", "is_method": true, "class_name": "FTPFeedStorage", "function_description": "Factory method in FTPFeedStorage that initializes an FTP feed storage instance based on crawler settings and specified URI, facilitating configuration-driven storage setup for data feeds."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_store_in_thread", "line_number": 208, "body": "def _store_in_thread(self, file):\n        ftp_store_file(\n            path=self.path, file=file, host=self.host,\n            port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.use_active_mode,\n            overwrite=self.overwrite,\n        )", "is_method": true, "class_name": "FTPFeedStorage", "function_description": "Internal method of FTPFeedStorage that uploads a given file to an FTP server using the instance\u2019s connection settings. It facilitates storing files on the remote FTP path with support for overwrite and connection mode options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "start_exporting", "line_number": 232, "body": "def start_exporting(self):\n        if not self._exporting:\n            self.exporter.start_exporting()\n            self._exporting = True", "is_method": true, "class_name": "_FeedSlot", "function_description": "Starts the export process if it isn't already running, ensuring that exporting is initiated only once per instance. This enables controlled activation of the export functionality within the _FeedSlot context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "finish_exporting", "line_number": 237, "body": "def finish_exporting(self):\n        if self._exporting:\n            self.exporter.finish_exporting()\n            self._exporting = False", "is_method": true, "class_name": "_FeedSlot", "function_description": "Method of the _FeedSlot class that finalizes the export process by signaling the exporter to complete and updating the exporting state accordingly. It ensures proper closure after data export operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "from_crawler", "line_number": 246, "body": "def from_crawler(cls, crawler):\n        exporter = cls(crawler)\n        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n        return exporter", "is_method": true, "class_name": "FeedExporter", "function_description": "Factory method of the FeedExporter class that initializes an exporter instance and connects it to crawler signals for managing export operations triggered by spider events and item scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "open_spider", "line_number": 289, "body": "def open_spider(self, spider):\n        for uri, feed_options in self.feeds.items():\n            uri_params = self._get_uri_params(spider, feed_options['uri_params'])\n            self.slots.append(self._start_new_batch(\n                batch_id=1,\n                uri=uri % uri_params,\n                feed_options=feed_options,\n                spider=spider,\n                uri_template=uri,\n            ))", "is_method": true, "class_name": "FeedExporter", "function_description": "Initializes and starts export batches for each configured feed when a spider begins running, preparing the FeedExporter to output scraped data to multiple destinations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "close_spider", "line_number": 300, "body": "def close_spider(self, spider):\n        deferred_list = []\n        for slot in self.slots:\n            d = self._close_slot(slot, spider)\n            deferred_list.append(d)\n        return defer.DeferredList(deferred_list) if deferred_list else None", "is_method": true, "class_name": "FeedExporter", "function_description": "Finalizer method in FeedExporter that asynchronously closes all active feed export slots when a spider finishes running, ensuring proper resource cleanup and finalization."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_close_slot", "line_number": 307, "body": "def _close_slot(self, slot, spider):\n        if not slot.itemcount and not slot.store_empty:\n            # We need to call slot.storage.store nonetheless to get the file\n            # properly closed.\n            return defer.maybeDeferred(slot.storage.store, slot.file)\n        slot.finish_exporting()\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': slot.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        d = defer.maybeDeferred(slot.storage.store, slot.file)\n\n        # Use `largs=log_args` to copy log_args into function's scope\n        # instead of using `log_args` from the outer scope\n        d.addCallback(\n            self._handle_store_success, log_args, logfmt, spider, type(slot.storage).__name__\n        )\n        d.addErrback(\n            self._handle_store_error, log_args, logfmt, spider, type(slot.storage).__name__\n        )\n        return d", "is_method": true, "class_name": "FeedExporter", "function_description": "Core method of FeedExporter that finalizes and stores feed data, handling empty feeds appropriately, and manages success or error callbacks during the storage process for seamless feed export completion."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_handle_store_error", "line_number": 329, "body": "def _handle_store_error(self, f, largs, logfmt, spider, slot_type):\n        logger.error(\n            logfmt % \"Error storing\", largs,\n            exc_info=failure_to_exc_info(f), extra={'spider': spider}\n        )\n        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")", "is_method": true, "class_name": "FeedExporter", "function_description": "Internal error handler of FeedExporter that logs storage failures and updates failure statistics, supporting monitoring and debugging of feed export operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_handle_store_success", "line_number": 336, "body": "def _handle_store_success(self, f, largs, logfmt, spider, slot_type):\n        logger.info(\n            logfmt % \"Stored\", largs, extra={'spider': spider}\n        )\n        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")", "is_method": true, "class_name": "FeedExporter", "function_description": "Private method in FeedExporter that logs successful data storage events and increments corresponding success statistics for monitoring feed export operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_start_new_batch", "line_number": 342, "body": "def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n        :param batch_id: sequence number of current batch\n        :param uri: uri of the new batch to start\n        :param feed_options: dict with parameters of feed\n        :param spider: user spider\n        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n        \"\"\"\n        storage = self._get_storage(uri, feed_options)\n        file = storage.open(spider)\n        exporter = self._get_exporter(\n            file=file,\n            format=feed_options['format'],\n            fields_to_export=feed_options['fields'],\n            encoding=feed_options['encoding'],\n            indent=feed_options['indent'],\n            **feed_options['item_export_kwargs'],\n        )\n        slot = _FeedSlot(\n            file=file,\n            exporter=exporter,\n            storage=storage,\n            uri=uri,\n            format=feed_options['format'],\n            store_empty=feed_options['store_empty'],\n            batch_id=batch_id,\n            uri_template=uri_template,\n        )\n        if slot.store_empty:\n            slot.start_exporting()\n        return slot", "is_method": true, "class_name": "FeedExporter", "function_description": "Initializes and configures a new data export batch, setting up storage and exporter instances for handling segmented output files in batch-oriented feed exporting workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "item_scraped", "line_number": 376, "body": "def item_scraped(self, item, spider):\n        slots = []\n        for slot in self.slots:\n            slot.start_exporting()\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n            if (\n                self.feeds[slot.uri_template]['batch_item_count']\n                and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']\n            ):\n                uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                self._close_slot(slot, spider)\n                slots.append(self._start_new_batch(\n                    batch_id=slot.batch_id + 1,\n                    uri=slot.uri_template % uri_params,\n                    feed_options=self.feeds[slot.uri_template],\n                    spider=spider,\n                    uri_template=slot.uri_template,\n                ))\n            else:\n                slots.append(slot)\n        self.slots = slots", "is_method": true, "class_name": "FeedExporter", "function_description": "Method of the FeedExporter class that processes a scraped item by exporting it, managing batching, and rotating feed slots when batch size limits are reached to efficiently handle segmented feed exports."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_load_components", "line_number": 400, "body": "def _load_components(self, setting_prefix):\n        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d", "is_method": true, "class_name": "FeedExporter", "function_description": "Private method of FeedExporter that loads and returns configured components based on a given settings prefix, ignoring any that are not properly configured."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_exporter_supported", "line_number": 410, "body": "def _exporter_supported(self, format):\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})", "is_method": true, "class_name": "FeedExporter", "function_description": "This method checks if a given export format is supported by the FeedExporter and logs an error if not. It helps validate export operations by confirming format compatibility."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_settings_are_valid", "line_number": 415, "body": "def _settings_are_valid(self):\n        \"\"\"\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n                logger.error(\n                    '%(batch_time)s or %(batch_id)d must be in the feed URI ({}) if FEED_EXPORT_BATCH_ITEM_COUNT '\n                    'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '\n                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n                    ''.format(uri_template)\n                )\n                return False\n        return True", "is_method": true, "class_name": "FeedExporter", "function_description": "Checks if feed export settings requiring batch-specific file URIs include necessary placeholders to ensure unique partial output file names, helping prevent data overwrite in batched feed exports."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_storage_supported", "line_number": 431, "body": "def _storage_supported(self, uri, feed_options):\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages:\n            try:\n                self._get_storage(uri, feed_options)\n                return True\n            except NotConfigured as e:\n                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n                             \"Reason: %(reason)s\",\n                             {'scheme': scheme, 'reason': str(e)})\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})", "is_method": true, "class_name": "FeedExporter", "function_description": "Checks if a given URI's storage scheme is supported and properly configured for exporting feeds, logging errors for unsupported or misconfigured storage options."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_instance", "line_number": 445, "body": "def _get_instance(self, objcls, *args, **kwargs):\n        return create_instance(\n            objcls, self.settings, getattr(self, 'crawler', None),\n            *args, **kwargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "Internal helper of FeedExporter that instantiates objects with shared settings and optional crawler context, supporting flexible creation of components needed during feed export processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_exporter", "line_number": 450, "body": "def _get_exporter(self, file, format, *args, **kwargs):\n        return self._get_instance(self.exporters[format], file, *args, **kwargs)", "is_method": true, "class_name": "FeedExporter", "function_description": "Core utility method of the FeedExporter class that initializes and returns a specific exporter instance based on the given format and file, enabling flexible export operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_storage", "line_number": 453, "body": "def _get_storage(self, uri, feed_options):\n        \"\"\"Fork of create_instance specific to feed storage classes\n\n        It supports not passing the *feed_options* parameters to classes that\n        do not support it, and issuing a deprecation warning instead.\n        \"\"\"\n        feedcls = self.storages[urlparse(uri).scheme]\n        crawler = getattr(self, 'crawler', None)\n\n        def build_instance(builder, *preargs):\n            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)\n\n        if crawler and hasattr(feedcls, 'from_crawler'):\n            instance = build_instance(feedcls.from_crawler, crawler)\n            method_name = 'from_crawler'\n        elif hasattr(feedcls, 'from_settings'):\n            instance = build_instance(feedcls.from_settings, self.settings)\n            method_name = 'from_settings'\n        else:\n            instance = build_instance(feedcls)\n            method_name = '__new__'\n        if instance is None:\n            raise TypeError(\"%s.%s returned None\" % (feedcls.__qualname__, method_name))\n        return instance", "is_method": true, "class_name": "FeedExporter", "function_description": "Private method of the FeedExporter class that creates and returns a storage instance based on a URI scheme, accommodating different constructor patterns and optionally passing configuration or crawler context. It abstracts feed storage initialization for flexible export destinations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/feedexport.py", "function": "_get_uri_params", "line_number": 478, "body": "def _get_uri_params(self, spider, uri_params, slot=None):\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        utc_now = datetime.utcnow()\n        params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n        params['batch_time'] = utc_now.isoformat().replace(':', '-')\n        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n        uripar_function(params, spider)\n        return params", "is_method": true, "class_name": "FeedExporter", "function_description": "Constructs and returns a dictionary of URI parameters by extracting attributes from a spider, adding timestamp and batch identifiers, and applying an optional customization function for feed export configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "from_crawler", "line_number": 32, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "StackTraceDump", "function_description": "Class method that initializes a StackTraceDump instance using a crawler object, facilitating integration with crawler-based workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "dump_stacktrace", "line_number": 35, "body": "def dump_stacktrace(self, signum, frame):\n        log_args = {\n            'stackdumps': self._thread_stacks(),\n            'enginestatus': format_engine_status(self.crawler.engine),\n            'liverefs': format_live_refs(),\n        }\n        logger.info(\"Dumping stack trace and engine status\\n\"\n                    \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n                    log_args, extra={'crawler': self.crawler})", "is_method": true, "class_name": "StackTraceDump", "function_description": "Utility method of the StackTraceDump class that logs the current thread stack traces, engine status, and live object references to aid in debugging or monitoring application state during signal handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "_thread_stacks", "line_number": 45, "body": "def _thread_stacks(self):\n        id2name = dict((th.ident, th.name) for th in threading.enumerate())\n        dumps = ''\n        for id_, frame in sys._current_frames().items():\n            name = id2name.get(id_, '')\n            dump = ''.join(traceback.format_stack(frame))\n            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n        return dumps", "is_method": true, "class_name": "StackTraceDump", "function_description": "Core method of StackTraceDump that collects and returns the current stack traces of all active threads, useful for debugging multi-threaded applications by providing a snapshot of their execution states."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/debug.py", "function": "_enter_debugger", "line_number": 63, "body": "def _enter_debugger(self, signum, frame):\n        Pdb().set_trace(frame.f_back)", "is_method": true, "class_name": "Debugger", "function_description": "Private method that triggers an interactive debugger at the caller's context in response to a signal, enabling real-time code inspection during execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj", "is_method": true, "class_name": "SpiderState", "function_description": "Factory method of the SpiderState class that initializes an instance using crawler settings and connects spider lifecycle signals for managing state during a spider's run."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "spider_closed", "line_number": 26, "body": "def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=4)", "is_method": true, "class_name": "SpiderState", "function_description": "Stores the current state of the spider to a file when the spider finishes, enabling job state persistence for resuming or analysis."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "spider_opened", "line_number": 31, "body": "def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}", "is_method": true, "class_name": "SpiderState", "function_description": "Restores or initializes the state of a spider when it starts, loading saved state data from a file if available to enable resuming or tracking crawl progress."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/spiderstate.py", "function": "statefn", "line_number": 39, "body": "def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')", "is_method": true, "class_name": "SpiderState", "function_description": "Returns the file path for the spider's current state file within the job directory, facilitating state management and persistence during spider execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memdebug.py", "function": "from_crawler", "line_number": 20, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n            raise NotConfigured\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "MemoryDebugger", "function_description": "Factory method of MemoryDebugger that initializes an instance if memory debugging is enabled and connects its cleanup method to the crawler's spider_closed signal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memdebug.py", "function": "spider_closed", "line_number": 27, "body": "def spider_closed(self, spider, reason):\n        gc.collect()\n        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n        for cls, wdict in live_refs.items():\n            if not wdict:\n                continue\n            self.stats.set_value(f'memdebug/live_refs/{cls.__name__}', len(wdict), spider=spider)", "is_method": true, "class_name": "MemoryDebugger", "function_description": "Tracks and records memory objects and garbage counts when a spider finishes, providing insights into live references and potential memory leaks for debugging purposes in a web crawling context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "from_crawler", "line_number": 41, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "CloseSpider", "function_description": "Factory method in the CloseSpider class that creates an instance using a crawler object, typically used to initialize the spider with crawler context or settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "error_count", "line_number": 44, "body": "def error_count(self, failure, response, spider):\n        self.counter['errorcount'] += 1\n        if self.counter['errorcount'] == self.close_on['errorcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_errorcount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors and increments an error count during spider runs, triggering spider closure when a predefined error threshold is reached to prevent excessive failures."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "page_count", "line_number": 49, "body": "def page_count(self, response, request, spider):\n        self.counter['pagecount'] += 1\n        if self.counter['pagecount'] == self.close_on['pagecount']:\n            self.crawler.engine.close_spider(spider, 'closespider_pagecount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors the number of processed pages and closes the spider when a specified page count limit is reached, enabling controlled termination based on crawl progress."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "spider_opened", "line_number": 54, "body": "def spider_opened(self, spider):\n        from twisted.internet import reactor\n        self.task = reactor.callLater(self.close_on['timeout'],\n                                      self.crawler.engine.close_spider, spider,\n                                      reason='closespider_timeout')", "is_method": true, "class_name": "CloseSpider", "function_description": "Method of CloseSpider that schedules automatic spider closure after a specified timeout, ensuring the spider stops running after a set period. This is useful for controlling spider runtime in crawling operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "item_scraped", "line_number": 60, "body": "def item_scraped(self, item, spider):\n        self.counter['itemcount'] += 1\n        if self.counter['itemcount'] == self.close_on['itemcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_itemcount')", "is_method": true, "class_name": "CloseSpider", "function_description": "Monitors scraped items count and closes the spider automatically once a specified item threshold is reached, enabling controlled termination of web crawling based on item quantity."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/closespider.py", "function": "spider_closed", "line_number": 65, "body": "def spider_closed(self, spider):\n        task = getattr(self, 'task', False)\n        if task and task.active():\n            task.cancel()", "is_method": true, "class_name": "CloseSpider", "function_description": "Terminates any active task associated with the spider when the spider finishes running, helping to clean up ongoing processes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "from_crawler", "line_number": 44, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "MemoryUsage", "function_description": "Class method that creates a MemoryUsage instance initialized with data from a given crawler object, facilitating memory tracking based on crawler state."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "get_virtual_size", "line_number": 47, "body": "def get_virtual_size(self):\n        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != 'darwin':\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size", "is_method": true, "class_name": "MemoryUsage", "function_description": "Method of the MemoryUsage class that returns the current process's peak virtual memory size in bytes, accounting for platform differences. It helps monitor and manage memory consumption during execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "engine_started", "line_number": 54, "body": "def engine_started(self):\n        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n        self.tasks = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)", "is_method": true, "class_name": "MemoryUsage", "function_description": "Initializes and starts periodic memory usage monitoring tasks within the MemoryUsage class to track virtual memory size and enforce warning or limit thresholds during engine operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "engine_stopped", "line_number": 69, "body": "def engine_stopped(self):\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()", "is_method": true, "class_name": "MemoryUsage", "function_description": "Stops all running tasks managed by the MemoryUsage instance to ensure no tasks are left active. It provides a clean shutdown mechanism for task management within the class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "update", "line_number": 74, "body": "def update(self):\n        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())", "is_method": true, "class_name": "MemoryUsage", "function_description": "Updates and records the maximum virtual memory size used, helping track and monitor memory consumption peaks during execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_check_limit", "line_number": 77, "body": "def _check_limit(self):\n        if self.get_virtual_size() > self.limit:\n            self.crawler.stats.set_value('memusage/limit_reached', 1)\n            mem = self.limit/1024/1024\n            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n                         {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                    f\"memory usage exceeded {mem}M at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/limit_notified', 1)\n\n            open_spiders = self.crawler.engine.open_spiders\n            if open_spiders:\n                for spider in open_spiders:\n                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n            else:\n                self.crawler.stop()", "is_method": true, "class_name": "MemoryUsage", "function_description": "Checks if memory usage exceeds a set limit and triggers shutdown procedures, including logging, notifications, and spider closure, to safely terminate crawling when memory constraints are breached."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_check_warning", "line_number": 98, "body": "def _check_warning(self):\n        if self.warned: # warn only once\n            return\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value('memusage/warning_reached', 1)\n            mem = self.warning/1024/1024\n            logger.warning(\"Memory usage reached %(memusage)dM\",\n                           {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                    f\"memory usage reached {mem}M at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/warning_notified', 1)\n            self.warned = True", "is_method": true, "class_name": "MemoryUsage", "function_description": "Checks memory usage against a threshold and issues a one-time warning with logging and optional email notification when the limit is exceeded."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/memusage.py", "function": "_send_report", "line_number": 115, "body": "def _send_report(self, rcpts, subject):\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        stats = self.crawler.stats\n        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup')/1024/1024}M\\r\\n\"\n        s += f\"Maximum memory usage          : {stats.get_value('memusage/max')/1024/1024}M\\r\\n\"\n        s += f\"Current memory usage          : {self.get_virtual_size()/1024/1024}M\\r\\n\"\n\n        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)", "is_method": true, "class_name": "MemoryUsage", "function_description": "Sends an email report detailing memory usage statistics and current engine status, providing recipients with important runtime diagnostics and resource information."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "from_crawler", "line_number": 21, "body": "def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n        if not interval:\n            raise NotConfigured\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "LogStats", "function_description": "Factory method in LogStats that initializes an instance using crawler settings and connects lifecycle signals to enable periodic logging of crawling statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "spider_opened", "line_number": 30, "body": "def spider_opened(self, spider):\n        self.pagesprev = 0\n        self.itemsprev = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)", "is_method": true, "class_name": "LogStats", "function_description": "Initializes counters and starts a periodic logging task when a spider begins, enabling regular capture of crawling statistics during the spider's execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "log", "line_number": 37, "body": "def log(self, spider):\n        items = self.stats.get_value('item_scraped_count', 0)\n        pages = self.stats.get_value('response_received_count', 0)\n        irate = (items - self.itemsprev) * self.multiplier\n        prate = (pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = pages, items\n\n        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n        log_args = {'pages': pages, 'pagerate': prate,\n                    'items': items, 'itemrate': irate}\n        logger.info(msg, log_args, extra={'spider': spider})", "is_method": true, "class_name": "LogStats", "function_description": "Logs the current crawl progress and scraping rates, reporting pages crawled and items scraped per minute. Useful for monitoring spider activity and performance during web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/logstats.py", "function": "spider_closed", "line_number": 50, "body": "def spider_closed(self, spider, reason):\n        if self.task and self.task.running:\n            self.task.stop()", "is_method": true, "class_name": "LogStats", "function_description": "Stops an ongoing task when a spider finishes its operation, ensuring proper cleanup or termination of related processes in the LogStats context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "parse_cachecontrol", "line_number": 347, "body": "def parse_cachecontrol(header):\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b','):\n        key, sep, val = directive.strip().partition(b'=')\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives", "is_method": false, "function_description": "Function that parses a Cache-Control HTTP header into a dictionary of directives and corresponding values, facilitating easy access to caching policies in HTTP communications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "rfc1123_to_epoch", "line_number": 367, "body": "def rfc1123_to_epoch(date_str):\n    try:\n        date_str = to_unicode(date_str, encoding='ascii')\n        return mktime_tz(parsedate_tz(date_str))\n    except Exception:\n        return None", "is_method": false, "function_description": "Converts an RFC 1123 formatted date string into an epoch timestamp, returning None if parsing fails. This function is useful for standardizing HTTP date headers or similar date representations into numeric time format."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_request", "line_number": 29, "body": "def should_cache_request(self, request):\n        return urlparse_cached(request).scheme not in self.ignore_schemes", "is_method": true, "class_name": "DummyPolicy", "function_description": "Determines if a request should be cached by checking if its URL scheme is not in a predefined list of ignored schemes, supporting selective caching decisions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_response", "line_number": 32, "body": "def should_cache_response(self, response, request):\n        return response.status not in self.ignore_http_codes", "is_method": true, "class_name": "DummyPolicy", "function_description": "Determines whether a response should be cached based on its HTTP status code, excluding those in a predefined ignore list. This helps control caching behavior in the DummyPolicy class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_fresh", "line_number": 35, "body": "def is_cached_response_fresh(self, cachedresponse, request):\n        return True", "is_method": true, "class_name": "DummyPolicy", "function_description": "Always indicates that a cached response is fresh regardless of its actual state, potentially serving as a placeholder or default policy in caching mechanisms."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_valid", "line_number": 38, "body": "def is_cached_response_valid(self, cachedresponse, response, request):\n        return True", "is_method": true, "class_name": "DummyPolicy", "function_description": "This method always considers cached responses as valid regardless of the input, effectively providing a policy that bypasses cache validation checks. It can be used to disable caching logic in scenarios where cache freshness is not a concern."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_parse_cachecontrol", "line_number": 54, "body": "def _parse_cachecontrol(self, r):\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b'Cache-Control', b'')\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Core method of RFC2616Policy that parses and caches Cache-Control headers from HTTP responses, filtering out specific directives to enforce caching policies consistently."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_request", "line_number": 64, "body": "def should_cache_request(self, request):\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Any other is eligible for caching\n        return True", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines whether an HTTP request is eligible for caching based on its URL scheme and cache-control directives. This enables efficient request handling by filtering out requests that should not be stored."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "should_cache_response", "line_number": 74, "body": "def should_cache_response(self, response, request):\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        elif response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        elif self.always_store:\n            return True\n        # Any hint on response expiration is good\n        elif b'max-age' in cc or b'Expires' in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        elif response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        elif response.status in (200, 203, 401):\n            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        else:\n            return False", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if an HTTP response should be cached based on RFC 2616 rules, response status, headers, and cache-control directives to guide proper caching behavior in web applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_fresh", "line_number": 103, "body": "def is_cached_response_fresh(self, cachedresponse, request):\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b'no-cache' in cc or b'no-cache' in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b'max-stale']\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Determines if a cached HTTP response is still fresh based on cache-control headers and request parameters, supporting conditional validation and client tolerance for stale responses according to RFC2616 rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "is_cached_response_valid", "line_number": 142, "body": "def is_cached_response_valid(self, cachedresponse, response, request):\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b'must-revalidate' not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Method of RFC2616Policy that determines whether a cached HTTP response can be reused based on the new response's status and cache-control rules, supporting efficient HTTP caching and conditional requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_set_conditional_validators", "line_number": 153, "body": "def _set_conditional_validators(self, request, cachedresponse):\n        if b'Last-Modified' in cachedresponse.headers:\n            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n\n        if b'ETag' in cachedresponse.headers:\n            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Private method of RFC2616Policy that sets HTTP conditional headers in a request based on the cached response\u2019s Last-Modified and ETag values to support efficient cache validation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_get_max_age", "line_number": 160, "body": "def _get_max_age(self, cc):\n        try:\n            return max(0, int(cc[b'max-age']))\n        except (KeyError, ValueError):\n            return None", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Private helper method in RFC2616Policy that extracts the 'max-age' value from cache-control directives, returning it as a non-negative integer or None if unavailable or invalid."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_compute_freshness_lifetime", "line_number": 166, "body": "def _compute_freshness_lifetime(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n\n        # Try HTTP/1.0 Expires header\n        if b'Expires' in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute fresshness lifetime\n        return 0", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Calculates the freshness lifetime of an HTTP response for caching decisions based on cache headers and heuristics. It helps determine how long a response can be reused before requiring validation or refresh."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_compute_current_age", "line_number": 197, "body": "def _compute_current_age(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n        if now > date:\n            currentage = now - date\n\n        if b'Age' in response.headers:\n            try:\n                age = int(response.headers[b'Age'])\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage", "is_method": true, "class_name": "RFC2616Policy", "function_description": "Calculates the current age of an HTTP response using its Date and Age headers, helping determine how fresh the response is relative to the current time. Useful for managing HTTP caching and validation policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "open_spider", "line_number": 225, "body": "def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, f'{spider.name}.db')\n        self.db = self.dbmodule.open(dbpath, 'c')\n\n        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Initializes and opens a DBM cache database file for a given spider, preparing the storage system to read and write cached data during the spider\u2019s operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "close_spider", "line_number": 231, "body": "def close_spider(self, spider):\n        self.db.close()", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Method of DbmCacheStorage that closes the database connection associated with the spider, ensuring proper resource cleanup when the spider finishes its execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "retrieve_response", "line_number": 234, "body": "def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Utility method in DbmCacheStorage that retrieves a cached HTTP response for a given spider and request, reconstructing it with headers, status, and body for reuse in web crawling or scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "store_response", "line_number": 246, "body": "def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        self.db[f'{key}_data'] = pickle.dumps(data, protocol=4)\n        self.db[f'{key}_time'] = str(time())", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Core method of DbmCacheStorage that saves HTTP response details paired with a request key, enabling efficient caching of web responses for spiders to avoid redundant network calls."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_read_data", "line_number": 257, "body": "def _read_data(self, spider, request):\n        key = self._request_key(request)\n        db = self.db\n        tkey = f'{key}_time'\n        if tkey not in db:\n            return  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        return pickle.loads(db[f'{key}_data'])", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Private method of DbmCacheStorage that retrieves and deserializes cached response data for a given request if it exists and has not expired, supporting efficient request caching with expiration management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_request_key", "line_number": 270, "body": "def _request_key(self, request):\n        return request_fingerprint(request)", "is_method": true, "class_name": "DbmCacheStorage", "function_description": "Private utility method in DbmCacheStorage that generates a unique fingerprint key for a given request, facilitating consistent cache indexing and lookup."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "open_spider", "line_number": 282, "body": "def open_spider(self, spider):\n        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n                     extra={'spider': spider})", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Logs the activation of filesystem cache storage when a spider starts, indicating the cache directory in use for debugging purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "retrieve_response", "line_number": 289, "body": "def retrieve_response(self, spider, request):\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return  # not cached\n        rpath = self._get_request_path(spider, request)\n        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n            body = f.read()\n        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n            rawheaders = f.read()\n        url = metadata.get('response_url')\n        status = metadata['status']\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Provides cached HTTP responses from the filesystem for given requests, returning None if no cache exists. This enables efficient reuse of previously fetched web data in web crawling contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "store_response", "line_number": 306, "body": "def store_response(self, spider, request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n            f.write(request.body)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Stores the HTTP response and related request metadata on the filesystem, enabling persistent caching for web crawling activities within the FilesystemCacheStorage class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_get_request_path", "line_number": 331, "body": "def _get_request_path(self, spider, request):\n        key = request_fingerprint(request)\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Constructs a filesystem path to store or retrieve a cached request for a given spider using the request's unique fingerprint. This enables organized, efficient access to cached web requests within the caching system."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/httpcache.py", "function": "_read_meta", "line_number": 335, "body": "def _read_meta(self, spider, request):\n        rpath = self._get_request_path(spider, request)\n        metapath = os.path.join(rpath, 'pickled_meta')\n        if not os.path.exists(metapath):\n            return  # not found\n        mtime = os.stat(metapath).st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return  # expired\n        with self._open(metapath, 'rb') as f:\n            return pickle.load(f)", "is_method": true, "class_name": "FilesystemCacheStorage", "function_description": "Internal method of FilesystemCacheStorage that reads and returns cached metadata for a given spider request if it exists and hasn't expired; otherwise, it returns None."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "from_crawler", "line_number": 22, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Class method that creates an AutoThrottle instance using a crawler object, facilitating integration with crawling framework components."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_spider_opened", "line_number": 25, "body": "def _spider_opened(self, spider):\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Initializes throttle delay settings on spider start, configuring minimum, maximum, and initial download delays for managing request pacing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_min_delay", "line_number": 30, "body": "def _min_delay(self, spider):\n        s = self.crawler.settings\n        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))", "is_method": true, "class_name": "AutoThrottle", "function_description": "Method in AutoThrottle that determines the current minimum download delay for a spider, considering its specific setting or falling back to the default crawler configuration. It helps regulate request timing to optimize crawling speed and server load."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_max_delay", "line_number": 34, "body": "def _max_delay(self, spider):\n        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')", "is_method": true, "class_name": "AutoThrottle", "function_description": "Returns the maximum delay allowed by the AutoThrottle settings to control crawl rate for a given spider. This helps manage request pacing to avoid overwhelming target servers."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_start_delay", "line_number": 37, "body": "def _start_delay(self, spider):\n        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))", "is_method": true, "class_name": "AutoThrottle", "function_description": "Returns the initial delay duration for request throttling, ensuring it is not less than the minimum configured delay. This supports controlled request pacing at the start of crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_response_downloaded", "line_number": 40, "body": "def _response_downloaded(self, response, request, spider):\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get('download_latency')\n        if latency is None or slot is None:\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    'slot': key, 'concurrency': conc,\n                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n                    'latency': latency * 1000, 'size': size\n                },\n                extra={'spider': spider}\n            )", "is_method": true, "class_name": "AutoThrottle", "function_description": "Monitors response download latency to adjust request delays dynamically, optimizing crawl rate for each slot. It optionally logs detailed timing and size info to aid debugging in the AutoThrottle component."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_get_slot", "line_number": 64, "body": "def _get_slot(self, request, spider):\n        key = request.meta.get('download_slot')\n        return key, self.crawler.engine.downloader.slots.get(key)", "is_method": true, "class_name": "AutoThrottle", "function_description": "Private method of the AutoThrottle class that obtains the download slot identifier and its corresponding slot object for a given request, facilitating management of concurrent downloads per slot."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/throttle.py", "function": "_adjust_delay", "line_number": 68, "body": "def _adjust_delay(self, slot, latency, response):\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay", "is_method": true, "class_name": "AutoThrottle", "function_description": "Adjusts request delay dynamically to maintain a target concurrency based on server latency and response status, ensuring delays stay within configured bounds to optimize request pacing and avoid premature throttle reduction on error responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "from_crawler", "line_number": 61, "body": "def from_crawler(cls, crawler):\n        return cls(crawler)", "is_method": true, "class_name": "TelnetConsole", "function_description": "Factory method for TelnetConsole that instantiates the class using a crawler, facilitating integration with crawling frameworks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "start_listening", "line_number": 64, "body": "def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\"Telnet console listening on %(host)s:%(port)d\",\n                    {'host': h.host, 'port': h.port},\n                    extra={'crawler': self.crawler})", "is_method": true, "class_name": "TelnetConsole", "function_description": "Starts a Telnet console server that listens for incoming TCP connections on a specified host and port range, enabling remote interaction with the system via Telnet."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "stop_listening", "line_number": 71, "body": "def stop_listening(self):\n        self.port.stopListening()", "is_method": true, "class_name": "TelnetConsole", "function_description": "Stops the TelnetConsole from accepting new incoming connections by halting the listening port. This method is useful for gracefully shutting down or disabling the Telnet service."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "protocol", "line_number": 74, "body": "def protocol(self):\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode('utf8')\n                    and credentials.checkPassword(self.password.encode('utf8'))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(\n            telnet.AuthenticatingTelnetProtocol,\n            Portal()\n        )", "is_method": true, "class_name": "TelnetConsole", "function_description": "Provides a Telnet protocol instance with authentication by validating user credentials and initializing an interactive Telnet session incorporating a server protocol and debugging interface. This supports secure remote console access within the TelnetConsole class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "_get_telnet_vars", "line_number": 97, "body": "def _get_telnet_vars(self):\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        telnet_vars = {\n            'engine': self.crawler.engine,\n            'spider': self.crawler.engine.spider,\n            'slot': self.crawler.engine.slot,\n            'crawler': self.crawler,\n            'extensions': self.crawler.extensions,\n            'stats': self.crawler.stats,\n            'settings': self.crawler.settings,\n            'est': lambda: print_engine_status(self.crawler.engine),\n            'p': pprint.pprint,\n            'prefs': print_live_refs,\n            'help': \"This is Scrapy telnet console. For more info see: \"\n                    \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars", "is_method": true, "class_name": "TelnetConsole", "function_description": "Returns a dictionary of key Scrapy crawler components and utilities for use within the Telnet console, enabling interactive inspection and control of the crawling process."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/telnet.py", "function": "login", "line_number": 78, "body": "def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode('utf8')\n                    and credentials.checkPassword(self.password.encode('utf8'))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)", "is_method": true, "class_name": "Portal", "function_description": "Core login function of the Portal class that validates user credentials and establishes a Telnet protocol session with debugging interfaces for remote access management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/statsmailer.py", "function": "from_crawler", "line_number": 19, "body": "def from_crawler(cls, crawler):\n        recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n        if not recipients:\n            raise NotConfigured\n        mail = MailSender.from_settings(crawler.settings)\n        o = cls(crawler.stats, recipients, mail)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o", "is_method": true, "class_name": "StatsMailer", "function_description": "Class method of StatsMailer that initializes an instance using crawler settings, retrieves email recipients, sets up a mail sender, and connects a signal to trigger stats emailing when a spider closes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/statsmailer.py", "function": "spider_closed", "line_number": 28, "body": "def spider_closed(self, spider):\n        spider_stats = self.stats.get_stats(spider)\n        body = \"Global stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)", "is_method": true, "class_name": "StatsMailer", "function_description": "Sends an email summarizing the global and specific spider statistics when a spider finishes running, enabling automated reporting of Scrapy crawl results to designated recipients."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "from_crawler", "line_number": 16, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o", "is_method": true, "class_name": "CoreStats", "function_description": "Creates and returns a CoreStats instance linked to crawler events, enabling it to automatically track and respond to spider and item lifecycle signals during web scraping sessions."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "spider_opened", "line_number": 25, "body": "def spider_opened(self, spider):\n        self.start_time = datetime.utcnow()\n        self.stats.set_value('start_time', self.start_time, spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Records and stores the UTC start time when a spider begins execution, enabling tracking of the spider's runtime duration and performance metrics in the CoreStats context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "spider_closed", "line_number": 29, "body": "def spider_closed(self, spider, reason):\n        finish_time = datetime.utcnow()\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n        self.stats.set_value('finish_time', finish_time, spider=spider)\n        self.stats.set_value('finish_reason', reason, spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Tracks and records the elapsed time, finish timestamp, and reason when a spider completes its run, updating these stats for monitoring or analysis purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "item_scraped", "line_number": 37, "body": "def item_scraped(self, item, spider):\n        self.stats.inc_value('item_scraped_count', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "CoreStats method that increments the count of items scraped by a specific spider, supporting tracking and aggregation of scraping statistics during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "response_received", "line_number": 40, "body": "def response_received(self, spider):\n        self.stats.inc_value('response_received_count', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Increments a counter tracking the number of responses received by a specified spider, supporting monitoring and statistics collection during web crawling operations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/extensions/corestats.py", "function": "item_dropped", "line_number": 43, "body": "def item_dropped(self, item, spider, exception):\n        reason = exception.__class__.__name__\n        self.stats.inc_value('item_dropped_count', spider=spider)\n        self.stats.inc_value(f'item_dropped_reasons_count/{reason}', spider=spider)", "is_method": true, "class_name": "CoreStats", "function_description": "Counts and records dropped items along with the specific reasons for dropping them, supporting detailed tracking of failures within the CoreStats class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "from_crawler", "line_number": 24, "body": "def from_crawler(cls, crawler):\n        return cls(crawler.settings)", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "Factory method for HttpErrorMiddleware that creates an instance using configuration settings from the provided crawler. It enables integration of middleware within a crawling framework by initializing with crawler-specific settings."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "process_spider_input", "line_number": 31, "body": "def process_spider_input(self, response, spider):\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if meta.get('handle_httpstatus_all', False):\n            return\n        if 'handle_httpstatus_list' in meta:\n            allowed_statuses = meta['handle_httpstatus_list']\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, 'Ignoring non-200 response')", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "Core method of HttpErrorMiddleware that checks HTTP response status codes and raises an error for unwanted statuses, enabling spiders to handle or ignore specific HTTP errors during web scraping."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/httperror.py", "function": "process_spider_exception", "line_number": 47, "body": "def process_spider_exception(self, response, exception, spider):\n        if isinstance(exception, HttpError):\n            spider.crawler.stats.inc_value('httperror/response_ignored_count')\n            spider.crawler.stats.inc_value(\n                f'httperror/response_ignored_status_count/{response.status}'\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {'response': response}, extra={'spider': spider},\n            )\n            return []", "is_method": true, "class_name": "HttpErrorMiddleware", "function_description": "Handles HTTP errors in web crawling by logging and updating stats, then discarding responses with unhandled or disallowed HTTP status codes during spider processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "from_settings", "line_number": 21, "body": "def from_settings(cls, settings):\n        maxlength = settings.getint('URLLENGTH_LIMIT')\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Factory method for UrlLengthMiddleware that creates an instance using a URL length limit from configuration settings, raising an error if the limit is not specified."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "process_spider_output", "line_number": 27, "body": "def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.info(\n                    \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                    {'maxlength': self.maxlength, 'url': request.url},\n                    extra={'spider': spider}\n                )\n                spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Filters out requests with URLs exceeding a specified maximum length during web crawling, helping to avoid processing overly long URLs and improve crawl efficiency and resource management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/urllength.py", "function": "_filter", "line_number": 28, "body": "def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.info(\n                    \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                    {'maxlength': self.maxlength, 'url': request.url},\n                    extra={'spider': spider}\n                )\n                spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                return False\n            else:\n                return True", "is_method": true, "class_name": "UrlLengthMiddleware", "function_description": "Filters out requests with URLs exceeding a specified maximum length, logging and counting such events. This function helps manage crawling efficiency by ignoring overly long URLs in web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "_load_policy_class", "line_number": 271, "body": "def _load_policy_class(policy, warning_only=False):\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return load_object(policy)\n    except ValueError:\n        try:\n            return _policy_classes[policy.lower()]\n        except KeyError:\n            msg = f\"Could not load referrer policy {policy!r}\"\n            if not warning_only:\n                raise RuntimeError(msg)\n            else:\n                warnings.warn(msg, RuntimeWarning)\n                return None", "is_method": false, "function_description": "Utility function that attempts to load a referrer policy class from a given string path or matches it against standard W3C referrer policies, optionally issuing warnings instead of errors on failure. It supports flexible policy retrieval for HTTP referrer handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "stripped_referrer", "line_number": 40, "body": "def stripped_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Returns a sanitized version of a URL's referrer unless its scheme is in a restricted list, ensuring controlled referrer information based on policy rules."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "origin_referrer", "line_number": 44, "body": "def origin_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Provides the origin part of a URL as the referrer unless the URL scheme is in a predefined no-referrer list, enabling controlled referrer policy handling in web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "strip_url", "line_number": 48, "body": "def strip_url(self, url, origin_only=False):\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(url,\n                         strip_credentials=True,\n                         strip_fragment=True,\n                         strip_default_port=True,\n                         origin_only=origin_only)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Provides a sanitized version of a URL by removing sensitive information and optionally stripping it down to its origin, supporting secure and privacy-conscious referrer handling in web contexts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "origin", "line_number": 70, "body": "def origin(self, url):\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Method of ReferrerPolicy that returns the serialized origin (scheme, host, and path) of a given URL, providing a standardized origin representation for requests or responses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "potentially_trustworthy", "line_number": 74, "body": "def potentially_trustworthy(self, url):\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in ('data',):\n            return False\n        return self.tls_protected(url)", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Method of ReferrerPolicy that determines if a URL is potentially trustworthy based on its scheme and TLS protection status, supporting secure context evaluations in web security policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "tls_protected", "line_number": 81, "body": "def tls_protected(self, url):\n        return urlparse(url).scheme in ('https', 'ftps')", "is_method": true, "class_name": "ReferrerPolicy", "function_description": "Determines if a given URL uses a transport layer security protocol by checking for secure schemes like HTTPS or FTPS. This helps enforce secure communication policies within the ReferrerPolicy context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 95, "body": "def referrer(self, response_url, request_url):\n        return None", "is_method": true, "class_name": "NoReferrerPolicy", "function_description": "Returns None as the referrer for any request, effectively disabling referrer information in HTTP requests within the NoReferrerPolicy context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 115, "body": "def referrer(self, response_url, request_url):\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)", "is_method": true, "class_name": "NoReferrerWhenDowngradePolicy", "function_description": "Method in NoReferrerWhenDowngradePolicy that determines the referrer header to send based on the security levels of the response and request URLs, helping to prevent leaking referrer information when downgrading from HTTPS to HTTP."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 132, "body": "def referrer(self, response_url, request_url):\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)", "is_method": true, "class_name": "SameOriginPolicy", "function_description": "Checks if two URLs share the same origin and, if so, returns a sanitized referrer string from the response URL. This aids in enforcing same-origin policy rules for referrer handling in web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 148, "body": "def referrer(self, response_url, request_url):\n        return self.origin_referrer(response_url)", "is_method": true, "class_name": "OriginPolicy", "function_description": "Returns the origin referrer URL for a given response, providing referrer information used in managing request origins and related policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 167, "body": "def referrer(self, response_url, request_url):\n        if (\n            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)", "is_method": true, "class_name": "StrictOriginPolicy", "function_description": "Method of StrictOriginPolicy that determines the appropriate referrer value based on the security properties of response and request URLs, enforcing strict origin-based referrer policies for web requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 188, "body": "def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        else:\n            return origin", "is_method": true, "class_name": "OriginWhenCrossOriginPolicy", "function_description": "Determines the appropriate referrer value based on cross-origin policy by comparing origins of response and request URLs, ensuring correct referrer information for same-origin or cross-origin requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 215, "body": "def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        elif (\n            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)", "is_method": true, "class_name": "StrictOriginWhenCrossOriginPolicy", "function_description": "Method of StrictOriginWhenCrossOriginPolicy that determines the appropriate referrer value based on the relationship and security of the response and request URLs, enforcing strict origin policy rules for cross-origin requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "referrer", "line_number": 241, "body": "def referrer(self, response_url, request_url):\n        return self.stripped_referrer(response_url)", "is_method": true, "class_name": "UnsafeUrlPolicy", "function_description": "Returns a sanitized referrer URL based on the response URL, used to enforce unsafe URL handling policies."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "from_crawler", "line_number": 300, "body": "def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('REFERER_ENABLED'):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Creates and configures a RefererMiddleware instance from the crawler settings, enabling referer handling if activated, and hooks into the crawler's request scheduling to manage referer headers during redirects."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "policy", "line_number": 310, "body": "def policy(self, resp_or_url, request):\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get('referrer_policy')\n        if policy_name is None:\n            if isinstance(resp_or_url, Response):\n                policy_header = resp_or_url.headers.get('Referrer-Policy')\n                if policy_header is not None:\n                    policy_name = to_unicode(policy_header.decode('latin1'))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Determines the appropriate Referrer-Policy for an HTTP request based on request metadata, a parent response's header, or default settings. It ensures valid policy selection for controlling referrer information sent with requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "process_spider_output", "line_number": 335, "body": "def process_spider_output(self, response, result, spider):\n        def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r\n        return (_set_referer(r) for r in result or ())", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Middleware method in RefererMiddleware that adds appropriate 'Referer' headers to outgoing requests based on the originating response, helping maintain referer information during web crawling and request handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "request_scheduled", "line_number": 344, "body": "def request_scheduled(self, request, spider):\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get('redirect_urls', [])\n        if redirected_urls:\n            request_referrer = request.headers.get('Referer')\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url)\n                if policy_referrer != request_referrer:\n                    if policy_referrer is None:\n                        request.headers.pop('Referer')\n                    else:\n                        request.headers['Referer'] = policy_referrer", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Middleware method that adjusts the \"Referer\" header on redirected requests to ensure it complies with referrer policies, preserving correct referrer information during HTTP redirections."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/referer.py", "function": "_set_referer", "line_number": 336, "body": "def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r", "is_method": true, "class_name": "RefererMiddleware", "function_description": "Private helper method in RefererMiddleware that sets the 'Referer' header on a Request object based on the current response and request URLs to maintain accurate referer information during HTTP requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "from_crawler", "line_number": 23, "body": "def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Factory method for OffsiteMiddleware that initializes an instance with crawler stats and connects its spider_opened handler to the crawler's spider_opened signal."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "process_spider_output", "line_number": 28, "body": "def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\n                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                            {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Filters and yields requests produced by a spider, allowing only those within allowed domains or marked to bypass filtering, while tracking stats and logging filtered offsite requests during web crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "should_follow", "line_number": 45, "body": "def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Determines whether a request's hostname matches the allowed pattern for processing. This method filters requests to control offsite link following in web crawling tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "get_host_regex", "line_number": 51, "body": "def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('')  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n                message = (\"allowed_domains accepts only domains, not URLs. \"\n                           f\"Ignoring URL entry {domain} in allowed_domains.\")\n                warnings.warn(message, URLWarning)\n            elif port_pattern.search(domain):\n                message = (\"allowed_domains accepts only domains without ports. \"\n                           f\"Ignoring entry {domain} in allowed_domains.\")\n                warnings.warn(message, PortWarning)\n            else:\n                domains.append(re.escape(domain))\n        regex = fr'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Provides a regex pattern to restrict spider requests to allowed domains, enforcing offsite policies by validating and compiling domain names from spider settings for URL filtering during crawling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/offsite.py", "function": "spider_opened", "line_number": 75, "body": "def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()", "is_method": true, "class_name": "OffsiteMiddleware", "function_description": "Initializes the middleware when a spider starts by setting up the host matching pattern and resetting the set of seen domains. This prepares the middleware to track and filter requests during the spider's crawl session."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "from_crawler", "line_number": 23, "body": "def from_crawler(cls, crawler):\n        settings = crawler.settings\n        maxdepth = settings.getint('DEPTH_LIMIT')\n        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n        prio = settings.getint('DEPTH_PRIORITY')\n        return cls(maxdepth, crawler.stats, verbose, prio)", "is_method": true, "class_name": "DepthMiddleware", "function_description": "Factory method for DepthMiddleware that initializes an instance using crawler settings, configuring depth limits, verbosity, and priority to control and monitor crawl depth during web scraping tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "process_spider_output", "line_number": 30, "body": "def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value(f'request_depth_count/{depth}',\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True\n\n        # base case (depth=0)\n        if 'depth' not in response.meta:\n            response.meta['depth'] = 0\n            if self.verbose_stats:\n                self.stats.inc_value('request_depth_count/0', spider=spider)\n\n        return (r for r in result or () if _filter(r))", "is_method": true, "class_name": "DepthMiddleware", "function_description": "Method of DepthMiddleware that tracks and limits crawl request depth by filtering out requests exceeding max depth, adjusting priorities, and updating crawl depth statistics."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/spidermiddlewares/depth.py", "function": "_filter", "line_number": 31, "body": "def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value(f'request_depth_count/{depth}',\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True", "is_method": true, "class_name": "DepthMiddleware", "function_description": "Core method of DepthMiddleware that filters requests based on their crawl depth, enforcing maximum depth limits and adjusting priorities to control crawling behavior in web scraping workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "_create_testcase", "line_number": 171, "body": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return f\"[{spider}] {method.__name__} ({desc})\"\n\n    name = f'{spider}_{method.__name__}'\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)", "is_method": false, "function_description": "Generates a dynamic test case class for a given method, labeling it with spider and description information, to facilitate contract testing within a spider-based testing framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "add_pre_hook", "line_number": 22, "body": "def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n\n            request.callback = wrapper\n\n        return request", "is_method": true, "class_name": "Contract", "function_description": "Method of the Contract class that wraps a request's callback to execute a pre-processing hook with test result tracking before running the original callback, facilitating validation or setup steps in request handling workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "add_post_hook", "line_number": 45, "body": "def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request", "is_method": true, "class_name": "Contract", "function_description": "Provides a mechanism to wrap a request's callback with a post-processing hook that executes additional test logic, capturing and reporting results on success, failure, or errors within a testing context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "adjust_request_args", "line_number": 69, "body": "def adjust_request_args(self, args):\n        return args", "is_method": true, "class_name": "Contract", "function_description": "This method returns the input arguments unchanged, serving as a placeholder for modifying request parameters if needed."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "tested_methods_from_spidercls", "line_number": 80, "body": "def tested_methods_from_spidercls(self, spidercls):\n        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n        methods = []\n        for key, value in getmembers(spidercls):\n            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                methods.append(key)\n\n        return methods", "is_method": true, "class_name": "ContractsManager", "function_description": "Core utility of ContractsManager that identifies and returns method names from a given class whose docstrings start with a decorator, aiding in recognizing specially annotated methods for contract-related processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "extract_contracts", "line_number": 89, "body": "def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts", "is_method": true, "class_name": "ContractsManager", "function_description": "Core function of ContractsManager that parses a method's docstring for contract annotations and extracts corresponding contract objects for validation or enforcement purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "from_spider", "line_number": 102, "body": "def from_spider(self, spider, results):\n        requests = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, 'contract')\n                results.addError(case, sys.exc_info())\n\n        return requests", "is_method": true, "class_name": "ContractsManager", "function_description": "Core method of ContractsManager that executes contract tests from a spider's methods, handling exceptions and collecting resulting requests to validate spider behavior against defined contracts."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "from_method", "line_number": 114, "body": "def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs['dont_filter'] = True\n            kwargs['callback'] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove('self')\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request", "is_method": true, "class_name": "ContractsManager", "function_description": "Constructs and configures a request object based on extracted contracts from a method, applying hooks and argument adjustments to tailor request behavior dynamically during execution."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "_clean_req", "line_number": 148, "body": "def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper", "is_method": true, "class_name": "ContractsManager", "function_description": "Internal method in ContractsManager that wraps a request's callbacks to suppress object returns and capture any errors for logging or testing purposes. It ensures all exceptions during request processing are recorded without propagating them."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "cb_wrapper", "line_number": 154, "body": "def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())", "is_method": true, "class_name": "ContractsManager", "function_description": "Utility method in ContractsManager that wraps a callback execution to safely process its output and capture any exceptions as test errors, facilitating robust callback handling in spider response processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/__init__.py", "function": "wrapper", "line_number": 27, "body": "def wrapper(response, **cb_kwargs):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response, **cb_kwargs)))", "is_method": true, "class_name": "Contract", "function_description": "Utility method in the Contract class that wraps a callback with pre-processing, handles test case result logging, and ensures the callback's output is converted into a list for further processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "adjust_request_args", "line_number": 18, "body": "def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args", "is_method": true, "class_name": "UrlContract", "function_description": "Method of UrlContract that injects the primary URL argument into request parameters, ensuring the URL value is included in the request arguments."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "adjust_request_args", "line_number": 32, "body": "def adjust_request_args(self, args):\n        args['cb_kwargs'] = json.loads(' '.join(self.args))\n        return args", "is_method": true, "class_name": "CallbackKeywordArgumentsContract", "function_description": "Service method of CallbackKeywordArgumentsContract that modifies a request argument dictionary by adding parsed callback keyword arguments from stored JSON-encoded strings. It enables dynamic injection of callback parameters into request processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "post_process", "line_number": 78, "body": "def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if self.obj_type_verifier(x):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = f'{self.min_bound}..{self.max_bound}'\n\n            raise ContractFail(f\"Returned {occurrences} {self.obj_name}, expected {expected}\")", "is_method": true, "class_name": "ReturnsContract", "function_description": "Method of the ReturnsContract class that validates the quantity of returned objects against predefined bounds, raising an error if the count falls outside the acceptable range."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/contracts/default.py", "function": "post_process", "line_number": 102, "body": "def post_process(self, output):\n        for x in output:\n            if is_item(x):\n                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                if missing:\n                    missing_fields = \", \".join(missing)\n                    raise ContractFail(f\"Missing fields: {missing_fields}\")", "is_method": true, "class_name": "ScrapesContract", "function_description": "Checks each item in the output for required fields and raises an error if any are missing. This ensures data integrity by validating that all specified arguments are present in scraped results."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_handle_statuses", "line_number": 49, "body": "def _handle_statuses(self, allow_redirects):\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))", "is_method": true, "class_name": "MediaPipeline", "function_description": "Private method in MediaPipeline that sets which HTTP status codes to handle, excluding redirect statuses when redirects are allowed. It configures the pipeline's behavior regarding HTTP response handling."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_key_for_pipe", "line_number": 54, "body": "def _key_for_pipe(self, key, base_class_name=None, settings=None):\n        \"\"\"\n        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n        'IMAGES'\n        >>> class MyPipe(MediaPipeline):\n        ...     pass\n        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n        'MYPIPE_IMAGES'\n        \"\"\"\n        class_name = self.__class__.__name__\n        formatted_key = f\"{class_name.upper()}_{key}\"\n        if (\n            not base_class_name\n            or class_name == base_class_name\n            or settings and not settings.get(formatted_key)\n        ):\n            return key\n        return formatted_key", "is_method": true, "class_name": "MediaPipeline", "function_description": "Generates a namespaced key by prefixing it with the pipeline class name to avoid conflicts, optionally considering base class and settings. Useful for uniquely identifying pipeline-specific resources or configurations."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "from_crawler", "line_number": 74, "body": "def from_crawler(cls, crawler):\n        try:\n            pipe = cls.from_settings(crawler.settings)\n        except AttributeError:\n            pipe = cls()\n        pipe.crawler = crawler\n        return pipe", "is_method": true, "class_name": "MediaPipeline", "function_description": "Class method that initializes a MediaPipeline instance using crawler settings if available, or creates a default instance, associating it with the given crawler for further media processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "open_spider", "line_number": 82, "body": "def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Initializes and stores metadata about the given spider within the MediaPipeline to prepare for processing related to that spider."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "process_item", "line_number": 85, "body": "def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info, item) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=True)\n        return dfd.addCallback(self.item_completed, item, info)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Handles media requests for a given item by processing all related requests concurrently and triggers a callback upon completion. It facilitates asynchronous management of media downloads within the MediaPipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_process_request", "line_number": 92, "body": "def _process_request(self, request, info, item):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n        dfd.addCallback(self._check_media_to_download, request, info, item=item)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Internal method of MediaPipeline that manages media download requests with deduplication, caching, and callback handling to prevent redundant downloads and coordinate asynchronous processing of identical requests."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_make_compatible", "line_number": 121, "body": "def _make_compatible(self):\n        \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n        methods = [\n            \"file_path\", \"media_to_download\", \"media_downloaded\",\n            \"file_downloaded\", \"image_downloaded\", \"get_images\"\n        ]\n\n        for method_name in methods:\n            method = getattr(self, method_name, None)\n            if callable(method):\n                setattr(self, method_name, self._compatible(method))", "is_method": true, "class_name": "MediaPipeline", "function_description": "Private method in MediaPipeline that ensures specified overridable methods remain backwards compatible by wrapping them with a compatibility handler, supporting smooth upgrades in subclasses without breaking existing functionality."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_compatible", "line_number": 133, "body": "def _compatible(self, func):\n        \"\"\"Wrapper for overridable methods to allow backwards compatibility\"\"\"\n        self._check_signature(func)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if self._expects_item[func.__name__]:\n                return func(*args, **kwargs)\n\n            kwargs.pop('item', None)\n            return func(*args, **kwargs)\n\n        return wrapper", "is_method": true, "class_name": "MediaPipeline", "function_description": "Utility method in MediaPipeline that wraps functions to ensure backwards compatibility by adapting arguments based on expected parameters, allowing seamless integration of updated or legacy methods."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_check_signature", "line_number": 147, "body": "def _check_signature(self, func):\n        sig = signature(func)\n        self._expects_item[func.__name__] = True\n\n        if 'item' not in sig.parameters:\n            old_params = str(sig)[1:-1]\n            new_params = old_params + \", *, item=None\"\n            warn(f'{func.__name__}(self, {old_params}) is deprecated, '\n                 f'please use {func.__name__}(self, {new_params})',\n                 ScrapyDeprecationWarning, stacklevel=2)\n            self._expects_item[func.__name__] = False", "is_method": true, "class_name": "MediaPipeline", "function_description": "Utility method in MediaPipeline that checks if a function accepts an 'item' parameter, issuing a deprecation warning if it does not, to ensure compatibility and guide method signature updates."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_modify_media_request", "line_number": 159, "body": "def _modify_media_request(self, request):\n        if self.handle_httpstatus_list:\n            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n        else:\n            request.meta['handle_httpstatus_all'] = True", "is_method": true, "class_name": "MediaPipeline", "function_description": "Private method in MediaPipeline that updates a media request's metadata to specify which HTTP status codes should be handled during processing. It enables customized error handling by either targeting specific statuses or all statuses."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_check_media_to_download", "line_number": 165, "body": "def _check_media_to_download(self, result, request, info, item):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            self._modify_media_request(request)\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd", "is_method": true, "class_name": "MediaPipeline", "function_description": "Handles conditional media downloading by either invoking a custom download function or triggering a default download process, managing callbacks for success or failure during a media pipeline operation."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "_cache_result_and_execute_waiters", "line_number": 182, "body": "def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # What happens when the media_downloaded callback raises an\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that the original\n            # StopIteration exception (which in turn contains the failed\n            # Response and by extension, the original Request) gets encapsulated\n            # within the FileException context.\n            #\n            # Originally, Scrapy was using twisted.internet.defer.returnValue\n            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n            # encapsulating the returned Response in a _DefGen_Return exception\n            # instead of a StopIteration.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the encapsulated exception when it is a StopIteration instance\n            #\n            # This problem does not occur in Python 2.7 since we don't have\n            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n            context = getattr(result.value, '__context__', None)\n            if isinstance(context, StopIteration):\n                setattr(result.value, '__context__', None)\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Private method of MediaPipeline that caches the download result (success or cleaned failure) for a file and triggers all waiting callbacks, ensuring efficient handling of concurrent requests and preventing memory leaks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_downloaded", "line_number": 228, "body": "def media_downloaded(self, response, request, info, *, item=None):\n        \"\"\"Handler for success downloads\"\"\"\n        return response", "is_method": true, "class_name": "MediaPipeline", "function_description": "Returns the successful response of a media download operation, serving as a callback to handle downloaded media within the MediaPipeline class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "media_failed", "line_number": 232, "body": "def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure", "is_method": true, "class_name": "MediaPipeline", "function_description": "Handles download failures by passing the failure information through, enabling consistent failure reporting in media download workflows."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "item_completed", "line_number": 236, "body": "def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item", "is_method": true, "class_name": "MediaPipeline", "function_description": "This MediaPipeline method handles post-processing after all media requests for an item finish, logging any failures encountered during processing. It ensures error tracking for media handling operations within the pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/media.py", "function": "wrapper", "line_number": 138, "body": "def wrapper(*args, **kwargs):\n            if self._expects_item[func.__name__]:\n                return func(*args, **kwargs)\n\n            kwargs.pop('item', None)\n            return func(*args, **kwargs)", "is_method": true, "class_name": "MediaPipeline", "function_description": "Utility method in MediaPipeline that conditionally removes the 'item' argument before calling a wrapped function, enabling flexible function invocation depending on expected parameters."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "from_settings", "line_number": 91, "body": "def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['IMAGES_STORE']\n        return cls(store_uri, settings=settings)", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Creates an ImagesPipeline instance configured with cloud storage credentials and settings from a provided configuration, enabling image storage across S3, GCS, or FTP backends."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "file_downloaded", "line_number": 113, "body": "def file_downloaded(self, response, request, info, *, item=None):\n        return self.image_downloaded(response, request, info, item=item)", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Delegates file download handling to the image_downloaded method, integrating file processing within an image pipeline workflow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "image_downloaded", "line_number": 116, "body": "def image_downloaded(self, response, request, info, *, item=None):\n        checksum = None\n        for path, image, buf in self.get_images(response, request, info, item=item):\n            if checksum is None:\n                buf.seek(0)\n                checksum = md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path, buf, info,\n                meta={'width': width, 'height': height},\n                headers={'Content-Type': 'image/jpeg'})\n        return checksum", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Provides functionality to download and store images from a response, returning a checksum for verifying the first image's integrity. This service supports handling, persisting, and tracking image files within an image processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "get_images", "line_number": 129, "body": "def get_images(self, response, request, info, *, item=None):\n        path = self.file_path(request, response=response, info=info, item=item)\n        orig_image = self._Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\"Image too small \"\n                                 f\"({width}x{height} < \"\n                                 f\"{self.min_width}x{self.min_height})\")\n\n        image, buf = self.convert_image(orig_image)\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n            thumb_image, thumb_buf = self.convert_image(image, size)\n            yield thumb_path, thumb_image, thumb_buf", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Core method of the ImagesPipeline class that processes a downloaded image, validates its dimensions, converts it, and generates its thumbnails, providing file paths and image data for further handling or storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "convert_image", "line_number": 147, "body": "def convert_image(self, image, size=None):\n        if image.format == 'PNG' and image.mode == 'RGBA':\n            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode != 'RGB':\n            image = image.convert('RGB')\n\n        if size:\n            image = image.copy()\n            image.thumbnail(size, self._Image.ANTIALIAS)\n\n        buf = BytesIO()\n        image.save(buf, 'JPEG')\n        return image, buf", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Core method of ImagesPipeline that standardizes image format by converting various modes to RGB, optionally resizes it, and returns the processed image along with a JPEG byte buffer suitable for downstream image handling or storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "get_media_requests", "line_number": 168, "body": "def get_media_requests(self, item, info):\n        urls = ItemAdapter(item).get(self.images_urls_field, [])\n        return [Request(u) for u in urls]", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Utility method of the ImagesPipeline class that generates download requests for image URLs extracted from an item, facilitating media retrieval during scraping or processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "item_completed", "line_number": 172, "body": "def item_completed(self, results, item, info):\n        with suppress(KeyError):\n            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n        return item", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Final description:  \nMethod that updates a scraped item with successfully processed image results, attaching them to a specified field for further use or storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "file_path", "line_number": 177, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return f'full/{image_guid}.jpg'", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Generates a unique file path for an image download request based on the request URL, ensuring consistent and collision-free storage naming within the ImagesPipeline processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/images.py", "function": "thumb_path", "line_number": 181, "body": "def thumb_path(self, request, thumb_id, response=None, info=None):\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return f'thumbs/{thumb_id}/{thumb_guid}.jpg'", "is_method": true, "class_name": "ImagesPipeline", "function_description": "Generates a consistent file path for storing or retrieving a thumbnail image based on the request URL and thumbnail identifier. This supports organized thumbnail management within the ImagesPipeline workflow."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 49, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(os.path.dirname(absolute_path), info)\n        with open(absolute_path, 'wb') as f:\n            f.write(buf.getvalue())", "is_method": true, "class_name": "FSFilesStore", "function_description": "Stores a file's binary content to a specified path in the filesystem, creating necessary directories. Provides a foundational method for saving file data within the FSFilesStore class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 55, "body": "def stat_file(self, path, info):\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = os.path.getmtime(absolute_path)\n        except os.error:\n            return {}\n\n        with open(absolute_path, 'rb') as f:\n            checksum = md5sum(f)\n\n        return {'last_modified': last_modified, 'checksum': checksum}", "is_method": true, "class_name": "FSFilesStore", "function_description": "Provides file metadata including last modification time and checksum for a given path, supporting file integrity checks and update detection within the FSFilesStore context."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_filesystem_path", "line_number": 67, "body": "def _get_filesystem_path(self, path):\n        path_comps = path.split('/')\n        return os.path.join(self.basedir, *path_comps)", "is_method": true, "class_name": "FSFilesStore", "function_description": "Private utility method of FSFilesStore that converts a relative path into an absolute filesystem path within the store's base directory. It standardizes file path construction for accessing stored files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_mkdir", "line_number": 71, "body": "def _mkdir(self, dirname, domain=None):\n        seen = self.created_directories[domain] if domain else set()\n        if dirname not in seen:\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n            seen.add(dirname)", "is_method": true, "class_name": "FSFilesStore", "function_description": "Private helper method of FSFilesStore that creates a directory if it doesn't already exist, tracking created directories optionally within a specified domain for organized directory management."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 110, "body": "def stat_file(self, path, info):\n        def _onsuccess(boto_key):\n            checksum = boto_key['ETag'].strip('\"')\n            last_modified = boto_key['LastModified']\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {'checksum': checksum, 'last_modified': modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)", "is_method": true, "class_name": "S3FilesStore", "function_description": "Provides file metadata including checksum and last modification timestamp from the S3 storage, facilitating file validation and synchronization tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_boto_key", "line_number": 119, "body": "def _get_boto_key(self, path):\n        key_name = f'{self.prefix}{path}'\n        return threads.deferToThread(\n            self.s3_client.head_object,\n            Bucket=self.bucket,\n            Key=key_name)", "is_method": true, "class_name": "S3FilesStore", "function_description": "Internal method of S3FilesStore that asynchronously checks for the existence or metadata of an S3 object by its path within a specified bucket and prefix. It supports background retrieval of S3 object headers to optimize file management tasks."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 126, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f'{self.prefix}{path}'\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return threads.deferToThread(\n            self.s3_client.put_object,\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra)", "is_method": true, "class_name": "S3FilesStore", "function_description": "Uploads a file-like object to an S3 bucket under a specified path, supporting custom metadata and headers. This method enables asynchronous storage of files in S3 with appropriate access control."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_headers_to_botocore_kwargs", "line_number": 142, "body": "def _headers_to_botocore_kwargs(self, headers):\n        \"\"\" Convert headers to botocore keyword agruments.\n        \"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaselessDict({\n            'Content-Type': 'ContentType',\n            'Cache-Control': 'CacheControl',\n            'Content-Disposition': 'ContentDisposition',\n            'Content-Encoding': 'ContentEncoding',\n            'Content-Language': 'ContentLanguage',\n            'Content-Length': 'ContentLength',\n            'Content-MD5': 'ContentMD5',\n            'Expires': 'Expires',\n            'X-Amz-Grant-Full-Control': 'GrantFullControl',\n            'X-Amz-Grant-Read': 'GrantRead',\n            'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',\n            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',\n            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',\n            'X-Amz-Request-Payer': 'RequestPayer',\n            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',\n            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',\n            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',\n            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',\n            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',\n            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',\n            'X-Amz-Storage-Class': 'StorageClass',\n            'X-Amz-Tagging': 'Tagging',\n            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n        })\n        extra = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            else:\n                extra[kwarg] = value\n        return extra", "is_method": true, "class_name": "S3FilesStore", "function_description": "Converts a dictionary of HTTP-style headers into corresponding botocore-compatible keyword arguments, enabling seamless integration with botocore's S3 API calls while validating header support."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 215, "body": "def stat_file(self, path, info):\n        def _onsuccess(blob):\n            if blob:\n                checksum = blob.md5_hash\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {'checksum': checksum, 'last_modified': last_modified}\n            else:\n                return {}\n\n        return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Provides asynchronous metadata retrieval for a file in Google Cloud Storage, returning its checksum and last modified timestamp if available. Enables efficient access to file attributes for validation or synchronization purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_content_type", "line_number": 226, "body": "def _get_content_type(self, headers):\n        if headers and 'Content-Type' in headers:\n            return headers['Content-Type']\n        else:\n            return 'application/octet-stream'", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Private helper method in GCSFilesStore that extracts the 'Content-Type' from HTTP headers, defaulting to 'application/octet-stream' when not specified. This helps identify file content types for processing or storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 232, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        blob = self.bucket.blob(self.prefix + path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY\n        )", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Method of GCSFilesStore that uploads and saves file content to a specified cloud storage path with metadata, caching, and access policies, supporting asynchronous operation for efficient remote file persistence."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "persist_file", "line_number": 261, "body": "def persist_file(self, path, buf, info, meta=None, headers=None):\n        path = f'{self.basedir}/{path}'\n        return threads.deferToThread(\n            ftp_store_file, path=path, file=buf,\n            host=self.host, port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.USE_ACTIVE_MODE\n        )", "is_method": true, "class_name": "FTPFilesStore", "function_description": "Method of FTPFilesStore that uploads a file buffer to a specified FTP path asynchronously, handling connection details and supporting optional metadata and headers for flexible file storage."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "stat_file", "line_number": 269, "body": "def stat_file(self, path, info):\n        def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary(f'RETR {file_path}', m.update)\n                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n        return threads.deferToThread(_stat_file, path)", "is_method": true, "class_name": "FTPFilesStore", "function_description": "Provides asynchronous retrieval of a file's last modification timestamp and checksum from an FTP server, supporting integrity checks and metadata access for remote files in concurrent applications."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "from_settings", "line_number": 348, "body": "def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)", "is_method": true, "class_name": "FilesPipeline", "function_description": "Class method of FilesPipeline that initializes and configures various storage backends (S3, GCS, FTP) using provided settings, then returns an instance linked to the specified storage URI."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_get_store", "line_number": 370, "body": "def _get_store(self, uri):\n        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n            scheme = 'file'\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)", "is_method": true, "class_name": "FilesPipeline", "function_description": "Provides the appropriate storage handler instance based on the URI scheme, enabling flexible file or protocol-specific storage access within the FilesPipeline class."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_to_download", "line_number": 378, "body": "def media_to_download(self, request, info, *, item=None):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}\n\n        path = self.file_path(request, info=info, item=item)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f:\n            logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n        )\n        return dfd", "is_method": true, "class_name": "FilesPipeline", "function_description": "Core method of the FilesPipeline class that determines whether a media file needs to be downloaded based on its modification time and expiration policy, helping to avoid unnecessary re-downloads of up-to-date files."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_failed", "line_number": 416, "body": "def media_failed(self, failure, request, info):\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                'File (unknown-error): Error downloading %(medianame)s from '\n                '%(request)s referred in <%(referer)s>: %(exception)s',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer, 'exception': failure.value},\n                extra={'spider': info.spider}\n            )\n\n        raise FileException", "is_method": true, "class_name": "FilesPipeline", "function_description": "Handles media download failures by logging errors except for ignored requests, then raises a file-specific exception to signal the failure during the file processing pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "media_downloaded", "line_number": 429, "body": "def media_downloaded(self, response, request, info, *, item=None):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('download-error')\n\n        if not response.body:\n            logger.warning(\n                'File (empty-content): Empty file from %(request)s referred '\n                'in <%(referer)s>: no-content',\n                {'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('empty-content')\n\n        status = 'cached' if 'cached' in response.flags else 'downloaded'\n        logger.debug(\n            'File (%(status)s): Downloaded file from %(request)s referred in '\n            '<%(referer)s>',\n            {'status': status, 'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                'File (error): Error processing file from %(request)s '\n                'referred in <%(referer)s>: %(errormsg)s',\n                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n                extra={'spider': info.spider}, exc_info=True\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                'File (unknown-error): Error processing file from %(request)s '\n                'referred in <%(referer)s>',\n                {'request': request, 'referer': referer},\n                exc_info=True, extra={'spider': info.spider}\n            )\n            raise FileException(str(exc))\n\n        return {'url': request.url, 'path': path, 'checksum': checksum, 'status': status}", "is_method": true, "class_name": "FilesPipeline", "function_description": "Handles file download responses by validating status and content, processing file storage, and logging outcomes. It returns file metadata including URL, storage path, checksum, and download status for downstream use."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "inc_stats", "line_number": 482, "body": "def inc_stats(self, spider, status):\n        spider.crawler.stats.inc_value('file_count', spider=spider)\n        spider.crawler.stats.inc_value(f'file_status_count/{status}', spider=spider)", "is_method": true, "class_name": "FilesPipeline", "function_description": "Tracks and increments the total number of processed files and the count of files for a specific status within a crawling task. It supports monitoring file-related statistics during a web scraping pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "get_media_requests", "line_number": 487, "body": "def get_media_requests(self, item, info):\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        return [Request(u) for u in urls]", "is_method": true, "class_name": "FilesPipeline", "function_description": "Returns a list of download requests for media URLs extracted from an item, enabling automated retrieval of associated files during pipeline processing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "file_downloaded", "line_number": 491, "body": "def file_downloaded(self, response, request, info, *, item=None):\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum", "is_method": true, "class_name": "FilesPipeline", "function_description": "Core method of FilesPipeline that saves a downloaded file, computes its MD5 checksum, and persists it to storage, allowing other functions to verify file integrity and access the stored content."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "item_completed", "line_number": 499, "body": "def item_completed(self, results, item, info):\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item", "is_method": true, "class_name": "FilesPipeline", "function_description": "Method of the FilesPipeline class that updates an item with successfully processed file results after completion, filtering out any failed file downloads or processing outcomes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "file_path", "line_number": 504, "body": "def file_path(self, request, response=None, info=None, *, item=None):\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        media_ext = os.path.splitext(request.url)[1]\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = ''\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = mimetypes.guess_extension(media_type)\n        return f'full/{media_guid}{media_ext}'", "is_method": true, "class_name": "FilesPipeline", "function_description": "Generates a unique file path for downloaded media by hashing the URL and appending an appropriate file extension, ensuring consistent and organized storage for media files during a scraping pipeline."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 111, "body": "def _onsuccess(boto_key):\n            checksum = boto_key['ETag'].strip('\"')\n            last_modified = boto_key['LastModified']\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {'checksum': checksum, 'last_modified': modified_stamp}", "is_method": true, "class_name": "S3FilesStore", "function_description": "Utility function in S3FilesStore that extracts the checksum and last modification timestamp from an S3 object's metadata for validation or synchronization purposes."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_onsuccess", "line_number": 216, "body": "def _onsuccess(blob):\n            if blob:\n                checksum = blob.md5_hash\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {'checksum': checksum, 'last_modified': last_modified}\n            else:\n                return {}", "is_method": true, "class_name": "GCSFilesStore", "function_description": "Internal helper function in GCSFilesStore that extracts and returns the MD5 checksum and last modified timestamp from a Google Cloud Storage blob, or an empty dictionary if the blob is missing."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/files.py", "function": "_stat_file", "line_number": 270, "body": "def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary(f'RETR {file_path}', m.update)\n                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}", "is_method": true, "class_name": "FTPFilesStore", "function_description": "Private method in FTPFilesStore that fetches a file's last modification timestamp and MD5 checksum from the FTP server, supporting file integrity verification and update tracking."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "_get_mwlist_from_settings", "line_number": 17, "body": "def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "Core utility function of ItemPipelineManager that extracts and constructs a list of middleware components defined under ITEM_PIPELINES in the given settings configuration."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "_add_middleware", "line_number": 20, "body": "def _add_middleware(self, pipe):\n        super(ItemPipelineManager, self)._add_middleware(pipe)\n        if hasattr(pipe, 'process_item'):\n            self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "Adds a pipeline component and registers its item processing coroutine, enabling asynchronous handling of items within the pipeline framework."}, {"file": "./dataset/RepoExec/test-apps/scrapy/scrapy/pipelines/__init__.py", "function": "process_item", "line_number": 25, "body": "def process_item(self, item, spider):\n        return self._process_chain('process_item', item, spider)", "is_method": true, "class_name": "ItemPipelineManager", "function_description": "Core method of ItemPipelineManager that processes an item through its configured pipeline stages, enabling sequential item transformations or validations during web scraping or data extraction workflows."}]