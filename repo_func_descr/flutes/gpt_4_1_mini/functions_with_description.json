[{"file": "./dataset/RepoExec/test-apps/flutes/examples/map_reduce.py", "function": "count_words", "line_number": 17, "body": "def count_words(sentences: List[str]) -> CounterT[str]:\n    counter: CounterT[str] = Counter()\n    for sent in sentences:\n        counter.update(word.lower() for word in sent.split())\n    return counter", "is_method": false, "function_description": "Function that counts the frequency of each word across a list of sentences, returning a case-insensitive tally useful for text analysis or word frequency statistics."}, {"file": "./dataset/RepoExec/test-apps/flutes/examples/map_reduce.py", "function": "main", "line_number": 24, "body": "def main() -> None:\n    if len(sys.argv) < 2:\n        print(f\"Usage: python {sys.argv[0]} [file]\")\n        sys.exit(1)\n\n    path = sys.argv[1]\n    with flutes.work_in_progress(\"Read file\"):\n        with open(path) as f:\n            sentences = []\n            for line in f:\n                sentences.append(line)\n                if len(sentences) >= 100000:\n                    break\n\n    with flutes.work_in_progress(\"Parallel\"):\n        with flutes.safe_pool(processes=4, state_class=WordCounter) as pool_stateful:\n            for _ in pool_stateful.imap_unordered(WordCounter.count_words, sentences, chunksize=1000):\n                pass\n            states = pool_stateful.get_states()\n        parallel_word_counter: CounterT[str] = Counter()\n        for state in states:\n            parallel_word_counter.update(state.word_cnt)\n\n    with flutes.work_in_progress(\"Naive parallel\"):\n        naive_word_counter: CounterT[str] = Counter()\n        data_chunks = flutes.chunk(1000, sentences)\n        with flutes.safe_pool(processes=4) as pool:\n            for counter in pool.imap_unordered(count_words, data_chunks):\n                naive_word_counter.update(counter)\n\n    with flutes.work_in_progress(\"Sequential\"):\n        seq_word_counter: CounterT[str] = Counter()\n        for sent in sentences:\n            seq_word_counter.update(word.lower() for word in sent.split())\n\n    assert seq_word_counter == naive_word_counter == parallel_word_counter", "is_method": false, "function_description": "Main script function that reads up to 100,000 lines from a specified file and compares word counting results across parallel and sequential processing methods to verify their consistency."}, {"file": "./dataset/RepoExec/test-apps/flutes/examples/map_reduce.py", "function": "count_words", "line_number": 13, "body": "def count_words(self, sentence: str):\n        self.word_cnt.update(word.lower() for word in sentence.split())", "is_method": true, "class_name": "WordCounter", "function_description": "Updates the WordCounter's internal tally by counting and normalizing words from the given sentence. This method supports incremental word frequency tracking for text analysis and processing tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "chunk", "line_number": 23, "body": "def chunk(n: int, iterable: Iterable[T]) -> Iterator[List[T]]:\n    r\"\"\"Split the iterable into chunks, with each chunk containing no more than ``n`` elements.\n\n    .. code:: python\n\n        >>> list(chunk(3, range(10)))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n\n    :param n: The maximum number of elements in one chunk.\n    :param iterable: The iterable.\n    :return: An iterator over chunks.\n    \"\"\"\n    if n <= 0:\n        raise ValueError(\"`n` should be positive\")\n    group = []\n    for x in iterable:\n        group.append(x)\n        if len(group) == n:\n            yield group\n            group = []\n    if len(group) > 0:\n        yield group", "is_method": false, "function_description": "Utility function that divides an iterable into consecutive chunks each containing up to n elements, facilitating batch processing or grouped iteration over large or streaming data sequences."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "take", "line_number": 47, "body": "def take(n: int, iterable: Iterable[T]) -> Iterator[T]:\n    r\"\"\"Take the first :attr:`n` elements from an iterable.\n\n    .. code:: python\n\n        >>> list(take(5, range(1000000)))\n        [0, 1, 2, 3, 4]\n\n    :param n: The number of elements to take.\n    :param iterable: The iterable.\n    :return: An iterator returning the first :attr:`n` elements from the iterable.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"`n` should be non-negative\")\n    try:\n        it = iter(iterable)\n        for _ in range(n):\n            yield next(it)\n    except StopIteration:\n        pass", "is_method": false, "function_description": "Utility function that returns an iterator yielding the first n elements from any iterable, supporting convenient extraction of a limited subset without consuming the entire sequence."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "drop", "line_number": 69, "body": "def drop(n: int, iterable: Iterable[T]) -> Iterator[T]:\n    r\"\"\"Drop the first :attr:`n` elements from an iterable, and return the rest as an iterator.\n\n    .. code:: python\n\n        >>> next(drop(5, range(1000000)))\n        5\n\n    :param n: The number of elements to drop.\n    :param iterable: The iterable.\n    :return: An iterator returning the remaining part of the iterable after the first :attr:`n` elements.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"`n` should be non-negative\")\n    try:\n        it = iter(iterable)\n        for _ in range(n):\n            next(it)\n        yield from it\n    except StopIteration:\n        pass", "is_method": false, "function_description": "Utility function that skips the first n elements of an iterable and returns an iterator over the remaining items, enabling efficient partial consumption of sequences or streams."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "drop_until", "line_number": 92, "body": "def drop_until(pred_fn: Callable[[T], bool], iterable: Iterable[T]) -> Iterator[T]:\n    r\"\"\"Drop elements from the iterable until an element that satisfies the predicate is encountered. Similar to the\n    built-in :py:func:`filter` function, but only applied to a prefix of the iterable.\n\n    .. code:: python\n\n        >>> list(drop_until(lambda x: x > 5, range(10)))\n        [6, 7, 8, 9]\n\n    :param pred_fn: The predicate that returned elements should satisfy.\n    :param iterable: The iterable.\n    :return: The iterator after dropping elements.\n    \"\"\"\n    iterator = iter(iterable)\n    for item in iterator:\n        if not pred_fn(item):\n            continue\n        yield item\n        break\n    yield from iterator", "is_method": false, "function_description": "Function that skips elements from an iterable until one meets a specified condition, then yields that element and all remaining ones. Useful for selectively processing data starting from a relevant point in a sequence."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "split_by", "line_number": 115, "body": "def split_by(iterable: Iterable[A], empty_segments: bool = False, *, criterion: Callable[[A], bool]) \\\n        -> Iterator[List[A]]: ...", "is_method": false, "function_description": "Utility function that splits an iterable into segments wherever a criterion function returns True, optionally including empty segments. It supports flexible partitioning of sequences based on custom conditions."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "split_by", "line_number": 120, "body": "def split_by(iterable: Iterable[A], empty_segments: bool = False, *, separator: A) \\\n        -> Iterator[List[A]]: ...", "is_method": false, "function_description": "Splits an iterable into consecutive segments separated by a specified separator element. It optionally includes empty segments when separators are adjacent, facilitating chunking or parsing tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "split_by", "line_number": 124, "body": "def split_by(iterable: Iterable[A], empty_segments: bool = False, *, criterion=None, separator=None) \\\n        -> Iterator[List[A]]:\n    r\"\"\"Split a list into sub-lists by dropping certain elements. Exactly one of ``criterion`` and ``separator`` must be\n    specified. For example:\n\n    .. code:: python\n\n        >>> list(split_by(range(10), criterion=lambda x: x % 3 == 0))\n        [[1, 2], [4, 5], [7, 8]]\n\n        >>> list(split_by(\" Split by: \", empty_segments=True, separator='.'))\n        [[], ['S', 'p', 'l', 'i', 't'], ['b', 'y', ':'], []]\n\n    :param iterable: The list to split.\n    :param empty_segments: If ``True``, include an empty list in cases where two adjacent elements satisfy\n        the criterion.\n    :param criterion: The criterion to decide whether to drop an element.\n    :param separator: The separator for sub-lists. An element is dropped if it is equal to ``parameter``.\n    :return: List of sub-lists.\n    \"\"\"\n    if not ((criterion is None) ^ (separator is None)):\n        raise ValueError(\"Exactly one of `criterion` and `separator` should be specified\")\n    if criterion is None:\n        criterion = lambda x: x == separator\n    group = []\n    for x in iterable:\n        if not criterion(x):\n            group.append(x)\n        else:\n            if len(group) > 0 or empty_segments:\n                yield group\n            group = []\n    if len(group) > 0 or empty_segments:\n        yield group", "is_method": false, "function_description": "Utility function that splits an iterable into sub-lists by removing elements matching a given criterion or separator, with optional inclusion of empty segments for adjacent matches. It facilitates customizable segmentation of sequences for data processing tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanl", "line_number": 161, "body": "def scanl(func: Callable[[A, A], A], iterable: Iterable[A]) -> Iterator[A]: ...", "is_method": false, "function_description": "Function applying a binary operation cumulatively from the left across an iterable, yielding intermediate results. Useful for progressive aggregations like running totals or cumulative computations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanl", "line_number": 165, "body": "def scanl(func: Callable[[B, A], B], iterable: Iterable[A], initial: B) -> Iterator[B]: ...", "is_method": false, "function_description": "Function providing a left-to-right cumulative computation over an iterable, yielding all intermediate results starting from an initial value. Useful for tasks requiring progressive aggregation or stateful transformations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanl", "line_number": 168, "body": "def scanl(func, iterable, *args):\n    r\"\"\"Computes the intermediate results of :py:func:`~functools.reduce`. Equivalent to Haskell's ``scanl``. For\n    example:\n\n    .. code:: python\n\n        >>> list(scanl(operator.add, [1, 2, 3, 4], 0))\n        [0, 1, 3, 6, 10]\n        >>> list(scanl(lambda s, x: x + s, ['a', 'b', 'c', 'd']))\n        ['a', 'ba', 'cba', 'dcba']\n\n    Learn more at `Learn You a Haskell: Higher Order Functions <http://learnyouahaskell.com/higher-order-functions>`_.\n\n    :param func: The function to apply. This should be a binary function where the arguments are: the accumulator,\n        and the current element.\n    :param iterable: The list of elements to iteratively apply the function to.\n    :param initial: The initial value for the accumulator. If not supplied, the first element in the list is used.\n    :return: The intermediate results at each step.\n    \"\"\"\n    iterable = iter(iterable)\n    if len(args) == 1:\n        acc = args[0]\n    elif len(args) == 0:\n        acc = next(iterable)\n    else:\n        raise ValueError(\"Too many arguments\")\n    yield acc\n    for x in iterable:\n        acc = func(acc, x)\n        yield acc", "is_method": false, "function_description": "Provides an iterator yielding all intermediate accumulator states of a reduction operation over an iterable, enabling step-by-step tracking of cumulative results similar to Haskell's `scanl`. Useful for debugging or incremental computations needing intermediate outputs."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanr", "line_number": 201, "body": "def scanr(func: Callable[[A, A], A], iterable: Iterable[A]) -> List[A]: ...", "is_method": false, "function_description": "Utility function that performs a right-to-left cumulative reduction on an iterable using a binary function, returning intermediate results as a list. Useful for computing suffix-based accumulations or scans over sequences."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanr", "line_number": 205, "body": "def scanr(func: Callable[[B, A], B], iterable: Iterable[A], initial: B) -> List[B]: ...", "is_method": false, "function_description": "Function performing a right-to-left cumulative reduction on an iterable using a binary function and an initial value, returning all intermediate accumulation results. Useful for computations requiring suffix-based aggregation."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "scanr", "line_number": 208, "body": "def scanr(func, iterable, *args):\n    r\"\"\"Computes the intermediate results of :py:func:`~functools.reduce` applied in reverse. Equivalent to Haskell's\n    ``scanr``. For example:\n\n    .. code:: python\n\n        >>> scanr(operator.add, [1, 2, 3, 4], 0)\n        [10, 9, 7, 4, 0]\n        >>> scanr(lambda s, x: x + s, ['a', 'b', 'c', 'd'])\n        ['abcd', 'bcd', 'cd', 'd']\n\n    Learn more at `Learn You a Haskell: Higher Order Functions <http://learnyouahaskell.com/higher-order-functions>`_.\n\n    :param func: The function to apply. This should be a binary function where the arguments are: the accumulator,\n        and the current element.\n    :param iterable: The list of elements to iteratively apply the function to.\n    :param initial: The initial value for the accumulator. If not supplied, the first element in the list is used.\n    :return: The intermediate results at each step, starting from the end.\n    \"\"\"\n    return list(scanl(func, reversed(iterable), *args))[::-1]", "is_method": false, "function_description": "Function that computes cumulative results of a binary function applied right-to-left over an iterable, returning all intermediate values like a reverse reduce operation. Useful for stepwise accumulation or suffix-based computations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__iter__", "line_number": 258, "body": "def __iter__(self):\n        if self.exhausted:\n            return iter(self.list)\n        return self.LazyListIterator(self)", "is_method": true, "class_name": "LazyList", "function_description": "Allows iteration over the LazyList, returning a cached iterator if fully loaded or a lazy iterator to retrieve elements on demand otherwise."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "_fetch_until", "line_number": 263, "body": "def _fetch_until(self, idx: Optional[int]) -> None:\n        if self.exhausted:\n            return\n        try:\n            if idx is not None and idx < 0:\n                idx = None  # otherwise we won't know when the sequence ends\n            while idx is None or len(self.list) <= idx:\n                self.list.append(next(self.iter))\n        except StopIteration:\n            self.exhausted = True\n            del self.iter", "is_method": true, "class_name": "LazyList", "function_description": "Internal LazyList method that incrementally fetches items from an iterator until reaching a specified index or exhausting the iterator, supporting lazy sequence evaluation and access."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 276, "body": "def __getitem__(self, idx: int) -> T: ...", "is_method": true, "class_name": "LazyList", "function_description": "Provides access to the element at the given index in the LazyList, potentially triggering computation or retrieval if the element is not yet loaded. It enables indexed retrieval with lazy evaluation behavior."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 279, "body": "def __getitem__(self, idx: slice) -> List[T]: ...", "is_method": true, "class_name": "LazyList", "function_description": "Supports retrieving a slice of items from the LazyList, allowing efficient access to a subset of elements without loading the entire list into memory."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 281, "body": "def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            self._fetch_until(idx.stop)\n        else:\n            self._fetch_until(idx)\n        return self.list[idx]", "is_method": true, "class_name": "LazyList", "function_description": "Core method of LazyList that provides indexed or sliced access by ensuring required elements are fetched on demand before retrieval. It supports lazy loading for efficient memory and data handling."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__len__", "line_number": 288, "body": "def __len__(self):\n        if self.exhausted:\n            return len(self.list)\n        else:\n            raise TypeError(\"__len__ is not available before the iterable is depleted\")", "is_method": true, "class_name": "LazyList", "function_description": "This method returns the length of the internal list only after the iterable has been fully consumed. It ensures length retrieval is valid only when the entire lazy-loaded data is available."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__iter__", "line_number": 330, "body": "def __iter__(self) -> Iterator[int]:\n        return Range(self.l, self.r, self.step)", "is_method": true, "class_name": "Range", "function_description": "Core iterator method in Range class that returns a new Range object to enable iteration over the defined numeric range with specified start, end, and step values."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__next__", "line_number": 333, "body": "def __next__(self) -> int:\n        if self.val >= self.r:\n            raise StopIteration\n        result = self.val\n        self.val += self.step\n        return result", "is_method": true, "class_name": "Range", "function_description": "Iterator method of the Range class that returns the next integer in the sequence, raising StopIteration when the sequence end is reached. It enables iteration over a range of values with a specified step increment."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__len__", "line_number": 340, "body": "def __len__(self) -> int:\n        return self.length", "is_method": true, "class_name": "Range", "function_description": "Returns the number of elements in the Range instance. This allows other functions to easily obtain the range size for iteration or size checking."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "_get_idx", "line_number": 343, "body": "def _get_idx(self, idx: int) -> int:\n        return self.l + self.step * idx", "is_method": true, "class_name": "Range", "function_description": "Core utility method of the Range class that calculates the actual value at a given index considering the range\u2019s start and step, enabling indexed access to values in a custom numeric sequence."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 347, "body": "def __getitem__(self, idx: int) -> int: ...", "is_method": true, "class_name": "Range", "function_description": "Provides indexed access to elements within the Range, enabling retrieval of individual values by position. This allows Range instances to be used like sequences in iteration and lookup."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 350, "body": "def __getitem__(self, idx: slice) -> List[int]: ...", "is_method": true, "class_name": "Range", "function_description": "Provides list-like slicing access to a Range object, returning the corresponding subset of integers within the specified slice boundaries. This enables convenient extraction of numeric subranges."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 352, "body": "def __getitem__(self, item):\n        if isinstance(item, slice):\n            return [self._get_idx(idx) for idx in range(*item.indices(self.length))]\n        if item < 0:\n            item = self.length + item\n        return self._get_idx(item)", "is_method": true, "class_name": "Range", "function_description": "Utility method of the Range class that enables indexed and sliced access to range elements, supporting both positive and negative indices for flexible element retrieval."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 387, "body": "def __getitem__(self, idx: int) -> R: ...", "is_method": true, "class_name": "MapList", "function_description": "Core method of the MapList class that retrieves the element at the specified index, providing list-like access to its items. It enables indexed retrieval of mapped or transformed elements stored in the collection."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 390, "body": "def __getitem__(self, idx: slice) -> List[R]: ...", "is_method": true, "class_name": "MapList", "function_description": "Supports slicing to retrieve a list of elements from the MapList instance, enabling convenient access to multiple items at once."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__getitem__", "line_number": 392, "body": "def __getitem__(self, item):\n        if isinstance(item, int):\n            return self.func(self.list[item])\n        return [self.func(x) for x in self.list[item]]", "is_method": true, "class_name": "MapList", "function_description": "Core utility method of MapList that applies a stored function to elements accessed by index or slice, returning either the function result of a single item or a list of results for multiple items."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__iter__", "line_number": 397, "body": "def __iter__(self) -> Iterator[R]:\n        return map(self.func, self.list)", "is_method": true, "class_name": "MapList", "function_description": "Iterator method of the MapList class that applies a stored function to each item in the underlying list, enabling iteration over the transformed elements."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__len__", "line_number": 400, "body": "def __len__(self) -> int:\n        return len(self.list)", "is_method": true, "class_name": "MapList", "function_description": "Returns the number of elements contained in the MapList instance. This enables easy retrieval of the list's size for iteration or validation purposes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__iter__", "line_number": 242, "body": "def __iter__(self):\n            return self", "is_method": true, "class_name": "LazyListIterator", "function_description": "Provides iterator protocol support by returning the iterator object itself, enabling iteration over LazyListIterator instances."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/iterator.py", "function": "__next__", "line_number": 245, "body": "def __next__(self):\n            try:\n                obj = self.list()[self.index]\n            except IndexError:\n                raise StopIteration\n            self.index += 1\n            return obj", "is_method": true, "class_name": "LazyListIterator", "function_description": "Iterator method of the LazyListIterator class that returns the next element from a lazily evaluated list, raising StopIteration when the end is reached. It enables sequential access to elements without upfront list loading."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "readable_size", "line_number": 36, "body": "def readable_size(size: float) -> str:\n    r\"\"\"Represent file size in human-readable format.\n\n    .. code:: python\n\n        >>> readable_size(1024 * 1024)\n        \"1.00M\"\n\n    :param size: File size in bytes.\n    \"\"\"\n    units = [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]\n    for unit in units[:-1]:\n        if size < 1024:\n            return f\"{size:.2f}{unit}\"\n        size /= 1024\n    return f\"{size:.2f}{units[-1]}\"", "is_method": false, "function_description": "Utility function that converts a file size in bytes into a human-readable string with appropriate size units (e.g., KB, MB), simplifying size representation for display or logging purposes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "get_file_lines", "line_number": 54, "body": "def get_file_lines(path: PathType) -> int:\n    r\"\"\"Get number of lines in text file.\n    \"\"\"\n    return int(subprocess.check_output(['wc', '-l', str(path)]).split()[0].decode('utf-8'))", "is_method": false, "function_description": "Utility function that returns the number of lines in a given text file, useful for quickly determining file length or validating file content size."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "remove_prefix", "line_number": 60, "body": "def remove_prefix(s: str, prefix: str) -> str:\n    r\"\"\"Remove the specified prefix from a string. If only parts of the prefix match, then only that part is removed.\n\n    .. code:: python\n\n        >>> remove_prefix(\"https://github.com/huzecong/flutes\", \"https://\")\n        \"github.com/huzecong/flutes\"\n\n        >>> remove_prefix(\"preface\", \"prefix\")\n        \"face\"\n\n    :param s: The string whose prefix we want to remove.\n    :param prefix: The prefix to remove.\n    \"\"\"\n    length = min(len(s), len(prefix))\n    prefix_len = next((idx for idx in range(length) if s[idx] != prefix[idx]), length)\n    return s[prefix_len:]", "is_method": false, "function_description": "Function that removes the matching portion of a specified prefix from the start of a string, even if only part of the prefix matches, facilitating flexible prefix trimming in string processing tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "copy_tree", "line_number": 79, "body": "def copy_tree(src: PathType, dst: PathType, overwrite: bool = False) -> None:\n    r\"\"\"Copy contents of folder ``src`` to folder ``dst``. The ``dst`` folder can exist or whatever (looking at you,\n    :py:func:`shutil.copytree`).\n\n    :param src: The source directory.\n    :param dst: The destination directory. If it doesn't exist, it will be created.\n    :param overwrite: If ``True``, files in ``dst`` will be overwritten if a file with the same relative path exists\n        in ``src``. If ``False``, these files are not copied. Defaults to ``False``.\n    \"\"\"\n    os.makedirs(dst, exist_ok=True)\n    for file in os.listdir(src):\n        src_path = os.path.join(src, file)\n        dst_path = os.path.join(dst, file)\n        if os.path.isdir(src_path):\n            copy_tree(src_path, dst_path, overwrite=overwrite)\n        elif overwrite or not os.path.exists(dst_path):\n            shutil.copy2(src_path, dst_path)\n    shutil.copystat(src, dst)", "is_method": false, "function_description": "Utility function that copies all contents from a source directory to a destination directory, optionally overwriting existing files. It ensures the destination folder exists and recursively preserves directory structure and file metadata."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "cache", "line_number": 99, "body": "def cache(path: Optional[PathType], verbose: bool = True, name: Optional[str] = None):\n    r\"\"\"A function decorator that caches the output of the function to disk. If the cache file exists, it is loaded from\n    disk and the function will not be executed.\n\n    :param path: Path to the cache file. If ``None``, no cache is loaded or saved.\n    :param verbose: If ``True``, will print to log.\n    :param name: Name of the object to load. Only used for logging purposes.\n    \"\"\"\n    name = (name or 'cache').capitalize()\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            if path is not None and os.path.exists(path):\n                with open(path, \"rb\") as f:\n                    ret = pickle.load(f)\n                if verbose:\n                    log(f\"{name} loaded from '{path}'\")\n            else:\n                ret = func(*args, **kwargs)\n                if path is not None:\n                    with open(path, \"wb\") as f:\n                        pickle.dump(ret, f)\n                    if verbose:\n                        log(f\"{name} saved to '{path}'\")\n            return ret\n\n        return wrapped\n\n    return decorator", "is_method": false, "function_description": "Decorator function that caches a function's output to a specified file, loading from cache to avoid recomputation and optionally logging these actions for efficiency in repeated function calls."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "scandir", "line_number": 131, "body": "def scandir(path: PathType) -> Iterator[PathType]:\n    r\"\"\"Lazily iterate over all files and directories under a directory. The returned path is the absolute path of the\n    child file or directory, with the same type as :attr:`path` (:py:class:`pathlib.Path` or :py:class:`str`).\n\n    :param path: Path to the directory.\n    :return: An iterator over children paths.\n    \"\"\"\n    if isinstance(path, Path):\n        with os.scandir(path) as it:\n            for entry in it:\n                yield Path(entry.path)\n    else:\n        with os.scandir(path) as it:\n            for entry in it:\n                yield entry.path", "is_method": false, "function_description": "Function that provides a lazy iterator over all immediate files and directories within a specified directory, yielding absolute paths matching the input path's type. Useful for efficiently listing directory contents without loading all at once."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "get_folder_size", "line_number": 24, "body": "def get_folder_size(path: PathType) -> int:\n        # Credit: https://stackoverflow.com/a/25574638/4909228\n        r\"\"\"Get disk usage of given path in bytes.\"\"\"\n        return int(subprocess.check_output(['du', '-s', str(path)],\n                                           env={\"BLOCKSIZE\": \"512\"}).split()[0].decode('utf-8')) * 512", "is_method": false, "function_description": "Function that returns the total disk usage in bytes for a specified file system path, useful for assessing storage consumption of files or directories."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "get_folder_size", "line_number": 30, "body": "def get_folder_size(path: PathType) -> int:\n        # Credit: https://stackoverflow.com/a/25574638/4909228\n        r\"\"\"Get disk usage of given path in bytes.\"\"\"\n        return int(subprocess.check_output(['du', '-bs', str(path)]).split()[0].decode('utf-8'))", "is_method": false, "function_description": "Utility function that returns the total disk usage of a specified folder path in bytes, useful for monitoring storage or assessing folder size programmatically."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/fs.py", "function": "wrapped", "line_number": 111, "body": "def wrapped(*args, **kwargs):\n            if path is not None and os.path.exists(path):\n                with open(path, \"rb\") as f:\n                    ret = pickle.load(f)\n                if verbose:\n                    log(f\"{name} loaded from '{path}'\")\n            else:\n                ret = func(*args, **kwargs)\n                if path is not None:\n                    with open(path, \"wb\") as f:\n                        pickle.dump(ret, f)\n                    if verbose:\n                        log(f\"{name} saved to '{path}'\")\n            return ret", "is_method": false, "function_description": "Utility wrapper function that caches a function's result to a file, loading from disk if available to avoid recomputation and saving the output for future use. Useful for expensive or time-consuming function calls with persistent caching."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "shut_up", "line_number": 20, "body": "def shut_up(stderr: bool = True, stdout: bool = False):\n    r\"\"\" Suppress output (probably generated by external script or badly-written libraries) for :py:data:`~sys.stderr`\n    or :py:data:`~sys.stdout`. This method can be used as a decorator, or a context manager:\n\n    .. code:: python\n\n        @shut_up(stderr=True)\n        def verbose_func(...):\n            ...\n\n        with shut_up(stderr=True):\n            ... # verbose stuff\n\n    :param stderr: If ``True``, suppress output from :py:data:`~sys.stderr`. Defaults to ``True``.\n    :param stdout: If ``True``, suppress output from :py:data:`~sys.stdout`. Defaults to ``False``.\n    \"\"\"\n    # redirect output to /dev/null\n    fds = ([sys.stdout.fileno()] if stdout else []) + ([sys.stderr.fileno()] if stderr else [])\n    null_fds = [os.open(os.devnull, os.O_RDWR) for _ in fds]\n    output_fds = [os.dup(fd) for fd in fds]\n    for null_fd, fd in zip(null_fds, fds):\n        os.dup2(null_fd, fd)\n    yield\n    # restore normal stderr\n    for null_fd, output_fd, fd in zip(null_fds, output_fds, fds):\n        os.dup2(output_fd, fd)\n        os.close(null_fd)", "is_method": false, "function_description": "Utility function that temporarily suppresses output to sys.stderr and/or sys.stdout, usable as a decorator or context manager to silence verbose or unwanted console messages."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "progress_open", "line_number": 105, "body": "def progress_open(path: PathType, mode: Literal['rb'], *, encoding: str = ...,\n                  verbose: bool = ..., buffer_size: int = ..., **kwargs) -> ProgressReader[bytes]: ...", "is_method": false, "function_description": "Returns a progress-tracking file reader opened in binary mode, providing real-time feedback during file reading operations. Useful for monitoring the progress of large file reads."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "progress_open", "line_number": 114, "body": "def progress_open(path, mode=\"r\", *, encoding='utf-8', verbose=True, buffer_size=io.DEFAULT_BUFFER_SIZE,\n                  bar_fn: Optional[Callable[..., tqdm]] = None, **kwargs):\n    r\"\"\"A replacement for :py:func:`open` that shows the progress of reading the file:\n\n    .. code:: python\n\n        with progress_open(path, mode=\"r\") as f:\n            # `f` is just what you'd get with `open(path)`, now with a progress bar\n            bar = f.progress_bar  # type: tqdm.tqdm\n\n    :param path: Path to the file.\n    :param mode: The file open mode. When progress bar is enabled, only read modes ``\"r\"`` and ``\"rb\"`` are supported\n        (write progress doesn't make a lot of sense). Defaults to ``\"r\"``.\n    :param encoding: Encoding for the file. Only required for ``\"r\"`` mode. Defaults to ``\"utf-8\"``.\n    :param verbose: If ``False``, the progress bar is not displayed and a normal file object is returned. Defaults to\n        ``True``.\n    :param buffer_size: The size of the file buffer. Defaults to :py:data:`io.DEFAULT_BUFFER_SIZE`.\n    :param bar_fn: An optional callable that constructs a progress bar when called. This is useful when you want to\n        override the default progress bar, for instance, to use with :class:`~flutes.ProgressBarManager`:\n\n        .. code:: python\n\n            def process(path: str, bar: flutes.ProgressBarManager.Proxy):\n                with flutes.progress_open(path, bar_fn=bar.new) as f:\n                    ...\n\n    :param kwargs: Additional arguments to pass to `tqdm <https://tqdm.github.io/>`_ initializer.\n    :return: A file object.\n    \"\"\"\n    if not verbose:\n        return open(path, mode)\n\n    if mode not in [\"r\", \"rb\"]:\n        raise ValueError(f\"Unsupported mode '{mode}'. Only read modes ('r', 'rb') are supported\")\n\n    kwargs.setdefault(\"bar_format\", \"{l_bar}{bar}| [{elapsed}<{remaining}{postfix}]\")\n    buffer = f = _ProgressBufferedReader(io.FileIO(str(path), mode=\"r\"), buffer_size,\n                                         bar_fn=bar_fn or tqdm, bar_kwargs=kwargs)\n    if mode == \"r\":\n        f = io.TextIOWrapper(f, encoding=encoding)  # type: ignore[assignment]\n        f.progress_bar = buffer.progress_bar\n    return f", "is_method": false, "function_description": "Utility function that opens a file for reading while displaying a progress bar to indicate read progress. It optionally supports customization of the progress bar and provides a seamless drop-in replacement for the built-in open function."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "reverse_open", "line_number": 232, "body": "def reverse_open(path: PathType, *, encoding: str = 'utf-8', allow_empty_lines: bool = False,\n                 buffer_size: int = io.DEFAULT_BUFFER_SIZE):\n    # Credits: https://stackoverflow.com/questions/2301789/read-a-file-in-reverse-order-using-python\n    r\"\"\"A generator that returns the lines of a file in reverse order. Usage and syntax is the same as built-in\n    method :py:func:`open`.\n\n    :param path: Path to file.\n    :param encoding: Encoding of file. Defaults to ``\"utf-8\"``.\n    :param allow_empty_lines: If ``False``, empty lines are skipped. Defaults to ``False``.\n    :param buffer_size: Buffer size. You probably won't need to change this for most cases. Defaults to\n        :py:data:`io.DEFAULT_BUFFER_SIZE`.\n    \"\"\"\n    if buffer_size < _ReverseReadlineFile.MAX_CHAR_BYTES:\n        raise ValueError(f\"`buf_size` must be at least {_ReverseReadlineFile.MAX_CHAR_BYTES}\")\n    fp = open(path, \"rb\")\n    gen = _ReverseReadlineFile.generator(fp, encoding=encoding, allow_empty_lines=allow_empty_lines,\n                                         buf_size=buffer_size)\n    return _ReverseReadlineFile(fp, gen)", "is_method": false, "function_description": "Utility function that opens a text file and provides a generator yielding its lines in reverse order, optionally skipping empty lines. It supports custom encodings and buffer sizes, facilitating backward file reading for use cases like reverse log analysis."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__enter__", "line_number": 57, "body": "def __enter__(self):\n        self.progress_bar.__enter__()\n        return super().__enter__()", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Method in _ProgressBufferedReader that manages resource setup by entering both its progress bar context and its own buffered reading context, enabling progress tracking during buffered reading operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__exit__", "line_number": 61, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        if super().__exit__(exc_type, exc_val, exc_tb):\n            return True\n        return self.progress_bar.__exit__(exc_type, exc_val, exc_tb)", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Handles cleanup when exiting a with-statement, ensuring both the superclass and the associated progress bar resources are properly closed or finalized. Useful for managing resource and progress bar lifecycle in file or stream reading contexts."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "close", "line_number": 66, "body": "def close(self) -> None:\n        self.progress_bar.close()", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Closes the progress bar associated with the buffered reader to finalize and clean up any ongoing progress tracking. This ensures proper termination of progress display resources."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "read", "line_number": 69, "body": "def read(self, size: Optional[int] = -1) -> bytes:\n        ret = super().read(size)\n        self._read_bytes += len(ret)\n        self.progress_bar.update(len(ret))\n        return ret", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Utility method in _ProgressBufferedReader that reads data while tracking and updating a progress bar to reflect the number of bytes read, facilitating progress monitoring during buffered reading operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "read1", "line_number": 75, "body": "def read1(self, size: int = -1) -> bytes:\n        ret = super().read1(size)\n        self._read_bytes += len(ret)\n        self.progress_bar.update(len(ret))\n        return ret", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "This method reads up to a specified number of bytes from a buffered stream while updating a progress indicator. It enables tracking read progress during streaming operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "readline", "line_number": 81, "body": "def readline(self, size: int = -1) -> bytes:\n        ret = super().readline(size)\n        self._read_bytes += len(ret)\n        self.progress_bar.update(len(ret))\n        return ret", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Method of _ProgressBufferedReader that reads a line from the buffer, updates the count of read bytes, and advances a progress bar accordingly, enabling progress tracking during buffered reading operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "seek", "line_number": 87, "body": "def seek(self, offset: int, whence: int = io.SEEK_SET) -> int:\n        ret = super().seek(offset, whence)\n        self.progress_bar.update(ret - self._read_bytes)\n        self._read_bytes = ret\n        return ret", "is_method": true, "class_name": "_ProgressBufferedReader", "function_description": "Overrides the seek method to update a progress bar based on the new file position, providing real-time feedback during buffered reading operations. Useful for tracking read progress in large stream or file processing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "generator", "line_number": 162, "body": "def generator(fp, *, encoding='utf-8', allow_empty_lines=False, buf_size=8192):\n        segment = None\n        offset = 0\n\n        fp.seek(0, os.SEEK_END)\n        file_size = remaining_size = fp.tell()\n        while remaining_size > 0:\n            cur_buf_size = buf_size\n            offset = min(file_size, offset + cur_buf_size)\n            fp.seek(file_size - offset)\n            buffer_bytes = fp.read(min(remaining_size, cur_buf_size))\n\n            trials = 0\n            while True:\n                trials += 1\n                try:\n                    buffer = buffer_bytes.decode(encoding)\n                    break\n                except UnicodeDecodeError:\n                    if trials >= _ReverseReadlineFile.MAX_CHAR_BYTES:\n                        raise\n                    buffer_bytes = buffer_bytes[1:]\n                    cur_buf_size -= 1\n                    offset -= 1\n            fp.seek(file_size - offset)\n\n            remaining_size -= cur_buf_size\n            lines = buffer.split('\\n')\n            # the first line of the buffer is probably not a complete line so\n            # we'll save it and append it to the last line of the next buffer\n            # we read\n            if segment is not None:\n                # if the previous chunk starts right from the beginning of line\n                # do not concat the segment to the last line of new chunk\n                # instead, yield the segment first\n                if buffer[-1] != '\\n':\n                    lines[-1] += segment\n                else:\n                    yield segment\n            segment = lines[0]\n            for index in range(len(lines) - 1, 0, -1):\n                if allow_empty_lines or len(lines[index]):\n                    yield lines[index]\n        # Don't yield None if the file was empty\n        if segment is not None:\n            yield segment", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Utility function in _ReverseReadlineFile that generates lines from a file pointer in reverse order, supporting custom encoding, optional empty line inclusion, and efficient buffered reading for processing large files backward."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__iter__", "line_number": 213, "body": "def __iter__(self):\n        return self", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Provides iterator protocol support by returning the iterator object itself, enabling instance iteration typically for reading lines in reverse."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__next__", "line_number": 216, "body": "def __next__(self):\n        return next(self.gen) + '\\n'", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Returns the next line from the file in reverse order, appending a newline character. It enables iterating backwards through a file line by line."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__enter__", "line_number": 219, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Enables use of the _ReverseReadlineFile object as a context manager, supporting the with-statement for resource management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "__exit__", "line_number": 222, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Context suggests this is a file-like class managing resources. The __exit__ method ensures the file is closed when exiting a context, providing safe resource cleanup. This is a standard context manager exit method for this class.\n\nOutput:\nContext manager exit method that closes the file resource to ensure proper cleanup on context exit."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "readline", "line_number": 225, "body": "def readline(self):\n        return next(self.gen)", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Utility method in _ReverseReadlineFile that returns the next line from the reversed file iterator, enabling line-by-line reading from the end towards the beginning."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/io.py", "function": "close", "line_number": 228, "body": "def close(self):\n        self.fp.close()", "is_method": true, "class_name": "_ReverseReadlineFile", "function_description": "Closes the underlying file pointer associated with the _ReverseReadlineFile instance, releasing the file resource. This method ensures proper file closure after reading operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/run.py", "function": "error_wrapper", "line_number": 28, "body": "def error_wrapper(err: ExcType) -> ExcType:\n    r\"\"\"Wrap exceptions raised in :py:mod:`subprocess` to output captured output by default.\n    \"\"\"\n    if not isinstance(err, (subprocess.CalledProcessError, subprocess.TimeoutExpired)):\n        return err\n\n    def __str__(self):\n        string = super(self.__class__, self).__str__()\n        if self.output:\n            try:\n                output = self.output.decode('utf-8')\n            except UnicodeEncodeError:  # ignore output\n                string += \"\\nFailed to parse output.\"\n            else:\n                string += \"\\nCaptured output:\\n\" + '\\n'.join([f'    {line}' for line in output.split('\\n')])\n        else:\n            string += \"\\nNo output was generated.\"\n        return string\n\n    # Dynamically create a new type that overrides __str__, because replacing __str__ on instances don't work.\n    err_type = type(err)\n    new_type = type(err_type.__name__, (err_type,), {\"__str__\": __str__})\n\n    err.__class__ = new_type\n    return err", "is_method": false, "function_description": "Enhances subprocess exceptions by overriding their string representation to include any captured output, improving error messages for easier debugging in subprocess-related operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/run.py", "function": "run_command", "line_number": 58, "body": "def run_command(args: Union[str, List[str]], *,\n                env: Optional[Dict[str, str]] = None, cwd: Optional[PathType] = None, timeout: Optional[float] = None,\n                verbose: bool = False, return_output: bool = False, ignore_errors: bool = False,\n                **kwargs) -> CommandResult:\n    r\"\"\"A wrapper over ``subprocess.check_output`` that prevents deadlock caused by the combination of pipes and\n    timeout. Output is redirected into a temporary file and returned only on exceptions or when return code is nonzero.\n\n    In case an OSError occurs, the function will retry for a maximum for 5 times with exponential back-off. If error\n    still occurs, we just re-raise it.\n\n    :param args: The command to run. Should be either a `str` or a list of `str` depending on whether ``shell`` is True.\n    :param env: Environment variables to set before running the command. Defaults to None.\n    :param cwd: The working directory of the command to run. If None, uses the default (probably user home).\n    :param timeout: Maximum running time for the command. If running time exceeds the specified limit,\n        ``subprocess.TimeoutExpired`` is thrown.\n    :param verbose: If ``True``, print out the executed command and output.\n    :param return_output: If ``True``, the captured output is returned. Otherwise, the return code is returned.\n    :param ignore_errors: If ``True``, exceptions will not be raised. A special return code of -32768 indicates a\n        ``subprocess.TimeoutExpired`` error.\n    :return: An instance of :class:`CommandResult`.\n    \"\"\"\n    cwd_str = str(cwd) if cwd is not None else None\n    if verbose:\n        log((cwd_str or \"\") + \"> \" + repr(args), timestamp=False, include_proc_id=False)\n    with tempfile.TemporaryFile() as f:\n        try:\n            ret = subprocess.run(args, check=True, stdout=f, stderr=subprocess.STDOUT,\n                                 timeout=timeout, env=env, cwd=cwd_str, **kwargs)\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n            f.seek(0)\n            output = f.read()\n            if len(output) > MAX_OUTPUT_LENGTH:  # truncate if longer than 8192 characters\n                output = b\"*** (previous output truncated) ***\\n\" + output[-MAX_OUTPUT_LENGTH:]\n            if ignore_errors:\n                return_code = e.returncode if isinstance(e, subprocess.CalledProcessError) else -32768\n                return CommandResult(args, return_code, output)\n            else:\n                e.output = output\n                raise error_wrapper(e) from None\n        if return_output or ret.returncode != 0 or verbose:\n            f.seek(0)\n            output = f.read()\n            if verbose:\n                try:\n                    log(output.decode('utf-8'), timestamp=False, include_proc_id=False)\n                except UnicodeDecodeError:\n                    for line in output.split(b\"\\n\"):\n                        log(str(line), timestamp=False, include_proc_id=False)\n            return CommandResult(args, ret.returncode, output)\n    return CommandResult(args, ret.returncode, None)", "is_method": false, "function_description": "Utility function that safely executes a system command with retries, optional timeout, and environment control, capturing its output and exit code while handling errors and preventing deadlocks. It supports verbose logging and flexible result retrieval for robust subprocess management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/run.py", "function": "__str__", "line_number": 34, "body": "def __str__(self):\n        string = super(self.__class__, self).__str__()\n        if self.output:\n            try:\n                output = self.output.decode('utf-8')\n            except UnicodeEncodeError:  # ignore output\n                string += \"\\nFailed to parse output.\"\n            else:\n                string += \"\\nCaptured output:\\n\" + '\\n'.join([f'    {line}' for line in output.split('\\n')])\n        else:\n            string += \"\\nNo output was generated.\"\n        return string", "is_method": false, "function_description": "Returns a string representation of the object including its standard info plus any captured output, formatted for readability. If no output exists or decoding fails, it indicates this in the string."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_pool_state_init", "line_number": 180, "body": "def _pool_state_init(state_class: Type[PoolState], *args, **kwargs) -> None:\n    # Wrapper for initializer function passed to stateful pools.\n    state_obj = state_class(*args, **kwargs)  # type: ignore[call-arg]\n    # _pool_state_init -> worker\n    local_vars = inspect.currentframe().f_back.f_locals  # type: ignore[union-attr]\n    local_vars['__state__'] = state_obj\n    del local_vars", "is_method": false, "function_description": "Internal helper function that initializes a state object of a given class and injects it into the local variables of the calling frame, supporting stateful worker pools in managing per-worker state."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_pool_fn_with_state", "line_number": 189, "body": "def _pool_fn_with_state(fn: Callable[..., R], *args, **kwds) -> R:\n    # Wrapper for compute function passed to stateful pools.\n    frame = cast(FrameType, inspect.currentframe().f_back)  # type: ignore[union-attr]\n    while '__state__' not in frame.f_locals:  # the function might be wrapped several types\n        frame = cast(FrameType, frame.f_back)  # _pool_fn_with_state -> mapper -> worker\n    local_vars = frame.f_locals\n    state_obj = local_vars['__state__']\n    del frame, local_vars\n    return fn(state_obj, *args, **kwds)", "is_method": false, "function_description": "Utility function that invokes a given callable by injecting a hidden state object from the caller's frame, enabling functions to access shared state within stateful multiprocessing pools."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_chain_fns", "line_number": 200, "body": "def _chain_fns(fns: List[Callable[..., R]], fn_arg_kwargs: List[Tuple[Tuple[Any, ...], Dict[str, Any]]]) -> List[R]:\n    rets = []\n    for fn, (args, kwargs) in zip(fns, fn_arg_kwargs):\n        rets.append(fn(*args, **kwargs))\n    return rets", "is_method": false, "function_description": "Utility function that executes a list of functions with their respective arguments and keyword arguments, returning a list of their results in order. It facilitates batch function invocation with varied parameters."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_gather_fn", "line_number": 224, "body": "def _gather_fn(queue: 'mp.Queue[R]', fn: Callable[[T], Iterator[R]], *args, **kwargs) -> Optional[bool]:\n    try:\n        for x in fn(*args, **kwargs):  # type: ignore[call-arg]\n            queue.put(x)\n    except Exception as e:\n        log_exception(e)\n    # No matter what happens, signal the end of generation.\n    queue.put(cast(R, END_SIGNATURE))\n    return True", "is_method": false, "function_description": "Utility function that executes a generator function, pushing yielded results into a multiprocessing queue and signaling completion, thus enabling asynchronous collection of streamed data or results across processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "safe_pool", "line_number": 638, "body": "def safe_pool(processes: int, *args, state_class: Type[State], init_args: Tuple[Any, ...] = (),\n              closing: Optional[List[Any]] = None, suppress_exceptions: bool = False,\n              **kwargs) -> StatefulPoolType[State]: ...", "is_method": false, "function_description": "Creates and manages a multiprocessing pool with a shared state, allowing safe parallel execution while optionally handling resource cleanup and suppressing exceptions. Useful for parallelizing tasks that require consistent state management across processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "safe_pool", "line_number": 649, "body": "def safe_pool(processes, *args, state_class=None, init_args=(), closing=None,\n              suppress_exceptions=False, **kwargs):\n    r\"\"\"A wrapper over :py:class:`multiprocessing.Pool <multiprocessing.pool.Pool>` with additional functionalities:\n\n    - Fallback to sequential execution when ``processes == 0``.\n    - Stateful processes: Functions run in the pool will have access to a mutable state class. See :class:`PoolState`\n      for details.\n    - Handles exceptions gracefully.\n    - All pool methods support ``args`` and ``kwds``, which allows passing arguments to the called function.\n\n    Please see :class:`PoolType` (non-stateful) and :class:`StatefulPoolType` for supported methods of the pool\n    instance.\n\n    :param processes: The number of worker processes to run. A value of 0 means sequential execution in the current\n        process.\n    :param state_class: The class of the pool state. This allows functions run by the pool to access a mutable\n        process-local state. The ``state_class`` must be a subclass of :class:`PoolState`. Defaults to ``None``.\n    :param init_args: Arguments to the initializer of the pool state. The state will be constructed with:\n\n        .. code:: python\n\n            state = state_class(*init_args)\n\n    :param closing: An optional list of objects to close at exit, routines to run at exit. For each element ``obj``:\n\n        - If it is a callable, ``obj`` is called with no arguments.\n        - If it has an ``close()`` method, ``obj.close()`` is invoked.\n        - Otherwise, an exception is raised before the pool is constructed.\n\n    :param suppress_exceptions: If ``True``, exceptions raised within the lifetime of the pool are suppressed. Defaults\n        to ``False``.\n    :return: A context manager that can be used in a ``with`` statement.\n    \"\"\"\n    if state_class is not None:\n        if not issubclass(state_class, PoolState) or state_class is PoolState:\n            raise ValueError(\"`state_class` must be a subclass of `flutes.PoolState`\")\n\n    if closing is not None and not isinstance(closing, list):\n        raise ValueError(\"`closing` should either be `None` or a list\")\n    closing_fns = []\n    for obj in (closing or []):\n        if callable(obj):\n            closing_fns.append(obj)\n        elif hasattr(obj, \"close\") and callable(getattr(obj, \"close\")):\n            closing_fns.append(obj.close)\n        else:\n            raise ValueError(\"Invalid object in `closing` list. \"\n                             \"The object must either be a callable or has a `close` method\")\n\n    def close_fn():\n        for fn in closing_fns:\n            fn()\n\n    if processes == 0:\n        pool_class = DummyPool\n    else:\n        pool_class = PoolWrapper\n\n    args = (processes,) + args\n    if state_class is not None:\n        pool = StatefulPool(pool_class, state_class, init_args, args, kwargs)\n    else:\n        pool = pool_class(*args, **kwargs)\n\n    if processes == 0:\n        # Don't swallow exceptions in the single-process case.\n        yield pool\n        close_fn()\n        return\n\n    try:\n        yield pool\n    except KeyboardInterrupt as e:\n        from .log import log  # prevent circular import\n        log(\"Gracefully shutting down...\", \"warning\", force_console=True)\n        log(\"Press Ctrl-C again to force terminate...\", force_console=True, timestamp=False)\n        try:\n            pool.close()\n            pool.join()\n        except KeyboardInterrupt:\n            pass\n        raise e  # keyboard interrupts are always reraised\n    except Exception as e:\n        if suppress_exceptions:\n            from .log import log  # prevent circular import\n            log(traceback.format_exc(), force_console=True, timestamp=False)\n        else:\n            raise e\n    finally:\n        close_fn()\n        # In Python 3.8, the interpreter hangs when the pool is not properly closed.\n        pool.close()\n        pool.terminate()", "is_method": false, "function_description": "Utility function providing a flexible multiprocessing pool with optional stateful workers, graceful exception handling, and fallback to sequential execution. It supports resource cleanup and can suppress or propagate exceptions, enabling robust parallel task management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "kill_proc_tree", "line_number": 777, "body": "def kill_proc_tree(pid: int, including_parent: bool = True) -> None:\n    r\"\"\"Kill all child processes of a given process.\n\n    :param pid: The process ID (PID) of the process whose children we want to kill. To commit suicide, use\n        :py:meth:`os.getpid`.\n    :param including_parent: If ``True``, the process itself is killed as well. Defaults to ``True``.\n    \"\"\"\n    import psutil\n    parent = psutil.Process(pid)\n    children = parent.children(recursive=True)\n    for child in children:\n        child.kill()\n    _ = psutil.wait_procs(children, timeout=5)\n    if including_parent:\n        parent.kill()\n        parent.wait(5)", "is_method": false, "function_description": "Utility function that terminates a process and all its child processes based on the given PID, optionally including the parent process itself. Useful for cleaning up process trees to avoid orphaned subprocesses."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "ready", "line_number": 42, "body": "def ready(self) -> bool:\n        return True", "is_method": true, "class_name": "DummyApplyResult", "function_description": "Simple status-check method of DummyApplyResult that always indicates readiness, potentially used as a placeholder or default implementation in workflows expecting a readiness state."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "success", "line_number": 45, "body": "def success(self) -> bool:\n        return True", "is_method": true, "class_name": "DummyApplyResult", "function_description": "This method in DummyApplyResult always indicates a successful result by returning True, providing a consistent success status for use in conditional flows or testing scenarios."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "get", "line_number": 51, "body": "def get(self, timeout: Optional[float] = None) -> T:\n        return self._value", "is_method": true, "class_name": "DummyApplyResult", "function_description": "Returns the stored result value immediately, optionally ignoring any timeout parameter. It provides synchronous access to a precomputed or placeholder result within the DummyApplyResult context."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap", "line_number": 74, "body": "def imap(self, fn: Callable[[T], R], iterable: Iterable[T], *_, args=(), kwds={}, **__) -> Iterator[R]:\n        if self._process_state is not None:\n            locals().update({\"__state__\": self._process_state})\n        for x in iterable:\n            yield fn(x, *args, **kwds)", "is_method": true, "class_name": "DummyPool", "function_description": "Utility method of DummyPool that applies a function to each item in an iterable, yielding results lazily. It mimics an asynchronous map interface synchronously for compatibility or testing purposes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap_unordered", "line_number": 80, "body": "def imap_unordered(self, fn: Callable[[T], R], iterable: Iterable[T], *_, args=(), kwds={}, **__) -> Iterator[R]:\n        return self.imap(fn, iterable, args=args, kwds=kwds)", "is_method": true, "class_name": "DummyPool", "function_description": "Utility method in DummyPool that applies a function asynchronously over an iterable without preserving order, delegating to its imap method while supporting additional arguments."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map", "line_number": 83, "body": "def map(self, fn: Callable[[T], R], iterable: Iterable[T], *_, args=(), kwds={}, **__) -> List[R]:\n        return list(self.imap(fn, iterable, args=args, kwds=kwds))", "is_method": true, "class_name": "DummyPool", "function_description": "Utility method in DummyPool that applies a function to all items in an iterable, returning the results as a list. It supports additional positional and keyword arguments for flexible function invocation."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map_async", "line_number": 86, "body": "def map_async(self, fn: Callable[[T], R], iterable: Iterable[T], *_, args=(), kwds={}, **__) \\\n            -> 'mp.pool.ApplyResult[List[R]]':\n        return DummyApplyResult(self.map(fn, iterable, args=args, kwds=kwds))", "is_method": true, "class_name": "DummyPool", "function_description": "Utility method of DummyPool that asynchronously applies a function over an iterable, returning a placeholder async result wrapping the synchronous map operation. It simulates asynchronous mapping behavior in a simplified or testing context."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "starmap", "line_number": 90, "body": "def starmap(self, fn: Callable[..., R], iterable: Iterable[Tuple[T, ...]], *_, args=(), kwds={}, **__) -> List[R]:\n        if self._process_state is not None:\n            locals().update({\"__state__\": self._process_state})\n        return [fn(*x, *args, **kwds) for x in iterable]", "is_method": true, "class_name": "DummyPool", "function_description": "Provides a simplified starmap function that applies a given function to argument tuples from an iterable, supporting additional fixed arguments and keyword arguments. It enables parallel-like function mapping without actual multiprocessing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "starmap_async", "line_number": 95, "body": "def starmap_async(self, fn: Callable[..., R], iterable: Iterable[Tuple[T, ...]], *_, args=(), kwds={}, **__) \\\n            -> 'mp.pool.ApplyResult[List[R]]':\n        return DummyApplyResult(self.starmap(fn, iterable, args=args, kwds=kwds))", "is_method": true, "class_name": "DummyPool", "function_description": "Provides an asynchronous-like interface for applying a function to an iterable of argument tuples, returning a result wrapper that mimics multiprocessing pool behavior without actual parallel execution."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "apply", "line_number": 99, "body": "def apply(self, fn: Callable[..., R], args: Iterable[Any] = (), kwds: Dict[str, Any] = {}, *_, **__) -> R:\n        if self._process_state is not None:\n            locals().update({\"__state__\": self._process_state})\n        return fn(*args, **kwds)", "is_method": true, "class_name": "DummyPool", "function_description": "The apply method of DummyPool executes a given function synchronously with specified arguments and keyword arguments, providing a simplified interface for function invocation without parallel processing. It suits use cases where multiprocessing behavior is not required."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "apply_async", "line_number": 104, "body": "def apply_async(self, fn: Callable[..., R], args: Iterable[Any] = (), kwds: Dict[str, Any] = {}, *_, **__) \\\n            -> 'mp.pool.ApplyResult[R]':\n        return DummyApplyResult(self.apply(fn, args, kwds))", "is_method": true, "class_name": "DummyPool", "function_description": "Provides asynchronous execution of a function by immediately applying it and returning a placeholder result, simulating an async interface without real concurrency. Useful for testing or synchronous fallback in multiprocessing contexts."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "gather", "line_number": 108, "body": "def gather(self, fn: Callable[[T], Iterator[R]], iterable: Iterable[T], *_, args=(), kwds={}, **__) -> Iterator[R]:\n        if self._process_state is not None:\n            locals().update({\"__state__\": self._process_state})\n        for x in iterable:\n            yield from fn(x, *args, **kwds)", "is_method": true, "class_name": "DummyPool", "function_description": "Provides an iterator that applies a given function to each item in an iterable, yielding all results sequentially. This enables flexible processing of iterable elements with custom functions in a streaming manner."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__getattr__", "line_number": 118, "body": "def __getattr__(self, item):\n        return types.MethodType(DummyPool._no_op, self)", "is_method": true, "class_name": "DummyPool", "function_description": "Returns a no-operation method for any undefined attribute access, effectively disabling functionality while preventing attribute errors. This supports creating a dummy object that safely ignores method calls."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__enter__", "line_number": 121, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "DummyPool", "function_description": "Contextually, this is the __enter__ method of the DummyPool class, part of Python's context management protocol. Its purpose is to allow instances of DummyPool to be used in a with statement, returning the instance itself when entering the context. It primarily serves to enable resource management style usage patterns.\n\nOutput:\nEnables using DummyPool instances in with statements by returning the instance upon entering the runtime context."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__exit__", "line_number": 124, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self._state = mp.pool.TERMINATE", "is_method": true, "class_name": "DummyPool", "function_description": "Handles cleanup by signaling the pool to terminate when exiting a context manager block, ensuring proper resource release in DummyPool."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__return_state__", "line_number": 169, "body": "def __return_state__(self):\n        r\"\"\"When :meth:`StatefulPoolType.get_states` is invoked, this method is called for each pool worker to return\n        its state. The default implementation returns the :class:`PoolState` object itself, but it might be beneficial\n        to override this method in cases such as:\n\n        - The :class:`PoolState` object contains unpickle-able attributes.\n        - You need to dynamically compute the state before it's retrieved.\n        \"\"\"\n        return self", "is_method": true, "class_name": "PoolState", "function_description": "Provides the current state of a pool worker, allowing customization for dynamic state computation or handling unpickleable attributes when retrieving states from the pool."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__call__", "line_number": 216, "body": "def __call__(self, *args):\n        return self.fn(*args, *self.args, **self.kwds)", "is_method": true, "class_name": "FuncWrapper", "function_description": "Utility method of FuncWrapper that invokes the wrapped function with a combination of call-time and preset positional and keyword arguments, facilitating flexible function execution with stored parameters."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_define_method", "line_number": 257, "body": "def _define_method(pool_method: Callable[..., R]) -> Callable[..., R]:\n        @functools.wraps(pool_method)\n        def wrapped_method(func, *_, args=(), kwds={}, **__):\n            if len(args) > 0 or len(kwds) > 0:\n                func = FuncWrapper(func, args, kwds)\n            return pool_method(func, *_, **__)\n\n        return wrapped_method", "is_method": true, "class_name": "PoolWrapper", "function_description": "Internal helper method in PoolWrapper that wraps a pool method to support passing arguments to the target function by encapsulating them, enabling flexible function invocation in multiprocessing or parallel execution contexts."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "gather", "line_number": 266, "body": "def gather(self, fn: Callable[[T], Iterator[R]], iterable: Iterable[T], chunksize: int = 1,\n               args: Iterable[Any] = (), kwds: Dict[str, Any] = {}) -> Iterator[R]:\n        # Refer to documentation at `PoolType.gather`.\n        ctx = mp.get_context()\n        ctx.reducer = CustomMPReducer  # type: ignore[assignment]\n        with ctx.Manager() as manager:\n            queue = manager.Queue()\n            gather_fn = functools.partial(_gather_fn, queue, fn)\n            if not isinstance(iterable, list):\n                iterable = list(iterable)\n            length = len(iterable)\n            end_count = 0\n            ret = self.map_async(  # type: ignore[call-arg]\n                gather_fn, iterable, chunksize=chunksize, args=args, kwds=kwds)\n            while True:\n                try:\n                    x = queue.get_nowait()\n                except Empty:\n                    if ret.ready():\n                        # Update length to the number of end signatures successfully returned.\n                        new_length = sum(map(bool, ret.get()))\n                        if end_count == new_length:\n                            break\n                        length = new_length\n                    time.sleep(0.1)  # queue empty, wait for a bit\n                    continue\n                except (OSError, ValueError):\n                    break  # data in queue could be corrupt, e.g. if worker process is terminated while enqueueing\n                if x == END_SIGNATURE:\n                    end_count += 1\n                    if end_count == length:\n                        break\n                else:\n                    yield x", "is_method": true, "class_name": "PoolWrapper", "function_description": "Utility method of the PoolWrapper class that concurrently executes a function over an iterable and yields results as they become available, managing inter-process communication and synchronization for efficient parallel processing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_wrap_fn", "line_number": 354, "body": "def _wrap_fn(self, func: Callable[[State, T], R], allow_function: bool = True) -> Callable[[T], R]:\n        # If the function is a `PoolState` method, wrap it to allow access to `self`.\n        if id(func) in self._class_methods:\n            return functools.partial(_pool_fn_with_state, func)\n        if inspect.ismethod(func):\n            if func.__self__.__class__ is self._state_class:\n                raise ValueError(f\"Bound methods of the pool state class {self._state_class.__name__} are not \"\n                                 f\"accepted; use an unbound method instead.\")\n        if not allow_function:\n            raise ValueError(f\"Only unbound methods of the pool state class {self._state_class.__name__} are accepted\")\n        if inspect.isfunction(func):\n            args = inspect.getfullargspec(func)\n            if len(args.args) > 0 and args.args[0] == \"self\":\n                raise ValueError(f\"Only unbound methods of the pool state class {self._state_class.__name__} are \"\n                                 f\"accepted\")\n        return func", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method in StatefulPool that validates and conditionally wraps functions to ensure they are unbound methods of the pool's state class, facilitating controlled access to the state within the pooling mechanism."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_define_method", "line_number": 371, "body": "def _define_method(self, pool_method: Callable[..., R]) -> Callable[..., R]:\n        @functools.wraps(pool_method)\n        def wrapped_method(func, *args, **kwargs):\n            return pool_method(self._wrap_fn(func), *args, **kwargs)\n\n        return wrapped_method", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method within StatefulPool that wraps pool methods to automatically apply internal function transformations, streamlining integration of user functions with the pool's processing mechanism."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_init_broadcast", "line_number": 379, "body": "def _init_broadcast(self: State, _dummy: int) -> int:\n        self.__broadcasted__ = False\n        worker_id = get_worker_id()\n        assert worker_id is not None\n        return worker_id", "is_method": true, "class_name": "StatefulPool", "function_description": "Initializes broadcast state within a worker pool by resetting a broadcast flag and returning the current worker's identifier for task coordination."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_apply_broadcast", "line_number": 386, "body": "def _apply_broadcast(self: State, broadcast_fn: Callable[[State], R], *args, **kwds) -> Optional[Tuple[R, int]]:\n        if not hasattr(self, '__broadcasted__'):\n            # Might be possible that a worker crashed and restarted.\n            self.__broadcasted__ = False\n        if self.__broadcasted__:\n            return None\n        self.__broadcasted__ = True\n        worker_id = get_worker_id()\n        assert worker_id is not None\n        result = broadcast_fn(self, *args, **kwds)  # type: ignore[call-arg]\n        return (result, worker_id)", "is_method": true, "class_name": "StatefulPool", "function_description": "Core utility of StatefulPool that ensures a broadcast function executes once per worker, preventing repeated invocations and returning the broadcast result with the worker's identifier."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "get_states", "line_number": 398, "body": "def get_states(self) -> List[State]:\n        r\"\"\"Return the states of each pool worker.\n\n        :return: A list of state for each worker process. Order is arbitrary.\n        \"\"\"\n        return self.broadcast(self._state_class.__return_state__)", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method of StatefulPool that returns the current state of each worker process as a list, facilitating monitoring or management of distributed worker states."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "broadcast", "line_number": 405, "body": "def broadcast(self, fn: Callable[[State], R], *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> List[R]:\n        r\"\"\"Broadcast a function to each pool worker, and gather results.\n\n        :param fn: The function to broadcast.\n        :param args: Positional arguments to apply to the function.\n        :param kwds: Keyword arguments to apply to the function.\n        :return: The broadcast result from each worker process. Order is arbitrary.\n        \"\"\"\n        if self._pool._state != mp.pool.RUN:\n            raise ValueError(\"Pool not running\")\n        _ = self._wrap_fn(fn, allow_function=False)  # ensure that the function is an unbound method\n        if isinstance(self._pool, DummyPool):\n            return [fn(self._pool._process_state, *args, **kwds)]\n        assert isinstance(self._pool, Pool)\n\n        # Initialize the worker states.\n        received_ids: Set[int] = set()\n        n_processes = self._pool._processes\n        broadcast_init_fn = functools.partial(_pool_fn_with_state, self._init_broadcast)\n        while len(received_ids) < n_processes:\n            init_ids: List[int] = self._pool.map(broadcast_init_fn, range(n_processes))  # type: ignore[arg-type]\n            received_ids.update(init_ids)\n\n        # Perform broadcast.\n        received_ids: Set[int] = set()\n        broadcast_results = []\n        broadcast_handler_fn = functools.partial(_pool_fn_with_state, self._apply_broadcast)\n        while len(received_ids) < n_processes:\n            results: List[Optional[Tuple[R, int]]] = self._pool.map(\n                broadcast_handler_fn, [fn] * n_processes, args=args, kwds=kwds)  # type: ignore[arg-type]\n            for result in results:\n                if result is not None:\n                    ret, worker_id = result\n                    received_ids.add(worker_id)\n                    broadcast_results.append(ret)\n        return broadcast_results", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method of StatefulPool that runs a given function simultaneously on all worker states in the pool, collecting and returning each worker\u2019s result in arbitrary order. It facilitates parallel execution with shared state management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__getattr__", "line_number": 442, "body": "def __getattr__(self, item):\n        return getattr(self._pool, item)", "is_method": true, "class_name": "StatefulPool", "function_description": "Enables the StatefulPool instance to delegate attribute access to its internal pool object, providing seamless access to the underlying pool's methods and properties."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "apply", "line_number": 458, "body": "def apply(self,\n              fn: Callable[..., T], args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> T:\n        r\"\"\"Calls ``fn`` with arguments ``args`` and keyword arguments ``kwds``, and blocks until the result is ready.\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.apply` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Provides a synchronous call to execute a given function with specified arguments, blocking until the result is available. Useful for running tasks within a pool and retrieving their results directly."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "apply_async", "line_number": 465, "body": "def apply_async(self,\n                    func: Callable[..., T], args: Iterable[Any] = (), kwds: Mapping[str, Any] = {},\n                    callback: Optional[Callable[[T], None]] = None,\n                    error_callback: Optional[Callable[[BaseException], None]] = None) -> 'mp.pool.ApplyResult[T]':\n        r\"\"\"Non-blocking version of :meth:`apply`.\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.apply_async` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "As a method of PoolType, this function provides asynchronous execution of a callable, allowing other processes or threads to continue running without waiting for the result. It supports callbacks for handling results or errors, facilitating concurrent task management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map", "line_number": 474, "body": "def map(self,  # type: ignore[override]\n            fn: Callable[[T], R], iterable: Iterable[T], chunksize: Optional[int] = None,\n            *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> List[R]:\n        r\"\"\"A parallel, eager, blocking equivalent of :meth:`map`, with support for additional arguments. The sequential\n        equivalent is:\n\n        .. code:: python\n\n            list(map(lambda x: fn(x, *args, **kwds), iterable))\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.map` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Provides a parallel, blocking map operation that applies a function to items in an iterable with optional extra arguments, enabling concurrent processing to speed up batch computations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map_async", "line_number": 487, "body": "def map_async(self,  # type: ignore[override]\n                  fn: Callable[[T], R], iterable: Iterable[T], chunksize: Optional[int] = None,\n                  callback: Optional[Callable[[T], None]] = None,\n                  error_callback: Optional[Callable[[BaseException], None]] = None,\n                  *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> 'mp.pool.ApplyResult[List[R]]':\n        r\"\"\"Non-blocking version of :meth:`map`.\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.map_async` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Provides an asynchronous, non-blocking map operation that applies a function to iterable items in parallel, allowing optional callbacks for results or errors."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap", "line_number": 497, "body": "def imap(self,  # type: ignore[override]\n             fn: Callable[[T], R], iterable: Iterable[T], chunksize: int = 1,\n             *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]:\n        r\"\"\"Lazy version of :meth:`map`.\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.imap` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Provides a lazy, iterator-based mapping of a function over an iterable, optionally supporting additional arguments and chunk processing for efficient parallel execution."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap_unordered", "line_number": 505, "body": "def imap_unordered(self,  # type: ignore[override]\n                       fn: Callable[[T], R], iterable: Iterable[T], chunksize: int = 1,\n                       *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]:\n        r\"\"\"Similar to :meth:`imap`, but the ordering of the results are not guaranteed.\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.imap_unordered` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Utility method in PoolType that applies a function asynchronously to items in an iterable and yields results as they complete, without preserving input order. It supports additional positional and keyword arguments for flexible parallel processing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "starmap", "line_number": 513, "body": "def starmap(self,  # type: ignore[override]\n                fn: Callable[..., R], iterable: Iterable[Iterable[Any]], chunksize: Optional[int] = None,\n                *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> List[R]:\n        r\"\"\"Similar to :meth:`map`, except that the elements of ``iterable`` are expected to be iterables that are\n        unpacked as arguments. The sequential equivalent is:\n\n        .. code:: python\n\n            list(map(lambda xs: fn(*xs, *args, **kwds), iterable))\n\n        Please refer to Python documentation on :py:meth:`multiprocessing.pool.Pool.starmap` for details.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Provides parallel execution of a function with argument unpacking over an iterable, returning a list of results. It enables concurrent processing where each iterable element supplies multiple arguments to the target function."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "gather", "line_number": 536, "body": "def gather(self,\n               fn: Callable[[T], Iterator[R]], iterable: Iterable[T], chunksize: int = 1,\n               *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]:\n        r\"\"\"Apply a function that returns a generator to each element in an iterable, and return an iterator over the\n        concatenation of all elements produced by the generators. Order is not guaranteed across generators, but\n        relative order is preserved for elements from the same generator.\n\n        This method chops the iterable into a number of chunks which it submits to the process pool as separate tasks.\n        The (approximate) size of these chunks can be specified by setting :attr:`chunksize` to a positive integer.\n\n        The underlying implementation uses a managed queue to hold the results. The sequential equivalent is:\n\n        .. code:: python\n\n            itertools.chain.from_iterable(fn(x, *args, **kwds) for x in iterable)\n\n        :param fn: The function returning generators.\n        :param iterable: The iterable.\n        :param chunksize: The (approximate) size of each chunk. Defaults to 1. A larger ``chunksize`` is beneficial\n            for performance.\n        :param args: Positional arguments to apply to the function.\n        :param kwds: Keyword arguments to apply to the function.\n        :return: An iterator over the concatenation of all elements produced by the generators.\n        \"\"\"", "is_method": true, "class_name": "PoolType", "function_description": "Concatenates the outputs of a generator-returning function applied concurrently to chunks of an iterable, providing an iterator over all generated elements without guaranteeing cross-generator order. Useful for parallelizing generator-based data processing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map", "line_number": 587, "body": "def map(self,  # type: ignore[override]\n            fn: Callable[[State, T], R], iterable: Iterable[T], chunksize: Optional[int] = None,\n            *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> List[R]: ...", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Core method of StatefulPoolType that applies a stateful function to each item in an iterable, supporting additional fixed arguments and optional chunking for parallel or batched processing."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "map_async", "line_number": 591, "body": "def map_async(self,  # type: ignore[override]\n                  fn: Callable[[State, T], R], iterable: Iterable[T], chunksize: Optional[int] = None,\n                  callback: Optional[Callable[[T], None]] = None,\n                  error_callback: Optional[Callable[[BaseException], None]] = None,\n                  *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> 'mp.pool.ApplyResult[List[R]]': ...", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Facilitates asynchronous execution of a function over an iterable with optional callbacks for results and errors, supporting additional arguments and chunking for parallel processing in a stateful multiprocessing pool context."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap", "line_number": 597, "body": "def imap(self,  # type: ignore[override]\n             fn: Callable[[State, T], R], iterable: Iterable[T], chunksize: int = 1,\n             *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]: ...", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Core method of StatefulPoolType that applies a stateful function asynchronously over an iterable, yielding results in order with optional chunking and additional arguments support. It enables parallel processing with shared state management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "imap_unordered", "line_number": 601, "body": "def imap_unordered(self,  # type: ignore[override]\n                       fn: Callable[[State, T], R], iterable: Iterable[T], chunksize: int = 1,\n                       *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]: ...", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Service method in StatefulPoolType that applies a function asynchronously to items from an iterable in any order, supporting additional arguments and optional chunking for efficiency. Useful for concurrent processing with stateful context across tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "gather", "line_number": 605, "body": "def gather(self,  # type: ignore[override]\n               fn: Callable[[State, T], Iterator[R]], iterable: Iterable[T], chunksize: int = 1,\n               *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> Iterator[R]: ...", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Core method of StatefulPoolType that applies a stateful function over an iterable in chunks, yielding results lazily. It supports passing additional arguments, enabling efficient parallel or sequential processing with state management."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "get_states", "line_number": 609, "body": "def get_states(self) -> List[State]:\n        r\"\"\"Return the states of each pool worker. The pool state class can override the\n        :meth:`PoolState.__return_state__` method to customize the returned value.\n\n        The implementation uses the :meth:`broadcast` mechanism to retrieve states. This function is blocking.\n\n        .. note::\n            :meth:`get_states` must be called within the ``with`` block, before the pool terminates. Calling\n            :meth:`get_states` while iterating over results from :meth:`imap`, :meth:`imap_unordered`, or :meth:`gather`\n            is likely to result in deadlock or long wait periods.\n\n        :return: A list of state for each worker process. Ordering of the states is arbitrary.\n        \"\"\"", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Returns the current state of each worker in the pool as a list, allowing inspection or monitoring of worker statuses within a managed pool context. Useful for managing or debugging concurrent worker processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "broadcast", "line_number": 623, "body": "def broadcast(self, fn: Callable[[State], R],\n                  *, args: Iterable[Any] = (), kwds: Mapping[str, Any] = {}) -> List[R]:\n        r\"\"\"Call the function on each worker process and gather results. It is guaranteed that the function is called\n        on each worker process exactly once.\n\n        This function is blocking.\n\n        :param fn: The function to call on workers. This must be an unbound method of the pool state class.\n        :param args: Positional arguments to apply to the function.\n        :param kwds: Keyword arguments to apply to the function.\n        :return: A list of results, one from each worker process. Ordering of the results is arbitrary.\n        \"\"\"", "is_method": true, "class_name": "StatefulPoolType", "function_description": "Method of StatefulPoolType that executes a given function exactly once on each worker process, collecting and returning their results in a blocking call. It enables coordinated parallel computation across worker states."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__enter__", "line_number": 758, "body": "def __enter__(self) -> IO[Any]:\n        return self._file", "is_method": true, "class_name": "MultiprocessingFileWriter", "function_description": "Returns the underlying file object to support context management, enabling safe and convenient file operations within a with-statement in MultiprocessingFileWriter."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__exit__", "line_number": 761, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self._thread.join()\n        self._file.close()", "is_method": true, "class_name": "MultiprocessingFileWriter", "function_description": "Ensures proper cleanup of resources by waiting for the background thread to finish and closing the file upon exiting the context, supporting safe file writing in multiprocessing environments."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "write", "line_number": 765, "body": "def write(self, s: str):\n        self._queue.put_nowait(s)", "is_method": true, "class_name": "MultiprocessingFileWriter", "function_description": "Utility method of the MultiprocessingFileWriter class that asynchronously queues strings for writing, enabling non-blocking file output in multiprocessing environments."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_receive", "line_number": 768, "body": "def _receive(self):\n        while True:\n            try:\n                record = self._queue.get()\n                self._file.write(record)\n            except EOFError:\n                break", "is_method": true, "class_name": "MultiprocessingFileWriter", "function_description": "Internal method of MultiprocessingFileWriter that continuously reads records from a queue and writes them to a file until an end-of-file signal is received. It manages asynchronous data writing in a multiprocessing environment."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "proxy", "line_number": 1020, "body": "def proxy(self):\n        r\"\"\"Return the proxy class for the progress bar manager. Subprocesses should communicate with the manager\n        through the proxy class.\n        \"\"\"\n        return self._proxy", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Returns the proxy class used by subprocesses to interact with the progress bar manager, enabling managed communication across process boundaries."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_close_bar", "line_number": 1026, "body": "def _close_bar(self, worker_id: Optional[int]) -> None:\n        if worker_id in self.progress_bars:\n            self.progress_bars[worker_id].close()\n            del self.progress_bars[worker_id]", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Internal method of ProgressBarManager that closes and removes the progress bar associated with a specific worker, helping manage and clean up progress tracking resources."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_run", "line_number": 1031, "body": "def _run(self):\n        from tqdm import tqdm\n        while True:\n            try:\n                event = self.queue.get()\n                if isinstance(event, NewEvent):\n                    position = self.worker_id_map[event.worker_id]\n                    self._close_bar(event.worker_id)\n                    kwargs = {**self.bar_kwargs, **event.kwargs, \"leave\": False, \"position\": position}\n                    bar = tqdm(**kwargs)\n                    self.progress_bars[event.worker_id] = bar\n                elif isinstance(event, UpdateEvent):\n                    bar = self.progress_bars[event.worker_id]\n                    if event.postfix is not None:\n                        # Only force refresh if we're only setting the postfix.\n                        bar.set_postfix(event.postfix, refresh=event.n == 0)\n                    bar.update(event.n)\n                elif isinstance(event, WriteEvent):\n                    tqdm.write(event.message)\n                elif isinstance(event, CloseEvent):\n                    self._close_bar(event.worker_id)\n                elif isinstance(event, QuitEvent):\n                    break\n                else:\n                    assert False\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except (EOFError, BrokenPipeError):\n                break\n            except:\n                traceback.print_exc(file=sys.stderr)", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Core method of ProgressBarManager that listens for progress events to create, update, write to, or close progress bars, managing multiple workers\u2019 progress displays concurrently until terminated."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__enter__", "line_number": 1063, "body": "def __enter__(self):\n        return self", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Standard context manager method for ProgressBarManager enabling use in with-statements to manage progress bar lifecycle automatically."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__exit__", "line_number": 1066, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Destructor-like method in ProgressBarManager that ensures the progress bar is properly closed when exiting a context manager block."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "close", "line_number": 1069, "body": "def close(self):\n        if not self.verbose:\n            return\n\n        self.queue.put_nowait(QuitEvent())\n        self.thread.join()\n        for bar in self.progress_bars.values():\n            bar.close()\n        self.manager.shutdown()\n        from .log import set_console_logging_function\n        set_console_logging_function(self._original_console_logging_fn)", "is_method": true, "class_name": "ProgressBarManager", "function_description": "Utility method of ProgressBarManager that gracefully stops progress tracking by terminating related threads, closing all progress bars, shutting down the manager, and restoring the original console logging function."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "wrapped_method", "line_number": 259, "body": "def wrapped_method(func, *_, args=(), kwds={}, **__):\n            if len(args) > 0 or len(kwds) > 0:\n                func = FuncWrapper(func, args, kwds)\n            return pool_method(func, *_, **__)", "is_method": true, "class_name": "PoolWrapper", "function_description": "Utility method in PoolWrapper that prepares and invokes a pool method by optionally wrapping the target function with preset arguments, enabling flexible parallel execution customization."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "get_arg", "line_number": 318, "body": "def get_arg(pos: int, name: str, default=None):\n            if len(args) > pos + 1:\n                return args[pos]\n            if name in kwargs:\n                return kwargs[name]\n            return default", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method in StatefulPool that retrieves an argument by position or name from given inputs, returning a default value if the argument is absent."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "set_arg", "line_number": 325, "body": "def set_arg(pos: int, name: str, val):\n            nonlocal args\n            if len(args) > pos + 1:\n                args = args[:pos] + (val,) + args[(pos + 1):]\n            else:\n                kwargs[name] = val", "is_method": true, "class_name": "StatefulPool", "function_description": "Utility method of StatefulPool that updates a positional argument at a given index or assigns a value to a keyword argument by name, enabling dynamic modification of stored function arguments."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "new", "line_number": 878, "body": "def new(self, iterable: Literal[None] = None, update_frequency: Union[int, float] = 1, **kwargs) -> tqdm:\n            ...", "is_method": true, "class_name": "Proxy", "function_description": "Creates and returns a tqdm progress bar optionally initialized from an iterable and configured with a customizable update frequency. Useful for tracking progress in iterative operations with flexible update control."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "new", "line_number": 881, "body": "def new(self, iterable=None, update_frequency=1, **kwargs):\n            r\"\"\"Construct a new progress bar.\n            \n            :param iterable: The iterable to decorate with a progress bar. If ``None``, then updates must be manually\n                managed with calls to :meth:`update`.\n            :param update_frequency: How many iterations per update. This argument only takes effect if :attr:`iterable`\n                is not ``None``:\n\n                - If :attr:`update_frequency` is a ``float``, then the progress bar is updated whenever the iterable\n                  progresses over that percentage of elements. For instance, a value of ``0.01`` results in an update\n                  per 1% of progress. Requires a sized iterable (having a valid ``__len__``).\n                - If :attr:`update_frequency` is an ``int``, then the progress bar is updated whenever the iterable\n                  progresses over that many elements. For instance, a value of ``10`` results in an update per 10\n                  elements.\n            :param kwargs: Additional arguments for the `tqdm <https://tqdm.github.io/>`_ progress bar initializer.\n                These can override the default arguments set in the constructor of :class:`ProgressBarManager`.\n            :return: The wrapped iterable, or the proxy class itself.\n            \"\"\"\n            length = kwargs.get(\"total\", None)\n            ret_val = self\n            if iterable is not None:\n                try:\n                    iter_len = len(iterable)\n                    if length is None:\n                        length = iter_len\n                        kwargs.update(total=iter_len)\n                    elif length != iter_len:\n                        import warnings\n                        warnings.warn(f\"Iterable has length {iter_len} but total={length} is specified\")\n                except TypeError:\n                    pass\n                if isinstance(update_frequency, float):\n                    if length is None:\n                        raise ValueError(\"`iterable` must have valid length, or `total` must be specified \"\n                                         \"if `update_frequency` is float\")\n                    if not (0.0 < update_frequency <= 1.0):\n                        raise ValueError(\"`update_frequency` must be within the range (0, 1]\")\n                    ret_val = self._iter_per_percentage(iterable, length, update_frequency)\n                else:\n                    if not (0 < update_frequency):\n                        raise ValueError(\"`update_frequency` must be positive\")\n                    ret_val = self._iter_per_elems(iterable, update_frequency)\n            self.queue.put_nowait(NewEvent(get_worker_id(), kwargs))\n            return ret_val", "is_method": true, "class_name": "Proxy", "function_description": "Creates and returns a progress bar proxy that wraps an iterable to visually track iteration progress, supporting customizable update frequencies by element count or percentage. It also allows manual progress updates if no iterable is provided."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_iter_per_elems", "line_number": 926, "body": "def _iter_per_elems(self, iterable: Iterable[T], update_frequency: int) -> Iterator[T]:\n            prev_index = -1\n            next_index = update_frequency - 1\n            idx = 0\n            for idx, x in enumerate(iterable):\n                yield x\n                if idx == next_index:\n                    self.update(idx - prev_index)\n                    next_index += update_frequency\n                    prev_index = idx\n            # At the end, `idx == len(iterable) - 1`.\n            if idx > prev_index:\n                self.update(idx - prev_index)", "is_method": true, "class_name": "Proxy", "function_description": "Provides an iterator over elements that periodically triggers an update callback at a specified frequency, useful for tracking progress or batching operations during iteration."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "_iter_per_percentage", "line_number": 940, "body": "def _iter_per_percentage(self, iterable: Iterable[T], length: int, update_frequency: float) -> Iterator[T]:\n            update_count = 0\n            prev_index = -1\n            next_index = max(0, int(update_frequency * length) - 1)\n            for idx, x in enumerate(iterable):\n                yield x\n                if idx == next_index:\n                    self.update(idx - prev_index)\n                    update_count += 1\n                    next_index = max(idx + 1, int(update_frequency * (update_count + 1) * length) - 1)\n                    prev_index = idx\n            if length > prev_index + 1:\n                self.update(length - prev_index - 1)", "is_method": true, "class_name": "Proxy", "function_description": "Core utility method of the Proxy class that iterates over items while periodically triggering an update callback based on a specified percentage of progress, useful for monitoring or reporting iteration progress in long-running processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "update", "line_number": 954, "body": "def update(self, n: int = 0, *, postfix: Optional[Dict[str, Any]] = None) -> None:\n            r\"\"\"Update progress for the current progress bar.\n\n            :param n: Increment to add to the counter.\n            :param postfix: An optional dictionary containing additional stats displayed at the end of the progress bar.\n                See `tqdm.set_postfix <https://tqdm.github.io/docs/tqdm/#set_postfix>`_ for more details.\n            \"\"\"\n            self.queue.put_nowait(UpdateEvent(get_worker_id(), n, postfix))", "is_method": true, "class_name": "Proxy", "function_description": "Utility method of the Proxy class that increments the current progress bar by a specified amount and optionally updates its displayed stats with additional information. It enables asynchronous progress tracking with custom status updates."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "write", "line_number": 963, "body": "def write(self, message: str) -> None:\n            r\"\"\"Write a message to console without disrupting the progress bars.\n\n            :param message: The message to write.\n            \"\"\"\n            self.queue.put_nowait(WriteEvent(get_worker_id(), message))", "is_method": true, "class_name": "Proxy", "function_description": "Utility method of the Proxy class that safely writes messages to the console without interfering with ongoing progress bars, ensuring smooth user feedback during concurrent operations."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "close", "line_number": 970, "body": "def close(self) -> None:\n            r\"\"\"Close the current progress bar.\"\"\"\n            self.queue.put_nowait(CloseEvent(get_worker_id()))", "is_method": true, "class_name": "Proxy", "function_description": "Closes the current progress bar by sending a close event to the internal queue. This allows coordinated termination of progress tracking across worker processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__enter__", "line_number": 975, "body": "def __enter__(self):\n            return self", "is_method": true, "class_name": "Proxy", "function_description": "Enables use of the Proxy instance as a context manager, supporting the with statement for resource management convenience."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "__exit__", "line_number": 978, "body": "def __exit__(self, exc_type, exc_val, exc_tb):\n            self.close()", "is_method": true, "class_name": "Proxy", "function_description": "Ensures the Proxy instance is properly closed when exiting a context, managing resource cleanup automatically."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/multiproc.py", "function": "new", "line_number": 985, "body": "def new(self, iterable=None, **kwargs):\n            if iterable is not None:\n                return iterable\n            return self", "is_method": true, "class_name": "_DummyProxy", "function_description": "Returns the given iterable if provided; otherwise, returns the current instance. This utility supports flexible object initialization or chaining by conditionally passing through an iterable input."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/timing.py", "function": "work_in_progress", "line_number": 10, "body": "def work_in_progress(desc: str = \"Work in progress\"):\n    r\"\"\"Time the execution time of a code block or function.\n\n    .. code:: python\n\n        >>> @work_in_progress(\"Loading file\")\n        ... def load_file(path):\n        ...     with open(path, \"rb\") as f:\n        ...         return pickle.load(f)\n        ...\n        ... obj = load_file(\"/path/to/some/file\")\n        Loading file... done. (3.52s)\n\n        >>> with work_in_progress(\"Saving file\"):\n        ...     with open(path, \"wb\") as f:\n        ...         pickle.dump(obj, f)\n        Saving file... done. (3.78s)\n\n    :param desc: Description of the task performed.\n    \"\"\"\n    print(desc + \"... \", end='', flush=True)\n    begin_time = time.time()\n    yield\n    time_consumed = time.time() - begin_time\n    print(f\"done. ({time_consumed:.2f}s)\")", "is_method": false, "function_description": "Utility function and context manager that times execution of code blocks or functions, displaying a custom descriptive message with the elapsed duration to track progress during runtime."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "get_worker_id", "line_number": 24, "body": "def get_worker_id() -> Optional[int]:\n    r\"\"\"Return the ID of the pool worker process, or ``None`` if the current process is not a pool worker.\"\"\"\n    proc_name = mp.current_process().name\n    if \"PoolWorker\" in proc_name:\n        worker_id = int(proc_name[(proc_name.find('-') + 1):])\n        return worker_id\n    return None", "is_method": false, "function_description": "Function that identifies if the current process is a pool worker and returns its numeric worker ID, or None if it isn\u2019t, facilitating worker-specific operations in multiprocessing pools."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "_remove_handlers", "line_number": 95, "body": "def _remove_handlers(logger):\n    while len(logger.handlers) > 0:\n        handler = logger.handlers[0]\n        handler.close()\n        logger.removeHandler(handler)", "is_method": false, "function_description": "Utility function that clears all handlers from a logger, ensuring proper release of resources and preventing duplicate logging outputs. It helps manage and reset logger configurations safely."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "get_logging_levels", "line_number": 131, "body": "def get_logging_levels() -> List[str]:\n    r\"\"\"Return a list of logging levels that the logging system supports.\"\"\"\n    return list(LEVEL_MAP.keys())", "is_method": false, "function_description": "Function that provides a list of all supported logging levels available in the logging system. Useful for configuring or validating log level settings in applications."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "set_log_file", "line_number": 136, "body": "def set_log_file(path: PathType, fmt: str = \"%(asctime)s %(levelname)s: %(message)s\") -> None:\n    r\"\"\"Set the path of the log file.\n\n    :param path: Path to the log file.\n    :param fmt: Logging format.\n    \"\"\"\n    _remove_handlers(LOGGER)\n    handler = MultiprocessingFileHandler(path, mode=\"a\")\n    handler.setFormatter(logging.Formatter(fmt))\n    LOGGER.addHandler(handler)", "is_method": false, "function_description": "Utility function that configures logging to write messages to a specified file path using a given format, supporting multiprocessing-safe log handling."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "log", "line_number": 148, "body": "def log(msg: str, level: str = \"info\", force_console: bool = False,\n        timestamp: bool = True, include_proc_id: bool = True) -> None:\n    r\"\"\"Write a line of log with the specified logging level.\n\n    :param msg: Message to log.\n    :param level: Logging level. Available options are ``success``, ``warning``, ``error``, and ``info``. Defaults to\n        ``info``.\n    :param force_console: If ``True``, will write to console regardless of logging level setting. Defaults to ``False``.\n    :param timestamp: If ``True``, will add a timestamp to the console logging output. Defaults to ``True``.\n\n        ..note::\n            The logging level colors apply to the timestamp only, so if :attr:`timestamp` is set to ``False``, then\n            it's not possible to differentiate between different logging levels under console.\n\n    :param include_proc_id: If ``True``, will include the process ID for multiprocessing pool workers. Defaults to\n        ``True``.\n    \"\"\"\n    if level not in LOGGING_MAP:\n        raise ValueError(f\"Incorrect logging level '{level}'\")\n    if include_proc_id:\n        worker_id = get_worker_id()\n        if worker_id is not None:\n            msg = f\"(Worker {worker_id:2d}) {msg}\"\n    if force_console or LEVEL_MAP[level] >= _CONSOLE_LOGGING_LEVEL.value:\n        if timestamp:\n            time_str = time.strftime(\"[%Y-%m-%d %H:%M:%S]\")\n            _CONSOLE_LOG_FN(colored(time_str, COLOR_MAP[level]) + \" \" + msg)\n        else:\n            _CONSOLE_LOG_FN(msg)\n    if LOGGER.hasHandlers():\n        LOGGING_MAP[level](msg)", "is_method": false, "function_description": "Function that logs messages with configurable level, timestamp, process ID, and console output options, supporting enhanced visibility and multiprocessing context for debugging or monitoring purposes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "set_logging_level", "line_number": 181, "body": "def set_logging_level(level: str, console: bool = True, file: bool = True) -> None:\n    r\"\"\"Set the global logging level to the specified level.\n\n    :param level: Logging level.\n    :param console: If ``True``, the specified logging level applies to console output.\n    :param file: If ``True``, the specified logging level applies to file output.\n    \"\"\"\n    if level not in LEVEL_MAP:\n        raise ValueError(f\"Incorrect logging level '{level}'\")\n    if console:\n        _CONSOLE_LOGGING_LEVEL.value = LEVEL_MAP[level]\n    if file:\n        LOGGER.setLevel(LEVEL_MAP[level])", "is_method": false, "function_description": "Function that sets the global logging verbosity level for console and/or file outputs, enabling dynamic control of log detail based on user preference or environment. It supports flexible configuration of logging destinations separately."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "_get_console_logging_function", "line_number": 196, "body": "def _get_console_logging_function():\n    return _CONSOLE_LOG_FN", "is_method": false, "function_description": "Returns the predefined console logging function for standardized logging output. This utility provides consistent access to the console logger used across the application."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "set_console_logging_function", "line_number": 200, "body": "def set_console_logging_function(log_fn: Callable[[str], None]) -> None:\n    r\"\"\"Set the console logging function **for current process only**.\"\"\"\n    global _CONSOLE_LOG_FN\n    _CONSOLE_LOG_FN = log_fn", "is_method": false, "function_description": "This function sets a custom console logging function for the current process, allowing other parts of the program to direct log output to a specified callable. It enables flexible, process-local logging behavior customization."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "setFormatter", "line_number": 52, "body": "def setFormatter(self, fmt):\n        logging.Handler.setFormatter(self, fmt)\n        self._handler.setFormatter(fmt)", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Sets the logging formatter for both the multiprocessing file handler and its underlying handler, ensuring consistent log message formatting across processes."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "receive", "line_number": 56, "body": "def receive(self):\n        while True:\n            try:\n                record = self.queue.get()\n                self._handler.emit(record)\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except EOFError:\n                break\n            except:\n                traceback.print_exc(file=sys.stderr)", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Continuously processes log records from a queue, emitting each through a handler until interrupted or the queue ends. This supports asynchronous log handling in multiprocessing contexts."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "send", "line_number": 68, "body": "def send(self, s):\n        self.queue.put_nowait(s)", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Utility method of MultiprocessingFileHandler that asynchronously sends data by putting it into a managed queue for inter-process communication."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "_format_record", "line_number": 71, "body": "def _format_record(self, record):\n        if record.args:\n            record.msg = record.msg % record.args\n            record.args = None\n        if record.exc_info:\n            _ = self.format(record)\n            record.exc_info = None\n\n        return record", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Internal utility method of the MultiprocessingFileHandler class that formats a log record by applying argument substitution and exception formatting, preparing it for safe multiprocessing-safe logging output."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "emit", "line_number": 81, "body": "def emit(self, record):\n        try:\n            s = self._format_record(record)\n            self.send(s)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Core logging method of MultiprocessingFileHandler that formats a log record and sends it, handling exceptions to ensure reliable log emission in multiprocessing contexts."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/log.py", "function": "close", "line_number": 90, "body": "def close(self):\n        self._handler.close()\n        logging.Handler.close(self)", "is_method": true, "class_name": "MultiprocessingFileHandler", "function_description": "Service method of MultiprocessingFileHandler that closes the associated file handler and properly terminates the logging handler to release resources."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/math.py", "function": "ceil_div", "line_number": 6, "body": "def ceil_div(a: int, b: int) -> int:\n    r\"\"\"Integer division that rounds up.\"\"\"\n    return (a - 1) // b + 1", "is_method": false, "function_description": "Utility function that performs integer division of two numbers, rounding the result up to the nearest whole number. Useful for calculations requiring ceiling division, such as pagination or chunking tasks."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "reverse_map", "line_number": 16, "body": "def reverse_map(d: Dict[T, int]) -> List[T]:\n    r\"\"\"Given a dict containing pairs of ``(item, id)``, return a list where the ``id``-th element is ``item``.\n\n    .. note::\n        It is assumed that the ``id``\\ s form a permutation.\n\n    .. code:: python\n\n        >>> words = ['a', 'aardvark', 'abandon', ...]\n        >>> word_to_id = {word: idx for idx, word in enumerate(words)}\n        >>> id_to_word = reverse_map(word_to_id)\n        >>> (words == id_to_word)\n        True\n\n    :param d: The dictionary mapping ``item`` to ``id``.\n    \"\"\"\n    return [k for k, _ in sorted(d.items(), key=lambda xs: xs[1])]", "is_method": false, "function_description": "Utility function that converts a dictionary mapping items to unique integer IDs into a list where each index corresponds to the original item, effectively reversing the mapping for index-based access."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "register_no_map_class", "line_number": 39, "body": "def register_no_map_class(container_type: Type[T]) -> None:\n    r\"\"\"Register a container type as `non-mappable`, i.e., instances of the class will be treated as singleton objects in\n    :func:`map_structure` and :func:`map_structure_zip`, their contents will not be traversed. This would be useful for\n    certain types that subclass built-in container types, such as ``torch.Size``.\n\n    :param container_type: The type of the container, e.g. :py:class:`list`, :py:class:`dict`.\n    \"\"\"\n    return _NO_MAP_TYPES.add(container_type)", "is_method": false, "function_description": "Registers a container type to be treated as a non-mappable singleton, preventing its contents from being traversed by structure-mapping functions. This is useful for container subclasses that should not be recursively processed."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "_no_map_type", "line_number": 50, "body": "def _no_map_type(container_type: Type[T]) -> Type[T]:\n    # Create a subtype of the container type that sets an normally inaccessible\n    # special attribute on instances.\n    # This is necessary because `setattr` does not work on built-in types\n    # (e.g. `list`).\n    new_type = type(\"_no_map\" + container_type.__name__,\n                    (container_type,), {_NO_MAP_INSTANCE_ATTR: True})\n    return new_type", "is_method": false, "function_description": "Utility function that creates a subtype of a given container type with a special marker attribute, enabling attribute-setting on otherwise immutable built-in types like lists."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "no_map_instance", "line_number": 61, "body": "def no_map_instance(instance: T) -> T:\n    r\"\"\"Register a container instance as `non-mappable`, i.e., it will be treated as a singleton object in\n    :func:`map_structure` and :func:`map_structure_zip`, its contents will not be traversed.\n\n    :param instance: The container instance.\n    \"\"\"\n    try:\n        setattr(instance, _NO_MAP_INSTANCE_ATTR, True)\n        return instance\n    except AttributeError:\n        return _no_map_type(type(instance))(instance)", "is_method": false, "function_description": "Utility function that marks a container instance as non-traversable in mapping operations, ensuring it is treated as a singleton and its contents are not recursively processed during structure mapping functions."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "map_structure", "line_number": 75, "body": "def map_structure(fn: Callable[[T], R], obj: Collection[T]) -> Collection[R]:\n    r\"\"\"Map a function over all elements in a (possibly nested) collection.\n\n    :param fn: The function to call on elements.\n    :param obj: The collection to map function over.\n    :return: The collection in the same structure, with elements mapped.\n    \"\"\"\n    if obj.__class__ in _NO_MAP_TYPES or hasattr(obj, _NO_MAP_INSTANCE_ATTR):\n        return fn(obj)\n    if isinstance(obj, list):\n        return [map_structure(fn, x) for x in obj]\n    if isinstance(obj, tuple):\n        if hasattr(obj, '_fields'):  # namedtuple\n            return type(obj)(*[map_structure(fn, x) for x in obj])\n        else:\n            return tuple(map_structure(fn, x) for x in obj)\n    if isinstance(obj, dict):\n        # could be `OrderedDict`\n        return type(obj)((k, map_structure(fn, v)) for k, v in obj.items())\n    if isinstance(obj, set):\n        return {map_structure(fn, x) for x in obj}\n    return fn(obj)", "is_method": false, "function_description": "Utility function that recursively applies a given function to all elements within a nested collection, maintaining the original collection structure while transforming each element."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/structure.py", "function": "map_structure_zip", "line_number": 100, "body": "def map_structure_zip(fn: Callable[..., R], objs: Sequence[Collection[T]]) -> Collection[R]:\n    r\"\"\"Map a function over tuples formed by taking one elements from each (possibly nested) collection. Each collection\n    must have identical structures.\n\n    .. note::\n        Although identical structures are required, it is not enforced by assertions. The structure of the first\n        collection is assumed to be the structure for all collections.\n\n    :param fn: The function to call on elements.\n    :param objs: The list of collections to map function over.\n    :return: A collection with the same structure, with elements mapped.\n    \"\"\"\n    obj = objs[0]\n    if obj.__class__ in _NO_MAP_TYPES or hasattr(obj, _NO_MAP_INSTANCE_ATTR):\n        return fn(*objs)\n    if isinstance(obj, list):\n        return [map_structure_zip(fn, xs) for xs in zip(*objs)]\n    if isinstance(obj, tuple):\n        if hasattr(obj, '_fields'):  # namedtuple\n            return type(obj)(*[map_structure_zip(fn, xs) for xs in zip(*objs)])\n        else:\n            return tuple(map_structure_zip(fn, xs) for xs in zip(*objs))\n    if isinstance(obj, dict):\n        # could be `OrderedDict`\n        return type(obj)((k, map_structure_zip(fn, [o[k] for o in objs])) for k in obj.keys())\n    if isinstance(obj, set):\n        raise ValueError(\"Structures cannot contain `set` because it's unordered\")\n    return fn(*objs)", "is_method": false, "function_description": "Utility function that applies a given function element-wise across identically structured collections, preserving the input structure. It supports nested lists, tuples (including namedtuples), and dictionaries but disallows unordered sets."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "register_ipython_excepthook", "line_number": 18, "body": "def register_ipython_excepthook() -> None:\n    r\"\"\"Register an exception hook that launches an interactive IPython session upon uncaught exceptions.\n    \"\"\"\n\n    def excepthook(type, value, traceback):\n        if type is KeyboardInterrupt or type is BdbQuit:\n            # Don't capture keyboard interrupts (Ctrl+C) or Python debugger exit events.\n            sys.__excepthook__(type, value, traceback)\n        else:\n            ipython_hook(type, value, traceback)\n\n    # Enter IPython debugger on exception.\n    from IPython.core import ultratb\n\n    ipython_hook = ultratb.FormattedTB(mode='Context', color_scheme='Linux', call_pdb=1)\n    sys.excepthook = excepthook", "is_method": false, "function_description": "Function that sets a custom exception handler to automatically launch an interactive IPython debugging session whenever an uncaught exception occurs, except for keyboard interrupts or debugger exits."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "log_exception", "line_number": 36, "body": "def log_exception(e, user_msg: Optional[str] = None, **kwargs):\n    r\"\"\"Convenience function to log an exception using the logging interface.\n\n    :param e: The exception instance.\n    :param user_msg: An optional user message to print.\n    :param kwargs: Additional arguments for :func:`~flutes.log.log`.\n    \"\"\"\n    exc_msg = f\"<{e.__class__.__qualname__}> {e}\"\n    if user_msg is not None:\n        exc_msg = f\"{user_msg}: {exc_msg}\"\n    try:\n        if not (isinstance(e, subprocess.CalledProcessError) and e.output is not None):\n            log(traceback.format_exc(), \"error\", **kwargs)\n        log(exc_msg, \"error\", **kwargs)\n    except Exception as log_e:\n        print(exc_msg)\n        print(f\"Another exception occurred while logging: <{log_e.__class__.__qualname__}> {log_e}\")\n        raise log_e", "is_method": false, "function_description": "Utility function that logs detailed information about exceptions, optionally including a user message, to aid in debugging and error tracking during program execution."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "exception_wrapper", "line_number": 56, "body": "def exception_wrapper(handler_fn=None):\n    r\"\"\"Function decorator that calls the specified handler function when a exception occurs inside the decorated\n    function. By default, ``handler_fn`` is ``None``, and :func:`log_exception` will be called to print the exception\n    details.\n\n    A custom handler function takes the following arguments:\n\n    - A positional argument for the exception object. This must be the first argument of the method.\n    - Arguments with matching names to arguments in the wrapped method. These arguments will be filled with values\n      passed to the wrapped method. These arguments cannot take default values.\n    - Arguments without matching names. These arguments must take default values.\n    - An optional variadic keyword argument (``**kwargs``). This will be filled with remaining argument name-value pairs\n      that are not captured by other arguments.\n\n    For example:\n\n    .. code:: python\n\n        def handler_fn(e, three, one, args, my_arg=None, **kw): ...\n\n        @exception_wrapper(handler_fn)\n        def foo(one, two, *args, three=None, **kwargs): ...\n\n        foo(1, \"2\", \"arg1\", \"arg2\", four=4)\n\n    Assuming a :py:exc:`ValueError` is thrown, the argument values for ``handler_fn`` would be:\n\n    .. code::\n\n        e:      <ValueError>\n        three:  None\n        one:    1\n        args:   (\"arg1\", \"arg2\")\n        my_arg: None\n        kw:     {\"two\": \"2\",\n                 \"kwargs\": {\"four\": 4}}\n    \"\"\"\n\n    def _unwrap(fn):\n        if hasattr(fn, \"__wrapped__\"):\n            return _unwrap(fn.__wrapped__)\n        return fn\n\n    def decorator(func):\n        if handler_fn is not None:\n            handler_argspec = inspect.getfullargspec(_unwrap(handler_fn))\n            if len(handler_argspec.args) == 0:\n                raise ValueError(\"Exception handler must have a positional argument for the exception object\")\n            if handler_argspec.varargs is not None:\n                raise ValueError(\"Exception handler cannot have a varargs argument (*args)\")\n            handler_arg_names = set(handler_argspec.args[1:] + handler_argspec.kwonlyargs)\n            handler_args_with_defaults = set((handler_argspec.kwonlydefaults or {}).keys())\n            if handler_argspec.defaults is not None:\n                handler_args_with_defaults |= set(handler_argspec.args[-len(handler_argspec.defaults):])\n            handler_arg_names -= handler_args_with_defaults\n            inner_signature = inspect.signature(func)\n            for name in handler_arg_names:\n                if name not in inner_signature.parameters:\n                    raise ValueError(f\"Argument '{name}' in exception handler does not match \"\n                                     f\"any argument in wrapped method\")\n            for name in handler_args_with_defaults:\n                if name in inner_signature.parameters:\n                    raise ValueError(f\"Argument '{name}' matches wrapped method argument, thus \"\n                                     f\"cannot have default values\")\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if handler_fn is None:\n                    log_exception(e)\n                else:\n                    # Credit: https://stackoverflow.com/questions/59831981/\n                    bound_args = inner_signature.bind(*args, **kwargs)\n                    bound_args.apply_defaults()\n                    handler_args = {name: bound_args.arguments[name] for name in handler_arg_names}\n                    if handler_argspec.varkw is not None:\n                        var_kw = {name: value for name, value in bound_args.arguments.items()\n                                  if name not in handler_arg_names}\n                        handler_args.update(var_kw)  # they would just match the kwargs\n                    return handler_fn(e, **handler_args)\n\n        return wrapped\n\n    return decorator", "is_method": false, "function_description": "Decorator that wraps a function to catch exceptions and invokes a specified handler with the exception and relevant arguments. It enables custom exception handling tailored to the wrapped function\u2019s signature for finer control over error responses."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "excepthook", "line_number": 22, "body": "def excepthook(type, value, traceback):\n        if type is KeyboardInterrupt or type is BdbQuit:\n            # Don't capture keyboard interrupts (Ctrl+C) or Python debugger exit events.\n            sys.__excepthook__(type, value, traceback)\n        else:\n            ipython_hook(type, value, traceback)", "is_method": false, "function_description": "Function that customizes exception handling by delegating KeyboardInterrupt or debugger exits to default behavior while processing other exceptions via an IPython-specific handler."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "_unwrap", "line_number": 94, "body": "def _unwrap(fn):\n        if hasattr(fn, \"__wrapped__\"):\n            return _unwrap(fn.__wrapped__)\n        return fn", "is_method": false, "function_description": "Utility function that recursively retrieves the original undecorated function by unwrapping any layers of decorators applied to it."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "decorator", "line_number": 99, "body": "def decorator(func):\n        if handler_fn is not None:\n            handler_argspec = inspect.getfullargspec(_unwrap(handler_fn))\n            if len(handler_argspec.args) == 0:\n                raise ValueError(\"Exception handler must have a positional argument for the exception object\")\n            if handler_argspec.varargs is not None:\n                raise ValueError(\"Exception handler cannot have a varargs argument (*args)\")\n            handler_arg_names = set(handler_argspec.args[1:] + handler_argspec.kwonlyargs)\n            handler_args_with_defaults = set((handler_argspec.kwonlydefaults or {}).keys())\n            if handler_argspec.defaults is not None:\n                handler_args_with_defaults |= set(handler_argspec.args[-len(handler_argspec.defaults):])\n            handler_arg_names -= handler_args_with_defaults\n            inner_signature = inspect.signature(func)\n            for name in handler_arg_names:\n                if name not in inner_signature.parameters:\n                    raise ValueError(f\"Argument '{name}' in exception handler does not match \"\n                                     f\"any argument in wrapped method\")\n            for name in handler_args_with_defaults:\n                if name in inner_signature.parameters:\n                    raise ValueError(f\"Argument '{name}' matches wrapped method argument, thus \"\n                                     f\"cannot have default values\")\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if handler_fn is None:\n                    log_exception(e)\n                else:\n                    # Credit: https://stackoverflow.com/questions/59831981/\n                    bound_args = inner_signature.bind(*args, **kwargs)\n                    bound_args.apply_defaults()\n                    handler_args = {name: bound_args.arguments[name] for name in handler_arg_names}\n                    if handler_argspec.varkw is not None:\n                        var_kw = {name: value for name, value in bound_args.arguments.items()\n                                  if name not in handler_arg_names}\n                        handler_args.update(var_kw)  # they would just match the kwargs\n                    return handler_fn(e, **handler_args)\n\n        return wrapped", "is_method": false, "function_description": "This function provides a decorator that wraps another function to catch and handle exceptions using a custom handler, validating handler signature compatibility beforehand. It enables customized exception processing while preserving the original function's interface."}, {"file": "./dataset/RepoExec/test-apps/flutes/flutes/exception.py", "function": "wrapped", "line_number": 122, "body": "def wrapped(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if handler_fn is None:\n                    log_exception(e)\n                else:\n                    # Credit: https://stackoverflow.com/questions/59831981/\n                    bound_args = inner_signature.bind(*args, **kwargs)\n                    bound_args.apply_defaults()\n                    handler_args = {name: bound_args.arguments[name] for name in handler_arg_names}\n                    if handler_argspec.varkw is not None:\n                        var_kw = {name: value for name, value in bound_args.arguments.items()\n                                  if name not in handler_arg_names}\n                        handler_args.update(var_kw)  # they would just match the kwargs\n                    return handler_fn(e, **handler_args)", "is_method": false, "function_description": "This function provides a flexible error handling wrapper that executes a target function and processes exceptions via a custom handler or default logging. It enables centralized and customizable exception management for any callable."}]