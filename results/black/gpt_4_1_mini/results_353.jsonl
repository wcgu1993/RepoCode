{"task_id": 353, "prediction_id": 0, "generation": "from typing import Iterable, Tuple, Union, Text\nfrom blib2to3.pgen2.token import TokenInfo\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # We use the Untokenizer class from the file context, which manages spacing and newlines\n    class Untokenizer:\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            # It is possible that row < prev_row in some cases (though unlikely here)\n            # We add whitespace only if on the same line\n            if row == self.prev_row:\n                col_offset = col - self.prev_col\n                if col_offset > 0:\n                    self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                # t can be either (type, value) or (type, value, start, end, line)\n                if len(t) == 2:\n                    # Older simple form\n                    tok_type, token = t\n                    # Add a token and a space for name/number tokens to avoid concatenation on output\n                    if tok_type in (1, 2):  # NAME=1, NUMBER=2 from token.py\n                        token += \" \"\n                    self.tokens.append(token)\n                    self.prev_row = 1\n                    self.prev_col = 0\n                    continue\n\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (4, 61):  # NEWLINE=4, NL=61\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n    return Untokenizer().untokenize(iterable)", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 369, in <module>\nImportError: cannot import name 'TokenInfo' from 'blib2to3.pgen2.token' (/input/test-apps/black/src/blib2to3/pgen2/token.py)\n", "passed": false}
{"task_id": 353, "prediction_id": 1, "generation": "from typing import Iterable, Union, Tuple, Text\n\nfrom blib2to3.pgen2.token import *\nfrom typing import Tuple\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # Use Untokenizer class defined in the file content to do the untokenizing.\n    # We reimplement its logic here.\n\n    tokens = []\n    prev_row = 1\n    prev_col = 0\n\n    for t in iterable:\n        if len(t) == 2:\n            # Legacy compatibility behavior similar to Untokenizer.compat()\n            toknum, tokval = t\n            startline = False\n            indents = []\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n\n            # The iterable might have more tokens after this, so try to process them\n            # but since we do not have full Untokenizer.compat implementation detail here,\n            # just append the tokens simply and break.\n            tokens.append(tokval)\n            break\n\n        tok_type, token, start, end, line = t\n\n        row, col = start\n        # Add whitespace if needed to move forward to the start position\n        # Note: row only goes backward or same (asserted in Untokenizer.add_whitespace)\n        if row == prev_row:\n            col_offset = col - prev_col\n            if col_offset > 0:\n                tokens.append(\" \" * col_offset)\n        else:\n            # New lines between previous token and this token\n            # Adding newlines to reach the 'row' line\n            newlines = row - prev_row\n            if newlines > 0:\n                tokens.append(\"\\n\" * newlines)\n            # And indentation on the new line\n            if col > 0:\n                tokens.append(\" \" * col)\n\n        tokens.append(token)\n        prev_row, prev_col = end\n\n        # After NEWLINE or NL token, the next token should start from next line, col=0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 729, in <module>\n  File \"<string>\", line 728, in test_2\nAssertionError\n", "passed": false}
{"task_id": 353, "prediction_id": 2, "generation": "from typing import Iterable, Union, Tuple, Text\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\nNEWLINE = 4\nNL = 61\nNAME = 1\nNUMBER = 2\nINDENT = 5\nDEDENT = 6\nASYNC = 100\nAWAIT = 101\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).__next__\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    class Untokenizer:\n        tokens: list[Text]\n        prev_row: int\n        prev_col: int\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            assert row <= self.prev_row\n            col_offset = col - self.prev_col\n            if col_offset:\n                self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                if len(t) == 2:\n                    self.compat(t, iterable)\n                    break\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (NEWLINE, NL):\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n        def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n            startline = False\n            indents = []\n            toks_append = self.tokens.append\n            toknum, tokval = token\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n            for tok in iterable:\n                toknum, tokval = tok[:2]\n\n                if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                    tokval += \" \"\n\n                if toknum == INDENT:\n                    indents.append(tokval)\n                    continue\n                elif toknum == DEDENT:\n                    indents.pop()\n                    continue\n                elif toknum in (NEWLINE, NL):\n                    startline = True\n                elif startline and indents:\n                    toks_append(indents[-1])\n                    startline = False\n                toks_append(tokval)\n\n    return Untokenizer().untokenize(iterable)", "message": "passed", "passed": true}
{"task_id": 353, "prediction_id": 3, "generation": "from typing import Iterable, Union, Tuple, Text, cast\nfrom blib2to3.pgen2.token import *\nfrom typing import List\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens: List[Text] = []\n    prev_row, prev_col = 1, 0\n\n    for t in iterable:\n        # If tuple is length 2, it is (type, string)\n        if len(t) == 2:\n            # Compatibility mode: just append token with a space if NAME or NUMBER,\n            # else just append token\n            tok_type, tok_val = cast(Tuple[int, Text], t)\n            if tok_type in (NAME, NUMBER):\n                tokens.append(tok_val + \" \")\n            elif tok_type in (NEWLINE, NL):\n                tokens.append(\"\\n\")\n            else:\n                tokens.append(tok_val)\n            prev_row = prev_col = 0  # Reset positions as we have no position info\n            continue\n\n        # Full token info tuple: (type, string, start, end, line)\n        tok_type, tok_val, start, end, line = cast(\n            Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text], t\n        )\n        srow, scol = start\n        erow, ecol = end\n\n        # Add whitespace to advance columns and lines if needed\n        if srow > prev_row:\n            # New lines in between tokens\n            tokens.append(\"\\n\" * (srow - prev_row))\n            prev_col = 0\n        if scol > prev_col:\n            tokens.append(\" \" * (scol - prev_col))\n\n        tokens.append(tok_val)\n\n        prev_row, prev_col = erow, ecol\n\n    return \"\".join(tokens)", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 764, in <module>\n  File \"<string>\", line 763, in test_7\nAssertionError\n", "passed": false}
{"task_id": 353, "prediction_id": 4, "generation": "from typing import Iterable, Tuple, Union, Text\n\n# Import token constants\nfrom blib2to3.pgen2.token import *\n\n# Type alias for input tokens\nTokenInfo = Union[Tuple[int, str], Tuple[int, str, Tuple[int, int], Tuple[int, int], Text]]\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens = []\n    prev_row, prev_col = 1, 0  # Track the position of the last token's end\n\n    for t in iterable:\n        if len(t) == 2:\n            # Compatibility mode: tuple of (tok_type, tok_val) only\n            # Append tokens separated by space for names and numbers\n            tok_type, tok_val = t\n            if tok_type in (NAME, NUMBER):\n                tok_val += \" \"\n            tokens.append(tok_val)\n            # We cannot reliably fix spacing here, so just continue\n            continue\n\n        # Full token info\n        tok_type, tok_val, (srow, scol), (erow, ecol), line = t\n\n        # Add appropriate whitespace based on start position relative to previous token end\n        # If this token is on the same line as previous token\n        if srow == prev_row:\n            space = scol - prev_col\n            if space > 0:\n                tokens.append(\" \" * space)\n        else:\n            # Different line(s) from previous token\n            # Insert the line breaks needed to reach the start row\n            for _ in range(srow - prev_row):\n                tokens.append(\"\\n\")\n            # Insert spaces to column start of this token\n            if scol > 0:\n                tokens.append(\" \" * scol)\n\n        tokens.append(tok_val)\n        prev_row, prev_col = erow, ecol\n\n        # Adjust after NEWLINE or NL tokens: newline increases prev_row, resets col to 0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)", "message": "passed", "passed": true}
