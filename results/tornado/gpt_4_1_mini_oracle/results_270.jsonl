{"task_id": 270, "prediction_id": 0, "generation": "import re\nfrom typing import Union, Callable, List\n\nfrom tornado.escape import xhtml_escape\n\n\ndef linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n    taking the link as an argument and returning the extra text\n    e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n    or::\n\n    def extra_params_cb(url):\n      if url.startswith(\"http://example.com\"):\n        return 'class=\"internal\"'\n      else:\n        return 'class=\"external\" rel=\"nofollow\"'\n    linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n    this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n    linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n    \"mailto\"])``. It is very unsafe to include protocols such as\n    ``javascript``.\n    \"\"\"\n    # Ensure text is a str (unicode) for processing\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\", \"replace\")\n\n    # Regex to detect URLs:\n    # 1) with protocol, e.g. http://example.com\n    # 2) without protocol, e.g. www.example.com (only if require_protocol is False)\n    # To keep it relatively simple yet effective, we'll use patterns inspired by the _URL_RE but simplified here.\n\n    # URL regex parts:\n    # Protocol capture group: scheme://\n    # Host + path: contiguous non-space characters except some punctuation at the end\n    # We will try to keep trailing punctuation like ., !, ?) out of the URL\n\n    # Regex explanation:\n    # (?P<full_url>...) - named group for the whole matched URL\n    # - protocol: (?P<proto>\\w+):\\/\\/\n    # - or www. prefix without protocol\n    # - domain + path: [^\\s<>\"'()]+ (anything except spaces and brackets/quotes)\n    # We'll anchor URLs at word boundaries to avoid mid-word captures.\n\n    url_pattern_with_proto = (\n        r\"(?P<full_url>(?P<proto>[a-zA-Z][a-zA-Z0-9+\\-.]*):\\/\\/\"  # scheme://\n        r\"[^ \\t\\r\\n<>\\\"'()]+)\"  # domain and path\n    )\n    url_pattern_without_proto = (\n        r\"(?P<full_url>www\\.[^ \\t\\r\\n<>\\\"'()]+)\"\n    )\n    if require_protocol:\n        url_pattern = url_pattern_with_proto\n    else:\n        url_pattern = f\"({url_pattern_with_proto}|{url_pattern_without_proto})\"\n\n    url_re = re.compile(url_pattern)\n\n    def shorten_url(url: str) -> str:\n        \"\"\"Shortens the URL display string if needed.\"\"\"\n        # Example policy: If URL is longer than 40 characters, truncate with ellipsis in the middle\n        max_length = 40\n        if len(url) <= max_length:\n            return url\n        # We keep start and end parts\n        prefix_len = max_length // 2 - 2\n        suffix_len = max_length - prefix_len - 3\n        return url[:prefix_len] + \"...\" + url[-suffix_len:]\n\n    def replace_url(match: re.Match) -> str:\n        url = match.group(\"full_url\")\n        proto = match.groupdict().get(\"proto\")\n\n        # Check protocol if required\n        if proto:\n            proto_lc = proto.lower()\n            if proto_lc not in permitted_protocols:\n                # Do not linkify, just escape and return original text\n                return xhtml_escape(url)\n        else:\n            if require_protocol:\n                # No protocol and require_protocol=True: do not linkify\n                return xhtml_escape(url)\n\n        # For URLs without protocol but allowed (www. prefix), prepend 'http://' in href\n        href = url\n        if not proto:\n            # If no protocol, assume http:// prefix for href\n            href = \"http://\" + url\n\n        display_url = url\n        if shorten:\n            display_url = shorten_url(display_url)\n\n        # Get extra params string\n        if callable(extra_params):\n            extra = extra_params(url)\n        else:\n            extra = extra_params\n\n        # Escape href and display_url\n        href_escaped = xhtml_escape(href)\n        display_escaped = xhtml_escape(display_url)\n\n        # Compose the final anchor tag\n        if extra:\n            return f'<a href=\"{href_escaped}\" {extra}>{display_escaped}</a>'\n        else:\n            return f'<a href=\"{href_escaped}\">{display_escaped}</a>'\n\n    # Escape the whole text first to safely handle HTML entities outside links\n    escaped_text = xhtml_escape(text)\n\n    # Now we want to linkify the URLs inside escaped_text.\n    # But since we escaped the text, URLs are escaped too (e.g., & becomes &amp;)\n    # We want to linkify on the original text to get raw URLs and build the final string\n    # To handle this cleanly, we'll do the linkifying on the original text, replacing matched URLs with anchors,\n    # and then escape any parts outside of URLs.\n\n    # So, step back: instead of escaping whole text at top, we will process with re.sub on original text,\n    # building up the final string by escaping non-URL parts and processing matched URLs.\n\n    # Implementation plan:\n    # Iterate over the text with re.finditer, for each matched URL:\n    # - Append escaped text from last position up to match start\n    # - Append anchor tag for matched URL\n    # - Update last position to match end\n    # At the end append escaped text after last match\n\n    parts = []\n    last_end = 0\n    for match in url_re.finditer(text):\n        start, end = match.span()\n        # Append escaped text before this URL\n        parts.append(xhtml_escape(text[last_end:start]))\n        # Append linkified URL\n        parts.append(replace_url(match))\n        last_end = end\n    # Append remaining escaped text after last URL\n    parts.append(xhtml_escape(text[last_end:]))\n\n    return \"\".join(parts)", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 459, in <module>\n  File \"<string>\", line 456, in test_0\n  File \"<string>\", line 341, in linkify\n  File \"/usr/local/lib/python3.10/re.py\", line 251, in compile\n    return _compile(pattern, flags)\n  File \"/usr/local/lib/python3.10/re.py\", line 303, in _compile\n    p = sre_compile.compile(pattern, flags)\n  File \"/usr/local/lib/python3.10/sre_compile.py\", line 764, in compile\n    p = sre_parse.parse(p, flags)\n  File \"/usr/local/lib/python3.10/sre_parse.py\", line 948, in parse\n    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n  File \"/usr/local/lib/python3.10/sre_parse.py\", line 443, in _parse_sub\n    itemsappend(_parse(source, state, verbose, nested + 1,\n  File \"/usr/local/lib/python3.10/sre_parse.py\", line 834, in _parse\n    p = _parse_sub(source, state, sub_verbose, nested + 1)\n  File \"/usr/local/lib/python3.10/sre_parse.py\", line 443, in _parse_sub\n    itemsappend(_parse(source, state, verbose, nested + 1,\n  File \"/usr/local/lib/python3.10/sre_parse.py\", line 831, in _parse\n    raise source.error(err.msg, len(name) + 1) from None\nre.error: redefinition of group name 'full_url' as group 4; was group 2 at position 78\n", "passed": false}
{"task_id": 270, "prediction_id": 1, "generation": "import re\nfrom typing import Union, List, Callable\n\n# Import the xhtml_escape function from the current file context (assuming the same module)\n# Here we redefine it fully since code must run independently.\nimport html.entities\nimport re as _re\n\n\ndef xhtml_escape(value: Union[str, bytes]) -> str:\n    \"\"\"Escapes a string so it is valid within HTML or XML.\n\n    Escapes the characters ``<``, ``>``, ``\"``, ``'``, and ``&``.\n    When used in attribute values the escaped strings must be enclosed\n    in quotes.\n    \"\"\"\n    _XHTML_ESCAPE_RE = re.compile(\"[&<>\\\"']\")\n    _XHTML_ESCAPE_DICT = {\n        \"&\": \"&amp;\",\n        \"<\": \"&lt;\",\n        \">\": \"&gt;\",\n        '\"': \"&quot;\",\n        \"'\": \"&#39;\",\n    }\n\n    def to_basestring(value):\n        if isinstance(value, bytes):\n            return value.decode(\"utf-8\")\n        return value\n\n    return _XHTML_ESCAPE_RE.sub(\n        lambda match: _XHTML_ESCAPE_DICT[match.group(0)], to_basestring(value)\n    )\n\n\ndef linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n    taking the link as an argument and returning the extra text\n    e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n    or::\n\n    def extra_params_cb(url):\n    if url.startswith(\"http://example.com\"):\n    return 'class=\"internal\"'\n    else:\n    return 'class=\"external\" rel=\"nofollow\"'\n    linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n    this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n    linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n    \"mailto\"])``. It is very unsafe to include protocols such as\n    ``javascript``.\n    \"\"\"\n\n    # Convert bytes input to string\n    if isinstance(text, bytes):\n        text = text.decode('utf-8')\n\n    # Regex to match URLs, simplified but respecting the requirement:\n    # match either: protocol://something or www.something (if require_protocol=False)\n    # We must extract the protocol to check allowed protocols\n    # Pattern parts:\n    # - Protocol: ([\\w+.-]+):\\/\\/\n    # - Host and path: [^\\s<>\"']+\n    # - If no protocol, allow www.[^\\s<>\"']+\n\n    url_re = re.compile(\n        r\"\"\"\n        \\b(\n            (?: # Group for protocol URLs\n                (?P<proto>[\\w+.-]+)://\n                [^\\s<>\"'()]+\n            )\n            |\n            (?: # Group for www URLs (only if require_protocol is False)\n                www\\.[^\\s<>\"'()]+\n            )\n        )\n        \"\"\",\n        re.VERBOSE | re.IGNORECASE,\n    )\n\n    def _shorten(url: str) -> str:\n        # Shorten long URLs (display part)\n        if len(url) <= 50:\n            return url\n        # Show start + ellipsis + end (e.g. 30 chars start and 15 chars end)\n        return url[:30] + \"\u2026\" + url[-15:]\n\n    def _escape_url(url: str) -> str:\n        # Escape &, <, >, \", '\n        return xhtml_escape(url)\n\n    def _extra_params_for_url(url: str) -> str:\n        if callable(extra_params):\n            return \" \" + extra_params(url)\n        elif isinstance(extra_params, str) and extra_params.strip():\n            return \" \" + extra_params.strip()\n        else:\n            return \"\"\n\n    def _linkify_match(match: re.Match) -> str:\n        full_url = match.group(0)\n        proto = match.group(\"proto\")\n        normalized_url = full_url\n\n        # If no protocol but starts with www and require_protocol is False, assume http://\n        if not proto:\n            if require_protocol:\n                # Don't linkify if protocol required but no protocol found\n                return full_url\n            else:\n                # This is a www URL without protocol\n                proto = \"http\"\n                normalized_url = \"http://\" + full_url\n\n        # Check allowed protocols\n        if proto.lower() not in (p.lower() for p in permitted_protocols):\n            # Do not linkify disallowed protocols\n            return full_url\n\n        # Escape url for href attribute\n        href = _escape_url(normalized_url)\n\n        # Determine text to display\n        display_text = full_url\n        if shorten:\n            display_text = _shorten(display_text)\n        display_text = xhtml_escape(display_text)\n\n        # Append extra params\n        params = _extra_params_for_url(normalized_url)\n\n        return f'<a href=\"{href}\"{params}>{display_text}</a>'\n\n    result = url_re.sub(_linkify_match, text)\n    return result", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 559, in <module>\n  File \"<string>\", line 557, in test_46\nAssertionError\n", "passed": false}
{"task_id": 270, "prediction_id": 2, "generation": "import re\nfrom typing import Union, List, Callable\n\n# We can directly use the provided xhtml_escape from context\n# but since context code is given in file content, no import needed here.\n# We'll just call xhtml_escape as defined in file content.\n\n# Regex to find URLs and www-like patterns, similar to _URL_RE in file content,\n# but we replicate here with modification to capture protocol and domain\n\n_URL_RE = re.compile(\n    r\"\"\"\n    \\b(\n        (?:\n            (?P<protocol>[\\w+-]+):\\/\\/            # protocol and ://\n            |                                    # OR\n            www\\.                                # www.\n        )\n        [^\\s<>\"']+                              # domain and path (no space or some delimiters)\n    )\n    \"\"\",\n    re.VERBOSE | re.IGNORECASE,\n)\n\n\ndef linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n    taking the link as an argument and returning the extra text\n    e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n    or::\n\n    def extra_params_cb(url):\n        if url.startswith(\"http://example.com\"):\n            return 'class=\"internal\"'\n        else:\n            return 'class=\"external\" rel=\"nofollow\"'\n    linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n    this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n    linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n    \"mailto\"])``. It is very unsafe to include protocols such as\n    ``javascript``.\n    \"\"\"\n    # Convert bytes to str if needed\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\", errors=\"replace\")\n\n    def _shorten_url(url: str) -> str:\n        # Simple shortening: If url length > 50, truncate and add ellipsis\n        MAX_LEN = 50\n        if len(url) <= MAX_LEN:\n            return url\n        else:\n            # We keep the start and the end, replaced middle by \"\u2026\"\n            # Example: http://verylonglong...com/path\n            part_length = (MAX_LEN - 1) // 2\n            return url[:part_length] + \"\u2026\" + url[-part_length:]\n\n    def _build_link(url: str) -> str:\n        # Check protocol if required or if present\n        # Extract protocol\n        # If url starts with www., consider protocol = None\n        protocol_match = re.match(r\"^([\\w+-]+):\\/\\/\", url, re.IGNORECASE)\n        if protocol_match:\n            protocol = protocol_match.group(1).lower()\n        elif url.lower().startswith(\"www.\"):\n            protocol = None\n        else:\n            protocol = None\n\n        if require_protocol and not protocol:\n            # Don't linkify if protocol required but missing\n            return xhtml_escape(url)\n\n        if protocol and protocol not in permitted_protocols:\n            # Skip protocols not permitted\n            return xhtml_escape(url)\n\n        href = xhtml_escape(url)\n        if shorten:\n            display_text = xhtml_escape(_shorten_url(url))\n        else:\n            display_text = href\n\n        if callable(extra_params):\n            extra = extra_params(url)\n            if extra:\n                extra = \" \" + extra\n        else:\n            extra = (\" \" + extra_params) if extra_params else \"\"\n\n        return f'<a href=\"{href}\"{extra}>{display_text}</a>'\n\n    last_end = 0\n    result = []\n    for match in _URL_RE.finditer(text):\n        start, end = match.span()\n        url = match.group(0)\n\n        # Append text before link, escaped\n        before = text[last_end:start]\n        result.append(xhtml_escape(before))\n\n        linked = _build_link(url)\n        result.append(linked)\n\n        last_end = end\n\n    # Append the remaining part\n    result.append(xhtml_escape(text[last_end:]))\n\n    return \"\".join(result)", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 436, in <module>\n  File \"<string>\", line 435, in test_3\nAssertionError\n", "passed": false}
{"task_id": 270, "prediction_id": 3, "generation": "import re\nfrom typing import Union, Callable, List\n\nfrom tornado.escape import xhtml_escape\n\n\ndef linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n    taking the link as an argument and returning the extra text\n    e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n    or::\n\n    def extra_params_cb(url):\n    if url.startswith(\"http://example.com\"):\n    return 'class=\"internal\"'\n    else:\n    return 'class=\"external\" rel=\"nofollow\"'\n    linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n    this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n    linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n    \"mailto\"])``. It is very unsafe to include protocols such as\n    ``javascript``.\n    \"\"\"\n    # Regex to match URLs and www domains\n    # We'll use a regex inspired by the _URL_RE in file content with tweaks for linkify\n    # Explanation:\n    # - Capture group 1: full matched URL\n    # - Capture group 2: protocol if present (e.g. http)\n    # - The regex matches URLs starting with protocol:// or www.\n    regex = re.compile(\n        r\"\\b(\"\n        r\"(?:(?P<proto>[\\w+-]+):/{1,3})?\"  # protocol, optional\n        r\"(?:\"\n        r\"(?:(?:[^\\s&()<>'\\\"`]+)|\"  # URL-safe characters except control and punctuations that usually end URLs\n        r\"\\([^\\s&()<>'\\\"`]+\\))+\"  # balanced parentheses inside URL\n        r\")\"\n        r\")\",\n        re.IGNORECASE,\n    )\n\n    def is_permitted_protocol(proto: str) -> bool:\n        return proto.lower() in [p.lower() for p in permitted_protocols]\n\n    def shorten_url(url: str, max_length: int = 50) -> str:\n        # If url is shorter than max_length, return as is\n        if len(url) <= max_length:\n            return url\n        # Strategy: show first 30 chars, '...', and last 15 chars\n        head_len = 30\n        tail_len = 15\n        return f\"{url[:head_len]}...{url[-tail_len:]}\"\n\n    def replace(match: re.Match) -> str:\n        matched_url = match.group(0)\n        proto = match.group(\"proto\")\n\n        # Determine if this URL should be linkified\n        # - If require_protocol is True, proto must be present and in permitted_protocols.\n        # - If require_protocol is False:\n        #   * if proto present, check if permitted.\n        #   * if proto absent, accept if proto is None and the url looks like www.domain...\n        #     (to be safe, only linkify if it starts with www.)\n        if require_protocol:\n            if not proto or not is_permitted_protocol(proto):\n                # Do not linkify, just escape and return\n                return xhtml_escape(matched_url)\n        else:\n            if proto:\n                if not is_permitted_protocol(proto):\n                    return xhtml_escape(matched_url)\n            else:\n                # No protocol - only linkify if it starts with 'www.'\n                if not matched_url.lower().startswith(\"www.\"):\n                    return xhtml_escape(matched_url)\n\n        # Construct href\n        href = matched_url\n        if not proto:\n            # No protocol in matched_url, but linkify is allowed, so prepend \"http://\"\n            href = \"http://\" + matched_url\n\n        # Escape href attribute value\n        href_escaped = xhtml_escape(href)\n\n        # Determine displayed text\n        display_text = matched_url\n        if shorten:\n            display_text = shorten_url(display_text)\n        display_text_escaped = xhtml_escape(display_text)\n\n        # Determine extra params\n        if callable(extra_params):\n            try:\n                extra = extra_params(href)\n                if not isinstance(extra, str):\n                    extra = str(extra)\n            except Exception:\n                extra = \"\"\n        else:\n            extra = extra_params\n        # Add leading space if extra_params is not empty and does not already start with space\n        if extra and not extra.startswith(\" \"):\n            extra = \" \" + extra\n\n        return f'<a href=\"{href_escaped}\"{extra}>{display_text_escaped}</a>'\n\n    # The re.sub with function replacement to convert all links\n    # We run xhtml_escape on non-links after replacement, but safer is to escape the entire text first and then replace.\n    # But replacing escaped text would be hard. Instead, we do substitution directly on original.\n    # So we must be careful that replacement never leaks unescaped input (it doesn't because of xhtml_escape calls).\n\n    # Process with re.sub\n    # To avoid linkifying partial URLs inside already existing links etc, the original Tornado implementation uses a more complex \n    # parser, but here we use the given regex and trust the input.\n    return regex.sub(replace, to_unicode(text))\n\n\ndef to_unicode(value: Union[None, str, bytes]) -> Union[None, str]:\n    \"\"\"Converts a string argument to a unicode string.\n\n    This helper duplicates functionality from the file content.\n    \"\"\"\n    if value is None or isinstance(value, str):\n        return value\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\")\n    raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 524, in <module>\n  File \"<string>\", line 523, in test_36\nAssertionError\n", "passed": false}
{"task_id": 270, "prediction_id": 4, "generation": "import re\nfrom typing import Union, List, Callable\n\nfrom tornado.escape import xhtml_escape\n\n# Precompiled regex to find URLs with optional protocols and www prefix.\n_URL_RE = re.compile(\n    r'\\b((?:([\\w-]+):(/{1,3})|www[.])(?:(?:(?:[^\\s&()]|&amp;|&quot;)*(?:[^!\"#$%&\\'()*+,.:;<=>?@[\\\\]^`{|}~\\s]))|(?:\\((?:[^\\s&()]|&amp;|&quot;)*\\)))+)'\n)\n\n\ndef linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n    taking the link as an argument and returning the extra text\n    e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n    or::\n\n    def extra_params_cb(url):\n    if url.startswith(\"http://example.com\"):\n    return 'class=\"internal\"'\n    else:\n    return 'class=\"external\" rel=\"nofollow\"'\n    linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n    this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n    linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n    \"mailto\"])``. It is very unsafe to include protocols such as\n    ``javascript``.\n    \"\"\"\n    # Convert bytes to str for processing\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\")\n\n    def _shorten_url(url: str) -> str:\n        # Shorten long URLs for display\n        if len(url) > 50:\n            # Show first 25 chars ... last 10 chars\n            return url[:25] + \"...\" + url[-10:]\n        return url\n\n    def _get_extra_params(url: str) -> str:\n        if callable(extra_params):\n            return extra_params(url)\n        return extra_params\n\n    def _is_protocol_allowed(scheme: str) -> bool:\n        # None or empty scheme is not allowed if require_protocol=True\n        if require_protocol:\n            if not scheme:\n                return False\n        # Permit only if scheme is in permitted_protocols (case insensitive match)\n        return scheme.lower() in (p.lower() for p in permitted_protocols)\n\n    # This function will be called on every URL-like match by re.sub\n    def _replace_link(match: re.Match) -> str:\n        matched_text = match.group(0)\n        scheme = match.group(2) or \"\"  # Protocol, e.g. \"http\"\n        # If require_protocol and no scheme, skip linkification\n        if require_protocol and not scheme:\n            return xhtml_escape(matched_text)\n\n        # If scheme exists and not permitted, escape and don't linkify\n        if scheme and not _is_protocol_allowed(scheme):\n            return xhtml_escape(matched_text)\n\n        # If no scheme, but require_protocol is False\n        # We only allow linking if no scheme but scheme not required\n        if not scheme and require_protocol:\n            return xhtml_escape(matched_text)\n\n        # Prepare href: add \"http://\" if no scheme (e.g. \"www.foo.com\") and require_protocol is False\n        href = matched_text\n        if not scheme:\n            href = \"http://\" + matched_text\n\n        # Escape href attribute to be safe\n        href_escaped = xhtml_escape(href)\n\n        # Display text: escape. Shorten if requested\n        display_text = matched_text\n        if shorten:\n            display_text = _shorten_url(display_text)\n        display_text_escaped = xhtml_escape(display_text)\n\n        # Handle extra_params, which could be str or callable\n        extra = _get_extra_params(href) or \"\"\n        if extra and not extra.startswith(\" \"):\n            extra = \" \" + extra  # ensure at least a space before extra\n\n        return f'<a href=\"{href_escaped}\"{extra}>{display_text_escaped}</a>'\n\n    # Use regex sub to replace all URL-like substrings with links\n    result = _URL_RE.sub(_replace_link, text)\n\n    return result", "message": "failed: Traceback (most recent call last):\n  File \"/codegendata/human-eval/human_eval/execution.py\", line 57, in unsafe_execute\n    exec(check_program, exec_globals)\n  File \"<string>\", line 411, in <module>\n  File \"<string>\", line 408, in test_0\nAssertionError\n", "passed": false}
