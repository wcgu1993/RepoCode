{"function": "detect_encoding", "target_function_prompt": "def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n", "f1_score": 0.5, "recall": 1.0, "precision": 0.3333333333333333, "context_size": 3, "target_api_invocations": ["_get_normal_name"], "context": [{"fname": "read_or_stop", "evidence": "The target function needs to read lines safely from the readline iterator without raising exceptions, which this function provides."}, {"fname": "find_cookie", "evidence": "This function detects and validates encoding cookies in lines, directly aligning with the target function's key task of encoding detection and validation."}, {"fname": "_get_normal_name", "evidence": "Encoding name normalization is essential for the target function and this utility performs that task, likely called by detect_encoding or related helpers."}]}
{"function": "untokenize", "target_function_prompt": "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n", "f1_score": 0.0, "recall": 0.0, "precision": 0.0, "context_size": 15, "target_api_invocations": ["Untokenizer.untokenize"], "context": [{"fname": "__str__", "evidence": "The '__str__' method of Node returns source code text by concatenating its children's string representations, directly aiding untokenize in reconstructing source code."}, {"fname": "enumerate_reversed", "evidence": "enumerate_reversed provides reverse enumeration functionality which could be relevant for untokenize implementation when processing tokens backward to properly reconstruct whitespace or for correct token placement."}, {"fname": "_combinations", "evidence": "_combinations function may be useful for handling string prefix combinations or text normalization during token reconstruction."}, {"fname": "is_empty_lpar", "evidence": "Untokenize reconstructs source code from tokens and may need to identify special token cases such as empty left parentheses, so a utility like is_empty_lpar is likely invoked for handling such tokens."}, {"fname": "type_repr", "evidence": "The untokenize function reconstructs source code from tokens and may need the symbolic name of token types to assist in formatting or debugging, making invocation of type_repr plausible."}, {"fname": "TErr", "evidence": "The 'TErr' function produces standardized transformation errors; if untokenize encounters errors when reconstructing source code, it may invoke this function to create consistent error responses."}, {"fname": "_maybe_empty_lines", "evidence": "Untokenize needs to properly manage spacing in the output source; this method determines how many empty lines to insert before and after a line, making it likely invoked for formatting."}, {"fname": "_maybe_empty_lines_for_class_or_def", "evidence": "This method refines empty line calculations specifically for class and function definitions and is likely called (directly or indirectly) when untokenize formats such structures."}, {"fname": "__maybe_normalize_string_quotes", "evidence": "Untokenize may need to normalize string quotes in tokens representing string literals to produce consistent source code output, thus this normalization helper is potentially invoked."}, {"fname": "__normalize_f_string", "evidence": "The method normalizes f-strings without expressions, which may be part of untokenize’s string token processing to maintain consistent formatting."}, {"fname": "maybe_empty_lines", "evidence": "This public method provides the appropriate counts of surrounding empty lines for formatting and is a probable entry point for untokenize to determine spacing around lines."}, {"fname": "dont_increase_indentation", "evidence": "untokenize reconstructs source lines with correct indentation; normalization of indentation prefixes via this decorator could be used in the process of line splitting or manipulation."}, {"fname": "line_to_string", "evidence": "Converting a Line object into its string representation aligns with untokenize’s task to produce source code text from tokenized structures."}, {"fname": "__str__", "evidence": "producing string representations of line tokens with correct formatting strongly aligns with untokenize functionality."}, {"fname": "prefix", "evidence": "untokenize must handle token prefixes to accurately reconstruct whitespace and spacing, so it could invoke this method to propagate prefix data to token children."}]}
