{"function": "detect_encoding", "target_function_prompt": "def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n", "f1_score": 0.0, "recall": 0.0, "precision": 0.0, "context_size": 0, "target_api_invocations": ["_get_normal_name"], "context": []}
{"function": "untokenize", "target_function_prompt": "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n", "f1_score": 0.0, "recall": 0.0, "precision": 0.0, "context_size": 0, "target_api_invocations": ["Untokenizer.untokenize"], "context": []}
