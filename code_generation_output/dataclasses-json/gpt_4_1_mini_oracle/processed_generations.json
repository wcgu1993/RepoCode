{"task_id": 345, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n\n    decoded_names = []\n    for name in field_names:\n        if name in overrides:\n            letter_case = overrides[name].letter_case\n            if letter_case is not None:\n                decoded_names.append(letter_case(name))\n                continue\n        decoded_names.append(name)\n\n    return decoded_names", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    result = []\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            transformed = overrides[name].letter_case(name)\n            result.append(transformed)\n        else:\n            result.append(name)\n    return result", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decoded_mapping = {}\n    for name in field_names:\n        letter_case = overrides.get(name, FieldOverride(None, None, None, None, None)).letter_case\n        transformed_name = letter_case(name) if letter_case is not None else name\n        decoded_mapping[transformed_name] = name\n    return decoded_mapping", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if overrides is None:\n        return field_names\n    # Determine if any overrides have a letter_case callable to apply\n    new_field_names = []\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            new_name = overrides[name].letter_case(name)\n            new_field_names.append(new_name)\n        else:\n            new_field_names.append(name)\n    return new_field_names", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    # field_names: iterable of strings - original field names\n    # overrides: dict {field_name: FieldOverride} with optional letter_case function\n\n    # We want to produce a mapping from the potentially letter-case converted field names (keys\n    # after applying letter_case) back to the original field names.\n    # This mapping helps in decoding JSON keys back to dataclass attribute names.\n\n    result = {}\n    for original_key in field_names:\n        if original_key in overrides and overrides[original_key].letter_case is not None:\n            changed_key = overrides[original_key].letter_case(original_key)\n            result[changed_key] = original_key\n        else:\n            # no letter_case override - key maps to itself\n            result[original_key] = original_key\n    return result"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n\n    decoded_names = []\n    for name in field_names:\n        if name in overrides:\n            letter_case = overrides[name].letter_case\n            if letter_case is not None:\n                decoded_names.append(letter_case(name))\n                continue\n        decoded_names.append(name)\n\n    return decoded_names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    result = []\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            transformed = overrides[name].letter_case(name)\n            result.append(transformed)\n        else:\n            result.append(name)\n    return result\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decoded_mapping = {}\n    for name in field_names:\n        letter_case = overrides.get(name, FieldOverride(None, None, None, None, None)).letter_case\n        transformed_name = letter_case(name) if letter_case is not None else name\n        decoded_mapping[transformed_name] = name\n    return decoded_mapping\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if overrides is None:\n        return field_names\n    # Determine if any overrides have a letter_case callable to apply\n    new_field_names = []\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            new_name = overrides[name].letter_case(name)\n            new_field_names.append(new_name)\n        else:\n            new_field_names.append(name)\n    return new_field_names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    # field_names: iterable of strings - original field names\n    # overrides: dict {field_name: FieldOverride} with optional letter_case function\n\n    # We want to produce a mapping from the potentially letter-case converted field names (keys\n    # after applying letter_case) back to the original field names.\n    # This mapping helps in decoding JSON keys back to dataclass attribute names.\n\n    result = {}\n    for original_key in field_names:\n        if original_key in overrides and overrides[original_key].letter_case is not None:\n            changed_key = overrides[original_key].letter_case(original_key)\n            result[changed_key] = original_key\n        else:\n            # no letter_case override - key maps to itself\n            result[original_key] = original_key\n    return result\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n"]}
{"task_id": 346, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["from dataclasses import is_dataclass\nfrom typing import Iterable\n\ndef _decode_dict_keys(key_type, xs: Iterable[str], infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If the target key type is str, no decoding is needed\n    if key_type == str:\n        return xs\n\n    # If the key_type is a dataclass or a supported generic type,\n    # decode each key string accordingly.\n    # Use the _decode_items context function to decode keys.\n    # Import _decode_items here to avoid circular imports (assuming it belongs to the same package)\n    from dataclasses_json.core import _decode_items\n\n    decoded_keys = _decode_items(key_type, xs, infer_missing)\n    # _decode_items returns a generator, so convert to list\n    return list(decoded_keys)", "from dataclasses import is_dataclass\nfrom dataclasses_json.core import _decode_items\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If keys are already of the target type, return as is\n    # Otherwise decode each key appropriately\n\n    # str is the default type for JSON keys; for str key_type no conversion needed\n    if key_type is str:\n        return xs\n\n    # If key_type is a dataclass or another supported generic, decode keys one by one\n    # For keys, we only have a sequence of strings, so we decode each individually\n    # Use _decode_items to decode keys according to key_type\n    decoded_keys = list(_decode_items(key_type, xs, infer_missing))\n\n    return decoded_keys", "from dataclasses import is_dataclass\nfrom dataclasses_json.core import _decode_items\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # JSON keys are strings; decode them to the target key_type\n    # xs is an iterable of keys as strings\n    # If key_type is a dataclass, decode each key as a dataclass instance\n    if is_dataclass(key_type):\n        # keys are likely dicts serialized as JSON strings, but JSON mandates keys to be strings\n        # but here we just convert each string key to a dict/json then decode as dataclass\n        # However, keys in JSON are strings, so we assume these strings represent serialized keys,\n        # it's not common, but follow the pattern of delegating to _decode_items\n        # since _decode_items handles sequences of values\n        return _decode_items(key_type, xs, infer_missing)\n    # For supported generics or other types, decode keys appropriately\n    return _decode_items(key_type, xs, infer_missing)", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # Use _decode_items from context to decode keys according to key_type\n    # _decode_items returns a generator, so convert to list before returning\n    return list(_decode_items(key_type, xs, infer_missing))", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # Import here since the context function specifies it comes from dataclasses_json.core\n    from dataclasses_json.core import _decode_items\n\n    # Use _decode_items to decode keys according to the specified key_type\n    # xs is an iterable of keys (strs), we want to convert them to key_type\n    decoded_keys = _decode_items(key_type, xs, infer_missing)\n    return decoded_keys"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom dataclasses import is_dataclass\nfrom typing import Iterable\n\ndef _decode_dict_keys(key_type, xs: Iterable[str], infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If the target key type is str, no decoding is needed\n    if key_type == str:\n        return xs\n\n    # If the key_type is a dataclass or a supported generic type,\n    # decode each key string accordingly.\n    # Use the _decode_items context function to decode keys.\n    # Import _decode_items here to avoid circular imports (assuming it belongs to the same package)\n    from dataclasses_json.core import _decode_items\n\n    decoded_keys = _decode_items(key_type, xs, infer_missing)\n    # _decode_items returns a generator, so convert to list\n    return list(decoded_keys)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import _decode_items\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If keys are already of the target type, return as is\n    # Otherwise decode each key appropriately\n\n    # str is the default type for JSON keys; for str key_type no conversion needed\n    if key_type is str:\n        return xs\n\n    # If key_type is a dataclass or another supported generic, decode keys one by one\n    # For keys, we only have a sequence of strings, so we decode each individually\n    # Use _decode_items to decode keys according to key_type\n    decoded_keys = list(_decode_items(key_type, xs, infer_missing))\n\n    return decoded_keys\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import _decode_items\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # JSON keys are strings; decode them to the target key_type\n    # xs is an iterable of keys as strings\n    # If key_type is a dataclass, decode each key as a dataclass instance\n    if is_dataclass(key_type):\n        # keys are likely dicts serialized as JSON strings, but JSON mandates keys to be strings\n        # but here we just convert each string key to a dict/json then decode as dataclass\n        # However, keys in JSON are strings, so we assume these strings represent serialized keys,\n        # it's not common, but follow the pattern of delegating to _decode_items\n        # since _decode_items handles sequences of values\n        return _decode_items(key_type, xs, infer_missing)\n    # For supported generics or other types, decode keys appropriately\n    return _decode_items(key_type, xs, infer_missing)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # Use _decode_items from context to decode keys according to key_type\n    # _decode_items returns a generator, so convert to list before returning\n    return list(_decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # Import here since the context function specifies it comes from dataclasses_json.core\n    from dataclasses_json.core import _decode_items\n\n    # Use _decode_items to decode keys according to the specified key_type\n    # xs is an iterable of keys (strs), we want to convert them to key_type\n    decoded_keys = _decode_items(key_type, xs, infer_missing)\n    return decoded_keys\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n"]}
{"task_id": 347, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["from dataclasses import is_dataclass\nfrom typing import Any, Iterable\n\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic, _is_supported_generic\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # For iterables (list, tuple, set, etc.), decode each item accordingly.\n    # This assumes xs is an iterable of values to decode.\n    # We create an appropriate collection by collecting decoded items.\n    # Because the expected output type can be any iterable, but typically a list or other collection,\n    # we return a list here. The caller (_decode_generic) will reconstruct the appropriate container.\n    \n    # Defensive check: xs could be a generator or a more general iterable\n    # Return a generator to be memory efficient and let caller decide the container later.\n    def generator():\n        for v in xs:\n            # If type_arg is a dataclass type but the value is already that dataclass,\n            # return it directly to avoid redundant decoding.\n            if is_dataclass(type_arg) and is_dataclass(v):\n                yield v\n            else:\n                # Decode each item by recursively calling _decode_generic\n                yield _decode_generic(type_arg, v, infer_missing)\n\n    # Return a generator expression for lazy evaluation.\n    return generator()", "from typing import Any, Iterable\n\ndef _decode_items(type_arg, xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    res = []\n    # We iterate over the items to decode each according to type_arg.\n    for vs in xs:\n        # If the item is already an instance of the expected type_arg and is a dataclass,\n        # we can use it directly to avoid redundant decoding.\n        from dataclasses import is_dataclass\n        if is_dataclass(vs) and (isinstance(type_arg, type) and is_dataclass(type_arg)):\n            res.append(vs)\n        else:\n            # Otherwise we decode the item using _decode_generic which covers generics, enums, etc.\n            res.append(_decode_generic(type_arg, vs, infer_missing))\n    return res", "from dataclasses import is_dataclass\nfrom typing import Any, Iterable\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    items = []\n    for vs in xs:\n        # If type_arg is a dataclass or the item itself is a dataclass instance,\n        # decode using _decode_dataclass. This handles cases where type_arg is\n        # a TypeVar and vs contains an instance or dict corresponding to that dataclass.\n        if is_dataclass(type_arg) or is_dataclass(vs):\n            from dataclasses_json.core import _decode_dataclass\n            decoded = _decode_dataclass(type_arg, vs, infer_missing)\n            items.append(decoded)\n        else:\n            from dataclasses_json.core import _decode_generic\n            decoded = _decode_generic(type_arg, vs, infer_missing)\n            items.append(decoded)\n    return items", "from dataclasses import is_dataclass\nfrom typing import Any\n\nfrom dataclasses_json.core import (_decode_dataclass,\n                                  _decode_generic,\n                                  _is_supported_generic)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n\n    # If xs is None, just return None (common for optional fields)\n    if xs is None:\n        return xs\n\n    # If the type_arg is a dataclass, decode each item by _decode_dataclass\n    if is_dataclass(type_arg):\n        # xs should be iterable to decode each item\n        return type(xs)(_decode_dataclass(type_arg, item, infer_missing)\n                        for item in xs)\n\n    # If the type_arg is a supported generic, recurse with _decode_generic\n    if _is_supported_generic(type_arg):\n        return type(xs)(_decode_generic(type_arg, item, infer_missing) for item in xs)\n\n    # For other types, fallback to converting using the type_arg as a constructor if possible\n    # This handles common built-in collections like list, set, tuple etc.\n    try:\n        return type_arg(xs)\n    except Exception:\n        # If direct conversion fails, just return the original xs\n        return xs", "from dataclasses import is_dataclass\nfrom typing import Any, Iterable\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic, _is_supported_generic\n\n\ndef _decode_items(type_arg, xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    decoded_items = []\n    for vs in xs:\n        if is_dataclass(type_arg) or is_dataclass(vs):\n            decoded = _decode_dataclass(type_arg, vs, infer_missing)\n        elif _is_supported_generic(type_arg):\n            decoded = _decode_generic(type_arg, vs, infer_missing)\n        else:\n            decoded = vs\n        decoded_items.append(decoded)\n    return decoded_items"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom typing import Any, Iterable\n\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic, _is_supported_generic\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # For iterables (list, tuple, set, etc.), decode each item accordingly.\n    # This assumes xs is an iterable of values to decode.\n    # We create an appropriate collection by collecting decoded items.\n    # Because the expected output type can be any iterable, but typically a list or other collection,\n    # we return a list here. The caller (_decode_generic) will reconstruct the appropriate container.\n    \n    # Defensive check: xs could be a generator or a more general iterable\n    # Return a generator to be memory efficient and let caller decide the container later.\n    def generator():\n        for v in xs:\n            # If type_arg is a dataclass type but the value is already that dataclass,\n            # return it directly to avoid redundant decoding.\n            if is_dataclass(type_arg) and is_dataclass(v):\n                yield v\n            else:\n                # Decode each item by recursively calling _decode_generic\n                yield _decode_generic(type_arg, v, infer_missing)\n\n    # Return a generator expression for lazy evaluation.\n    return generator()\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom typing import Any, Iterable\n\ndef _decode_items(type_arg, xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    res = []\n    # We iterate over the items to decode each according to type_arg.\n    for vs in xs:\n        # If the item is already an instance of the expected type_arg and is a dataclass,\n        # we can use it directly to avoid redundant decoding.\n        from dataclasses import is_dataclass\n        if is_dataclass(vs) and (isinstance(type_arg, type) and is_dataclass(type_arg)):\n            res.append(vs)\n        else:\n            # Otherwise we decode the item using _decode_generic which covers generics, enums, etc.\n            res.append(_decode_generic(type_arg, vs, infer_missing))\n    return res\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom typing import Any, Iterable\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    items = []\n    for vs in xs:\n        # If type_arg is a dataclass or the item itself is a dataclass instance,\n        # decode using _decode_dataclass. This handles cases where type_arg is\n        # a TypeVar and vs contains an instance or dict corresponding to that dataclass.\n        if is_dataclass(type_arg) or is_dataclass(vs):\n            from dataclasses_json.core import _decode_dataclass\n            decoded = _decode_dataclass(type_arg, vs, infer_missing)\n            items.append(decoded)\n        else:\n            from dataclasses_json.core import _decode_generic\n            decoded = _decode_generic(type_arg, vs, infer_missing)\n            items.append(decoded)\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom typing import Any\n\nfrom dataclasses_json.core import (_decode_dataclass,\n                                  _decode_generic,\n                                  _is_supported_generic)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n\n    # If xs is None, just return None (common for optional fields)\n    if xs is None:\n        return xs\n\n    # If the type_arg is a dataclass, decode each item by _decode_dataclass\n    if is_dataclass(type_arg):\n        # xs should be iterable to decode each item\n        return type(xs)(_decode_dataclass(type_arg, item, infer_missing)\n                        for item in xs)\n\n    # If the type_arg is a supported generic, recurse with _decode_generic\n    if _is_supported_generic(type_arg):\n        return type(xs)(_decode_generic(type_arg, item, infer_missing) for item in xs)\n\n    # For other types, fallback to converting using the type_arg as a constructor if possible\n    # This handles common built-in collections like list, set, tuple etc.\n    try:\n        return type_arg(xs)\n    except Exception:\n        # If direct conversion fails, just return the original xs\n        return xs\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom typing import Any, Iterable\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic, _is_supported_generic\n\n\ndef _decode_items(type_arg, xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    decoded_items = []\n    for vs in xs:\n        if is_dataclass(type_arg) or is_dataclass(vs):\n            decoded = _decode_dataclass(type_arg, vs, infer_missing)\n        elif _is_supported_generic(type_arg):\n            decoded = _decode_generic(type_arg, vs, infer_missing)\n        else:\n            decoded = vs\n        decoded_items.append(decoded)\n    return decoded_items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n"]}
{"task_id": 348, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["from collections.abc import Mapping, Collection\nfrom dataclasses import fields, is_dataclass\nfrom dataclasses_json.core import (_encode_overrides, _user_overrides_or_exts)\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            overrides = _user_overrides_or_exts(type(obj_inner))\n            result = {}\n            for f in fields(obj_inner):\n                value = getattr(obj_inner, f.name)\n                result[f.name] = _convert(value)\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n        elif isinstance(obj_inner, Mapping):\n            # Convert mappings preserving keys and converting values recursively\n            return type(obj_inner)((_convert(k), _convert(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # Convert collections recursively preserving type\n            return type(obj_inner)(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _convert(obj)", "from collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import _encode_overrides, _user_overrides_or_exts\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, \"\n                        f\"got {type(obj)}\")\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            cls = type(obj_inner)\n            overrides = _user_overrides_or_exts(cls)\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init:\n                    continue\n\n                value = getattr(obj_inner, f.name)\n                # Recursively convert the field's value\n                value = _convert(value)\n                result[f.name] = value\n\n            # Apply overrides: encoding, exclusions, letter casing and optionally encode JSON\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n\n        elif isinstance(obj_inner, Mapping):\n            # recursively convert keys and values of mapping types\n            # keys are assumed to be str or immutable so just keep keys as is\n            return obj_inner.__class__(\n                (k, _convert(v)) for k, v in obj_inner.items()\n            )\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # convert each element in the collection\n            return obj_inner.__class__(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _convert(obj)", "import copy\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import (_encode_overrides,\n                                  _user_overrides_or_exts)\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(\"`_asdict` should be called with dataclass instances\")\n\n    def _iterate(obj):\n        if not is_dataclass(obj):\n            # If obj is a Collection or Mapping (but not string/bytes),\n            # recursively handle items; else return as is.\n            if isinstance(obj, Mapping):\n                return type(obj)((k, _iterate(v)) for k, v in obj.items())\n            elif isinstance(obj, Collection) and not isinstance(obj, (str, bytes)):\n                return type(obj)(_iterate(v) for v in obj)\n            else:\n                return obj\n\n        overrides = _user_overrides_or_exts(type(obj))\n        result = {}\n        for f in fields(obj):\n            if not f.init:\n                # Skip fields that are not included in __init__\n                continue\n            value = getattr(obj, f.name)\n            # Recursively convert value to dict\n            value = _iterate(value)\n            result[f.name] = value\n\n        # Apply field overrides (exclude, letter_case, encoder)\n        result = _encode_overrides(result, overrides, encode_json=encode_json)\n        return result\n\n    return _iterate(obj)", "from collections.abc import Collection, Mapping\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _user_overrides_or_exts, _encode_overrides\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    # If the object is a dataclass instance\n    cls = type(obj)\n    overrides = _user_overrides_or_exts(cls)\n\n    result = {}\n    for field in fields(obj):\n        if not field.init:\n            # Skip fields that are not intended for init (and thus not serialized)\n            continue\n        value = getattr(obj, field.name)\n\n        # Recursively handle nested dataclasses and collections\n        if is_dataclass(value):\n            # Nested dataclass, recurse\n            value = _asdict(value, encode_json=encode_json)\n        elif isinstance(value, Mapping):\n            # Mapping: convert keys and values appropriately\n            value = type(value)(\n                (k, _asdict(v, encode_json=encode_json) if is_dataclass(v) else v)\n                for k, v in value.items()\n            )\n        elif isinstance(value, Collection) and not isinstance(value, (str, bytes, bytearray)):\n            # Collection (but not string/bytes), convert items recursively if dataclass\n            value = type(value)(\n                _asdict(v, encode_json=encode_json) if is_dataclass(v) else v\n                for v in value\n            )\n\n        result[field.name] = value\n\n    # Apply overrides: encode, exclude, letter case transformation, and encode_json if requested\n    return _encode_overrides(result, overrides, encode_json=encode_json)", "import copy\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _encode_overrides, _user_overrides_or_exts\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got: {type(obj)}\")\n\n    def _call(obj_inner):\n        if is_dataclass(obj_inner):\n            overrides = _user_overrides_or_exts(type(obj_inner))\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init:\n                    # Skip fields that are not part of the constructor/init\n                    continue\n                value = getattr(obj_inner, f.name)\n                # Recursively convert dataclass fields\n                result[f.name] = _call(value)\n            # Apply encoding overrides (exclusion, letter_case, custom encoding)\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n        elif isinstance(obj_inner, tuple) and hasattr(obj_inner, '_fields'):\n            # Namedtuple - convert each element recursively and rebuild\n            return type(obj_inner)(*[_call(v) for v in obj_inner])\n        elif isinstance(obj_inner, Mapping):\n            # For Mappings, recursively convert values\n            return type(obj_inner)((_call(k), _call(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # For Collections (lists, sets, etc.) except str/bytes, recursively convert elements\n            return type(obj_inner)(_call(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _call(obj)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Mapping, Collection\nfrom dataclasses import fields, is_dataclass\nfrom dataclasses_json.core import (_encode_overrides, _user_overrides_or_exts)\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            overrides = _user_overrides_or_exts(type(obj_inner))\n            result = {}\n            for f in fields(obj_inner):\n                value = getattr(obj_inner, f.name)\n                result[f.name] = _convert(value)\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n        elif isinstance(obj_inner, Mapping):\n            # Convert mappings preserving keys and converting values recursively\n            return type(obj_inner)((_convert(k), _convert(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # Convert collections recursively preserving type\n            return type(obj_inner)(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import _encode_overrides, _user_overrides_or_exts\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, \"\n                        f\"got {type(obj)}\")\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            cls = type(obj_inner)\n            overrides = _user_overrides_or_exts(cls)\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init:\n                    continue\n\n                value = getattr(obj_inner, f.name)\n                # Recursively convert the field's value\n                value = _convert(value)\n                result[f.name] = value\n\n            # Apply overrides: encoding, exclusions, letter casing and optionally encode JSON\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n\n        elif isinstance(obj_inner, Mapping):\n            # recursively convert keys and values of mapping types\n            # keys are assumed to be str or immutable so just keep keys as is\n            return obj_inner.__class__(\n                (k, _convert(v)) for k, v in obj_inner.items()\n            )\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # convert each element in the collection\n            return obj_inner.__class__(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nimport copy\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import (_encode_overrides,\n                                  _user_overrides_or_exts)\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(\"`_asdict` should be called with dataclass instances\")\n\n    def _iterate(obj):\n        if not is_dataclass(obj):\n            # If obj is a Collection or Mapping (but not string/bytes),\n            # recursively handle items; else return as is.\n            if isinstance(obj, Mapping):\n                return type(obj)((k, _iterate(v)) for k, v in obj.items())\n            elif isinstance(obj, Collection) and not isinstance(obj, (str, bytes)):\n                return type(obj)(_iterate(v) for v in obj)\n            else:\n                return obj\n\n        overrides = _user_overrides_or_exts(type(obj))\n        result = {}\n        for f in fields(obj):\n            if not f.init:\n                # Skip fields that are not included in __init__\n                continue\n            value = getattr(obj, f.name)\n            # Recursively convert value to dict\n            value = _iterate(value)\n            result[f.name] = value\n\n        # Apply field overrides (exclude, letter_case, encoder)\n        result = _encode_overrides(result, overrides, encode_json=encode_json)\n        return result\n\n    return _iterate(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _user_overrides_or_exts, _encode_overrides\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    # If the object is a dataclass instance\n    cls = type(obj)\n    overrides = _user_overrides_or_exts(cls)\n\n    result = {}\n    for field in fields(obj):\n        if not field.init:\n            # Skip fields that are not intended for init (and thus not serialized)\n            continue\n        value = getattr(obj, field.name)\n\n        # Recursively handle nested dataclasses and collections\n        if is_dataclass(value):\n            # Nested dataclass, recurse\n            value = _asdict(value, encode_json=encode_json)\n        elif isinstance(value, Mapping):\n            # Mapping: convert keys and values appropriately\n            value = type(value)(\n                (k, _asdict(v, encode_json=encode_json) if is_dataclass(v) else v)\n                for k, v in value.items()\n            )\n        elif isinstance(value, Collection) and not isinstance(value, (str, bytes, bytearray)):\n            # Collection (but not string/bytes), convert items recursively if dataclass\n            value = type(value)(\n                _asdict(v, encode_json=encode_json) if is_dataclass(v) else v\n                for v in value\n            )\n\n        result[field.name] = value\n\n    # Apply overrides: encode, exclude, letter case transformation, and encode_json if requested\n    return _encode_overrides(result, overrides, encode_json=encode_json)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nimport copy\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _encode_overrides, _user_overrides_or_exts\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got: {type(obj)}\")\n\n    def _call(obj_inner):\n        if is_dataclass(obj_inner):\n            overrides = _user_overrides_or_exts(type(obj_inner))\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init:\n                    # Skip fields that are not part of the constructor/init\n                    continue\n                value = getattr(obj_inner, f.name)\n                # Recursively convert dataclass fields\n                result[f.name] = _call(value)\n            # Apply encoding overrides (exclusion, letter_case, custom encoding)\n            return _encode_overrides(result, overrides, encode_json=encode_json)\n        elif isinstance(obj_inner, tuple) and hasattr(obj_inner, '_fields'):\n            # Namedtuple - convert each element recursively and rebuild\n            return type(obj_inner)(*[_call(v) for v in obj_inner])\n        elif isinstance(obj_inner, Mapping):\n            # For Mappings, recursively convert values\n            return type(obj_inner)((_call(k), _call(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # For Collections (lists, sets, etc.) except str/bytes, recursively convert elements\n            return type(obj_inner)(_call(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    return _call(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n"]}
{"task_id": 349, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.api", "predictions": ["from typing import Optional, TypeVar, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json import LetterCase\n\nA = TypeVar('A')\n\n\ndef dataclass_json(_cls: Optional[A] = None,\n                   *,\n                   letter_case: Optional[Union[str, LetterCase]] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls: A) -> A:\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "from typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json import LetterCase\nfrom dataclasses_json.api import _process_class\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # decorator called with parameters\n        return wrap\n    else:\n        # decorator called without parameters\n        return wrap(_cls)", "from typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # Decorator called with arguments.\n        return wrap\n    # Decorator called without arguments.\n    return wrap(_cls)", "from typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # Called as @dataclass_json(), with optional params\n        return wrap\n    else:\n        # Called as @dataclass_json without params\n        return wrap(_cls)", "from typing import Optional, Union\n\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # decorator called with parentheses\n        return wrap\n    else:\n        # decorator called without parentheses\n        return wrap(_cls)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, TypeVar, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json import LetterCase\n\nA = TypeVar('A')\n\n\ndef dataclass_json(_cls: Optional[A] = None,\n                   *,\n                   letter_case: Optional[Union[str, LetterCase]] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls: A) -> A:\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json import LetterCase\nfrom dataclasses_json.api import _process_class\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # decorator called with parameters\n        return wrap\n    else:\n        # decorator called without parameters\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # Decorator called with arguments.\n        return wrap\n    # Decorator called without arguments.\n    return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        # Called as @dataclass_json(), with optional params\n        return wrap\n    else:\n        # Called as @dataclass_json without params\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\n\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json import LetterCase\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # decorator called with parentheses\n        return wrap\n    else:\n        # decorator called without parentheses\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n"]}
{"task_id": 350, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.utils", "predictions": ["def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # The goal is to extract the \"origin\" type of a typing construct (like List[int] -> List)\n    # but Python 3.6 and 3.7+ deal differently with the typing module internals.\n    import sys\n\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__extra__\n        except AttributeError:\n            try:\n                origin = type_.__origin__\n            except AttributeError:\n                origin = type_\n            else:\n                origin = type_ if origin is None else origin\n        else:\n            try:\n                origin = type_.__origin__ if origin is None else origin\n            except AttributeError:\n                origin = type_\n    else:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = type_\n\n    return origin", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # Python 3.7+ typing types have __origin__ attribute for generic types.\n    # Python 3.6's typing module had some differences, so we need to accommodate.\n    try:\n        # Attempt to get __origin__ directly, works for 3.7+\n        return type_.__origin__\n    except AttributeError:\n        # For 3.6, try __extra__ first (typing internal attribute)\n        try:\n            return type_.__extra__\n        except AttributeError:\n            # As a fallback, return None or the type itself\n            return None", "import sys\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, '__origin__', None)\n    return origin", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # For Python 3.6, typing internals are less consistent; try __extra__, then __origin__, else fallback\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__extra__\n        except AttributeError:\n            try:\n                origin = type_.__origin__\n            except AttributeError:\n                origin = type_\n            else:\n                origin = type_ if origin is None else origin\n        else:\n            try:\n                origin = type_.__origin__ if origin is None else origin\n            except AttributeError:\n                origin = type_\n    else:\n        # For Python 3.7 and above, __origin__ is standardized\n        origin = type_.__origin__\n    return origin", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, '__origin__', None)\n    return origin"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # The goal is to extract the \"origin\" type of a typing construct (like List[int] -> List)\n    # but Python 3.6 and 3.7+ deal differently with the typing module internals.\n    import sys\n\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__extra__\n        except AttributeError:\n            try:\n                origin = type_.__origin__\n            except AttributeError:\n                origin = type_\n            else:\n                origin = type_ if origin is None else origin\n        else:\n            try:\n                origin = type_.__origin__ if origin is None else origin\n            except AttributeError:\n                origin = type_\n    else:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = type_\n\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # Python 3.7+ typing types have __origin__ attribute for generic types.\n    # Python 3.6's typing module had some differences, so we need to accommodate.\n    try:\n        # Attempt to get __origin__ directly, works for 3.7+\n        return type_.__origin__\n    except AttributeError:\n        # For 3.6, try __extra__ first (typing internal attribute)\n        try:\n            return type_.__extra__\n        except AttributeError:\n            # As a fallback, return None or the type itself\n            return None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\nimport sys\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, '__origin__', None)\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # For Python 3.6, typing internals are less consistent; try __extra__, then __origin__, else fallback\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__extra__\n        except AttributeError:\n            try:\n                origin = type_.__origin__\n            except AttributeError:\n                origin = type_\n            else:\n                origin = type_ if origin is None else origin\n        else:\n            try:\n                origin = type_.__origin__ if origin is None else origin\n            except AttributeError:\n                origin = type_\n    else:\n        # For Python 3.7 and above, __origin__ is standardized\n        origin = type_.__origin__\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, '__origin__', None)\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n"]}
