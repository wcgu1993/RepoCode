{"task_id": 345, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for field_name in field_names:\n        fo = overrides.get(field_name)\n        if fo is not None and fo.letter_case is not None:\n            encoded_name = fo.letter_case(field_name)\n            decode_names[encoded_name] = field_name\n        else:\n            decode_names[field_name] = field_name\n    return decode_names", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_map = {}\n    for field_name in field_names:\n        if field_name in overrides and overrides[field_name].letter_case is not None:\n            encoded_name = overrides[field_name].letter_case(field_name)\n        else:\n            encoded_name = field_name\n        decode_map[encoded_name] = field_name\n    return decode_map", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            # Decode name using the letter_case function provided in overrides\n            # We use the letter_case function on the original field name to get the expected key\n            # that should be mapped back to the original field name.\n            decoded_name = overrides[name].letter_case(name)\n            decode_names[decoded_name] = name\n    return decode_names", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for field_name in field_names:\n        override = overrides.get(field_name)\n        if override is not None and override.letter_case is not None:\n            encoded_name = override.letter_case(field_name)\n        else:\n            encoded_name = field_name\n        decode_names[encoded_name] = field_name\n    return decode_names", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    inverse = {}\n    for field_name in field_names:\n        override = overrides.get(field_name)\n        if override is None or override.letter_case is None:\n            encoded_name = field_name\n        else:\n            encoded_name = override.letter_case(field_name)\n        inverse[encoded_name] = field_name\n    return inverse"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for field_name in field_names:\n        fo = overrides.get(field_name)\n        if fo is not None and fo.letter_case is not None:\n            encoded_name = fo.letter_case(field_name)\n            decode_names[encoded_name] = field_name\n        else:\n            decode_names[field_name] = field_name\n    return decode_names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_map = {}\n    for field_name in field_names:\n        if field_name in overrides and overrides[field_name].letter_case is not None:\n            encoded_name = overrides[field_name].letter_case(field_name)\n        else:\n            encoded_name = field_name\n        decode_map[encoded_name] = field_name\n    return decode_map\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for name in field_names:\n        if name in overrides and overrides[name].letter_case is not None:\n            # Decode name using the letter_case function provided in overrides\n            # We use the letter_case function on the original field name to get the expected key\n            # that should be mapped back to the original field name.\n            decoded_name = overrides[name].letter_case(name)\n            decode_names[decoded_name] = name\n    return decode_names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    decode_names = {}\n    for field_name in field_names:\n        override = overrides.get(field_name)\n        if override is not None and override.letter_case is not None:\n            encoded_name = override.letter_case(field_name)\n        else:\n            encoded_name = field_name\n        decode_names[encoded_name] = field_name\n    return decode_names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    inverse = {}\n    for field_name in field_names:\n        override = overrides.get(field_name)\n        if override is None or override.letter_case is None:\n            encoded_name = field_name\n        else:\n            encoded_name = override.letter_case(field_name)\n        inverse[encoded_name] = field_name\n    return inverse\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n"]}
{"task_id": 346, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["from dataclasses_json.utils import _is_new_type, _issubclass_safe\nfrom dataclasses_json.core import _support_extended_types, _decode_generic, _decode_items\nfrom typing import Any, Iterator\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing) -> Iterator[Any]:\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    for x in xs:\n        # If key_type is a new-type, unwrap to its supertype\n        while _is_new_type(key_type):\n            key_type = key_type.__supertype__\n\n        # If the key is None and the target type is not optional, yield None with a warning?\n        # But since keys cannot be None in JSON objects, likely no keys would be None.\n        # So we skip that complexity here.\n\n        if _issubclass_safe(key_type, str):\n            # Keys from JSON are always str, so if the target type is str, no conversion needed.\n            yield x\n        elif _issubclass_safe(key_type, (int, float, bool)):\n            # Attempt to convert str key to int/float/bool\n            # bool keys are rare but theoretically possible if the user expects key_type=bool\n            if _issubclass_safe(key_type, bool):\n                # Convert common boolean string representations\n                lower = x.lower()\n                if lower in ('true', '1'):\n                    yield True\n                elif lower in ('false', '0'):\n                    yield False\n                else:\n                    # Fallback: Python bool conversion of non-empty string is True,\n                    # but this is not intuitive, so raise or yield as is?\n                    # Here, yield as is and warn or just yield as bool(x)\n                    yield bool(x)\n            else:\n                try:\n                    yield key_type(x)\n                except Exception:\n                    # If conversion fails, yield original\n                    yield x\n        elif _issubclass_safe(key_type, (Enum,)):\n            # Enums: try to instantiate with the string key\n            try:\n                yield key_type(x)\n            except Exception:\n                yield x\n        elif _issubclass_safe(key_type, (tuple, list, set)):\n            # Complex generic types for keys are unusual, but try to decode generically\n            # Use _decode_generic for generic handling when appropriate\n            yield _decode_generic(key_type, x, infer_missing)\n        else:\n            # For other types, try generic decoding\n            yield _support_extended_types(key_type, x)", "from typing import Any, Iterable, Type, List\n\ndef _decode_dict_keys(key_type: Type[Any], xs: Iterable[str], infer_missing: bool) -> List[Any]:\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # The input xs represents keys from a JSON object, which are always strings.\n    # We should convert each key string to the target key_type.\n    # For cases where key_type is a \"new type\", unwrap (__supertype__ chain).\n    # Then for each key string:\n    #  - If key_type is str, return as is\n    #  - Else if key_type is a supported generic, decode generically\n    #  - Else if key_type is an extended type (datetime, UUID, Decimal), convert using _support_extended_types\n    #  - Else try to construct key_type(key_string)\n    # Return the transformed keys as a list.\n\n    # Importing context functions used here:\n    from dataclasses_json.utils import _is_new_type\n    from dataclasses_json.core import _decode_generic\n    from dataclasses_json.core import _support_extended_types\n\n    # Unwrap new types to their underlying type\n    original_key_type = key_type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    decoded_keys = []\n    for key in xs:\n        if key_type is str:\n            # No decoding needed\n            decoded_key = key\n        elif _decode_generic is not None and (_issubclass_safe := False):\n            # Use _decode_generic if key_type is supported generic (but we don't have _issubclass_safe here)\n            # We cannot import _issubclass_safe from utils or core per instruction,\n            # but since we have access to it in file content and it is imported in the file,\n            # we can replicate usage using try-except here for safe subclass checking.\n\n            # Actually, to keep consistent with context, importing _issubclass_safe and using it.\n            from dataclasses_json.utils import _issubclass_safe\n\n            if _issubclass_safe(key_type, str):\n                decoded_key = key\n            elif _issubclass_safe(key_type, type(None)):\n                # Possibly Optional None, just use key as is\n                decoded_key = key\n            elif _issubclass_safe(key_type, Iterable) and key_type != str:\n                # Actually keys like dict keys should not be a collection, skip decoding generically here.\n                # Just try conversion to the type constructor.\n                decoded_key = key_type(key)\n            elif _is_new_type(key_type):\n                # We should not get here because we unwrap new_type above.\n                decoded_key = key_type(key)\n            elif key_type in (int, float, bool):\n                # Convert basic types directly\n                decoded_key = key_type(key)\n            else:\n                # Try to decode as a generic\n                try:\n                    decoded_key = _decode_generic(key_type, key, infer_missing)\n                except Exception:\n                    # fallback to direct constructor\n                    decoded_key = key_type(key)\n        else:\n            # Fallback: attempt to convert using type constructor or _support_extended_types\n            try:\n                # _support_extended_types handles datetime, Decimal, UUID etc.\n                decoded_key = _support_extended_types(key_type, key)\n                # If _support_extended_types didn't convert (returns key), try constructor\n                if decoded_key is key:\n                    decoded_key = key_type(key)\n            except Exception:\n                decoded_key = key  # fallback: leave as string in case of failure\n\n        decoded_keys.append(decoded_key)\n\n    return decoded_keys", "from typing import Any, Iterable, Type\n\ndef _decode_dict_keys(key_type: Type[Any], xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # For each key in xs, decode it to the given key_type.\n    # Handle new types by unwrapping their __supertype__.\n    # If the key_type is a supported generic (collection, optional, union, enum),\n    # perform generic decoding.\n    # Otherwise, attempt to convert extended types (datetime, Decimal, UUID).\n    # If the key is of the desired type already, return as is.\n\n    # Import dependencies locally to avoid global import conflicts\n    from dataclasses_json.utils import _is_new_type\n    from dataclasses_json.utils import _issubclass_safe\n    from dataclasses_json.core import _is_supported_generic, _decode_generic\n    from dataclasses_json.core import _support_extended_types\n\n    # Unwrap newtypes to their supertype\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # If key_type is a generic (like Optional, Union, Enum, Collection)\n    if _is_supported_generic(key_type):\n        return (_decode_generic(key_type, x, infer_missing) for x in xs)\n\n    # Otherwise, apply extended types conversion or leave as is\n    return (_support_extended_types(key_type, x) if not isinstance(x, key_type) else x\n            for x in xs)", "from typing import Iterable, Type\n\ndef _decode_dict_keys(key_type: Type, xs: Iterable, infer_missing: bool):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If key_type is already str, no conversion needed; just return xs as is\n    if key_type is str:\n        return xs\n\n    # Handle new types by unwrapping to the base supertype\n    from dataclasses_json.utils import _is_new_type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # For extended types (datetime, Decimal, UUID), convert accordingly\n    from dataclasses_json.core import _support_extended_types\n    # For supported generics (like collections, enums, optionals), delegate to _decode_generic\n    from dataclasses_json.core import _decode_generic\n    from dataclasses_json.utils import _issubclass_safe\n    from enum import Enum\n\n    if key_type is Any:\n        # If Any, no transformation needed\n        return xs\n\n    # Try to handle enum keys specially\n    if _issubclass_safe(key_type, Enum):\n        return (key_type(k) for k in xs)\n\n    # For extended types (datetime, Decimal, UUID) and primitives, attempt conversion\n    def convert_key(k):\n        # None keys are invalid for JSON objects, but tolerate if infer_missing is True\n        if k is None:\n            return None if infer_missing else k\n\n        # When key_type is generic (like Optional, Union), use _decode_generic\n        # But _decode_generic expects a value (key) and type (key_type)\n        # so we can use it to decode each key.\n        try:\n            decoded = _decode_generic(key_type, k, infer_missing)\n        except Exception:\n            # fallback to raw key if decoding fails\n            decoded = k\n\n        # If decode_generic returns a value different from k, return it\n        if decoded is not None:\n            return decoded\n\n        # Otherwise try extended types support fallback\n        return _support_extended_types(key_type, k)\n\n    return (convert_key(k) for k in xs)", "from dataclasses_json.utils import _is_new_type, _issubclass_safe\nfrom dataclasses_json.core import _support_extended_types, _decode_generic\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If the key_type is a new_type, unwrap it to get the core type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # If key_type is str, keys are already decoded correctly\n    if key_type is str:\n        return xs\n\n    # If the key_type is a supported generic type (e.g., collections, optionals, enums),\n    # decode each key using _decode_generic\n    if _issubclass_safe(key_type, str):\n        # Covers subclasses of str if any, just return keys as-is\n        return xs\n\n    # For enums and other supported generics decode via _decode_generic per key\n    if _issubclass_safe(key_type, type) or _issubclass_safe(type(xs), (list, tuple)):\n        # Defensive fallback, just try decode_generic anyway\n        return (_decode_generic(key_type, x, infer_missing) for x in xs) if xs else xs\n\n    # Default fallback: decode every key using _support_extended_types\n    # This covers datetime, Decimal, UUID etc.\n    decoded_keys = []\n    for x in xs:\n        try:\n            # Try decoding as extended type\n            k = _support_extended_types(key_type, x)\n        except Exception:\n            # If error, fallback to identity (original str key)\n            k = x\n        decoded_keys.append(k)\n    return decoded_keys"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom dataclasses_json.utils import _is_new_type, _issubclass_safe\nfrom dataclasses_json.core import _support_extended_types, _decode_generic, _decode_items\nfrom typing import Any, Iterator\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing) -> Iterator[Any]:\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    for x in xs:\n        # If key_type is a new-type, unwrap to its supertype\n        while _is_new_type(key_type):\n            key_type = key_type.__supertype__\n\n        # If the key is None and the target type is not optional, yield None with a warning?\n        # But since keys cannot be None in JSON objects, likely no keys would be None.\n        # So we skip that complexity here.\n\n        if _issubclass_safe(key_type, str):\n            # Keys from JSON are always str, so if the target type is str, no conversion needed.\n            yield x\n        elif _issubclass_safe(key_type, (int, float, bool)):\n            # Attempt to convert str key to int/float/bool\n            # bool keys are rare but theoretically possible if the user expects key_type=bool\n            if _issubclass_safe(key_type, bool):\n                # Convert common boolean string representations\n                lower = x.lower()\n                if lower in ('true', '1'):\n                    yield True\n                elif lower in ('false', '0'):\n                    yield False\n                else:\n                    # Fallback: Python bool conversion of non-empty string is True,\n                    # but this is not intuitive, so raise or yield as is?\n                    # Here, yield as is and warn or just yield as bool(x)\n                    yield bool(x)\n            else:\n                try:\n                    yield key_type(x)\n                except Exception:\n                    # If conversion fails, yield original\n                    yield x\n        elif _issubclass_safe(key_type, (Enum,)):\n            # Enums: try to instantiate with the string key\n            try:\n                yield key_type(x)\n            except Exception:\n                yield x\n        elif _issubclass_safe(key_type, (tuple, list, set)):\n            # Complex generic types for keys are unusual, but try to decode generically\n            # Use _decode_generic for generic handling when appropriate\n            yield _decode_generic(key_type, x, infer_missing)\n        else:\n            # For other types, try generic decoding\n            yield _support_extended_types(key_type, x)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom typing import Any, Iterable, Type, List\n\ndef _decode_dict_keys(key_type: Type[Any], xs: Iterable[str], infer_missing: bool) -> List[Any]:\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # The input xs represents keys from a JSON object, which are always strings.\n    # We should convert each key string to the target key_type.\n    # For cases where key_type is a \"new type\", unwrap (__supertype__ chain).\n    # Then for each key string:\n    #  - If key_type is str, return as is\n    #  - Else if key_type is a supported generic, decode generically\n    #  - Else if key_type is an extended type (datetime, UUID, Decimal), convert using _support_extended_types\n    #  - Else try to construct key_type(key_string)\n    # Return the transformed keys as a list.\n\n    # Importing context functions used here:\n    from dataclasses_json.utils import _is_new_type\n    from dataclasses_json.core import _decode_generic\n    from dataclasses_json.core import _support_extended_types\n\n    # Unwrap new types to their underlying type\n    original_key_type = key_type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    decoded_keys = []\n    for key in xs:\n        if key_type is str:\n            # No decoding needed\n            decoded_key = key\n        elif _decode_generic is not None and (_issubclass_safe := False):\n            # Use _decode_generic if key_type is supported generic (but we don't have _issubclass_safe here)\n            # We cannot import _issubclass_safe from utils or core per instruction,\n            # but since we have access to it in file content and it is imported in the file,\n            # we can replicate usage using try-except here for safe subclass checking.\n\n            # Actually, to keep consistent with context, importing _issubclass_safe and using it.\n            from dataclasses_json.utils import _issubclass_safe\n\n            if _issubclass_safe(key_type, str):\n                decoded_key = key\n            elif _issubclass_safe(key_type, type(None)):\n                # Possibly Optional None, just use key as is\n                decoded_key = key\n            elif _issubclass_safe(key_type, Iterable) and key_type != str:\n                # Actually keys like dict keys should not be a collection, skip decoding generically here.\n                # Just try conversion to the type constructor.\n                decoded_key = key_type(key)\n            elif _is_new_type(key_type):\n                # We should not get here because we unwrap new_type above.\n                decoded_key = key_type(key)\n            elif key_type in (int, float, bool):\n                # Convert basic types directly\n                decoded_key = key_type(key)\n            else:\n                # Try to decode as a generic\n                try:\n                    decoded_key = _decode_generic(key_type, key, infer_missing)\n                except Exception:\n                    # fallback to direct constructor\n                    decoded_key = key_type(key)\n        else:\n            # Fallback: attempt to convert using type constructor or _support_extended_types\n            try:\n                # _support_extended_types handles datetime, Decimal, UUID etc.\n                decoded_key = _support_extended_types(key_type, key)\n                # If _support_extended_types didn't convert (returns key), try constructor\n                if decoded_key is key:\n                    decoded_key = key_type(key)\n            except Exception:\n                decoded_key = key  # fallback: leave as string in case of failure\n\n        decoded_keys.append(decoded_key)\n\n    return decoded_keys\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom typing import Any, Iterable, Type\n\ndef _decode_dict_keys(key_type: Type[Any], xs: Iterable[Any], infer_missing: bool):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # For each key in xs, decode it to the given key_type.\n    # Handle new types by unwrapping their __supertype__.\n    # If the key_type is a supported generic (collection, optional, union, enum),\n    # perform generic decoding.\n    # Otherwise, attempt to convert extended types (datetime, Decimal, UUID).\n    # If the key is of the desired type already, return as is.\n\n    # Import dependencies locally to avoid global import conflicts\n    from dataclasses_json.utils import _is_new_type\n    from dataclasses_json.utils import _issubclass_safe\n    from dataclasses_json.core import _is_supported_generic, _decode_generic\n    from dataclasses_json.core import _support_extended_types\n\n    # Unwrap newtypes to their supertype\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # If key_type is a generic (like Optional, Union, Enum, Collection)\n    if _is_supported_generic(key_type):\n        return (_decode_generic(key_type, x, infer_missing) for x in xs)\n\n    # Otherwise, apply extended types conversion or leave as is\n    return (_support_extended_types(key_type, x) if not isinstance(x, key_type) else x\n            for x in xs)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom typing import Iterable, Type\n\ndef _decode_dict_keys(key_type: Type, xs: Iterable, infer_missing: bool):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If key_type is already str, no conversion needed; just return xs as is\n    if key_type is str:\n        return xs\n\n    # Handle new types by unwrapping to the base supertype\n    from dataclasses_json.utils import _is_new_type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # For extended types (datetime, Decimal, UUID), convert accordingly\n    from dataclasses_json.core import _support_extended_types\n    # For supported generics (like collections, enums, optionals), delegate to _decode_generic\n    from dataclasses_json.core import _decode_generic\n    from dataclasses_json.utils import _issubclass_safe\n    from enum import Enum\n\n    if key_type is Any:\n        # If Any, no transformation needed\n        return xs\n\n    # Try to handle enum keys specially\n    if _issubclass_safe(key_type, Enum):\n        return (key_type(k) for k in xs)\n\n    # For extended types (datetime, Decimal, UUID) and primitives, attempt conversion\n    def convert_key(k):\n        # None keys are invalid for JSON objects, but tolerate if infer_missing is True\n        if k is None:\n            return None if infer_missing else k\n\n        # When key_type is generic (like Optional, Union), use _decode_generic\n        # But _decode_generic expects a value (key) and type (key_type)\n        # so we can use it to decode each key.\n        try:\n            decoded = _decode_generic(key_type, k, infer_missing)\n        except Exception:\n            # fallback to raw key if decoding fails\n            decoded = k\n\n        # If decode_generic returns a value different from k, return it\n        if decoded is not None:\n            return decoded\n\n        # Otherwise try extended types support fallback\n        return _support_extended_types(key_type, k)\n\n    return (convert_key(k) for k in xs)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\nfrom dataclasses_json.utils import _is_new_type, _issubclass_safe\nfrom dataclasses_json.core import _support_extended_types, _decode_generic\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # If the key_type is a new_type, unwrap it to get the core type\n    while _is_new_type(key_type):\n        key_type = key_type.__supertype__\n\n    # If key_type is str, keys are already decoded correctly\n    if key_type is str:\n        return xs\n\n    # If the key_type is a supported generic type (e.g., collections, optionals, enums),\n    # decode each key using _decode_generic\n    if _issubclass_safe(key_type, str):\n        # Covers subclasses of str if any, just return keys as-is\n        return xs\n\n    # For enums and other supported generics decode via _decode_generic per key\n    if _issubclass_safe(key_type, type) or _issubclass_safe(type(xs), (list, tuple)):\n        # Defensive fallback, just try decode_generic anyway\n        return (_decode_generic(key_type, x, infer_missing) for x in xs) if xs else xs\n\n    # Default fallback: decode every key using _support_extended_types\n    # This covers datetime, Decimal, UUID etc.\n    decoded_keys = []\n    for x in xs:\n        try:\n            # Try decoding as extended type\n            k = _support_extended_types(key_type, x)\n        except Exception:\n            # If error, fallback to identity (original str key)\n            k = x\n        decoded_keys.append(k)\n    return decoded_keys\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n"]}
{"task_id": 347, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["from dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n    _is_supported_generic,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n)\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    # Defensive: if xs is None, no data to decode, return it immediately.\n    if xs is None:\n        return xs\n\n    # Prepare the result container matching the input type (e.g., list, tuple, set)\n    # We will accumulate decoded items here.\n    try:\n        res_container = type(xs)()\n    except Exception:\n        # If xs is not a proper collection instance or type() construction fails,\n        # fallback to using list\n        res_container = []\n\n    # Iterate over each item to decode it individually\n    # decode each element according to the type_arg\n    items = []\n    for vs in xs:\n        # If type_arg is a new type (has __supertype__), unwrap it\n        inner_type = type_arg\n        while _is_new_type(inner_type):\n            inner_type = inner_type.__supertype__\n\n        # If the element itself is a dataclass instance, decode it as a dataclass\n        # even if type_arg is a generic, this handles cases where the input already contains dataclasses.\n        if is_dataclass(inner_type) or is_dataclass(vs):\n            decoded = _decode_dataclass(inner_type, vs, infer_missing)\n        elif _is_supported_generic(inner_type):\n            decoded = _decode_generic(inner_type, vs, infer_missing)\n        else:\n            decoded = _support_extended_types(inner_type, vs)\n\n        items.append(decoded)\n\n    # Reconstruct the collection of decoded items with the same type as xs,\n    # e.g. list, tuple, set, etc.\n    # Some collections like tuple might need special construction\n    origin_type = type(xs)\n    if origin_type is tuple:\n        # tuple is immutable, use tuple constructor\n        return tuple(items)\n    # For immutable sets (frozenset), reconstruct accordingly\n    if origin_type.__name__ == 'frozenset':\n        return frozenset(items)\n    # For generators or other special collection types fallback to list\n    try:\n        return origin_type(items)\n    except Exception:\n        return items", "from copy import deepcopy\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n    _issubclass_safe,\n)\nfrom enum import Enum\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # We want to \"peek\" into the items of xs, if xs is iterable\n    # For mappings, xs would be dict_values or similar\n    # But xs can be any iterable\n\n    # Defensive conversion to list to be able to peek\n    # but only if xs is not a list already\n    # To avoid unnecessary copy if xs is large, handle carefully\n\n    # Try to get the first element safely, then work accordingly\n    iterator = iter(xs)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        # Empty iterable -> return empty instance of container type_arg\n        try:\n            constructor = type(xs)\n            return constructor()\n        except Exception:\n            return xs\n\n    # If type_arg is a new type, unwrap it to its supertype\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the type_arg is a dataclass type and first element is a dict-like,\n    # decode each item using _decode_dataclass\n    if is_dataclass(type_arg):\n        # If first is already an instance of dataclass type_arg,\n        # just return the iterator prepended with first\n        if is_dataclass(first):\n            # Rebuild iterable with first + rest\n            return (x if isinstance(x, type_arg) else\n                    _decode_dataclass(type_arg, x, infer_missing) for x in ( [first] + list(iterator) ))\n        else:\n            # Decode each item as dataclass\n            return (_decode_dataclass(type_arg, v, infer_missing) for v in ( [first] + list(iterator) ))\n\n    # If type_arg is Enum, decode each value to Enum\n    if _issubclass_safe(type_arg, Enum):\n        return (type_arg(v) for v in ( [first] + list(iterator) ))\n\n    # If type_arg is a supported generic (collection, optional, union, enum)\n    # and not str, use _decode_generic on each item\n    from dataclasses_json.core import _is_supported_generic\n    if _is_supported_generic(type_arg) and type_arg is not str:\n        # _decode_generic expects the value to be the whole element, so decode each element\n        return (_decode_generic(type_arg, v, infer_missing) for v in ( [first] + list(iterator) ))\n\n    # Fall back to _support_extended_types for each item\n    # This handles extended types like datetime, Decimal, UUID\n    return (_support_extended_types(type_arg, v) for v in ( [first] + list(iterator) ))", "from dataclasses import is_dataclass\nfrom dataclasses_json.utils import (_is_new_type, _issubclass_safe, _is_supported_generic)\nfrom dataclasses_json.core import (_decode_dataclass, _decode_generic, _support_extended_types)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # If the type is a new type (a function with __supertype__), unwrap it.\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the annotated type is a dataclass, decode using _decode_dataclass\n    if is_dataclass(type_arg):\n        # xs might be already decoded; if not, decode it\n        if is_dataclass(xs):\n            return xs\n        return _decode_dataclass(type_arg, xs, infer_missing)\n\n    # If the type is a supported generic (collections, optionals, enums)\n    if _is_supported_generic(type_arg) and type_arg is not str:\n        return _decode_generic(type_arg, xs, infer_missing)\n\n    # If xs is already an instance of type_arg, return as is\n    # Use isinstance check safely\n    try:\n        if isinstance(xs, type_arg):\n            return xs\n    except TypeError:\n        # Type_arg might not be a class/type that isinstance can check,\n        # just ignore and continue to _support_extended_types\n        pass\n\n    # Otherwise, attempt to support extended types (like datetime, Decimal, UUID)\n    return _support_extended_types(type_arg, xs)", "from dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n    _issubclass_safe,\n)\nfrom enum import Enum\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    # If xs is None, just return None (or empty iterable could be considered,\n    # but here consistent with other decode functions)\n    if xs is None:\n        return xs\n\n    # If type_arg is a newtype, unwrap it\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    origin_type = type(type_arg)\n\n    # If type_arg is a dataclass, but value(s) may not be decoded yet,\n    # decode each item in xs as a dataclass instance\n    if is_dataclass(type_arg):\n        # xs should be an iterable of dataclass data\n        return [(_decode_dataclass(type_arg, vs, infer_missing)) for vs in xs]\n\n    # If type_arg is an Enum class, decode each item to Enum\n    if _issubclass_safe(type_arg, Enum):\n        return [type_arg(vs) for vs in xs]\n\n    # If type_arg is a generic type that supports generic decoding,\n    # delegate to _decode_generic directly (will handle collection types)\n    # Here xs is an iterable of raw items, but we want to decode each item\n    # using the type argument(s)\n    # Note: if type_arg is a generic Collection with subtypes, use _decode_generic for each item\n\n    # To do this safely, use _decode_generic on xs (which is the collection)\n    # But _decode_generic expects the whole collection, so just pass xs directly\n    # It handles mapping or collection construction internally.\n\n    # However, _decode_items expects to decode the items in xs (an iterable of values)\n    # to the target type given by type_arg\n\n    # Because _decode_generic can handle the whole collection at once (including mapping), \n    # If type_arg is a collection typing, decode the whole xs using _decode_generic.\n    # But _decode_items is intended to decode each item of it's iterable xs according to type_arg.\n    # So recursion during _decode_generic will call _decode_items again.\n\n    # So we will try to decode each item according to type_arg, if type_arg is not a mapping.\n    \n    # If the type_arg is a collection, but not mapping (e.g. List), decode each element\n    # If mapping, this function should not be called directly for mapping keys or values,\n    # but should be handled by _decode_generic itself.\n\n    from dataclasses_json.utils import _is_collection, _is_mapping\n\n    if _is_collection(type_arg) and not _is_mapping(type_arg):\n        # Here, type_arg.__args__[0] is the element type\n        elem_type = getattr(type_arg, \"__args__\", [Any])[0]\n        return [_decode_generic(elem_type, vs, infer_missing) for vs in xs]\n\n    # Otherwise, treat xs as a simple iterable of values to decode individually:\n    # decode each value individually, using _support_extended_types for extended types\n\n    return [_support_extended_types(type_arg, vs) for vs in xs]", "from dataclasses import is_dataclass\nfrom dataclasses_json.utils import (_is_new_type, _support_extended_types)\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # Normalize the type_arg for wrapped new types\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the xs is already the decoded object(s), just return them\n    # For collections, we expect xs to be iterable\n    # For mappings and other collections, _decode_generic handles those\n\n    # If type_arg is a dataclass type:\n    if is_dataclass(type_arg):\n        # If xs is a collection of dataclasses, decode each item\n        # But we must check if xs is a collection or a single instance:\n        # A dict or list could mean multiple items. We'll treat xs as a collection if it is iterable (but not str).\n        # Since this function is used by _decode_generic which checks collection or mapping types,\n        # here xs is usually iterable.\n        # But here, per typical usage, xs is an iterable of items needing decoding (e.g. a list of dicts)\n        try:\n            iterator = iter(xs)\n        except TypeError:\n            # xs is a single item, decode directly\n            return _decode_dataclass(type_arg, xs, infer_missing)\n        else:\n            # xs is iterable: decode each item\n            # But need to check if xs is a mapping: if so decode accordingly\n            # This function is called recursively in _decode_generic for collection types.\n            decoded = []\n            for vs in xs:\n                if is_dataclass(vs) and isinstance(vs, type_arg):\n                    decoded.append(vs)\n                else:\n                    decoded.append(_decode_dataclass(type_arg, vs, infer_missing))\n            return decoded\n\n    # If the type_arg is a supported generic (collections, optionals, enums, unions),\n    # delegate to _decode_generic\n    # _decode_generic internally calls this function recursively if needed.\n    from dataclasses_json.core import _is_supported_generic\n    if _is_supported_generic(type_arg):\n        # If xs is a iterable of primitive types, recursively decode elements\n        # _decode_generic handles recursively calling _decode_items if needed\n        return _decode_generic(type_arg, xs, infer_missing)\n\n    # Otherwise, it's a base type or extended type (datetime, Decimal, UUID)\n    # Use the support extended types logic on each item in xs if xs is iterable\n    try:\n        iterator = iter(xs)\n    except TypeError:\n        # xs is a single value, convert directly\n        return _support_extended_types(type_arg, xs)\n    else:\n        # xs is iterable, convert each item\n        return [ _support_extended_types(type_arg, v) for v in xs ]"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n    _is_supported_generic,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n)\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    # Defensive: if xs is None, no data to decode, return it immediately.\n    if xs is None:\n        return xs\n\n    # Prepare the result container matching the input type (e.g., list, tuple, set)\n    # We will accumulate decoded items here.\n    try:\n        res_container = type(xs)()\n    except Exception:\n        # If xs is not a proper collection instance or type() construction fails,\n        # fallback to using list\n        res_container = []\n\n    # Iterate over each item to decode it individually\n    # decode each element according to the type_arg\n    items = []\n    for vs in xs:\n        # If type_arg is a new type (has __supertype__), unwrap it\n        inner_type = type_arg\n        while _is_new_type(inner_type):\n            inner_type = inner_type.__supertype__\n\n        # If the element itself is a dataclass instance, decode it as a dataclass\n        # even if type_arg is a generic, this handles cases where the input already contains dataclasses.\n        if is_dataclass(inner_type) or is_dataclass(vs):\n            decoded = _decode_dataclass(inner_type, vs, infer_missing)\n        elif _is_supported_generic(inner_type):\n            decoded = _decode_generic(inner_type, vs, infer_missing)\n        else:\n            decoded = _support_extended_types(inner_type, vs)\n\n        items.append(decoded)\n\n    # Reconstruct the collection of decoded items with the same type as xs,\n    # e.g. list, tuple, set, etc.\n    # Some collections like tuple might need special construction\n    origin_type = type(xs)\n    if origin_type is tuple:\n        # tuple is immutable, use tuple constructor\n        return tuple(items)\n    # For immutable sets (frozenset), reconstruct accordingly\n    if origin_type.__name__ == 'frozenset':\n        return frozenset(items)\n    # For generators or other special collection types fallback to list\n    try:\n        return origin_type(items)\n    except Exception:\n        return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom copy import deepcopy\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n    _issubclass_safe,\n)\nfrom enum import Enum\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # We want to \"peek\" into the items of xs, if xs is iterable\n    # For mappings, xs would be dict_values or similar\n    # But xs can be any iterable\n\n    # Defensive conversion to list to be able to peek\n    # but only if xs is not a list already\n    # To avoid unnecessary copy if xs is large, handle carefully\n\n    # Try to get the first element safely, then work accordingly\n    iterator = iter(xs)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        # Empty iterable -> return empty instance of container type_arg\n        try:\n            constructor = type(xs)\n            return constructor()\n        except Exception:\n            return xs\n\n    # If type_arg is a new type, unwrap it to its supertype\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the type_arg is a dataclass type and first element is a dict-like,\n    # decode each item using _decode_dataclass\n    if is_dataclass(type_arg):\n        # If first is already an instance of dataclass type_arg,\n        # just return the iterator prepended with first\n        if is_dataclass(first):\n            # Rebuild iterable with first + rest\n            return (x if isinstance(x, type_arg) else\n                    _decode_dataclass(type_arg, x, infer_missing) for x in ( [first] + list(iterator) ))\n        else:\n            # Decode each item as dataclass\n            return (_decode_dataclass(type_arg, v, infer_missing) for v in ( [first] + list(iterator) ))\n\n    # If type_arg is Enum, decode each value to Enum\n    if _issubclass_safe(type_arg, Enum):\n        return (type_arg(v) for v in ( [first] + list(iterator) ))\n\n    # If type_arg is a supported generic (collection, optional, union, enum)\n    # and not str, use _decode_generic on each item\n    from dataclasses_json.core import _is_supported_generic\n    if _is_supported_generic(type_arg) and type_arg is not str:\n        # _decode_generic expects the value to be the whole element, so decode each element\n        return (_decode_generic(type_arg, v, infer_missing) for v in ( [first] + list(iterator) ))\n\n    # Fall back to _support_extended_types for each item\n    # This handles extended types like datetime, Decimal, UUID\n    return (_support_extended_types(type_arg, v) for v in ( [first] + list(iterator) ))\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.utils import (_is_new_type, _issubclass_safe, _is_supported_generic)\nfrom dataclasses_json.core import (_decode_dataclass, _decode_generic, _support_extended_types)\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # If the type is a new type (a function with __supertype__), unwrap it.\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the annotated type is a dataclass, decode using _decode_dataclass\n    if is_dataclass(type_arg):\n        # xs might be already decoded; if not, decode it\n        if is_dataclass(xs):\n            return xs\n        return _decode_dataclass(type_arg, xs, infer_missing)\n\n    # If the type is a supported generic (collections, optionals, enums)\n    if _is_supported_generic(type_arg) and type_arg is not str:\n        return _decode_generic(type_arg, xs, infer_missing)\n\n    # If xs is already an instance of type_arg, return as is\n    # Use isinstance check safely\n    try:\n        if isinstance(xs, type_arg):\n            return xs\n    except TypeError:\n        # Type_arg might not be a class/type that isinstance can check,\n        # just ignore and continue to _support_extended_types\n        pass\n\n    # Otherwise, attempt to support extended types (like datetime, Decimal, UUID)\n    return _support_extended_types(type_arg, xs)\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.core import (\n    _decode_dataclass,\n    _decode_generic,\n    _support_extended_types,\n)\nfrom dataclasses_json.utils import (\n    _is_new_type,\n    _issubclass_safe,\n)\nfrom enum import Enum\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    # If xs is None, just return None (or empty iterable could be considered,\n    # but here consistent with other decode functions)\n    if xs is None:\n        return xs\n\n    # If type_arg is a newtype, unwrap it\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    origin_type = type(type_arg)\n\n    # If type_arg is a dataclass, but value(s) may not be decoded yet,\n    # decode each item in xs as a dataclass instance\n    if is_dataclass(type_arg):\n        # xs should be an iterable of dataclass data\n        return [(_decode_dataclass(type_arg, vs, infer_missing)) for vs in xs]\n\n    # If type_arg is an Enum class, decode each item to Enum\n    if _issubclass_safe(type_arg, Enum):\n        return [type_arg(vs) for vs in xs]\n\n    # If type_arg is a generic type that supports generic decoding,\n    # delegate to _decode_generic directly (will handle collection types)\n    # Here xs is an iterable of raw items, but we want to decode each item\n    # using the type argument(s)\n    # Note: if type_arg is a generic Collection with subtypes, use _decode_generic for each item\n\n    # To do this safely, use _decode_generic on xs (which is the collection)\n    # But _decode_generic expects the whole collection, so just pass xs directly\n    # It handles mapping or collection construction internally.\n\n    # However, _decode_items expects to decode the items in xs (an iterable of values)\n    # to the target type given by type_arg\n\n    # Because _decode_generic can handle the whole collection at once (including mapping), \n    # If type_arg is a collection typing, decode the whole xs using _decode_generic.\n    # But _decode_items is intended to decode each item of it's iterable xs according to type_arg.\n    # So recursion during _decode_generic will call _decode_items again.\n\n    # So we will try to decode each item according to type_arg, if type_arg is not a mapping.\n    \n    # If the type_arg is a collection, but not mapping (e.g. List), decode each element\n    # If mapping, this function should not be called directly for mapping keys or values,\n    # but should be handled by _decode_generic itself.\n\n    from dataclasses_json.utils import _is_collection, _is_mapping\n\n    if _is_collection(type_arg) and not _is_mapping(type_arg):\n        # Here, type_arg.__args__[0] is the element type\n        elem_type = getattr(type_arg, \"__args__\", [Any])[0]\n        return [_decode_generic(elem_type, vs, infer_missing) for vs in xs]\n\n    # Otherwise, treat xs as a simple iterable of values to decode individually:\n    # decode each value individually, using _support_extended_types for extended types\n\n    return [_support_extended_types(type_arg, vs) for vs in xs]\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\nfrom dataclasses import is_dataclass\nfrom dataclasses_json.utils import (_is_new_type, _support_extended_types)\nfrom dataclasses_json.core import _decode_dataclass, _decode_generic\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if xs is None:\n        return xs\n\n    # Normalize the type_arg for wrapped new types\n    while _is_new_type(type_arg):\n        type_arg = type_arg.__supertype__\n\n    # If the xs is already the decoded object(s), just return them\n    # For collections, we expect xs to be iterable\n    # For mappings and other collections, _decode_generic handles those\n\n    # If type_arg is a dataclass type:\n    if is_dataclass(type_arg):\n        # If xs is a collection of dataclasses, decode each item\n        # But we must check if xs is a collection or a single instance:\n        # A dict or list could mean multiple items. We'll treat xs as a collection if it is iterable (but not str).\n        # Since this function is used by _decode_generic which checks collection or mapping types,\n        # here xs is usually iterable.\n        # But here, per typical usage, xs is an iterable of items needing decoding (e.g. a list of dicts)\n        try:\n            iterator = iter(xs)\n        except TypeError:\n            # xs is a single item, decode directly\n            return _decode_dataclass(type_arg, xs, infer_missing)\n        else:\n            # xs is iterable: decode each item\n            # But need to check if xs is a mapping: if so decode accordingly\n            # This function is called recursively in _decode_generic for collection types.\n            decoded = []\n            for vs in xs:\n                if is_dataclass(vs) and isinstance(vs, type_arg):\n                    decoded.append(vs)\n                else:\n                    decoded.append(_decode_dataclass(type_arg, vs, infer_missing))\n            return decoded\n\n    # If the type_arg is a supported generic (collections, optionals, enums, unions),\n    # delegate to _decode_generic\n    # _decode_generic internally calls this function recursively if needed.\n    from dataclasses_json.core import _is_supported_generic\n    if _is_supported_generic(type_arg):\n        # If xs is a iterable of primitive types, recursively decode elements\n        # _decode_generic handles recursively calling _decode_items if needed\n        return _decode_generic(type_arg, xs, infer_missing)\n\n    # Otherwise, it's a base type or extended type (datetime, Decimal, UUID)\n    # Use the support extended types logic on each item in xs if xs is iterable\n    try:\n        iterator = iter(xs)\n    except TypeError:\n        # xs is a single value, convert directly\n        return _support_extended_types(type_arg, xs)\n    else:\n        # xs is iterable, convert each item\n        return [ _support_extended_types(type_arg, v) for v in xs ]\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n"]}
{"task_id": 348, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["import copy\nfrom collections import abc\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    # If obj is not a dataclass instance, return it as is or encode if requested\n    if not is_dataclass(obj):\n        if encode_json:\n            return _encode_json_type(obj)\n        return obj\n\n    # Prepare field overrides for this dataclass type \n    overrides = _user_overrides_or_exts(obj.__class__)\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            # We will create a dict of field names to their values, recursively calling _convert\n            result = {}\n            for f in fields(obj_inner):\n                value = getattr(obj_inner, f.name)\n                result[f.name] = _convert(value)\n            # Apply overrides: includes filtering by exclude predicate, letter case, encoder etc.\n            result = _encode_overrides(result, overrides, encode_json)\n            return result\n        elif _isinstance_safe(obj_inner, abc.Mapping):\n            # For mappings, convert keys and values by recursively calling _convert on values\n            return type(obj_inner)(\n                (key, _convert(value)) for key, value in obj_inner.items()\n            )\n        elif _isinstance_safe(obj_inner, abc.Collection) and not isinstance(obj_inner, (str, bytes, bytearray)):\n            # For other collections (lists, sets, tuples), convert each element recursively\n            converted = [_convert(v) for v in obj_inner]\n            # Preserve original collection type if possible\n            try:\n                return type(obj_inner)(converted)\n            except TypeError:\n                # fallback to list if the type constructor fails\n                return converted\n        else:\n            # Primitive types or objects that are not dataclasses/collections:\n            # If encode_json is requested, encode basic JSON types, else return as is\n            if encode_json:\n                return _encode_json_type(obj_inner)\n            return obj_inner\n\n    return _convert(obj)", "from collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n\n    # Obtain field-specific overrides (e.g. encoders, lettercase functions, excludes)\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(value):\n        # For dataclass instances, recurse into their fields\n        if is_dataclass(value):\n            return _convert(_asdict(value, encode_json=encode_json))\n\n        # For general Mappings, recursively convert both keys and values\n        elif _isinstance_safe(value, Mapping):\n            # Dict keys are typically strings (JSON keys), \n            # so keys are left as-is (not converted to dict keys as in decode)\n            return type(value)(\n                (_convert(k), _convert(v)) for k, v in value.items()\n            )\n\n        # For Collections (but not strings), recursively convert items\n        elif (_isinstance_safe(value, Collection)\n              and not isinstance(value, (str, bytes, bytearray))):\n            return type(value)(_convert(v) for v in value)\n\n        # Otherwise, encode json type if requested or return as-is\n        else:\n            if encode_json:\n                return _encode_json_type(value)\n            else:\n                return value\n\n    # Build dictionary of field names to converted values, respecting overrides (excludes, encoders, letter case)\n    kw = {}\n    for field in fields(obj):\n        value = getattr(obj, field.name)\n        # Check exclude predicate from overrides - skip if exclude says so\n        override = overrides.get(field.name)\n        if override is not None and override.exclude is not None and override.exclude(value):\n            continue\n\n        # Apply encoder if present on the field override before recursion (to encode or transform field value)\n        if override is not None and override.encoder is not None:\n            value = override.encoder(value)\n\n        # Recurse conversion\n        value = _convert(value)\n\n        # Handle letter_case renaming if set\n        if override is not None and override.letter_case is not None:\n            key = override.letter_case(field.name)\n        else:\n            key = field.name\n\n        kw[key] = value\n\n    return kw", "import copy\nfrom collections.abc import Mapping, Collection\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _encode_json_type\nfrom dataclasses_json.core import _encode_overrides\nfrom dataclasses_json.core import _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    # Recursively convert dataclass fields to dict\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            cls = type(obj_inner)\n            overrides = _user_overrides_or_exts(cls)\n\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init and f.repr is False:\n                    # Skip fields not included in initializer and representation, per dataclass standard asdict\n                    continue\n                value = getattr(obj_inner, f.name)\n                # Recursively convert value\n                value = _convert(value)\n                result[f.name] = value\n\n            # Apply overrides (encoder, letter_case, exclusion) and optionally encode JSON types\n            result = _encode_overrides(result, overrides, encode_json=encode_json)\n\n            return result\n\n        # For general Mapping (e.g. dict, OrderedDict, etc.), apply to keys and values recursively\n        elif _isinstance_safe(obj_inner, Mapping):\n            # Preserve the mapping type if possible\n            # Create a new mapping instance of the same type if possible, else dict\n            res_mapping = obj_inner.__class__() if type(obj_inner) != dict else {}\n            for k, v in obj_inner.items():\n                # Keys are usually strings in JSON/dict, so do not convert keys\n                res_mapping[k] = _convert(v)\n            return res_mapping\n\n        # For Collections (list, set, tuple, etc.) but not strings, convert each element recursively\n        elif _isinstance_safe(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # Preserve the type where possible\n            cons = type(obj_inner)\n            # Convert each element\n            converted = (_convert(v) for v in obj_inner)\n            if cons in (list, set, tuple):\n                return cons(converted)\n            else:\n                # If unknown collection type, try to create from iterable, fallback to list\n                try:\n                    return cons(converted)\n                except Exception:\n                    return list(converted)\n\n        else:\n            # Base case: not a dataclass, not mapping, not collection\n            # Optionally encode_json types (e.g. Decimal, UUID, Enum, datetime)\n            if encode_json:\n                return _encode_json_type(obj_inner)\n            else:\n                return obj_inner\n\n    # Make a deep copy before conversion to avoid mutability side-effects\n    obj_copy = copy.deepcopy(obj)\n    return _convert(obj_copy)", "from collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Dict\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(obj):\n        if _isinstance_safe(obj, tuple(fields(type(obj)))):\n            # Should never happen as fields return Field objects, not types,\n            # so this is not useful here; skip this condition.\n            pass\n\n        if is_dataclass(obj) and not isinstance(obj, type):\n            # Convert dataclass instance to dict with keys encoded and values converted recursively\n            result = {}\n            for f in fields(obj):\n                if not f.init:\n                    continue\n                value = getattr(obj, f.name)\n                if value is None:\n                    result[f.name] = None\n                elif is_dataclass(value):\n                    result[f.name] = _convert(value)\n                elif isinstance(value, (Mapping, Collection)) and not isinstance(value, (str, bytes)):\n                    # Collections handled normally below\n                    pass\n                else:\n                    result[f.name] = value\n            result = _encode_overrides(result, overrides, encode_json)\n            return result\n\n        if _isinstance_safe(obj, Mapping):\n            return obj.__class__((_convert(k), _convert(v)) for k, v in obj.items())\n        elif _isinstance_safe(obj, Collection) and not isinstance(obj, (str, bytes)):\n            return obj.__class__(_convert(v) for v in obj)\n        else:\n            if encode_json:\n                return _encode_json_type(obj)\n            return obj\n\n    return _convert(obj)", "from collections.abc import Mapping, Collection\nfrom dataclasses import fields, is_dataclass\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _ExtendedEncoder, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\nfrom typing import Any\n\ndef _asdict(obj: Any, encode_json: bool = False) -> Any:\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        # Not a dataclass instance, return it (optionally encode to JSON-compatible)\n        return _encode_json_type(obj) if encode_json else obj\n\n    # Retrieve field overrides for encoding/decoding to honor encoders, letter_case, exclude etc.\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(value):\n        # If value is a dataclass instance, recurse\n        if is_dataclass(value) and not isinstance(value, type):\n            return _asdict(value, encode_json=encode_json)\n        elif _isinstance_safe(value, Mapping):\n            # For mappings, preserve type (e.g. dict or OrderedDict)\n            # map keys/values recursively converting values, keys remain the same\n            return type(value)(\n                (k, _convert(v))\n                for k, v in value.items()\n            )\n        elif _isinstance_safe(value, Collection) and not isinstance(value, (str, bytes, bytearray)):\n            # For collections (list, set, tuple, etc) but not strings, bytes\n            return type(value)(_convert(v) for v in value)\n        else:\n            # For primitive or unknown types, optionally encode json or pass as is\n            return _encode_json_type(value) if encode_json else value\n\n    # Build a dict of field values keyed by override lettercase and filtered by exclude predicate\n    kvs = {}\n    for field in fields(obj):\n        value = getattr(obj, field.name)\n        # Check exclude using overrides; if true skip field\n        field_override = overrides.get(field.name)\n        if field_override and field_override.exclude and field_override.exclude(value):\n            continue\n\n        # Determine output key with letter_case override if any\n        key = field.name\n        if field_override and field_override.letter_case:\n            key = field_override.letter_case(field.name)\n\n        # Use encoder if present on field override for the value\n        if field_override and field_override.encoder is not None:\n            value = field_override.encoder(value)\n        else:\n            value = _convert(value)\n\n        if encode_json:\n            value = _encode_json_type(value)\n\n        kvs[key] = value\n\n    return kvs"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nimport copy\nfrom collections import abc\nfrom dataclasses import fields, is_dataclass\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    # If obj is not a dataclass instance, return it as is or encode if requested\n    if not is_dataclass(obj):\n        if encode_json:\n            return _encode_json_type(obj)\n        return obj\n\n    # Prepare field overrides for this dataclass type \n    overrides = _user_overrides_or_exts(obj.__class__)\n\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            # We will create a dict of field names to their values, recursively calling _convert\n            result = {}\n            for f in fields(obj_inner):\n                value = getattr(obj_inner, f.name)\n                result[f.name] = _convert(value)\n            # Apply overrides: includes filtering by exclude predicate, letter case, encoder etc.\n            result = _encode_overrides(result, overrides, encode_json)\n            return result\n        elif _isinstance_safe(obj_inner, abc.Mapping):\n            # For mappings, convert keys and values by recursively calling _convert on values\n            return type(obj_inner)(\n                (key, _convert(value)) for key, value in obj_inner.items()\n            )\n        elif _isinstance_safe(obj_inner, abc.Collection) and not isinstance(obj_inner, (str, bytes, bytearray)):\n            # For other collections (lists, sets, tuples), convert each element recursively\n            converted = [_convert(v) for v in obj_inner]\n            # Preserve original collection type if possible\n            try:\n                return type(obj_inner)(converted)\n            except TypeError:\n                # fallback to list if the type constructor fails\n                return converted\n        else:\n            # Primitive types or objects that are not dataclasses/collections:\n            # If encode_json is requested, encode basic JSON types, else return as is\n            if encode_json:\n                return _encode_json_type(obj_inner)\n            return obj_inner\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n\n    # Obtain field-specific overrides (e.g. encoders, lettercase functions, excludes)\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(value):\n        # For dataclass instances, recurse into their fields\n        if is_dataclass(value):\n            return _convert(_asdict(value, encode_json=encode_json))\n\n        # For general Mappings, recursively convert both keys and values\n        elif _isinstance_safe(value, Mapping):\n            # Dict keys are typically strings (JSON keys), \n            # so keys are left as-is (not converted to dict keys as in decode)\n            return type(value)(\n                (_convert(k), _convert(v)) for k, v in value.items()\n            )\n\n        # For Collections (but not strings), recursively convert items\n        elif (_isinstance_safe(value, Collection)\n              and not isinstance(value, (str, bytes, bytearray))):\n            return type(value)(_convert(v) for v in value)\n\n        # Otherwise, encode json type if requested or return as-is\n        else:\n            if encode_json:\n                return _encode_json_type(value)\n            else:\n                return value\n\n    # Build dictionary of field names to converted values, respecting overrides (excludes, encoders, letter case)\n    kw = {}\n    for field in fields(obj):\n        value = getattr(obj, field.name)\n        # Check exclude predicate from overrides - skip if exclude says so\n        override = overrides.get(field.name)\n        if override is not None and override.exclude is not None and override.exclude(value):\n            continue\n\n        # Apply encoder if present on the field override before recursion (to encode or transform field value)\n        if override is not None and override.encoder is not None:\n            value = override.encoder(value)\n\n        # Recurse conversion\n        value = _convert(value)\n\n        # Handle letter_case renaming if set\n        if override is not None and override.letter_case is not None:\n            key = override.letter_case(field.name)\n        else:\n            key = field.name\n\n        kw[key] = value\n\n    return kw\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nimport copy\nfrom collections.abc import Mapping, Collection\nfrom dataclasses import is_dataclass, fields\n\nfrom dataclasses_json.core import _encode_json_type\nfrom dataclasses_json.core import _encode_overrides\nfrom dataclasses_json.core import _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    # Recursively convert dataclass fields to dict\n    def _convert(obj_inner):\n        if is_dataclass(obj_inner):\n            cls = type(obj_inner)\n            overrides = _user_overrides_or_exts(cls)\n\n            result = {}\n            for f in fields(obj_inner):\n                if not f.init and f.repr is False:\n                    # Skip fields not included in initializer and representation, per dataclass standard asdict\n                    continue\n                value = getattr(obj_inner, f.name)\n                # Recursively convert value\n                value = _convert(value)\n                result[f.name] = value\n\n            # Apply overrides (encoder, letter_case, exclusion) and optionally encode JSON types\n            result = _encode_overrides(result, overrides, encode_json=encode_json)\n\n            return result\n\n        # For general Mapping (e.g. dict, OrderedDict, etc.), apply to keys and values recursively\n        elif _isinstance_safe(obj_inner, Mapping):\n            # Preserve the mapping type if possible\n            # Create a new mapping instance of the same type if possible, else dict\n            res_mapping = obj_inner.__class__() if type(obj_inner) != dict else {}\n            for k, v in obj_inner.items():\n                # Keys are usually strings in JSON/dict, so do not convert keys\n                res_mapping[k] = _convert(v)\n            return res_mapping\n\n        # For Collections (list, set, tuple, etc.) but not strings, convert each element recursively\n        elif _isinstance_safe(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            # Preserve the type where possible\n            cons = type(obj_inner)\n            # Convert each element\n            converted = (_convert(v) for v in obj_inner)\n            if cons in (list, set, tuple):\n                return cons(converted)\n            else:\n                # If unknown collection type, try to create from iterable, fallback to list\n                try:\n                    return cons(converted)\n                except Exception:\n                    return list(converted)\n\n        else:\n            # Base case: not a dataclass, not mapping, not collection\n            # Optionally encode_json types (e.g. Decimal, UUID, Enum, datetime)\n            if encode_json:\n                return _encode_json_type(obj_inner)\n            else:\n                return obj_inner\n\n    # Make a deep copy before conversion to avoid mutability side-effects\n    obj_copy = copy.deepcopy(obj)\n    return _convert(obj_copy)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Collection, Mapping\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Dict\n\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        raise TypeError(f\"_asdict() should be called on dataclass instances, got {type(obj)}\")\n\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(obj):\n        if _isinstance_safe(obj, tuple(fields(type(obj)))):\n            # Should never happen as fields return Field objects, not types,\n            # so this is not useful here; skip this condition.\n            pass\n\n        if is_dataclass(obj) and not isinstance(obj, type):\n            # Convert dataclass instance to dict with keys encoded and values converted recursively\n            result = {}\n            for f in fields(obj):\n                if not f.init:\n                    continue\n                value = getattr(obj, f.name)\n                if value is None:\n                    result[f.name] = None\n                elif is_dataclass(value):\n                    result[f.name] = _convert(value)\n                elif isinstance(value, (Mapping, Collection)) and not isinstance(value, (str, bytes)):\n                    # Collections handled normally below\n                    pass\n                else:\n                    result[f.name] = value\n            result = _encode_overrides(result, overrides, encode_json)\n            return result\n\n        if _isinstance_safe(obj, Mapping):\n            return obj.__class__((_convert(k), _convert(v)) for k, v in obj.items())\n        elif _isinstance_safe(obj, Collection) and not isinstance(obj, (str, bytes)):\n            return obj.__class__(_convert(v) for v in obj)\n        else:\n            if encode_json:\n                return _encode_json_type(obj)\n            return obj\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\nfrom collections.abc import Mapping, Collection\nfrom dataclasses import fields, is_dataclass\nfrom dataclasses_json.core import _encode_json_type, _encode_overrides, _ExtendedEncoder, _user_overrides_or_exts\nfrom dataclasses_json.utils import _isinstance_safe\nfrom typing import Any\n\ndef _asdict(obj: Any, encode_json: bool = False) -> Any:\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if not is_dataclass(obj):\n        # Not a dataclass instance, return it (optionally encode to JSON-compatible)\n        return _encode_json_type(obj) if encode_json else obj\n\n    # Retrieve field overrides for encoding/decoding to honor encoders, letter_case, exclude etc.\n    overrides = _user_overrides_or_exts(type(obj))\n\n    def _convert(value):\n        # If value is a dataclass instance, recurse\n        if is_dataclass(value) and not isinstance(value, type):\n            return _asdict(value, encode_json=encode_json)\n        elif _isinstance_safe(value, Mapping):\n            # For mappings, preserve type (e.g. dict or OrderedDict)\n            # map keys/values recursively converting values, keys remain the same\n            return type(value)(\n                (k, _convert(v))\n                for k, v in value.items()\n            )\n        elif _isinstance_safe(value, Collection) and not isinstance(value, (str, bytes, bytearray)):\n            # For collections (list, set, tuple, etc) but not strings, bytes\n            return type(value)(_convert(v) for v in value)\n        else:\n            # For primitive or unknown types, optionally encode json or pass as is\n            return _encode_json_type(value) if encode_json else value\n\n    # Build a dict of field values keyed by override lettercase and filtered by exclude predicate\n    kvs = {}\n    for field in fields(obj):\n        value = getattr(obj, field.name)\n        # Check exclude using overrides; if true skip field\n        field_override = overrides.get(field.name)\n        if field_override and field_override.exclude and field_override.exclude(value):\n            continue\n\n        # Determine output key with letter_case override if any\n        key = field.name\n        if field_override and field_override.letter_case:\n            key = field_override.letter_case(field.name)\n\n        # Use encoder if present on field override for the value\n        if field_override and field_override.encoder is not None:\n            value = field_override.encoder(value)\n        else:\n            value = _convert(value)\n\n        if encode_json:\n            value = _encode_json_type(value)\n\n        kvs[key] = value\n\n    return kvs\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n"]}
{"task_id": 349, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.api", "predictions": ["from typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # called with parentheses\n        return wrap\n    else:\n        # called without parentheses\n        return wrap(_cls)", "import functools\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.cfg import LetterCase\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # Called with kwargs, return decorator function to wrap class\n        return wrap\n    else:\n        # Called without kwargs, directly decorate the class\n        return wrap(_cls)", "import functools\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.cfg import LetterCase\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "from typing import Optional, Union\nfrom dataclasses_json.utils import _undefined_parameter_action_safe\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\nfrom enum import Enum\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[Enum] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Save config on class if letter_case or undefined are provided\n        if letter_case is not None or undefined is not None:\n            cls.dataclass_json_config = config(\n                letter_case=letter_case, undefined=undefined)['dataclasses_json']\n\n        # Process and extend the class with JSON (de)serialization methods\n        return _process_class(cls, letter_case, undefined)\n\n    # If decorator is used without arguments: @dataclass_json\n    if _cls is not None:\n        return wrap(_cls)\n\n    # If decorator is used with arguments: @dataclass_json(...)\n    return wrap", "from typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # called with parentheses\n        return wrap\n    else:\n        # called without parentheses\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nimport functools\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.cfg import LetterCase\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case, undefined)\n\n    if _cls is None:\n        # Called with kwargs, return decorator function to wrap class\n        return wrap\n    else:\n        # Called without kwargs, directly decorate the class\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nimport functools\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.cfg import LetterCase\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[LetterCase] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\nfrom dataclasses_json.utils import _undefined_parameter_action_safe\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\nfrom enum import Enum\n\n\ndef dataclass_json(_cls=None, *, letter_case: Optional[Enum] = None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Save config on class if letter_case or undefined are provided\n        if letter_case is not None or undefined is not None:\n            cls.dataclass_json_config = config(\n                letter_case=letter_case, undefined=undefined)['dataclasses_json']\n\n        # Process and extend the class with JSON (de)serialization methods\n        return _process_class(cls, letter_case, undefined)\n\n    # If decorator is used without arguments: @dataclass_json\n    if _cls is not None:\n        return wrap(_cls)\n\n    # If decorator is used with arguments: @dataclass_json(...)\n    return wrap\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\nfrom typing import Optional, Union\n\nfrom dataclasses_json.api import _process_class\nfrom dataclasses_json.undefined import Undefined\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        return _process_class(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n"]}
{"task_id": 350, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.utils", "predictions": ["def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # In Python typing, __origin__ is the attribute that indicates the base generic type.\n    # However, in Python 3.6, __origin__ may be None or missing, and __extra__ was used.\n    # This function tries to retrieve the type origin attribute robustly.\n\n    import sys\n\n    # For Python 3.6, __origin__ can be missing or None, and __extra__ was used in some types.\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        # For Python 3.7+, the __origin__ attribute is the way to get the origin type.\n        origin = getattr(type_, \"__origin__\", None)\n\n    # Fallback to the type itself if origin is None or not set\n    return origin or type_", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # Based on the pattern from _get_type_cons, we handle differences in typing internals\n    import sys\n\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, \"__origin__\", None)\n\n    return origin", "import sys\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # In Python 3.6, the typing internals used __extra__ and __origin__ differently,\n    # whereas in 3.7+ __origin__ is the standard attribute for getting the underlying \n    # origin of a generic type.\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = None\n\n        # For 3.6, if __origin__ is None, fallback to __extra__, which holds container info\n        if origin is None:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n\n        # If still None, fallback to the type itself\n        return origin if origin is not None else type_\n    else:\n        # For 3.7+, simply use __origin__, fallback to type_ if it doesn't exist\n        origin = getattr(type_, \"__origin__\", None)\n        return origin if origin is not None else type_", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = type_.__origin__\n    return origin", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = None\n        if origin is None:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n        return origin\n    else:\n        return getattr(type_, '__origin__', None)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # In Python typing, __origin__ is the attribute that indicates the base generic type.\n    # However, in Python 3.6, __origin__ may be None or missing, and __extra__ was used.\n    # This function tries to retrieve the type origin attribute robustly.\n\n    import sys\n\n    # For Python 3.6, __origin__ can be missing or None, and __extra__ was used in some types.\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        # For Python 3.7+, the __origin__ attribute is the way to get the origin type.\n        origin = getattr(type_, \"__origin__\", None)\n\n    # Fallback to the type itself if origin is None or not set\n    return origin or type_\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # Based on the pattern from _get_type_cons, we handle differences in typing internals\n    import sys\n\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = getattr(type_, \"__origin__\", None)\n\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\nimport sys\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    # In Python 3.6, the typing internals used __extra__ and __origin__ differently,\n    # whereas in 3.7+ __origin__ is the standard attribute for getting the underlying \n    # origin of a generic type.\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = None\n\n        # For 3.6, if __origin__ is None, fallback to __extra__, which holds container info\n        if origin is None:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n\n        # If still None, fallback to the type itself\n        return origin if origin is not None else type_\n    else:\n        # For 3.7+, simply use __origin__, fallback to type_ if it doesn't exist\n        origin = getattr(type_, \"__origin__\", None)\n        return origin if origin is not None else type_\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n    else:\n        origin = type_.__origin__\n    return origin\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            origin = type_.__origin__\n        except AttributeError:\n            origin = None\n        if origin is None:\n            try:\n                origin = type_.__extra__\n            except AttributeError:\n                origin = None\n        return origin\n    else:\n        return getattr(type_, '__origin__', None)\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n"]}
