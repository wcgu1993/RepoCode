{"task_id": 345, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    result = {}\n    for key, value in overrides.items():\n        if key in field_names:\n            result[key] = value\n    return result", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    result = []\n    for name in field_names:\n        if name in overrides:\n            result.append(overrides[name])\n        else:\n            result.append(name)\n    return result", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    result = []\n    for field in field_names:\n        if field in overrides:\n            result.append(overrides[field])\n        else:\n            result.append(field)\n    return result", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    overridden = []\n    for name in field_names:\n        overridden.append(overrides.get(name, name))\n    return overridden", "def _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    overridden_fields = {}\n    for field in field_names:\n        if field in overrides:\n            overridden_fields[field] = overrides[field]\n        else:\n            overridden_fields[field] = field\n    return overridden_fields"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    result = {}\n    for key, value in overrides.items():\n        if key in field_names:\n            result[key] = value\n    return result\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    result = []\n    for name in field_names:\n        if name in overrides:\n            result.append(overrides[name])\n        else:\n            result.append(name)\n    return result\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    result = []\n    for field in field_names:\n        if field in overrides:\n            result.append(overrides[field])\n        else:\n            result.append(field)\n    return result\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    overridden = []\n    for name in field_names:\n        overridden.append(overrides.get(name, name))\n    return overridden\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    if not overrides:\n        return field_names\n    overridden_fields = {}\n    for field in field_names:\n        if field in overrides:\n            overridden_fields[field] = overrides[field]\n        else:\n            overridden_fields[field] = field\n    return overridden_fields\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_13():\n    assert _decode_letter_case_overrides([\"TestField\"], {}) == {}\ntest_13()\n\ndef test_24():\n    assert _decode_letter_case_overrides(['name'], {}) == {}\ntest_24()\n\ndef test_30():\n    assert _decode_letter_case_overrides([\"a\"], {}) == {}\ntest_30()\n\ndef test_43():\n    assert _decode_letter_case_overrides([\"x\", \"y\", \"z\"], {}) == {}\ntest_43()\n\ndef test_51():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"], {}) == {}\ntest_51()\n\ndef test_67():\n    assert _decode_letter_case_overrides(['a', 'b', 'c', 'd'], {}) == {}\ntest_67()\n\ndef test_72():\n    assert _decode_letter_case_overrides([\"a\",\"b\",\"c\"],{}) == {}\ntest_72()\n\ndef test_73():\n    assert _decode_letter_case_overrides({'a', 'b'}, {}) == {}\ntest_73()\n\ndef test_74():\n    assert _decode_letter_case_overrides([\"fieldName\"], {}) == {}\ntest_74()\n\ndef test_96():\n    assert _decode_letter_case_overrides(('firstName', 'familyName'), {}) == {}\ntest_96()\n\ndef test_141():\n    assert _decode_letter_case_overrides({\"a\", \"b\"}, {}) == {}\ntest_141()\n\ndef test_143():\n    assert _decode_letter_case_overrides([\"name\"], {}) == {}\ntest_143()\n\ndef test_145():\n    assert _decode_letter_case_overrides(['name', 'last_name'], {}) == {}\ntest_145()\n\ndef test_150():\n    assert _decode_letter_case_overrides([\"a\", \"b\", \"c\"], {}) == {}\ntest_150()\n\ndef test_157():\n    assert _decode_letter_case_overrides(['key1', 'key2', 'key3'], {}) == {}\ntest_157()\n\ndef test_169():\n    assert _decode_letter_case_overrides([''], {}) == {}\ntest_169()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'A', 'B'}, {}) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'b':None, 'c':None}) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({\"lol\", \"Lol\", \"LOL\"}, {}) == output\ntest_11()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"a\",\"B\",\"C\"], {\"a\": None, \"B\": None}) == output\ntest_23()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None, 'c':None}) == output\ntest_42()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"Id\", \"Name\", \"Url\"], {}) == output\ntest_53()\n\ndef test_79():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"A\"], {}) == output\ntest_79()\n\ndef test_84():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides({'a','b','c'},{'a':None}) == output\ntest_84()\n\ndef test_98():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"userName\", \"id\", \"isAdmin\"],{}) == output\ntest_98()\n\ndef test_110():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['name', 'is_blocked', 'updated_at', 'id', 'age', 'city', 'last_login'], {}) == output\ntest_110()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"name\", \"Id\", \"Age\"], {}) == output\ntest_128()\n\ndef test_136():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides(['f_Oo', 'b_aR'], {}) == output\ntest_136()\n\ndef test_161():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_letter_case_overrides/test_161\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_letter_case_overrides([\"xyz\", \"abc\"], {}) == output\ntest_161()\n\n\n"]}
{"task_id": 346, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    decoded = {}\n    for key, value in xs.items():\n        try:\n            decoded_key = key_type(key)\n        except (ValueError, TypeError):\n            if not infer_missing:\n                raise\n            decoded_key = key\n        decoded[decoded_key] = value\n    return decoded", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    if key_type is str:\n        return xs\n    decoded = {}\n    for k, v in xs.items():\n        try:\n            decoded_key = key_type(k)\n        except Exception:\n            if infer_missing:\n                decoded_key = k\n            else:\n                raise\n        decoded[decoded_key] = v\n    return decoded", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    decoded_dict = {}\n    for k, v in xs.items():\n        try:\n            new_key = key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                new_key = k\n            else:\n                raise\n        decoded_dict[new_key] = v\n    return decoded_dict", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    if key_type is str or key_type is None:\n        return xs\n\n    def decode_key(k):\n        try:\n            return key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                return k\n            else:\n                raise\n\n    return {decode_key(k): v for k, v in xs.items()}", "def _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    new_dict = {}\n    for k, v in xs.items():\n        try:\n            new_key = key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                new_key = k\n            else:\n                raise\n        new_dict[new_key] = v\n    return new_dict"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    decoded = {}\n    for key, value in xs.items():\n        try:\n            decoded_key = key_type(key)\n        except (ValueError, TypeError):\n            if not infer_missing:\n                raise\n            decoded_key = key\n        decoded[decoded_key] = value\n    return decoded\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    if key_type is str:\n        return xs\n    decoded = {}\n    for k, v in xs.items():\n        try:\n            decoded_key = key_type(k)\n        except Exception:\n            if infer_missing:\n                decoded_key = k\n            else:\n                raise\n        decoded[decoded_key] = v\n    return decoded\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    decoded_dict = {}\n    for k, v in xs.items():\n        try:\n            new_key = key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                new_key = k\n            else:\n                raise\n        decoded_dict[new_key] = v\n    return decoded_dict\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    if key_type is str or key_type is None:\n        return xs\n\n    def decode_key(k):\n        try:\n            return key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                return k\n            else:\n                raise\n\n    return {decode_key(k): v for k, v in xs.items()}\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    new_dict = {}\n    for k, v in xs.items():\n        try:\n            new_key = key_type(k)\n        except (ValueError, TypeError):\n            if infer_missing:\n                new_key = k\n            else:\n                raise\n        new_dict[new_key] = v\n    return new_dict\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert 1 == len(list(_decode_dict_keys(int, [1], False)))\ntest_0()\n\ndef test_5():\n    assert all(\n        isinstance(i, float) for i in _decode_dict_keys(float, ['1', '2', '3'], False))\ntest_5()\n\ndef test_10():\n    assert {'a': 1, 'b': 2} == dict(zip( \n        _decode_dict_keys(Any, ['a', 'b'], True), [1, 2]))\ntest_10()\n\ndef test_11():\n    assert 123 == list(_decode_dict_keys(int, [\"123\"], True))[0]\ntest_11()\n\ndef test_20():\n    assert 1.0 == next(_decode_dict_keys(float, [1], False))\ntest_20()\n\ndef test_23():\n    assert 1 in _decode_dict_keys(int, [1, 2, 3, 4], None)\ntest_23()\n\ndef test_26():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], True))\ntest_26()\n\ndef test_30():\n    assert all(\n        isinstance(i, int) for i in _decode_dict_keys(int, ['1', '2', '3'], False))\ntest_30()\n\ndef test_31():\n    assert \"1\" == next(_decode_dict_keys(None, [\"1\"], False))\ntest_31()\n\ndef test_34():\n    assert 1.0 == next(_decode_dict_keys(float, [1], True))\ntest_34()\n\ndef test_37():\n    assert \"1\" == next(_decode_dict_keys(str, [1], True))\ntest_37()\n\ndef test_39():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], True))\ntest_39()\n\ndef test_40():\n    assert \"1\" == next(_decode_dict_keys(str, [\"1\"], False))\ntest_40()\n\ndef test_44():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], True))\ntest_44()\n\ndef test_49():\n    assert [str(i) for i in range(10)] == list(_decode_dict_keys(str, range(10), True))\ntest_49()\n\ndef test_52():\n    assert 1 == len(list(_decode_dict_keys(int, [1], True)))\ntest_52()\n\ndef test_60():\n    assert \"1\" == next(_decode_dict_keys(str, [1], False))\ntest_60()\n\ndef test_63():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], True))\ntest_63()\n\ndef test_66():\n    assert 1 == next(_decode_dict_keys(Any, [1], False))\ntest_66()\n\ndef test_74():\n    assert '1' in _decode_dict_keys(str, [1, 2, 3, 4], None)\ntest_74()\n\ndef test_79():\n    assert \"1\" == next(_decode_dict_keys(Any, [\"1\"], False))\ntest_79()\n\ndef test_82():\n    assert 1.0 == next(_decode_dict_keys(float, [\"1\"], False))\ntest_82()\n\ndef test_86():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], True)))\ntest_86()\n\ndef test_88():\n    assert 1 == len(list(_decode_dict_keys(int, [\"1\"], False)))\ntest_88()\n\ndef test_94():\n    assert \"123\" == list(_decode_dict_keys(str, [\"123\"], True))[0]\ntest_94()\n\ndef test_97():\n    assert 1 == next(_decode_dict_keys(Any, [1], True))\ntest_97()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(int, [1, 2, 3], True))) == output\ntest_7()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [\"123\"], True)) == output\ntest_21()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(str, [1, 2, 3], False)) == output\ntest_36()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert sum(list(_decode_dict_keys(int, {\"1\": 0, \"2\": 1, \"3\": 2}, True))) == output\ntest_38()\n\ndef test_45():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(None, [1, 2, 3], False)) == output\ntest_45()\n\ndef test_53():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [\"123\"], True)) == output\ntest_53()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(map(type, _decode_dict_keys(Any, [1, 2, 3], True))) == output\ntest_70()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(Any, [1, 2, 3], False)) == output\ntest_75()\n\ndef test_76():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_76\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_dict_keys(int, ['1', '2', '3'], False)) == output\ntest_76()\n\ndef test_95():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_dict_keys/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert dict(zip(\n        _decode_dict_keys(int, ['1', '2'], True), [1, 2])) == output\ntest_95()\n\n\n"]}
{"task_id": 347, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from typing import TypeVar, _GenericAlias, get_args, get_origin\n    from dataclasses import is_dataclass\n\n    # Helper function to decode a single item\n    def decode_one(item, typ):\n        # If type argument is a type variable, try to infer from the item\n        if isinstance(typ, TypeVar):\n            if is_dataclass(item):\n                return item\n            return item\n        # If type_arg is a generic alias (like List[int], Optional[str])\n        origin = get_origin(typ)\n        if origin is not None:\n            # For containers like List, Tuple, Dict, recursively decode\n            args = get_args(typ)\n            if origin is list and args:\n                return [decode_one(i, args[0]) for i in item]\n            elif origin is tuple and args:\n                if len(args) == 2 and args[1] is Ellipsis:\n                    return tuple(decode_one(i, args[0]) for i in item)\n                else:\n                    return tuple(decode_one(i, t) for i, t in zip(item, args))\n            elif origin is dict and args:\n                return {decode_one(k, args[0]): decode_one(v, args[1]) for k, v in item.items()}\n            else:\n                # fallback for other generics\n                return item\n        # Non-generic, just check if infer_missing is True and try casting\n        if infer_missing:\n            try:\n                return typ(item)\n            except Exception:\n                return item\n        return item\n\n    return [decode_one(x, type_arg) for x in xs]", "def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    from typing import TypeVar, _GenericAlias\n\n    # Helper to check if type_arg is a TypeVar\n    def _is_typevar(t):\n        return isinstance(t, TypeVar)\n\n    # Helper to check if type_arg is generic\n    def _is_generic(t):\n        return isinstance(t, _GenericAlias)\n\n    decoded = []\n    for v in xs:\n        # If the type is generic, use the annotated type directly\n        if _is_generic(type_arg):\n            # Attempt to create an instance of type_arg with the value v\n            try:\n                decoded.append(type_arg(v))\n            except Exception:\n                decoded.append(v)\n        # If the type_arg is a TypeVar and v is dataclass, infer type from v\n        elif _is_typevar(type_arg) and is_dataclass(v):\n            decoded.append(v)\n        # If infer_missing is True, try to infer type from the value if no annotation\n        elif infer_missing:\n            decoded.append(v)\n        else:\n            decoded.append(v)\n    return decoded", "def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    from typing import TypeVar, _GenericAlias\n\n    # If type_arg is a TypeVar, try to infer concrete type from xs\n    if isinstance(type_arg, TypeVar):\n        # Attempt to infer type from the first item that is not None\n        for v in xs:\n            if v is not None:\n                if is_dataclass(v):\n                    # The type of v is the dataclass's type\n                    return [v]\n                else:\n                    # The type can be inferred from the actual type of v\n                    type_arg = type(v)\n                break\n        else:\n            # If no value found in xs, fallback to original type_arg\n            pass\n\n    # If type_arg is a generic (like List[int]) - _GenericAlias in Python 3.7+\n    if hasattr(type_arg, '__origin__') and type_arg.__origin__ is not None:\n        # We will decode each item according to the generic arguments if possible\n        origin = type_arg.__origin__\n        args = getattr(type_arg, '__args__', ())\n        if args:\n            # Single arg generics like List[int], Dict[str, int] etc\n            inner_type = args[0]\n            return [v if isinstance(v, inner_type) else v for v in xs]\n        else:\n            return xs\n\n    # Default fallback: just return xs as is\n    return xs", "def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from typing import TypeVar, _GenericAlias, get_origin, get_args\n    from dataclasses import is_dataclass\n\n    # Helper to decode a single item based on inferred or annotated type\n    def decode_item(type_hint, item):\n        # If item is None and infer_missing is True, return None directly\n        if item is None and infer_missing:\n            return None\n        origin = get_origin(type_hint)\n        args = get_args(type_hint)\n\n        # If the type_hint is still a TypeVar, try to infer real type from item\n        if isinstance(type_hint, TypeVar):\n            # If item is dataclass instance, use its type\n            if is_dataclass(item):\n                return item\n            # Otherwise try to decode based on item's type\n            else:\n                return item\n\n        # If generic alias (like List[int], Dict[str, int])\n        if origin is not None:\n            # Decode containers recursively\n            if origin in (list, tuple, set, frozenset):\n                # Assume single argument generics\n                inner_type = args[0] if args else type(None)\n                return origin(decode_item(inner_type, x) for x in item)\n            if origin is dict:\n                key_type = args[0] if len(args) > 0 else type(None)\n                val_type = args[1] if len(args) > 1 else type(None)\n                return {decode_item(key_type, k): decode_item(val_type, v) for k, v in item.items()}\n\n        # Otherwise, if the type_hint is a normal class/type, try recursive decode if dataclass\n        if is_dataclass(type_hint):\n            # For dataclasses, assume we have a from_dict or similar\n            # if item is a dict, create instance\n            if isinstance(item, dict):\n                # Assume a from_dict classmethod or similar deserialization\n                try:\n                    # If type_hint has a from_dict method\n                    if hasattr(type_hint, \"from_dict\"):\n                        return type_hint.from_dict(item)\n                    else:\n                        # Fallback: just instantiate with **item\n                        return type_hint(**item)\n                except Exception:\n                    return item\n            else:\n                return item\n\n        # Fallback: return item as is\n        return item\n\n    return [decode_item(type_arg, x) for x in xs]", "def _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    import typing\n\n    decoded_items = []\n    for v in xs:\n        actual_type = type_arg\n        # If type_arg is a type var or a generic, infer actual type from value\n        if hasattr(type_arg, '__origin__') and type_arg.__origin__ is not None:\n            actual_type = type_arg\n        elif hasattr(type_arg, '__constraints__') and type_arg.__constraints__:\n            # TypeVar with constraints, try to get the type from value\n            for constraint in type_arg.__constraints__:\n                if isinstance(v, constraint):\n                    actual_type = constraint\n                    break\n        elif hasattr(type_arg, '__bound__') and type_arg.__bound__ is not None:\n            if isinstance(v, type_arg.__bound__):\n                actual_type = type_arg.__bound__\n        else:\n            # If the type_arg is a typevar or unknown, try to get from instance\n            t = type(v)\n            if is_dataclass(v):\n                actual_type = t\n            else:\n                actual_type = type_arg\n\n        # Use typing.get_origin and get_args for generics\n        origin = getattr(typing, 'get_origin', None)\n        args = getattr(typing, 'get_args', None)\n        if origin and args:\n            origin_type = origin(actual_type)\n            if origin_type in (list, tuple):\n                # for collections, recursively decode items\n                inner_type = args(actual_type)[0] if args(actual_type) else None\n                if inner_type:\n                    if origin_type is list:\n                        decoded_items.append([_decode_items(inner_type, v, infer_missing) for v in xs])\n                    elif origin_type is tuple:\n                        # Handle tuple of fixed length with possible heterogeneous types\n                        decoded_items.append(tuple(_decode_items(t, [elem], infer_missing)[0] for elem, t in zip(xs, args(actual_type))))\n                else:\n                    decoded_items.append(v)\n            else:\n                decoded_items.append(v)\n        else:\n            decoded_items.append(v)\n    # Flatten the case where nested lists were appended\n    # If decode produced a list-of-lists, flatten it into a single list\n    if decoded_items and isinstance(decoded_items[0], list) and len(decoded_items) == 1:\n        return decoded_items[0]\n    return decoded_items"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from typing import TypeVar, _GenericAlias, get_args, get_origin\n    from dataclasses import is_dataclass\n\n    # Helper function to decode a single item\n    def decode_one(item, typ):\n        # If type argument is a type variable, try to infer from the item\n        if isinstance(typ, TypeVar):\n            if is_dataclass(item):\n                return item\n            return item\n        # If type_arg is a generic alias (like List[int], Optional[str])\n        origin = get_origin(typ)\n        if origin is not None:\n            # For containers like List, Tuple, Dict, recursively decode\n            args = get_args(typ)\n            if origin is list and args:\n                return [decode_one(i, args[0]) for i in item]\n            elif origin is tuple and args:\n                if len(args) == 2 and args[1] is Ellipsis:\n                    return tuple(decode_one(i, args[0]) for i in item)\n                else:\n                    return tuple(decode_one(i, t) for i, t in zip(item, args))\n            elif origin is dict and args:\n                return {decode_one(k, args[0]): decode_one(v, args[1]) for k, v in item.items()}\n            else:\n                # fallback for other generics\n                return item\n        # Non-generic, just check if infer_missing is True and try casting\n        if infer_missing:\n            try:\n                return typ(item)\n            except Exception:\n                return item\n        return item\n\n    return [decode_one(x, type_arg) for x in xs]\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    from typing import TypeVar, _GenericAlias\n\n    # Helper to check if type_arg is a TypeVar\n    def _is_typevar(t):\n        return isinstance(t, TypeVar)\n\n    # Helper to check if type_arg is generic\n    def _is_generic(t):\n        return isinstance(t, _GenericAlias)\n\n    decoded = []\n    for v in xs:\n        # If the type is generic, use the annotated type directly\n        if _is_generic(type_arg):\n            # Attempt to create an instance of type_arg with the value v\n            try:\n                decoded.append(type_arg(v))\n            except Exception:\n                decoded.append(v)\n        # If the type_arg is a TypeVar and v is dataclass, infer type from v\n        elif _is_typevar(type_arg) and is_dataclass(v):\n            decoded.append(v)\n        # If infer_missing is True, try to infer type from the value if no annotation\n        elif infer_missing:\n            decoded.append(v)\n        else:\n            decoded.append(v)\n    return decoded\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    from typing import TypeVar, _GenericAlias\n\n    # If type_arg is a TypeVar, try to infer concrete type from xs\n    if isinstance(type_arg, TypeVar):\n        # Attempt to infer type from the first item that is not None\n        for v in xs:\n            if v is not None:\n                if is_dataclass(v):\n                    # The type of v is the dataclass's type\n                    return [v]\n                else:\n                    # The type can be inferred from the actual type of v\n                    type_arg = type(v)\n                break\n        else:\n            # If no value found in xs, fallback to original type_arg\n            pass\n\n    # If type_arg is a generic (like List[int]) - _GenericAlias in Python 3.7+\n    if hasattr(type_arg, '__origin__') and type_arg.__origin__ is not None:\n        # We will decode each item according to the generic arguments if possible\n        origin = type_arg.__origin__\n        args = getattr(type_arg, '__args__', ())\n        if args:\n            # Single arg generics like List[int], Dict[str, int] etc\n            inner_type = args[0]\n            return [v if isinstance(v, inner_type) else v for v in xs]\n        else:\n            return xs\n\n    # Default fallback: just return xs as is\n    return xs\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from typing import TypeVar, _GenericAlias, get_origin, get_args\n    from dataclasses import is_dataclass\n\n    # Helper to decode a single item based on inferred or annotated type\n    def decode_item(type_hint, item):\n        # If item is None and infer_missing is True, return None directly\n        if item is None and infer_missing:\n            return None\n        origin = get_origin(type_hint)\n        args = get_args(type_hint)\n\n        # If the type_hint is still a TypeVar, try to infer real type from item\n        if isinstance(type_hint, TypeVar):\n            # If item is dataclass instance, use its type\n            if is_dataclass(item):\n                return item\n            # Otherwise try to decode based on item's type\n            else:\n                return item\n\n        # If generic alias (like List[int], Dict[str, int])\n        if origin is not None:\n            # Decode containers recursively\n            if origin in (list, tuple, set, frozenset):\n                # Assume single argument generics\n                inner_type = args[0] if args else type(None)\n                return origin(decode_item(inner_type, x) for x in item)\n            if origin is dict:\n                key_type = args[0] if len(args) > 0 else type(None)\n                val_type = args[1] if len(args) > 1 else type(None)\n                return {decode_item(key_type, k): decode_item(val_type, v) for k, v in item.items()}\n\n        # Otherwise, if the type_hint is a normal class/type, try recursive decode if dataclass\n        if is_dataclass(type_hint):\n            # For dataclasses, assume we have a from_dict or similar\n            # if item is a dict, create instance\n            if isinstance(item, dict):\n                # Assume a from_dict classmethod or similar deserialization\n                try:\n                    # If type_hint has a from_dict method\n                    if hasattr(type_hint, \"from_dict\"):\n                        return type_hint.from_dict(item)\n                    else:\n                        # Fallback: just instantiate with **item\n                        return type_hint(**item)\n                except Exception:\n                    return item\n            else:\n                return item\n\n        # Fallback: return item as is\n        return item\n\n    return [decode_item(type_arg, x) for x in xs]\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    from dataclasses import is_dataclass\n    import typing\n\n    decoded_items = []\n    for v in xs:\n        actual_type = type_arg\n        # If type_arg is a type var or a generic, infer actual type from value\n        if hasattr(type_arg, '__origin__') and type_arg.__origin__ is not None:\n            actual_type = type_arg\n        elif hasattr(type_arg, '__constraints__') and type_arg.__constraints__:\n            # TypeVar with constraints, try to get the type from value\n            for constraint in type_arg.__constraints__:\n                if isinstance(v, constraint):\n                    actual_type = constraint\n                    break\n        elif hasattr(type_arg, '__bound__') and type_arg.__bound__ is not None:\n            if isinstance(v, type_arg.__bound__):\n                actual_type = type_arg.__bound__\n        else:\n            # If the type_arg is a typevar or unknown, try to get from instance\n            t = type(v)\n            if is_dataclass(v):\n                actual_type = t\n            else:\n                actual_type = type_arg\n\n        # Use typing.get_origin and get_args for generics\n        origin = getattr(typing, 'get_origin', None)\n        args = getattr(typing, 'get_args', None)\n        if origin and args:\n            origin_type = origin(actual_type)\n            if origin_type in (list, tuple):\n                # for collections, recursively decode items\n                inner_type = args(actual_type)[0] if args(actual_type) else None\n                if inner_type:\n                    if origin_type is list:\n                        decoded_items.append([_decode_items(inner_type, v, infer_missing) for v in xs])\n                    elif origin_type is tuple:\n                        # Handle tuple of fixed length with possible heterogeneous types\n                        decoded_items.append(tuple(_decode_items(t, [elem], infer_missing)[0] for elem, t in zip(xs, args(actual_type))))\n                else:\n                    decoded_items.append(v)\n            else:\n                decoded_items.append(v)\n        else:\n            decoded_items.append(v)\n    # Flatten the case where nested lists were appended\n    # If decode produced a list-of-lists, flatten it into a single list\n    if decoded_items and isinstance(decoded_items[0], list) and len(decoded_items) == 1:\n        return decoded_items[0]\n    return decoded_items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    if _is_dataclass_instance(obj):\n        result = []\n        for field in fields(obj):\n            value = _asdict(getattr(obj, field.name), encode_json=encode_json)\n            result.append((field.name, value))\n\n        result = _handle_undefined_parameters_safe(cls=obj, kvs=dict(result),\n                                                   usage=\"to\")\n        return _encode_overrides(dict(result), _user_overrides_or_exts(obj),\n                                 encode_json=encode_json)\n    elif isinstance(obj, Mapping):\n        return dict((_asdict(k, encode_json=encode_json),\n                     _asdict(v, encode_json=encode_json)) for k, v in\n                    obj.items())\n    elif isinstance(obj, Collection) and not isinstance(obj, str) \\\n            and not isinstance(obj, bytes):\n        return list(_asdict(v, encode_json=encode_json) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\nimport pickle\ndef test_0():\n    assert [None, None, None] == list(_decode_items(Optional[int], [None, None, None], True))\ntest_0()\n\ndef test_5():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], False))[0]\ntest_5()\n\ndef test_11():\n    assert [None, '2', None] == list(_decode_items(Union[int, str, None], [None, '2', None], True))\ntest_11()\n\ndef test_13():\n    assert [1,2,3] == list(_decode_items(Optional[int], [1,2,3], True))\ntest_13()\n\ndef test_14():\n    assert [1,2,3] == _decode_items(int, [1,2,3], True)\ntest_14()\n\ndef test_16():\n    assert [1, 2] == list(_decode_items(Union[int, str], [1, 2], False))\ntest_16()\n\ndef test_25():\n    assert [4, 4] == list(_decode_items(int, (4, 4), False))\ntest_25()\n\ndef test_27():\n    assert [1, 2, 3, 4, 5] == _decode_items(int, [1, 2, 3, 4, 5], False)\ntest_27()\n\ndef test_28():\n    assert [1, 2] == list(_decode_items(int, [1, 2], True))\ntest_28()\n\ndef test_29():\n    assert \"42\" == list(_decode_items(Union[str, int], [\"42\"], True))[0]\ntest_29()\n\ndef test_31():\n    assert [1, 2] == list(_decode_items(int, [1, 2], False))\ntest_31()\n\ndef test_32():\n    assert [Decimal(\"1.0\"), Decimal(\"2.0\"), Decimal(\"3.0\")] == _decode_items(Decimal, [1.0, 2.0, 3.0], True)\ntest_32()\n\ndef test_35():\n    assert _decode_items(str, [\"1\", \"2\", \"3\"], False) == [\"1\", \"2\", \"3\"]\ntest_35()\n\ndef test_36():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str, None], [1, '2', 3], True))\ntest_36()\n\ndef test_37():\n    assert 42 == list(_decode_items(Union[str, int], [42], True))[0]\ntest_37()\n\ndef test_41():\n    assert [None, None, None] == list(_decode_items(Union[int, str, None], [None, None, None], True))\ntest_41()\n\ndef test_42():\n    assert [None, 2, 3] == list(_decode_items(Optional[int], [None, 2, 3], True))\ntest_42()\n\ndef test_45():\n    assert 42 == list(_decode_items(int, [42], False))[0]\ntest_45()\n\ndef test_48():\n    assert [1,2,3] == list(_decode_items(int, [1,2,3], True))\ntest_48()\n\ndef test_50():\n    assert [1,2,3] == list(_decode_items(Any, [1,2,3], True))\ntest_50()\n\ndef test_61():\n    assert [\"test\", 42] == list(_decode_items(List[Union[str, int]], [[\"test\", 42]], True))[0]\ntest_61()\n\ndef test_63():\n    assert [1,2,3] == list(_decode_items(Union[int, str], [1,2,3], True))\ntest_63()\n\ndef test_68():\n    assert [1, '2', 3] == list(_decode_items(Union[int, str], [1, '2', 3], True))\ntest_68()\n\ndef test_69():\n    assert [\"Hello\", \"World\", \"!\"] == _decode_items(str, [\"Hello\", \"World\", \"!\"], False)\ntest_69()\n\ndef test_70():\n    assert _decode_items(str, [1, 2, 3], False) == [1, 2, 3]\ntest_70()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2, 3], True) == output\ntest_1()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(bool, [True, False], True) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(int, [1, 2], True) == output\ntest_7()\n\ndef test_10():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(int, [\"42\"], True)) == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], True)) == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [\"42\"], False)) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2021-10-20T21:00:00Z\", \"2021-10-20T22:00:00Z\"], True) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], True)) == output\ntest_23()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [\"2019-01-01T00:00:00Z\"], True) == output\ntest_33()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(float, [1.0, 2.0], True) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(datetime, [datetime(2020,1,1), datetime(2020,1,2)], True) == output\ntest_44()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(Decimal, [Decimal(1), Decimal(2)], True) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(complex, [1+0j, 2+0j], True) == output\ntest_52()\n\ndef test_54():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_54\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Union[str, int], [42], False)) == output\ntest_54()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _decode_items(str, [\"a\", \"b\"], True) == output\ntest_56()\n\ndef test_57():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(Tuple[str, int], [[\"test\", 42]], False)) == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_decode_items/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert list(_decode_items(str, [42], False)) == output\ntest_60()\n\n\n"]}
{"task_id": 348, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.core", "predictions": ["def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    # Define a recursive function to convert dataclasses and collections to dicts\n    def _convert(obj_inner):\n        if hasattr(obj_inner, \"__dataclass_fields__\"):\n            result = {}\n            for f in obj_inner.__dataclass_fields__:\n                value = getattr(obj_inner, f)\n                result[f] = _convert(value)\n            return result\n        elif isinstance(obj_inner, Mapping):\n            return obj_inner.__class__((_convert(k), _convert(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            return obj_inner.__class__(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    result = _convert(obj)\n    if encode_json:\n        import json\n        return json.dumps(result)\n    return result", "def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    def _convert(obj):\n        if hasattr(obj, \"__dataclass_fields__\"):\n            result = {}\n            for f in obj.__dataclass_fields__:  # type: ignore\n                value = getattr(obj, f)\n                result[f] = _convert(value)\n            return result\n        elif isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        elif isinstance(obj, str):\n            # Strings are also collections, but we want to keep them as is\n            return obj\n        elif isinstance(obj, Collection):\n            return type(obj)(_convert(v) for v in obj)\n        else:\n            return obj\n\n    converted = _convert(obj)\n    if encode_json:\n        try:\n            return json.dumps(converted)\n        except (TypeError, OverflowError):\n            # If cannot serialize, just return the converted dict\n            return converted\n    return converted", "def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    import dataclasses\n\n    def _convert(obj):\n        if dataclasses.is_dataclass(obj) and not isinstance(obj, type):\n            result = {}\n            for f in dataclasses.fields(obj):\n                value = getattr(obj, f.name)\n                result[f.name] = _convert(value)\n            return result\n        elif isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        elif isinstance(obj, Collection) and not isinstance(obj, (str, bytes, bytearray)):\n            return type(obj)(_convert(v) for v in obj)\n        else:\n            if encode_json:\n                try:\n                    json.dumps(obj)\n                    return obj\n                except (TypeError, OverflowError):\n                    return json.dumps(obj, default=str)\n            else:\n                return obj\n\n    return _convert(obj)", "def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import dataclasses\n    import json\n    \n    def _serialize(o):\n        if dataclasses.is_dataclass(o) and not isinstance(o, type):\n            result = {}\n            for f in dataclasses.fields(o):\n                value = getattr(o, f.name)\n                result[f.name] = _serialize(value)\n            return result\n        elif isinstance(o, Mapping):\n            return type(o)((_serialize(k), _serialize(v)) for k, v in o.items())\n        elif isinstance(o, Collection) and not isinstance(o, (str, bytes, bytearray)):\n            return type(o)(_serialize(v) for v in o)\n        else:\n            return o\n\n    result = _serialize(obj)\n    if encode_json:\n        return json.dumps(result)\n    return result", "def _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    \n    def _convert(obj):\n        # Terminal types that should be returned as is\n        if isinstance(obj, (str, bytes, int, float, bool, type(None))):\n            return obj\n        # Check for Mapping types\n        if isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        # Check for Collection but not str/type that is already checked\n        if isinstance(obj, Collection):\n            # If it's a dataclass, call _asdict recursively\n            if hasattr(obj, '__dataclass_fields__'):\n                return _asdict(obj, encode_json=encode_json)\n            return type(obj)(_convert(v) for v in obj)\n        # If obj is a dataclass instance (alternative check)\n        if hasattr(obj, '__dataclass_fields__'):\n            result = {}\n            for field in obj.__dataclass_fields__:\n                value = getattr(obj, field)\n                result[field] = _convert(value)\n            return result\n\n        # Fallback: if encode_json is True, try json serialization\n        if encode_json:\n            try:\n                return json.loads(json.dumps(obj))\n            except Exception:\n                pass\n\n        return obj\n\n    return _convert(obj)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    # Define a recursive function to convert dataclasses and collections to dicts\n    def _convert(obj_inner):\n        if hasattr(obj_inner, \"__dataclass_fields__\"):\n            result = {}\n            for f in obj_inner.__dataclass_fields__:\n                value = getattr(obj_inner, f)\n                result[f] = _convert(value)\n            return result\n        elif isinstance(obj_inner, Mapping):\n            return obj_inner.__class__((_convert(k), _convert(v)) for k, v in obj_inner.items())\n        elif isinstance(obj_inner, Collection) and not isinstance(obj_inner, (str, bytes)):\n            return obj_inner.__class__(_convert(v) for v in obj_inner)\n        else:\n            return obj_inner\n\n    result = _convert(obj)\n    if encode_json:\n        import json\n        return json.dumps(result)\n    return result\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    def _convert(obj):\n        if hasattr(obj, \"__dataclass_fields__\"):\n            result = {}\n            for f in obj.__dataclass_fields__:  # type: ignore\n                value = getattr(obj, f)\n                result[f] = _convert(value)\n            return result\n        elif isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        elif isinstance(obj, str):\n            # Strings are also collections, but we want to keep them as is\n            return obj\n        elif isinstance(obj, Collection):\n            return type(obj)(_convert(v) for v in obj)\n        else:\n            return obj\n\n    converted = _convert(obj)\n    if encode_json:\n        try:\n            return json.dumps(converted)\n        except (TypeError, OverflowError):\n            # If cannot serialize, just return the converted dict\n            return converted\n    return converted\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    import dataclasses\n\n    def _convert(obj):\n        if dataclasses.is_dataclass(obj) and not isinstance(obj, type):\n            result = {}\n            for f in dataclasses.fields(obj):\n                value = getattr(obj, f.name)\n                result[f.name] = _convert(value)\n            return result\n        elif isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        elif isinstance(obj, Collection) and not isinstance(obj, (str, bytes, bytearray)):\n            return type(obj)(_convert(v) for v in obj)\n        else:\n            if encode_json:\n                try:\n                    json.dumps(obj)\n                    return obj\n                except (TypeError, OverflowError):\n                    return json.dumps(obj, default=str)\n            else:\n                return obj\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import dataclasses\n    import json\n    \n    def _serialize(o):\n        if dataclasses.is_dataclass(o) and not isinstance(o, type):\n            result = {}\n            for f in dataclasses.fields(o):\n                value = getattr(o, f.name)\n                result[f.name] = _serialize(value)\n            return result\n        elif isinstance(o, Mapping):\n            return type(o)((_serialize(k), _serialize(v)) for k, v in o.items())\n        elif isinstance(o, Collection) and not isinstance(o, (str, bytes, bytearray)):\n            return type(o)(_serialize(v) for v in o)\n        else:\n            return o\n\n    result = _serialize(obj)\n    if encode_json:\n        return json.dumps(result)\n    return result\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport copy\nimport json\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom dataclasses import (MISSING,\n                         _is_dataclass_instance,\n                         fields,\n                         is_dataclass  # type: ignore\n                         )\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Collection, Mapping, Union, get_type_hints\nfrom uuid import UUID\n\nfrom typing_inspect import is_union_type \n\nfrom dataclasses_json import cfg\nfrom dataclasses_json.utils import (_get_type_cons,\n                                    _handle_undefined_parameters_safe,\n                                    _is_collection, _is_mapping, _is_new_type,\n                                    _is_optional, _isinstance_safe,\n                                    _issubclass_safe)\n\nJson = Union[dict, list, str, int, float, bool, None]\n\nconfs = ['encoder', 'decoder', 'mm_field', 'letter_case', 'exclude']\nFieldOverride = namedtuple('FieldOverride', confs)\n\n\nclass _ExtendedEncoder(json.JSONEncoder):\n    def default(self, o) -> Json:\n        result: Json\n        if _isinstance_safe(o, Collection):\n            if _isinstance_safe(o, Mapping):\n                result = dict(o)\n            else:\n                result = list(o)\n        elif _isinstance_safe(o, datetime):\n            result = o.timestamp()\n        elif _isinstance_safe(o, UUID):\n            result = str(o)\n        elif _isinstance_safe(o, Enum):\n            result = o.value\n        elif _isinstance_safe(o, Decimal):\n            result = str(o)\n        else:\n            result = json.JSONEncoder.default(self, o)\n        return result\n\n\ndef _user_overrides_or_exts(cls):\n    global_metadata = defaultdict(dict)\n    encoders = cfg.global_config.encoders\n    decoders = cfg.global_config.decoders\n    mm_fields = cfg.global_config.mm_fields\n    for field in fields(cls):\n        if field.type in encoders:\n            global_metadata[field.name]['encoder'] = encoders[field.type]\n        if field.type in decoders:\n            global_metadata[field.name]['decoder'] = decoders[field.type]\n        if field.type in mm_fields:\n            global_metadata[field.name]['mm_fields'] = mm_fields[field.type]\n    try:\n        cls_config = (cls.dataclass_json_config\n                      if cls.dataclass_json_config is not None else {})\n    except AttributeError:\n        cls_config = {}\n\n    overrides = {}\n    for field in fields(cls):\n        field_config = {}\n        # first apply global overrides or extensions\n        field_metadata = global_metadata[field.name]\n        if 'encoder' in field_metadata:\n            field_config['encoder'] = field_metadata['encoder']\n        if 'decoder' in field_metadata:\n            field_config['decoder'] = field_metadata['decoder']\n        if 'mm_field' in field_metadata:\n            field_config['mm_field'] = field_metadata['mm_field']\n        # then apply class-level overrides or extensions\n        field_config.update(cls_config)\n        # last apply field-level overrides or extensions\n        field_config.update(field.metadata.get('dataclasses_json', {}))\n        overrides[field.name] = FieldOverride(*map(field_config.get, confs))\n    return overrides\n\n\ndef _encode_json_type(value, default=_ExtendedEncoder().default):\n    if isinstance(value, Json.__args__):  # type: ignore\n        return value\n    return default(value)\n\n\ndef _encode_overrides(kvs, overrides, encode_json=False):\n    override_kvs = {}\n    for k, v in kvs.items():\n        if k in overrides:\n            exclude = overrides[k].exclude\n            # If the exclude predicate returns true, the key should be\n            #  excluded from encoding, so skip the rest of the loop\n            if exclude and exclude(v):\n                continue\n            letter_case = overrides[k].letter_case\n            original_key = k\n            k = letter_case(k) if letter_case is not None else k\n\n            encoder = overrides[original_key].encoder\n            v = encoder(v) if encoder is not None else v\n\n        if encode_json:\n            v = _encode_json_type(v)\n        override_kvs[k] = v\n    return override_kvs\n\n\ndef _decode_letter_case_overrides(field_names, overrides):\n    \"\"\"Override letter case of field names for encode/decode\"\"\"\n    names = {}\n    for field_name in field_names:\n        field_override = overrides.get(field_name)\n        if field_override is not None:\n            letter_case = field_override.letter_case\n            if letter_case is not None:\n                names[letter_case(field_name)] = field_name\n    return names\n\n\ndef _decode_dataclass(cls, kvs, infer_missing):\n    if isinstance(kvs, cls):\n        return kvs\n    overrides = _user_overrides_or_exts(cls)\n    kvs = {} if kvs is None and infer_missing else kvs\n    field_names = [field.name for field in fields(cls)]\n    decode_names = _decode_letter_case_overrides(field_names, overrides)\n    kvs = {decode_names.get(k, k): v for k, v in kvs.items()}\n    missing_fields = {field for field in fields(cls) if field.name not in kvs}\n\n    for field in missing_fields:\n        if field.default is not MISSING:\n            kvs[field.name] = field.default\n        elif field.default_factory is not MISSING:\n            kvs[field.name] = field.default_factory()\n        elif infer_missing:\n            kvs[field.name] = None\n\n    # Perform undefined parameter action\n    kvs = _handle_undefined_parameters_safe(cls, kvs, usage=\"from\")\n\n    init_kwargs = {}\n    types = get_type_hints(cls)\n    for field in fields(cls):\n        # The field should be skipped from being added\n        # to init_kwargs as it's not intended as a constructor argument.\n        if not field.init:\n            continue\n\n        field_value = kvs[field.name]\n        field_type = types[field.name]\n        if field_value is None and not _is_optional(field_type):\n            warning = (f\"value of non-optional type {field.name} detected \"\n                       f\"when decoding {cls.__name__}\")\n            if infer_missing:\n                warnings.warn(\n                    f\"Missing {warning} and was defaulted to None by \"\n                    f\"infer_missing=True. \"\n                    f\"Set infer_missing=False (the default) to prevent this \"\n                    f\"behavior.\", RuntimeWarning)\n            else:\n                warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n            init_kwargs[field.name] = field_value\n            continue\n\n        while True:\n            if not _is_new_type(field_type):\n                break\n\n            field_type = field_type.__supertype__\n\n        if (field.name in overrides\n                and overrides[field.name].decoder is not None):\n            # FIXME hack\n            if field_type is type(field_value):\n                init_kwargs[field.name] = field_value\n            else:\n                init_kwargs[field.name] = overrides[field.name].decoder(\n                    field_value)\n        elif is_dataclass(field_type):\n            # FIXME this is a band-aid to deal with the value already being\n            # serialized when handling nested marshmallow schema\n            # proper fix is to investigate the marshmallow schema generation\n            # code\n            if is_dataclass(field_value):\n                value = field_value\n            else:\n                value = _decode_dataclass(field_type, field_value,\n                                          infer_missing)\n            init_kwargs[field.name] = value\n        elif _is_supported_generic(field_type) and field_type != str:\n            init_kwargs[field.name] = _decode_generic(field_type,\n                                                      field_value,\n                                                      infer_missing)\n        else:\n            init_kwargs[field.name] = _support_extended_types(field_type,\n                                                              field_value)\n\n    return cls(**init_kwargs)\n\n\ndef _support_extended_types(field_type, field_value):\n    if _issubclass_safe(field_type, datetime):\n        # FIXME this is a hack to deal with mm already decoding\n        # the issue is we want to leverage mm fields' missing argument\n        # but need this for the object creation hook\n        if isinstance(field_value, datetime):\n            res = field_value\n        else:\n            tz = datetime.now(timezone.utc).astimezone().tzinfo\n            res = datetime.fromtimestamp(field_value, tz=tz)\n    elif _issubclass_safe(field_type, Decimal):\n        res = (field_value\n               if isinstance(field_value, Decimal)\n               else Decimal(field_value))\n    elif _issubclass_safe(field_type, UUID):\n        res = (field_value\n               if isinstance(field_value, UUID)\n               else UUID(field_value))\n    else:\n        res = field_value\n    return res\n\n\ndef _is_supported_generic(type_):\n    not_str = not _issubclass_safe(type_, str)\n    is_enum = _issubclass_safe(type_, Enum)\n    return (not_str and _is_collection(type_)) or _is_optional(\n        type_) or is_union_type(type_) or is_enum\n\n\ndef _decode_generic(type_, value, infer_missing):\n    if value is None:\n        res = value\n    elif _issubclass_safe(type_, Enum):\n        # Convert to an Enum using the type as a constructor.\n        # Assumes a direct match is found.\n        res = type_(value)\n    # FIXME this is a hack to fix a deeper underlying issue. A refactor is due.\n    elif _is_collection(type_):\n        if _is_mapping(type_):\n            k_type, v_type = getattr(type_, \"__args__\", (Any, Any))\n            # a mapping type has `.keys()` and `.values()`\n            # (see collections.abc)\n            ks = _decode_dict_keys(k_type, value.keys(), infer_missing)\n            vs = _decode_items(v_type, value.values(), infer_missing)\n            xs = zip(ks, vs)\n        else:\n            xs = _decode_items(type_.__args__[0], value, infer_missing)\n\n        # get the constructor if using corresponding generic type in `typing`\n        # otherwise fallback on constructing using type_ itself\n        try:\n            res = _get_type_cons(type_)(xs)\n        except (TypeError, AttributeError):\n            res = type_(xs)\n    else:  # Optional or Union\n        if not hasattr(type_, \"__args__\"):\n            # Any, just accept\n            res = value\n        elif _is_optional(type_) and len(type_.__args__) == 2:  # Optional\n            type_arg = type_.__args__[0]\n            if is_dataclass(type_arg) or is_dataclass(value):\n                res = _decode_dataclass(type_arg, value, infer_missing)\n            elif _is_supported_generic(type_arg):\n                res = _decode_generic(type_arg, value, infer_missing)\n            else:\n                res = _support_extended_types(type_arg, value)\n        else:  # Union (already decoded or unsupported 'from_json' used)\n            res = value\n    return res\n\n\ndef _decode_dict_keys(key_type, xs, infer_missing):\n    \"\"\"\n    Because JSON object keys must be strs, we need the extra step of decoding\n    them back into the user's chosen python type\n    \"\"\"\n    # handle NoneType keys... it's weird to type a Dict as NoneType keys\n    # but it's valid...\n    key_type = ((lambda x: x) if key_type is None or key_type == Any\n                else key_type)  # noqa: E721\n    return map(key_type, _decode_items(key_type, xs, infer_missing))\n\n\ndef _decode_items(type_arg, xs, infer_missing):\n    \"\"\"\n    This is a tricky situation where we need to check both the annotated\n    type info (which is usually a type from `typing`) and check the\n    value's type directly using `type()`.\n\n    If the type_arg is a generic we can use the annotated type, but if the\n    type_arg is a typevar we need to extract the reified type information\n    hence the check of `is_dataclass(vs)`\n    \"\"\"\n    if is_dataclass(type_arg) or is_dataclass(xs):\n        items = (_decode_dataclass(type_arg, x, infer_missing)\n                 for x in xs)\n    elif _is_supported_generic(type_arg):\n        items = (_decode_generic(type_arg, x, infer_missing) for x in xs)\n    else:\n        items = xs\n    return items\n\n\ndef _asdict(obj, encode_json=False):\n    \"\"\"\n    A re-implementation of `asdict` (based on the original in the `dataclasses`\n    source) to support arbitrary Collection and Mapping types.\n    \"\"\"\n    from collections.abc import Mapping, Collection\n    import json\n    \n    def _convert(obj):\n        # Terminal types that should be returned as is\n        if isinstance(obj, (str, bytes, int, float, bool, type(None))):\n            return obj\n        # Check for Mapping types\n        if isinstance(obj, Mapping):\n            return type(obj)((_convert(k), _convert(v)) for k, v in obj.items())\n        # Check for Collection but not str/type that is already checked\n        if isinstance(obj, Collection):\n            # If it's a dataclass, call _asdict recursively\n            if hasattr(obj, '__dataclass_fields__'):\n                return _asdict(obj, encode_json=encode_json)\n            return type(obj)(_convert(v) for v in obj)\n        # If obj is a dataclass instance (alternative check)\n        if hasattr(obj, '__dataclass_fields__'):\n            result = {}\n            for field in obj.__dataclass_fields__:\n                value = getattr(obj, field)\n                result[field] = _convert(value)\n            return result\n\n        # Fallback: if encode_json is True, try json serialization\n        if encode_json:\n            try:\n                return json.loads(json.dumps(obj))\n            except Exception:\n                pass\n\n        return obj\n\n    return _convert(obj)\n\n\nimport pickle\ndef test_2():\n    assert _asdict([{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]) == [{\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}]\ntest_2()\n\ndef test_3():\n    assert _asdict([(\"a\", (\"b\", \"c\"),)]) == [[\"a\", [\"b\", \"c\"]]]\ntest_3()\n\ndef test_4():\n    assert _asdict(\n        frozenset({1, 2, 3, 4, 5, 6}), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_4()\n\ndef test_5():\n    assert _asdict(Decimal(2)) == 2\ntest_5()\n\ndef test_6():\n    assert _asdict([1,2,3]) == [1, 2, 3]\ntest_6()\n\ndef test_8():\n    assert _asdict(dict(x=1, y=dict(z=datetime(2018, 4, 1, 16, 30))), encode_json=False) == {\"x\": 1, \"y\": {\"z\": datetime(2018, 4, 1, 16, 30)}}\ntest_8()\n\ndef test_9():\n    assert _asdict({1: [2,3], 4: [5,6]}) == {1: [2,3], 4: [5,6]}\ntest_9()\n\ndef test_14():\n    assert _asdict(\"hello\", encode_json=True) == \"hello\"\ntest_14()\n\ndef test_15():\n    assert _asdict(\n        (1, 2, 3, 4, 5, 6), \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_15()\n\ndef test_17():\n    assert {\"a\": {\"a\": 1}} == _asdict({\"a\": {\"a\": 1}}, encode_json=False)\ntest_17()\n\ndef test_18():\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=False) == {\"x\": 1, \"y\": {\"z\": 123}}\ntest_18()\n\ndef test_19():\n    assert _asdict([{\"hello\":\"world\"},[\"hello\",\"world\"]]) == [{\"hello\":\"world\"},[\"hello\",\"world\"]]\ntest_19()\n\ndef test_20():\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n    ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_20()\n\ndef test_21():\n    assert _asdict(\"hello\") == \"hello\"\ntest_21()\n\ndef test_23():\n    assert _asdict({1: 'a', 2: 'b', 3: 'c'}) == {1: 'a', 2: 'b', 3: 'c'}\ntest_23()\n\ndef test_24():\n    assert _asdict(Decimal(\"1.0\")) == Decimal(\"1.0\")\ntest_24()\n\ndef test_26():\n    assert _asdict(tuple('abc')) == ['a','b','c']\ntest_26()\n\ndef test_28():\n    assert [1, 2, 3] == _asdict([1, 2, 3])\ntest_28()\n\ndef test_30():\n    assert _asdict({1: (2,3), 4: (5,6)}) == {1: [2,3], 4: [5,6]}\ntest_30()\n\ndef test_31():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=False) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_31()\n\ndef test_32():\n    assert _asdict({\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}) == {\"hello\":\"world\",\"hi\":[{\"hello\":\"world\"},[\"hello\",\"world\"]]}\ntest_32()\n\ndef test_35():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28)) == datetime(\n        2018, 11, 17, 16, 55, 28)\ntest_35()\n\ndef test_36():\n    assert _asdict(2) == 2\ntest_36()\n\ndef test_37():\n    assert _asdict(\n        {\n            'hello': {\n                'world': 1,\n                'what': 'is happening',\n                'today': ['should', 'be', 'good'],\n                'so': None,\n                'he': [{'should': 'be'}],\n                'here': {\n                    'in': [\n                        'the',\n                        'lab',\n                        'as',\n                        'well',\n                    ]\n                }\n            },\n            'oh': 'no',\n            'you': [1, 2, 3],\n        },\n        encode_json=False\n    ) == {\n        'hello': {\n            'world': 1,\n            'what': 'is happening',\n            'today': ['should', 'be', 'good'],\n            'so': None,\n            'he': [{'should': 'be'}],\n            'here': {\n                'in': [\n                    'the',\n                    'lab',\n                    'as',\n                    'well',\n                ]\n            }\n        },\n        'oh': 'no',\n        'you': [1, 2, 3],\n    }\ntest_37()\n\ndef test_38():\n    assert _asdict(True) is True\ntest_38()\n\ndef test_40():\n    assert _asdict(True, encode_json=False) == True\ntest_40()\n\ndef test_43():\n    assert {'key': 'value'} == _asdict({'key': 'value'})\ntest_43()\n\ndef test_45():\n    assert _asdict((\"a\", (\"b\", \"c\"),)) == [\"a\", [\"b\", \"c\"]]\ntest_45()\n\ndef test_46():\n    assert _asdict({'a':[1,2,3], 'b': {'c': [4,5,6]}}) == {'a': [1, 2, 3], 'b': {'c': [4, 5, 6]}}\ntest_46()\n\ndef test_47():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"a\": [1]}}, encode_json=False)\ntest_47()\n\ndef test_48():\n    assert {\"a\": [1], \"b\": [2]} == _asdict({\"a\": [1], \"b\": [2]}, encode_json=False)\ntest_48()\n\ndef test_49():\n    assert _asdict(\n        {1, 2, 3, 4, 5, 6}, \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_49()\n\ndef test_52():\n    assert ['item1', 'item2'] == _asdict(['item1', 'item2'])\ntest_52()\n\ndef test_54():\n    assert _asdict(1, encode_json=False) == 1\ntest_54()\n\ndef test_55():\n    assert [[1, 2], [3]] == _asdict([[1, 2], [3]])\ntest_55()\n\ndef test_57():\n    assert _asdict(UUID(\"12345678-1234-5678-1234-567812345678\")) == UUID(\n        \"12345678-1234-5678-1234-567812345678\")\ntest_57()\n\ndef test_58():\n    assert _asdict(None) is None\ntest_58()\n\ndef test_59():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567)\ntest_59()\n\ndef test_60():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=False) == {\"a\":1, \"b\":2, \"c\":3}\ntest_60()\n\ndef test_61():\n    assert _asdict(\n        [1, 2, 3, 4, 5, 6], \n        encode_json=False\n    ) == [1, 2, 3, 4, 5, 6]\ntest_61()\n\ndef test_62():\n    assert _asdict([{\"hello\":\"world\"}]) == [{\"hello\":\"world\"}]\ntest_62()\n\ndef test_65():\n    assert _asdict({'a':1, 'b':2}) == {'a': 1, 'b': 2}\ntest_65()\n\ndef test_67():\n    assert {'a': 1, 'b': 2} == _asdict({'a': 1, 'b': 2})\ntest_67()\n\ndef test_69():\n    assert _asdict({\"hello\":\"world\"}) == {\"hello\":\"world\"}\ntest_69()\n\ndef test_70():\n    assert _asdict(None) == None\ntest_70()\n\ndef test_74():\n    assert _asdict([1, 2, 3, 4, 5, 6], encode_json=False) == [1, 2, 3, 4, 5, 6]\ntest_74()\n\ndef test_75():\n    assert _asdict(1234) == 1234\ntest_75()\n\ndef test_79():\n    assert _asdict({'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}, encode_json=True) == {'a':[1,2,3], 'b':{'c':[1,2,3]}, 'd':[1,2,3]}\ntest_79()\n\ndef test_80():\n    assert _asdict((1,2,3)) == [1,2,3]\ntest_80()\n\ndef test_81():\n    assert {\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}} == _asdict({\"a\": {\"a\": [1], \"b\": {\"b\": 2}}, \"b\": {\"b\": [2]}}, encode_json=False)\ntest_81()\n\ndef test_83():\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=False) == {\"x\": 1, \"y\": {\"z\": [1, 2, 3, {\"a\": 1, \"b\": 2}]}}\ntest_83()\n\ndef test_84():\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=False) == {1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}\ntest_84()\n\ndef test_85():\n    assert 1 == _asdict(1)\ntest_85()\n\ndef test_86():\n    assert _asdict('a') == 'a'\ntest_86()\n\ndef test_87():\n    assert _asdict(1.234) == 1.234\ntest_87()\n\ndef test_88():\n    assert _asdict(1) == 1\ntest_88()\n\ndef test_89():\n    assert _asdict(datetime(2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)) == datetime(\n        2018, 11, 17, 16, 55, 28, 4567, tzinfo=timezone.utc)\ntest_89()\n\ndef test_91():\n    assert _asdict({\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}) == {\"a\": {\"b\": \"c\"}, \"d\": [1, 2, {\"e\": 3}]}\ntest_91()\n\ndef test_94():\n    assert _asdict(1.0) == 1.0\ntest_94()\n\ndef test_95():\n    assert [{'item1': 1}, {'item2': 2}] == _asdict([{'item1': 1}, {'item2': 2}])\ntest_95()\n\ndef test_96():\n    assert _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}}, encode_json=True)[\"x\"][\"y\"][\n        \"z\"][\"a\"] == 2\ntest_96()\n\ndef test_97():\n    assert _asdict(\n            {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=False\n        ) == {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}\ntest_97()\n\ndef test_103():\n    assert _asdict([\"hello\",\"world\"]) == [\"hello\",\"world\"]\ntest_103()\n\ndef test_104():\n    assert {\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}} == _asdict({\"a\": [{\"a\": 1, \"b\": {\"b\": 2}}], \"b\": {\"b\": 2}}, encode_json=False)\ntest_104()\n\ndef test_105():\n    assert {\"a\": [1]} == _asdict({\"a\": [1]}, encode_json=False)\ntest_105()\n\ndef test_107():\n    assert _asdict([1,2,3]) == [1,2,3]\ntest_107()\n\ndef test_108():\n    assert _asdict({1: {'a': 'b'}, 4: {'c': 'd'}}) == {1: {'a': 'b'}, 4: {'c': 'd'}}\ntest_108()\n\ndef test_109():\n    assert {\"a\": 1} == _asdict({\"a\": 1}, encode_json=False)\ntest_109()\n\ndef test_111():\n    assert _asdict(False) == False\ntest_111()\n\ndef test_112():\n    assert _asdict(1.123, encode_json=False) == 1.123\ntest_112()\n\ndef test_114():\n    assert _asdict({'a':1, 'b':2, 'c':3}, encode_json=False) == {'a':1, 'b':2, 'c':3}\ntest_114()\n\ndef test_115():\n    assert _asdict(Decimal(2), encode_json=True) == 2\ntest_115()\n\ndef test_118():\n    assert _asdict(True) == True\ntest_118()\n\ndef test_119():\n    assert _asdict(\"1\") == \"1\"\ntest_119()\n\ndef test_120():\n    assert _asdict({1: 'first', 'a': {2: 'second', 'b': 'third'}}) == {1: 'first', 'a': {2: 'second', 'b': 'third'}}\ntest_120()\n\ndef test_123():\n    assert _asdict({'a':{'b':1}}, encode_json=False) == {'a':{'b':1}}\ntest_123()\n\ndef test_125():\n    assert {\"a\": {\"a\": 1}, \"b\": {\"b\": 2}} == _asdict({\"a\": {\"a\": 1}, \"b\": {\"b\": 2}}, encode_json=False)\ntest_125()\n\ndef test_127():\n    assert _asdict(1+2j) == 1+2j\ntest_127()\n\ndef test_130():\n    assert {\"key1\": 123, \"key2\": 456} == _asdict({\"key1\": 123, \"key2\": 456})\ntest_130()\n\ndef test_132():\n    assert _asdict(\n            ({\"c\": 1}, {\"d\": 2}), encode_json=False\n        ) == [{\"c\": 1}, {\"d\": 2}]\ntest_132()\n\ndef test_133():\n    assert {'a': 1} == _asdict({'a': 1})\ntest_133()\n\ndef test_134():\n    assert 2 == _asdict(2)\ntest_134()\n\ndef test_135():\n    assert _asdict({'a':1, 'b':2}) == {'a':1, 'b':2}\ntest_135()\n\ndef test_136():\n    assert [1, '2', [3, 4]] == _asdict([1, '2', [3, 4]])\ntest_136()\n\ndef test_137():\n    assert {\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456} == _asdict({\"key1\": {\"key1.1\": 123, \"key1.2\": 456}, \"key2\": 456})\ntest_137()\n\ndef test_139():\n    assert {'key': 1} == _asdict({'key': 1})\ntest_139()\n\ndef test_140():\n    assert {\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456} == _asdict({\"key1\": [123, {\"key1.1\": 123, \"key1.2\": 456}, 456], \"key2\": 456})\ntest_140()\n\ndef test_141():\n    assert _asdict({\"a\":1, \"b\":2, \"c\":3}, encode_json=True) == {\"a\":1, \"b\":2, \"c\":3}\ntest_141()\n\ndef test_142():\n    assert _asdict(\n        {1: 'one', 2: 'two', 3: 'three'}, \n        encode_json=False\n    ) == {1: 'one', 2: 'two', 3: 'three'}\ntest_142()\n\ndef test_145():\n    assert [1, 2] == _asdict([1, 2])\ntest_145()\n\ndef test_146():\n    assert 2 == _asdict({\"x\": {\"y\": {\"z\": {\"a\": 2}}}})[\"x\"][\"y\"][\"z\"][\"a\"]\ntest_146()\n\ndef test_147():\n    assert _asdict(2, encode_json=True) == 2\ntest_147()\n\ndef test_149():\n    assert _asdict({'1': 'a', '2': 'b', '3': 'c'}) == {'1': 'a', '2': 'b', '3': 'c'}\ntest_149()\n\ndef test_152():\n    assert _asdict({1: {'a', 'b'}, 4: {'c'}}) == {1: ['a','b'], 4: ['c']}\ntest_152()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_0()\n\ndef test_1():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_1\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}, encode_json=True) == output\ntest_1()\n\ndef test_7():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": (123, {\"key1.1\": 123, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_7()\n\ndef test_12():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=123)), encode_json=True) == output\ntest_12()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')(namedtuple('PersonName', 'first_name last_name')('John', 'Doe'), 25)) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(1, encode_json=True) == output\ntest_27()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(dict(x=1, y=dict(z=[1,2,3,{\"a\":1,\"b\":2}])), encode_json=True) == output\ntest_33()\n\ndef test_39():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_39\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45,tzinfo=timezone.utc)) == output\ntest_39()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"{12345678-1234-5678-1234-567812345678}\")) == output\ntest_41()\n\ndef test_42():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_42\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(Decimal(\"3.14159265359\")) == output\ntest_42()\n\ndef test_50():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"key1\": ({\"key1.1\": {\"key1.1.1\": 123, \"key1.1.2\": 456}, \"key1.2\": 456}, 456), \"key2\": 456}) == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a':1, 'b':2}, encode_json=True) == output\ntest_51()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(UUID(\"7e9d206b-dc02-4240-8bdb-ffa0ff505cca\")) == output\ntest_56()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2021, 4, 2, 20, 30, 0, tzinfo=timezone.utc), encode_json=False) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=True) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({1: 2, 3: {4: 5}, 6: {7: [8, 9, {10: 11}]}}, encode_json=True) == output\ntest_66()\n\ndef test_78():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict((1,2,3)) == output\ntest_78()\n\ndef test_92():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': 1, 'b': 2}) == output\ntest_92()\n\ndef test_100():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'key': {'subkey': 'value'}},\n                                       encode_json=False) == output\ntest_100()\n\ndef test_102():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_102\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc), encode_json=True) == output\ntest_102()\n\ndef test_113():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_113\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_113()\n\ndef test_117():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2018, 11, 17, 20, 53, 0, 173430,\n                            tzinfo=timezone.utc)) == output\ntest_117()\n\ndef test_121():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_121\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict([1, 2, 3], encode_json=True) == output\ntest_121()\n\ndef test_124():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(namedtuple('Person', 'name age')('John Doe', 25)) == output\ntest_124()\n\ndef test_126():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_126\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(datetime(2020,12,31,10,30,45)) == output\ntest_126()\n\ndef test_128():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_128\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({'a': {'b': 1, 'c': 2}}) == output\ntest_128()\n\ndef test_144():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_144\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict('a', encode_json=True) == output\ntest_144()\n\ndef test_150():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict(\n        {\"a\": 1, \"b\": 2, \"c\": [1, 2], \"d\": {\"x\": 5, \"y\": 10}}, encode_json=True\n    ) == output\ntest_150()\n\ndef test_151():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+core/_asdict/test_151\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _asdict({\"a\": 1}, encode_json=False) == output\ntest_151()\n\n\n"]}
{"task_id": 349, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.api", "predictions": ["def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Import here to avoid circular imports if needed\n        from dataclasses_json import DataClassJsonMixin\n\n        # Attach the mixin to the class dynamically\n        if not issubclass(cls, DataClassJsonMixin):\n            cls = type(cls.__name__, (DataClassJsonMixin, cls), dict(cls.__dict__))\n\n        # Configure letter_case and undefined if given\n        if letter_case is not None:\n            cls._dataclass_json_letter_case = letter_case\n        if undefined is not None:\n            cls._dataclass_json_undefined = undefined\n\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        cls = dataclass(cls)\n        cls = JsonMixin(cls, letter_case=letter_case, undefined=undefined)\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # This is where the actual decorator logic would be applied.\n        # For this example, we assume a _wrap_dataclass_json function exists\n        # that handles the decoration logic.\n        return _wrap_dataclass_json(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Attach serialization metadata to the class\n        cls._json_letter_case = letter_case\n        cls._json_undefined = undefined\n        # Normally would add methods or modify class here\n        # For example: cls.to_json = ...\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)", "def dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        from dataclasses_json import DataClassJsonMixin\n        # Create a new class that inherits from the original and DataClassJsonMixin\n        # to add JSON serialization/deserialization capabilities.\n        # If letter_case or undefined are specified, set them on the class's config.\n        mixin_bases = (DataClassJsonMixin,)\n        # If the class is already a dataclass, this will only add json methods\n        # without changing dataclass behavior.\n        # Compose new class dynamically.\n        class_name = cls.__name__\n        # Prepare config attributes dictionary\n        config_attrs = {}\n        if letter_case is not None:\n            config_attrs['letter_case'] = letter_case\n        if undefined is not None:\n            config_attrs['undefined'] = undefined\n\n        # Dynamically create a Config subclass if necessary\n        if config_attrs:\n            class Config:\n                pass\n            for k, v in config_attrs.items():\n                setattr(Config, k, v)\n            setattr(cls, 'Config', Config)\n\n        # Create new class with the mixin\n        new_cls = type(class_name, mixin_bases + (cls,), dict(cls.__dict__))\n        return new_cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Import here to avoid circular imports if needed\n        from dataclasses_json import DataClassJsonMixin\n\n        # Attach the mixin to the class dynamically\n        if not issubclass(cls, DataClassJsonMixin):\n            cls = type(cls.__name__, (DataClassJsonMixin, cls), dict(cls.__dict__))\n\n        # Configure letter_case and undefined if given\n        if letter_case is not None:\n            cls._dataclass_json_letter_case = letter_case\n        if undefined is not None:\n            cls._dataclass_json_undefined = undefined\n\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        cls = dataclass(cls)\n        cls = JsonMixin(cls, letter_case=letter_case, undefined=undefined)\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # This is where the actual decorator logic would be applied.\n        # For this example, we assume a _wrap_dataclass_json function exists\n        # that handles the decoration logic.\n        return _wrap_dataclass_json(cls, letter_case=letter_case, undefined=undefined)\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        # Attach serialization metadata to the class\n        cls._json_letter_case = letter_case\n        cls._json_undefined = undefined\n        # Normally would add methods or modify class here\n        # For example: cls.to_json = ...\n        return cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport abc\nimport json\nfrom enum import Enum\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar,\n                    Union)\n\nfrom stringcase import (camelcase, pascalcase, snakecase,\n                        spinalcase) \n\nfrom dataclasses_json.cfg import config\nfrom dataclasses_json.core import (Json, _ExtendedEncoder, _asdict,\n                                   _decode_dataclass)\nfrom dataclasses_json.mm import (JsonData, SchemaType, build_schema)\nfrom dataclasses_json.undefined import Undefined\nfrom dataclasses_json.utils import (_handle_undefined_parameters_safe,\n                                    _undefined_parameter_action_safe)\n\nA = TypeVar('A', bound=\"DataClassJsonMixin\")\nB = TypeVar('B')\nC = TypeVar('C')\nFields = List[Tuple[str, Any]]\n\n\nclass LetterCase(Enum):\n    CAMEL = camelcase\n    KEBAB = spinalcase\n    SNAKE = snakecase\n    PASCAL = pascalcase\n\n\nclass DataClassJsonMixin(abc.ABC):\n    \"\"\"\n    DataClassJsonMixin is an ABC that functions as a Mixin.\n\n    As with other ABCs, it should not be instantiated directly.\n    \"\"\"\n    dataclass_json_config = None\n\n    def to_json(self,\n                *,\n                skipkeys: bool = False,\n                ensure_ascii: bool = True,\n                check_circular: bool = True,\n                allow_nan: bool = True,\n                indent: Optional[Union[int, str]] = None,\n                separators: Tuple[str, str] = None,\n                default: Callable = None,\n                sort_keys: bool = False,\n                **kw) -> str:\n        return json.dumps(self.to_dict(encode_json=False),\n                          cls=_ExtendedEncoder,\n                          skipkeys=skipkeys,\n                          ensure_ascii=ensure_ascii,\n                          check_circular=check_circular,\n                          allow_nan=allow_nan,\n                          indent=indent,\n                          separators=separators,\n                          default=default,\n                          sort_keys=sort_keys,\n                          **kw)\n\n    @classmethod\n    def from_json(cls: Type[A],\n                  s: JsonData,\n                  *,\n                  parse_float=None,\n                  parse_int=None,\n                  parse_constant=None,\n                  infer_missing=False,\n                  **kw) -> A:\n        kvs = json.loads(s,\n                         parse_float=parse_float,\n                         parse_int=parse_int,\n                         parse_constant=parse_constant,\n                         **kw)\n        return cls.from_dict(kvs, infer_missing=infer_missing)\n\n    @classmethod\n    def from_dict(cls: Type[A],\n                  kvs: Json,\n                  *,\n                  infer_missing=False) -> A:\n        return _decode_dataclass(cls, kvs, infer_missing)\n\n    def to_dict(self, encode_json=False) -> Dict[str, Json]:\n        return _asdict(self, encode_json=encode_json)\n\n    @classmethod\n    def schema(cls: Type[A],\n               *,\n               infer_missing: bool = False,\n               only=None,\n               exclude=(),\n               many: bool = False,\n               context=None,\n               load_only=(),\n               dump_only=(),\n               partial: bool = False,\n               unknown=None) -> SchemaType:\n        Schema = build_schema(cls, DataClassJsonMixin, infer_missing, partial)\n\n        if unknown is None:\n            undefined_parameter_action = _undefined_parameter_action_safe(cls)\n            if undefined_parameter_action is not None:\n                # We can just make use of the same-named mm keywords\n                unknown = undefined_parameter_action.name.lower()\n\n        return Schema(only=only,\n                      exclude=exclude,\n                      many=many,\n                      context=context,\n                      load_only=load_only,\n                      dump_only=dump_only,\n                      partial=partial,\n                      unknown=unknown)\n\n\ndef dataclass_json(_cls=None, *, letter_case=None,\n                   undefined: Optional[Union[str, Undefined]] = None):\n    \"\"\"\n    Based on the code in the `dataclasses` module to handle optional-parens\n    decorators. See example below:\n\n    @dataclass_json\n    @dataclass_json(letter_case=LetterCase.CAMEL)\n    class Example:\n    ...\n    \"\"\"\n    def wrap(cls):\n        from dataclasses_json import DataClassJsonMixin\n        # Create a new class that inherits from the original and DataClassJsonMixin\n        # to add JSON serialization/deserialization capabilities.\n        # If letter_case or undefined are specified, set them on the class's config.\n        mixin_bases = (DataClassJsonMixin,)\n        # If the class is already a dataclass, this will only add json methods\n        # without changing dataclass behavior.\n        # Compose new class dynamically.\n        class_name = cls.__name__\n        # Prepare config attributes dictionary\n        config_attrs = {}\n        if letter_case is not None:\n            config_attrs['letter_case'] = letter_case\n        if undefined is not None:\n            config_attrs['undefined'] = undefined\n\n        # Dynamically create a Config subclass if necessary\n        if config_attrs:\n            class Config:\n                pass\n            for k, v in config_attrs.items():\n                setattr(Config, k, v)\n            setattr(cls, 'Config', Config)\n\n        # Create new class with the mixin\n        new_cls = type(class_name, mixin_bases + (cls,), dict(cls.__dict__))\n        return new_cls\n\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)\n\n\ndef _process_class(cls, letter_case, undefined):\n    if letter_case is not None or undefined is not None:\n        cls.dataclass_json_config = config(letter_case=letter_case,\n                                           undefined=undefined)[\n            'dataclasses_json']\n\n    cls.to_json = DataClassJsonMixin.to_json\n    # unwrap and rewrap classmethod to tag it to cls rather than the literal\n    # DataClassJsonMixin ABC\n    cls.from_json = classmethod(DataClassJsonMixin.from_json.__func__)\n    cls.to_dict = DataClassJsonMixin.to_dict\n    cls.from_dict = classmethod(DataClassJsonMixin.from_dict.__func__)\n    cls.schema = classmethod(DataClassJsonMixin.schema.__func__)\n\n    cls.__init__ = _handle_undefined_parameters_safe(cls, kvs=(), usage=\"init\")\n    # register cls as a virtual subclass of DataClassJsonMixin\n    DataClassJsonMixin.register(cls)\n    return cls\n\n\nimport pickle\ndef test_4():\n    assert dataclass_json(letter_case=LetterCase.CAMEL) != dataclass_json()\ntest_4()\n\ndef test_5():\n    assert dataclass_json\ntest_5()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(letter_case=LetterCase.CAMEL), type) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(), type) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+api/dataclass_json/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(dataclass_json(undefined=Undefined.EXCLUDE), type) == output\ntest_3()\n\n\n"]}
{"task_id": 350, "project": "test-apps/dataclasses-json", "module": "dataclasses_json.utils", "predictions": ["def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        # Python 3.8+\n        from typing import get_origin\n        return get_origin(type_)\n    except ImportError:\n        pass\n\n    # For Python 3.7 and 3.6, manually try to get __origin__ attribute:\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n\n    # Fallback: for example, plain types don't have __origin__\n    return None", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, \"__origin__\"):\n        # Python 3.7+\n        return type_.__origin__\n    elif _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        # Python 3.7 internal _GenericAlias\n        return type_._special\n    elif hasattr(type_, \"__extra__\"):\n        # Python 3.6 typing\n        return type_.__extra__\n    else:\n        return None", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # Python 3.7+\n    except ImportError:\n        _GenericAlias = None\n\n    if _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        return type_.__origin__\n    else:\n        # For Python 3.6 and other typing implementations\n        return getattr(type_, '__origin__', None) or getattr(type_, '_gorg', None) or None", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # type: ignore\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n    elif _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        return type_._special\n    else:\n        return None", "def _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # type: ignore\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n    elif _GenericAlias and isinstance(type_, _GenericAlias):\n        return type_._origin\n    else:\n        return None"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        # Python 3.8+\n        from typing import get_origin\n        return get_origin(type_)\n    except ImportError:\n        pass\n\n    # For Python 3.7 and 3.6, manually try to get __origin__ attribute:\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n\n    # Fallback: for example, plain types don't have __origin__\n    return None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, \"__origin__\"):\n        # Python 3.7+\n        return type_.__origin__\n    elif _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        # Python 3.7 internal _GenericAlias\n        return type_._special\n    elif hasattr(type_, \"__extra__\"):\n        # Python 3.6 typing\n        return type_.__extra__\n    else:\n        return None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # Python 3.7+\n    except ImportError:\n        _GenericAlias = None\n\n    if _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        return type_.__origin__\n    else:\n        # For Python 3.6 and other typing implementations\n        return getattr(type_, '__origin__', None) or getattr(type_, '_gorg', None) or None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # type: ignore\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n    elif _GenericAlias is not None and isinstance(type_, _GenericAlias):\n        return type_._special\n    else:\n        return None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/dataclasses-json\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\nimport inspect\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Collection, Mapping, Optional, TypeVar, Any\n\n\ndef _get_type_cons(type_):\n    \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\"\n    if sys.version_info.minor == 6:\n        try:\n            cons = type_.__extra__\n        except AttributeError:\n            try:\n                cons = type_.__origin__\n            except AttributeError:\n                cons = type_\n            else:\n                cons = type_ if cons is None else cons\n        else:\n            try:\n                cons = type_.__origin__ if cons is None else cons\n            except AttributeError:\n                cons = type_\n    else:\n        cons = type_.__origin__\n    return cons\n\n\ndef _get_type_origin(type_):\n    \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in\n    the typing api\"\"\"\n    try:\n        from typing import _GenericAlias  # type: ignore\n    except ImportError:\n        _GenericAlias = None\n\n    if hasattr(type_, '__origin__'):\n        return type_.__origin__\n    elif _GenericAlias and isinstance(type_, _GenericAlias):\n        return type_._origin\n    else:\n        return None\n\n\ndef _hasargs(type_, *args):\n    try:\n        res = all(arg in type_.__args__ for arg in args)\n    except AttributeError:\n        return False\n    else:\n        return res\n\n\ndef _isinstance_safe(o, t):\n    try:\n        result = isinstance(o, t)\n    except Exception:\n        return False\n    else:\n        return result\n\n\ndef _issubclass_safe(cls, classinfo):\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return (_is_new_type_subclass_safe(cls, classinfo)\n                if _is_new_type(cls)\n                else False)\n\n\ndef _is_new_type_subclass_safe(cls, classinfo):\n    super_type = getattr(cls, \"__supertype__\", None)\n\n    if super_type:\n        return _is_new_type_subclass_safe(super_type, classinfo)\n\n    try:\n        return issubclass(cls, classinfo)\n    except Exception:\n        return False\n\n\ndef _is_new_type(type_):\n    return inspect.isfunction(type_) and hasattr(type_, \"__supertype__\")\n\n\ndef _is_optional(type_):\n    return (_issubclass_safe(type_, Optional) or\n            _hasargs(type_, type(None)) or\n            type_ is Any)\n\n\ndef _is_mapping(type_):\n    return _issubclass_safe(_get_type_origin(type_), Mapping)\n\n\ndef _is_collection(type_):\n    return _issubclass_safe(_get_type_origin(type_), Collection)\n\n\ndef _is_nonstr_collection(type_):\n    return (_issubclass_safe(_get_type_origin(type_), Collection)\n            and not _issubclass_safe(type_, str))\n\n\ndef _timestamp_to_dt_aware(timestamp: float):\n    tz = datetime.now(timezone.utc).astimezone().tzinfo\n    dt = datetime.fromtimestamp(timestamp, tz=tz)\n    return dt\n\n\ndef _undefined_parameter_action_safe(cls):\n    try:\n        if cls.dataclass_json_config is None:\n            return\n        action_enum = cls.dataclass_json_config['undefined']\n    except (AttributeError, KeyError):\n        return\n\n    if action_enum is None or action_enum.value is None:\n        return\n\n    return action_enum\n\n\ndef _handle_undefined_parameters_safe(cls, kvs, usage: str):\n    \"\"\"\n    Checks if an undefined parameters action is defined and performs the\n    according action.\n    \"\"\"\n    undefined_parameter_action = _undefined_parameter_action_safe(cls)\n    usage = usage.lower()\n    if undefined_parameter_action is None:\n        return kvs if usage != \"init\" else cls.__init__\n    if usage == \"from\":\n        return undefined_parameter_action.value.handle_from_dict(cls=cls,\n                                                                 kvs=kvs)\n    elif usage == \"to\":\n        return undefined_parameter_action.value.handle_to_dict(obj=cls,\n                                                               kvs=kvs)\n    elif usage == \"dump\":\n        return undefined_parameter_action.value.handle_dump(obj=cls)\n    elif usage == \"init\":\n        return undefined_parameter_action.value.create_init(obj=cls)\n    else:\n        raise ValueError(\n            f\"usage must be one of ['to', 'from', 'dump', 'init'], \"\n            f\"but is '{usage}'\")\n\n\nCatchAllVar = TypeVar(\"CatchAllVar\", bound=Mapping)\n\n\nimport pickle\ndef test_1():\n    assert _get_type_origin(Any) == Any\ntest_1()\n\ndef test_7():\n    assert _get_type_origin(int) == int\ntest_7()\n\ndef test_15():\n    assert isinstance(_get_type_origin(Optional[int]), type(Optional))\ntest_15()\n\ndef test_20():\n    assert (_get_type_origin(inspect.Signature) is inspect.Signature)\ntest_20()\n\ndef test_26():\n    assert _get_type_origin(Collection[str]) != Collection[str]\ntest_26()\n\ndef test_28():\n    assert list == _get_type_origin(list)\ntest_28()\n\ndef test_45():\n    assert _get_type_origin(Optional[str]) != Optional\ntest_45()\n\ndef test_46():\n    assert _get_type_origin(Optional) == Optional\ntest_46()\n\ndef test_48():\n    assert _get_type_origin(Mapping[str, str]) != Mapping[str, str]\ntest_48()\n\ndef test_50():\n    assert _get_type_origin(str) == str\ntest_50()\n\ndef test_53():\n    assert _get_type_origin(datetime) is datetime\ntest_53()\n\ndef test_55():\n    assert _get_type_origin(Optional[str]) != Optional[str]\ntest_55()\n\ndef test_57():\n    assert dict == _get_type_origin(dict)\ntest_57()\n\ndef test_58():\n    assert _get_type_origin(Any) is Any\ntest_58()\n\ndef test_61():\n    assert str == _get_type_origin(str)\ntest_61()\n\ndef test_65():\n    assert _get_type_origin(dict) is dict\ntest_65()\n\ndef test_67():\n    assert isinstance(_get_type_origin(Optional[Mapping[str, str]]), type(Optional))\ntest_67()\n\ndef test_0():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_0\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_0()\n\ndef test_2():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_2()\n\ndef test_3():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_3()\n\ndef test_4():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(type(None)) == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_6()\n\ndef test_8():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[int,int]]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_9()\n\ndef test_11():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_11\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_11()\n\ndef test_14():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_14\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_14()\n\ndef test_16():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,int]) == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Collection[Mapping[str, str]]), type(Collection)) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping), type(Mapping)) == output\ntest_18()\n\ndef test_19():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_19\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Mapping)) == output\ntest_19()\n\ndef test_21():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_21()\n\ndef test_22():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_22()\n\ndef test_23():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_23\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection) == output\ntest_23()\n\ndef test_24():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int,str]) == output\ntest_24()\n\ndef test_25():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_25()\n\ndef test_27():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_27()\n\ndef test_29():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[int]) == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_30()\n\ndef test_32():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_32\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, Collection[int]]]) == output\ntest_32()\n\ndef test_33():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_33\", \"rb\") as f:\n        output = pickle.load(f)\n    assert type(_get_type_origin(Optional[int])) == output\ntest_33()\n\ndef test_34():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_34\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_34()\n\ndef test_35():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_35\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_35()\n\ndef test_36():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_36\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_36()\n\ndef test_37():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_37\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_37()\n\ndef test_38():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_38\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_38()\n\ndef test_40():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[str]]) == output\ntest_41()\n\ndef test_43():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_43\", \"rb\") as f:\n        output = pickle.load(f)\n    assert isinstance(_get_type_origin(Mapping[str, str]), type(Mapping)) == output\ntest_43()\n\ndef test_44():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Collection[Mapping[str, int]]]) == output\ntest_44()\n\ndef test_47():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_47\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping) == output\ntest_47()\n\ndef test_49():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_49\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_49()\n\ndef test_51():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, int]) == output\ntest_51()\n\ndef test_52():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_52\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_52()\n\ndef test_56():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_56\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Any]) == output\ntest_56()\n\ndef test_60():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[Mapping[str, int]]) == output\ntest_60()\n\ndef test_62():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, str]) == output\ntest_62()\n\ndef test_63():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[float]) == output\ntest_63()\n\ndef test_64():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_64\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[Mapping[str, int]]) == output\ntest_64()\n\ndef test_66():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, Any]) == output\ntest_66()\n\ndef test_68():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[int]) == output\ntest_69()\n\ndef test_70():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Optional[str]) == output\ntest_70()\n\ndef test_72():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[str, str]) == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Collection[str]) == output\ntest_73()\n\ndef test_75():\n    with open(\"/output/test-apps+dataclasses-json/test_output/dataclasses_json+utils/_get_type_origin/test_75\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_type_origin(Mapping[int, int]) == output\ntest_75()\n\n\n"]}
