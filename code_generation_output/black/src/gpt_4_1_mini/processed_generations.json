{"task_id": 351, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["import re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n\n    # Per Python's tokenizer.c, encodings are normalized by:\n    # 1. Lowercasing the encoding name\n    # 2. Removing all non-alphanumeric characters: spaces, dashes, underscores, and dots.\n    # This makes e.g. 'UTF-8', 'utf_8', 'utf.8', 'uTf8' all normalize to 'utf8'.\n\n    # Lowercase the input\n    enc = orig_enc.lower()\n    # Remove characters space, dash, underscore, dot (and any non-alphanum actually)\n    # Original behavior strips '-', '_', '.', and spaces in the C source.\n    normalized = re.sub(r\"[-_.\\s]\", \"\", enc)\n    return normalized", "import re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # According to tokenizer.c from CPython:\n    # The encoding name should be normalized by:\n    # 1. Lowercasing and removing all non-alphanumeric characters (including dash and underscore).\n    # 2. Special handling: Replace any occurrences of 'utf8' with 'utf-8'.\n    #\n    # From the context it seems to:\n    #   - remove all characters except letters and digits\n    #   - lowercase the result\n    #   - then replace utf8 by utf-8\n\n    # Remove any character that is not alphanumeric\n    normalized = re.sub(r\"[^a-zA-Z0-9]\", \"\", orig_enc)\n    # lowercase\n    normalized = normalized.lower()\n    # Replace utf8 with utf-8\n    normalized = normalized.replace(\"utf8\", \"utf-8\")\n\n    return normalized", "import re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Normalize encoding name: strip spaces, convert to lowercase and keep only alphanumerics and '-' and '_'.\n    # According to Python tokenizer.c get_normal_name logic:\n    # 1. Strip leading/trailing whitespace\n    # 2. Lowercase the encoding name\n    # 3. Remove any characters not in [a-z0-9._-]\n    # 4. Replace multiple occurrences of '.' and '_' with single ones? (not strictly needed)\n    # 5. No whitespace inside final name\n\n    enc = orig_enc.strip().lower()\n    # Keep only ASCII lowercase letters, digits, underscores, dashes and dots as in Python standard encoding names\n    enc = re.sub(r\"[^a-z0-9._-]\", \"\", enc)\n    return enc", "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # According to Python tokenizer.c, normalizing encoding names means:\n    # - case-insensitive mapping (casefold)\n    # - replace any '_' or '-' with '-' to unify separators\n    # - replace 'latin-1' and 'iso-8859-1' with 'latin-1' normalized as 'latin-1'\n    # For simplicity, here we:\n    # - strip whitespace\n    # - lower-case the string\n    # - replace '_' by '-' (common normalization for encodings)\n    # - remove duplicates of '-' (not strictly necessary)\n    # This function might be called from detect_encoding to normalize cookie encoding names.\n\n    # Step 1: strip whitespace and lower case\n    enc = orig_enc.strip().casefold()\n\n    # Step 2: replace '_' by '-'\n    enc = enc.replace('_', '-')\n\n    # Additional common normalizations done in tokenizer.c:\n    # \"latin-1\" and \"iso-8859-1\" are equivalent, normalize both to \"latin-1\"\n    if enc in {\"iso-8859-1\", \"iso8859-1\"}:\n        enc = \"latin-1\"\n\n    # Remove any possible trailing whitespace again just in case\n    return enc", "import re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Per the C tokenizer's get_normal_name:\n    # - underscores are removed\n    # - name is lowered\n    # - encoding names are normalized by removing underscores and lowercasing\n    # Implemented by removing underscores and lowercasing\n    \n    return orig_enc.replace(\"_\", \"\").lower()"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\nimport re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n\n    # Per Python's tokenizer.c, encodings are normalized by:\n    # 1. Lowercasing the encoding name\n    # 2. Removing all non-alphanumeric characters: spaces, dashes, underscores, and dots.\n    # This makes e.g. 'UTF-8', 'utf_8', 'utf.8', 'uTf8' all normalize to 'utf8'.\n\n    # Lowercase the input\n    enc = orig_enc.lower()\n    # Remove characters space, dash, underscore, dot (and any non-alphanum actually)\n    # Original behavior strips '-', '_', '.', and spaces in the C source.\n    normalized = re.sub(r\"[-_.\\s]\", \"\", enc)\n    return normalized\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\nimport re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # According to tokenizer.c from CPython:\n    # The encoding name should be normalized by:\n    # 1. Lowercasing and removing all non-alphanumeric characters (including dash and underscore).\n    # 2. Special handling: Replace any occurrences of 'utf8' with 'utf-8'.\n    #\n    # From the context it seems to:\n    #   - remove all characters except letters and digits\n    #   - lowercase the result\n    #   - then replace utf8 by utf-8\n\n    # Remove any character that is not alphanumeric\n    normalized = re.sub(r\"[^a-zA-Z0-9]\", \"\", orig_enc)\n    # lowercase\n    normalized = normalized.lower()\n    # Replace utf8 with utf-8\n    normalized = normalized.replace(\"utf8\", \"utf-8\")\n\n    return normalized\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\nimport re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Normalize encoding name: strip spaces, convert to lowercase and keep only alphanumerics and '-' and '_'.\n    # According to Python tokenizer.c get_normal_name logic:\n    # 1. Strip leading/trailing whitespace\n    # 2. Lowercase the encoding name\n    # 3. Remove any characters not in [a-z0-9._-]\n    # 4. Replace multiple occurrences of '.' and '_' with single ones? (not strictly needed)\n    # 5. No whitespace inside final name\n\n    enc = orig_enc.strip().lower()\n    # Keep only ASCII lowercase letters, digits, underscores, dashes and dots as in Python standard encoding names\n    enc = re.sub(r\"[^a-z0-9._-]\", \"\", enc)\n    return enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # According to Python tokenizer.c, normalizing encoding names means:\n    # - case-insensitive mapping (casefold)\n    # - replace any '_' or '-' with '-' to unify separators\n    # - replace 'latin-1' and 'iso-8859-1' with 'latin-1' normalized as 'latin-1'\n    # For simplicity, here we:\n    # - strip whitespace\n    # - lower-case the string\n    # - replace '_' by '-' (common normalization for encodings)\n    # - remove duplicates of '-' (not strictly necessary)\n    # This function might be called from detect_encoding to normalize cookie encoding names.\n\n    # Step 1: strip whitespace and lower case\n    enc = orig_enc.strip().casefold()\n\n    # Step 2: replace '_' by '-'\n    enc = enc.replace('_', '-')\n\n    # Additional common normalizations done in tokenizer.c:\n    # \"latin-1\" and \"iso-8859-1\" are equivalent, normalize both to \"latin-1\"\n    if enc in {\"iso-8859-1\", \"iso8859-1\"}:\n        enc = \"latin-1\"\n\n    # Remove any possible trailing whitespace again just in case\n    return enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\nimport re\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Per the C tokenizer's get_normal_name:\n    # - underscores are removed\n    # - name is lowered\n    # - encoding names are normalized by removing underscores and lowercasing\n    # Implemented by removing underscores and lowercasing\n    \n    return orig_enc.replace(\"_\", \"\").lower()\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n"]}
{"task_id": 352, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["from typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, *, bom_found: bool) -> Optional[str]:\n    \"\"\"\n    Detect encoding cookie inside a bytes line.\n    Returns the normalized encoding as string if found and valid.\n    Raises SyntaxError if unknown encoding or incompatible with BOM.\n    \"\"\"\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n\n    if bom_found:\n        if codec.name != \"utf-8\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    # Read first line of input, or empty bytes if none\n    first_line = readline()\n    if not first_line:\n        # Empty file, default encoding utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    # Check for UTF-8 BOM at start of file\n    bom_found = first_line.startswith(BOM_UTF8)\n    if bom_found:\n        # Encoding is \"utf-8-sig\" if BOM detected\n        default_encoding = \"utf-8-sig\"\n    else:\n        default_encoding = \"utf-8\"\n\n    # Attempt to find encoding cookie in first line\n    cookie_encoding = find_cookie(first_line, bom_found=bom_found)\n\n    # If cookie encoding found, possibly return it after checks\n    if cookie_encoding is not None:\n        # If BOM and cookie disagree, raise SyntaxError\n        if bom_found and cookie_encoding != \"utf-8-sig\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        return cookie_encoding, lines\n\n    # No cookie in first line, read second line (at most twice)\n    second_line = readline()\n    if second_line:\n        lines.append(second_line)\n        cookie_encoding = find_cookie(second_line, bom_found=False)\n        if cookie_encoding is not None:\n            # If BOM was found in first line, but cookie is not utf-8-sig, error\n            if bom_found and cookie_encoding != \"utf-8-sig\":\n                raise SyntaxError(\"encoding problem: utf-8\")\n            return cookie_encoding, lines\n\n    # No cookie found, just return default encoding and the lines read\n    return default_encoding, lines", "from typing import Callable, List, Optional, Tuple\nfrom codecs import lookup, BOM_UTF8\nimport regex as re\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, *, bom_found: bool) -> Optional[str]:\n    \"\"\"\n    Try to detect the encoding cookie in a line and return the normalized encoding name.\n    Raise SyntaxError for unknown encoding or encoding conflicts (with BOM).\n    Return None if no cookie is found.\n    \"\"\"\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        # Mimic the Python interpreter behaviour for unknown encoding\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n\n    if bom_found:\n        if codec.name != \"utf-8\":\n            # Encoding conflict with BOM\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n    # Read the first line\n    try:\n        first_line = readline()\n    except Exception:\n        # In case readline fails (e.g. StopIteration), treat as EOF\n        first_line = b\"\"\n\n    if not first_line:\n        # Empty file, default utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Check for UTF-8 BOM at start of first line\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Remove BOM bytes from the first line (the rest lines left untouched)\n        first_line = first_line[len(BOM_UTF8) :]\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    # Try to find cookie in the first line if encoding not yet determined (or even if BOM found)\n    cookie_enc = find_cookie(first_line, bom_found=bom_found)\n    if cookie_enc:\n        if encoding:\n            # Both BOM and cookie present, must agree\n            if encoding != cookie_enc:\n                raise SyntaxError(\"encoding conflict: \" + encoding + \" vs \" + cookie_enc)\n        else:\n            encoding = cookie_enc\n\n    # Read second line if we need to search cookie beyond the first line,\n    # but only if encoding not found yet (there may be a cookie in line 2)\n    if (not encoding) and (not bom_found):\n        try:\n            second_line = readline()\n        except Exception:\n            second_line = b\"\"\n        if second_line:\n            lines.append(second_line)\n            cookie_enc = find_cookie(second_line, bom_found=bom_found)\n            if cookie_enc:\n                encoding = cookie_enc\n\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines", "from typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return b\"\"\n\n    # Read first line\n    first_line = read_or_stop()\n    lines.append(first_line)\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Check for BOM_UTF8 at start of first line\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        first_line = first_line[len(BOM_UTF8) :]\n        # Replace lines[0] with line sans BOM for cookie detection\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        enc = _get_normal_name(match.group(1))\n        try:\n            # Validate encoding by lookup\n            codec = lookup(enc)\n        except LookupError:\n            raise SyntaxError(\"unknown encoding: \" + enc)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # If BOM utf-8 found but cookie encoding is not utf-8 raise error\n                raise SyntaxError(\"encoding problem: utf-8\")\n            # If BOM found, add \"-sig\" to encoding to signal BOM aware\n            enc += \"-sig\"\n        return enc\n\n    # Try to find encoding cookie in first line (possibly after skipping BOM)\n    cookie_enc = find_cookie(first_line)\n    if cookie_enc:\n        encoding = cookie_enc\n\n    # If no cookie found on first line, try second line\n    if encoding is None:\n        second_line = read_or_stop()\n        lines.append(second_line)\n        cookie_enc = find_cookie(second_line)\n        if cookie_enc:\n            encoding = cookie_enc\n\n    if encoding is None:\n        encoding = \"utf-8\"\n\n    return encoding, lines", "from typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    # Read the first line (possibly containing BOM or encoding cookie)\n    first_line = readline()\n    if not first_line:\n        # empty file: default to utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Detect BOM UTF-8 marker\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Adjust first line to remove BOM bytes\n        first_line = first_line[len(BOM_UTF8):]\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_str = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        m = cookie_re.match(line_str)\n        if not m:\n            return None\n        enc = _get_normal_name(m.group(1))\n        try:\n            codec = lookup(enc)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {enc}\")\n        # If BOM found and codec is not utf-8, raise SyntaxError (later handled)\n        return codec.name\n\n    cookie_enc = find_cookie(first_line)\n    if cookie_enc:\n        if bom_found and cookie_enc != \"utf-8\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding = cookie_enc if not bom_found else \"utf-8-sig\"\n        return encoding, lines\n\n    # If no encoding found in first line, check second line\n    second_line = readline()\n    if second_line:\n        lines.append(second_line)\n        cookie_enc2 = find_cookie(second_line)\n        if cookie_enc2:\n            if bom_found and cookie_enc2 != \"utf-8\":\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding = cookie_enc2 if not bom_found else \"utf-8-sig\"\n            return encoding, lines\n\n    # If no BOM or cookie found, default encoding is utf-8\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines", "from typing import Callable, List, Tuple, Optional\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, bom_found: bool) -> Optional[str]:\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        # This behaviour mimics the Python interpreter\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n    if bom_found:\n        if codec.name != \"utf-8\":\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    # Read first line (possibly empty bytes at EOF)\n    first_line = readline()\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Strip BOM for cookie detection\n        line_strip_bom = first_line[len(BOM_UTF8):]\n        encoding = \"utf-8-sig\"\n    else:\n        bom_found = False\n        line_strip_bom = first_line\n\n    lines = [first_line]\n\n    # Check encoding cookie on first line\n    encoding_cookie = find_cookie(line_strip_bom, bom_found)\n\n    if encoding_cookie:\n        encoding = encoding_cookie\n\n    # If no cookie found, read a second line to check for cookie there\n    if not encoding_cookie:\n        second_line = readline()\n        if second_line:\n            lines.append(second_line)\n            second_encoding_cookie = find_cookie(second_line, bom_found=False)\n            if second_encoding_cookie:\n                encoding = second_encoding_cookie\n    else:\n        # Cookie was found on first line, do not read second line\n        second_encoding_cookie = None\n\n    # If BOM and cookie both found but disagree, raise SyntaxError\n    if bom_found and encoding_cookie:\n        # encoding from BOM is 'utf-8-sig', from cookie is something else possibly\n        # Normalize encoding_cookie for comparison\n        normalized_cookie_enc = encoding_cookie.lower().replace(\"_\", \"-\")\n        if normalized_cookie_enc != \"utf-8\" and normalized_cookie_enc != \"utf-8-sig\":\n            # This check is mostly covered by find_cookie raising already,\n            # but double check semantics here:\n            raise SyntaxError(\"encoding problem: utf-8\")\n        # Actually encoding should be 'utf-8-sig' with BOM and cookie utf-8\n        encoding = \"utf-8-sig\"\n\n    # If no encoding detected yet, default to 'utf-8'\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, *, bom_found: bool) -> Optional[str]:\n    \"\"\"\n    Detect encoding cookie inside a bytes line.\n    Returns the normalized encoding as string if found and valid.\n    Raises SyntaxError if unknown encoding or incompatible with BOM.\n    \"\"\"\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n\n    if bom_found:\n        if codec.name != \"utf-8\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    # Read first line of input, or empty bytes if none\n    first_line = readline()\n    if not first_line:\n        # Empty file, default encoding utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    # Check for UTF-8 BOM at start of file\n    bom_found = first_line.startswith(BOM_UTF8)\n    if bom_found:\n        # Encoding is \"utf-8-sig\" if BOM detected\n        default_encoding = \"utf-8-sig\"\n    else:\n        default_encoding = \"utf-8\"\n\n    # Attempt to find encoding cookie in first line\n    cookie_encoding = find_cookie(first_line, bom_found=bom_found)\n\n    # If cookie encoding found, possibly return it after checks\n    if cookie_encoding is not None:\n        # If BOM and cookie disagree, raise SyntaxError\n        if bom_found and cookie_encoding != \"utf-8-sig\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        return cookie_encoding, lines\n\n    # No cookie in first line, read second line (at most twice)\n    second_line = readline()\n    if second_line:\n        lines.append(second_line)\n        cookie_encoding = find_cookie(second_line, bom_found=False)\n        if cookie_encoding is not None:\n            # If BOM was found in first line, but cookie is not utf-8-sig, error\n            if bom_found and cookie_encoding != \"utf-8-sig\":\n                raise SyntaxError(\"encoding problem: utf-8\")\n            return cookie_encoding, lines\n\n    # No cookie found, just return default encoding and the lines read\n    return default_encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, List, Optional, Tuple\nfrom codecs import lookup, BOM_UTF8\nimport regex as re\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, *, bom_found: bool) -> Optional[str]:\n    \"\"\"\n    Try to detect the encoding cookie in a line and return the normalized encoding name.\n    Raise SyntaxError for unknown encoding or encoding conflicts (with BOM).\n    Return None if no cookie is found.\n    \"\"\"\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        # Mimic the Python interpreter behaviour for unknown encoding\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n\n    if bom_found:\n        if codec.name != \"utf-8\":\n            # Encoding conflict with BOM\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n    # Read the first line\n    try:\n        first_line = readline()\n    except Exception:\n        # In case readline fails (e.g. StopIteration), treat as EOF\n        first_line = b\"\"\n\n    if not first_line:\n        # Empty file, default utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Check for UTF-8 BOM at start of first line\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Remove BOM bytes from the first line (the rest lines left untouched)\n        first_line = first_line[len(BOM_UTF8) :]\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    # Try to find cookie in the first line if encoding not yet determined (or even if BOM found)\n    cookie_enc = find_cookie(first_line, bom_found=bom_found)\n    if cookie_enc:\n        if encoding:\n            # Both BOM and cookie present, must agree\n            if encoding != cookie_enc:\n                raise SyntaxError(\"encoding conflict: \" + encoding + \" vs \" + cookie_enc)\n        else:\n            encoding = cookie_enc\n\n    # Read second line if we need to search cookie beyond the first line,\n    # but only if encoding not found yet (there may be a cookie in line 2)\n    if (not encoding) and (not bom_found):\n        try:\n            second_line = readline()\n        except Exception:\n            second_line = b\"\"\n        if second_line:\n            lines.append(second_line)\n            cookie_enc = find_cookie(second_line, bom_found=bom_found)\n            if cookie_enc:\n                encoding = cookie_enc\n\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return b\"\"\n\n    # Read first line\n    first_line = read_or_stop()\n    lines.append(first_line)\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Check for BOM_UTF8 at start of first line\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        first_line = first_line[len(BOM_UTF8) :]\n        # Replace lines[0] with line sans BOM for cookie detection\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        enc = _get_normal_name(match.group(1))\n        try:\n            # Validate encoding by lookup\n            codec = lookup(enc)\n        except LookupError:\n            raise SyntaxError(\"unknown encoding: \" + enc)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # If BOM utf-8 found but cookie encoding is not utf-8 raise error\n                raise SyntaxError(\"encoding problem: utf-8\")\n            # If BOM found, add \"-sig\" to encoding to signal BOM aware\n            enc += \"-sig\"\n        return enc\n\n    # Try to find encoding cookie in first line (possibly after skipping BOM)\n    cookie_enc = find_cookie(first_line)\n    if cookie_enc:\n        encoding = cookie_enc\n\n    # If no cookie found on first line, try second line\n    if encoding is None:\n        second_line = read_or_stop()\n        lines.append(second_line)\n        cookie_enc = find_cookie(second_line)\n        if cookie_enc:\n            encoding = cookie_enc\n\n    if encoding is None:\n        encoding = \"utf-8\"\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, List, Optional, Tuple\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    lines: List[bytes] = []\n\n    # Read the first line (possibly containing BOM or encoding cookie)\n    first_line = readline()\n    if not first_line:\n        # empty file: default to utf-8\n        return \"utf-8\", []\n\n    lines.append(first_line)\n\n    bom_found = False\n    encoding: Optional[str] = None\n\n    # Detect BOM UTF-8 marker\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Adjust first line to remove BOM bytes\n        first_line = first_line[len(BOM_UTF8):]\n        lines[0] = first_line\n        encoding = \"utf-8-sig\"\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_str = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        m = cookie_re.match(line_str)\n        if not m:\n            return None\n        enc = _get_normal_name(m.group(1))\n        try:\n            codec = lookup(enc)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {enc}\")\n        # If BOM found and codec is not utf-8, raise SyntaxError (later handled)\n        return codec.name\n\n    cookie_enc = find_cookie(first_line)\n    if cookie_enc:\n        if bom_found and cookie_enc != \"utf-8\":\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding = cookie_enc if not bom_found else \"utf-8-sig\"\n        return encoding, lines\n\n    # If no encoding found in first line, check second line\n    second_line = readline()\n    if second_line:\n        lines.append(second_line)\n        cookie_enc2 = find_cookie(second_line)\n        if cookie_enc2:\n            if bom_found and cookie_enc2 != \"utf-8\":\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding = cookie_enc2 if not bom_found else \"utf-8-sig\"\n            return encoding, lines\n\n    # If no BOM or cookie found, default encoding is utf-8\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, List, Tuple, Optional\nfrom codecs import BOM_UTF8, lookup\nimport re\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef find_cookie(line: bytes, bom_found: bool) -> Optional[str]:\n    try:\n        line_string = line.decode(\"ascii\")\n    except UnicodeDecodeError:\n        return None\n    match = cookie_re.match(line_string)\n    if not match:\n        return None\n    encoding = _get_normal_name(match.group(1))\n    try:\n        codec = lookup(encoding)\n    except LookupError:\n        # This behaviour mimics the Python interpreter\n        raise SyntaxError(\"unknown encoding: \" + encoding)\n    if bom_found:\n        if codec.name != \"utf-8\":\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"encoding problem: utf-8\")\n        encoding += \"-sig\"\n    return encoding\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    # Read first line (possibly empty bytes at EOF)\n    first_line = readline()\n    if first_line.startswith(BOM_UTF8):\n        bom_found = True\n        # Strip BOM for cookie detection\n        line_strip_bom = first_line[len(BOM_UTF8):]\n        encoding = \"utf-8-sig\"\n    else:\n        bom_found = False\n        line_strip_bom = first_line\n\n    lines = [first_line]\n\n    # Check encoding cookie on first line\n    encoding_cookie = find_cookie(line_strip_bom, bom_found)\n\n    if encoding_cookie:\n        encoding = encoding_cookie\n\n    # If no cookie found, read a second line to check for cookie there\n    if not encoding_cookie:\n        second_line = readline()\n        if second_line:\n            lines.append(second_line)\n            second_encoding_cookie = find_cookie(second_line, bom_found=False)\n            if second_encoding_cookie:\n                encoding = second_encoding_cookie\n    else:\n        # Cookie was found on first line, do not read second line\n        second_encoding_cookie = None\n\n    # If BOM and cookie both found but disagree, raise SyntaxError\n    if bom_found and encoding_cookie:\n        # encoding from BOM is 'utf-8-sig', from cookie is something else possibly\n        # Normalize encoding_cookie for comparison\n        normalized_cookie_enc = encoding_cookie.lower().replace(\"_\", \"-\")\n        if normalized_cookie_enc != \"utf-8\" and normalized_cookie_enc != \"utf-8-sig\":\n            # This check is mostly covered by find_cookie raising already,\n            # but double check semantics here:\n            raise SyntaxError(\"encoding problem: utf-8\")\n        # Actually encoding should be 'utf-8-sig' with BOM and cookie utf-8\n        encoding = \"utf-8-sig\"\n\n    # If no encoding detected yet, default to 'utf-8'\n    if not encoding:\n        encoding = \"utf-8\"\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()"]}
{"task_id": 353, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["from typing import Iterable, Tuple, Union, Text\nfrom blib2to3.pgen2.token import TokenInfo\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # We use the Untokenizer class from the file context, which manages spacing and newlines\n    class Untokenizer:\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            # It is possible that row < prev_row in some cases (though unlikely here)\n            # We add whitespace only if on the same line\n            if row == self.prev_row:\n                col_offset = col - self.prev_col\n                if col_offset > 0:\n                    self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                # t can be either (type, value) or (type, value, start, end, line)\n                if len(t) == 2:\n                    # Older simple form\n                    tok_type, token = t\n                    # Add a token and a space for name/number tokens to avoid concatenation on output\n                    if tok_type in (1, 2):  # NAME=1, NUMBER=2 from token.py\n                        token += \" \"\n                    self.tokens.append(token)\n                    self.prev_row = 1\n                    self.prev_col = 0\n                    continue\n\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (4, 61):  # NEWLINE=4, NL=61\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n    return Untokenizer().untokenize(iterable)", "from typing import Iterable, Union, Tuple, Text\n\nfrom blib2to3.pgen2.token import *\nfrom typing import Tuple\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # Use Untokenizer class defined in the file content to do the untokenizing.\n    # We reimplement its logic here.\n\n    tokens = []\n    prev_row = 1\n    prev_col = 0\n\n    for t in iterable:\n        if len(t) == 2:\n            # Legacy compatibility behavior similar to Untokenizer.compat()\n            toknum, tokval = t\n            startline = False\n            indents = []\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n\n            # The iterable might have more tokens after this, so try to process them\n            # but since we do not have full Untokenizer.compat implementation detail here,\n            # just append the tokens simply and break.\n            tokens.append(tokval)\n            break\n\n        tok_type, token, start, end, line = t\n\n        row, col = start\n        # Add whitespace if needed to move forward to the start position\n        # Note: row only goes backward or same (asserted in Untokenizer.add_whitespace)\n        if row == prev_row:\n            col_offset = col - prev_col\n            if col_offset > 0:\n                tokens.append(\" \" * col_offset)\n        else:\n            # New lines between previous token and this token\n            # Adding newlines to reach the 'row' line\n            newlines = row - prev_row\n            if newlines > 0:\n                tokens.append(\"\\n\" * newlines)\n            # And indentation on the new line\n            if col > 0:\n                tokens.append(\" \" * col)\n\n        tokens.append(token)\n        prev_row, prev_col = end\n\n        # After NEWLINE or NL token, the next token should start from next line, col=0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)", "from typing import Iterable, Union, Tuple, Text\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\nNEWLINE = 4\nNL = 61\nNAME = 1\nNUMBER = 2\nINDENT = 5\nDEDENT = 6\nASYNC = 100\nAWAIT = 101\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).__next__\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    class Untokenizer:\n        tokens: list[Text]\n        prev_row: int\n        prev_col: int\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            assert row <= self.prev_row\n            col_offset = col - self.prev_col\n            if col_offset:\n                self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                if len(t) == 2:\n                    self.compat(t, iterable)\n                    break\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (NEWLINE, NL):\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n        def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n            startline = False\n            indents = []\n            toks_append = self.tokens.append\n            toknum, tokval = token\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n            for tok in iterable:\n                toknum, tokval = tok[:2]\n\n                if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                    tokval += \" \"\n\n                if toknum == INDENT:\n                    indents.append(tokval)\n                    continue\n                elif toknum == DEDENT:\n                    indents.pop()\n                    continue\n                elif toknum in (NEWLINE, NL):\n                    startline = True\n                elif startline and indents:\n                    toks_append(indents[-1])\n                    startline = False\n                toks_append(tokval)\n\n    return Untokenizer().untokenize(iterable)", "from typing import Iterable, Union, Tuple, Text, cast\nfrom blib2to3.pgen2.token import *\nfrom typing import List\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens: List[Text] = []\n    prev_row, prev_col = 1, 0\n\n    for t in iterable:\n        # If tuple is length 2, it is (type, string)\n        if len(t) == 2:\n            # Compatibility mode: just append token with a space if NAME or NUMBER,\n            # else just append token\n            tok_type, tok_val = cast(Tuple[int, Text], t)\n            if tok_type in (NAME, NUMBER):\n                tokens.append(tok_val + \" \")\n            elif tok_type in (NEWLINE, NL):\n                tokens.append(\"\\n\")\n            else:\n                tokens.append(tok_val)\n            prev_row = prev_col = 0  # Reset positions as we have no position info\n            continue\n\n        # Full token info tuple: (type, string, start, end, line)\n        tok_type, tok_val, start, end, line = cast(\n            Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text], t\n        )\n        srow, scol = start\n        erow, ecol = end\n\n        # Add whitespace to advance columns and lines if needed\n        if srow > prev_row:\n            # New lines in between tokens\n            tokens.append(\"\\n\" * (srow - prev_row))\n            prev_col = 0\n        if scol > prev_col:\n            tokens.append(\" \" * (scol - prev_col))\n\n        tokens.append(tok_val)\n\n        prev_row, prev_col = erow, ecol\n\n    return \"\".join(tokens)", "from typing import Iterable, Tuple, Union, Text\n\n# Import token constants\nfrom blib2to3.pgen2.token import *\n\n# Type alias for input tokens\nTokenInfo = Union[Tuple[int, str], Tuple[int, str, Tuple[int, int], Tuple[int, int], Text]]\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens = []\n    prev_row, prev_col = 1, 0  # Track the position of the last token's end\n\n    for t in iterable:\n        if len(t) == 2:\n            # Compatibility mode: tuple of (tok_type, tok_val) only\n            # Append tokens separated by space for names and numbers\n            tok_type, tok_val = t\n            if tok_type in (NAME, NUMBER):\n                tok_val += \" \"\n            tokens.append(tok_val)\n            # We cannot reliably fix spacing here, so just continue\n            continue\n\n        # Full token info\n        tok_type, tok_val, (srow, scol), (erow, ecol), line = t\n\n        # Add appropriate whitespace based on start position relative to previous token end\n        # If this token is on the same line as previous token\n        if srow == prev_row:\n            space = scol - prev_col\n            if space > 0:\n                tokens.append(\" \" * space)\n        else:\n            # Different line(s) from previous token\n            # Insert the line breaks needed to reach the start row\n            for _ in range(srow - prev_row):\n                tokens.append(\"\\n\")\n            # Insert spaces to column start of this token\n            if scol > 0:\n                tokens.append(\" \" * scol)\n\n        tokens.append(tok_val)\n        prev_row, prev_col = erow, ecol\n\n        # Adjust after NEWLINE or NL tokens: newline increases prev_row, resets col to 0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\nfrom typing import Iterable, Tuple, Union, Text\nfrom blib2to3.pgen2.token import TokenInfo\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # We use the Untokenizer class from the file context, which manages spacing and newlines\n    class Untokenizer:\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            # It is possible that row < prev_row in some cases (though unlikely here)\n            # We add whitespace only if on the same line\n            if row == self.prev_row:\n                col_offset = col - self.prev_col\n                if col_offset > 0:\n                    self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                # t can be either (type, value) or (type, value, start, end, line)\n                if len(t) == 2:\n                    # Older simple form\n                    tok_type, token = t\n                    # Add a token and a space for name/number tokens to avoid concatenation on output\n                    if tok_type in (1, 2):  # NAME=1, NUMBER=2 from token.py\n                        token += \" \"\n                    self.tokens.append(token)\n                    self.prev_row = 1\n                    self.prev_col = 0\n                    continue\n\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (4, 61):  # NEWLINE=4, NL=61\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n    return Untokenizer().untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\nfrom typing import Iterable, Union, Tuple, Text\n\nfrom blib2to3.pgen2.token import *\nfrom typing import Tuple\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    # Use Untokenizer class defined in the file content to do the untokenizing.\n    # We reimplement its logic here.\n\n    tokens = []\n    prev_row = 1\n    prev_col = 0\n\n    for t in iterable:\n        if len(t) == 2:\n            # Legacy compatibility behavior similar to Untokenizer.compat()\n            toknum, tokval = t\n            startline = False\n            indents = []\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n\n            # The iterable might have more tokens after this, so try to process them\n            # but since we do not have full Untokenizer.compat implementation detail here,\n            # just append the tokens simply and break.\n            tokens.append(tokval)\n            break\n\n        tok_type, token, start, end, line = t\n\n        row, col = start\n        # Add whitespace if needed to move forward to the start position\n        # Note: row only goes backward or same (asserted in Untokenizer.add_whitespace)\n        if row == prev_row:\n            col_offset = col - prev_col\n            if col_offset > 0:\n                tokens.append(\" \" * col_offset)\n        else:\n            # New lines between previous token and this token\n            # Adding newlines to reach the 'row' line\n            newlines = row - prev_row\n            if newlines > 0:\n                tokens.append(\"\\n\" * newlines)\n            # And indentation on the new line\n            if col > 0:\n                tokens.append(\" \" * col)\n\n        tokens.append(token)\n        prev_row, prev_col = end\n\n        # After NEWLINE or NL token, the next token should start from next line, col=0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\nfrom typing import Iterable, Union, Tuple, Text\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\nNEWLINE = 4\nNL = 61\nNAME = 1\nNUMBER = 2\nINDENT = 5\nDEDENT = 6\nASYNC = 100\nAWAIT = 101\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).__next__\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    class Untokenizer:\n        tokens: list[Text]\n        prev_row: int\n        prev_col: int\n\n        def __init__(self) -> None:\n            self.tokens = []\n            self.prev_row = 1\n            self.prev_col = 0\n\n        def add_whitespace(self, start: Tuple[int, int]) -> None:\n            row, col = start\n            assert row <= self.prev_row\n            col_offset = col - self.prev_col\n            if col_offset:\n                self.tokens.append(\" \" * col_offset)\n\n        def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n            for t in iterable:\n                if len(t) == 2:\n                    self.compat(t, iterable)\n                    break\n                tok_type, token, start, end, line = t\n                self.add_whitespace(start)\n                self.tokens.append(token)\n                self.prev_row, self.prev_col = end\n                if tok_type in (NEWLINE, NL):\n                    self.prev_row += 1\n                    self.prev_col = 0\n            return \"\".join(self.tokens)\n\n        def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n            startline = False\n            indents = []\n            toks_append = self.tokens.append\n            toknum, tokval = token\n            if toknum in (NAME, NUMBER):\n                tokval += \" \"\n            if toknum in (NEWLINE, NL):\n                startline = True\n            for tok in iterable:\n                toknum, tokval = tok[:2]\n\n                if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                    tokval += \" \"\n\n                if toknum == INDENT:\n                    indents.append(tokval)\n                    continue\n                elif toknum == DEDENT:\n                    indents.pop()\n                    continue\n                elif toknum in (NEWLINE, NL):\n                    startline = True\n                elif startline and indents:\n                    toks_append(indents[-1])\n                    startline = False\n                toks_append(tokval)\n\n    return Untokenizer().untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\nfrom typing import Iterable, Union, Tuple, Text, cast\nfrom blib2to3.pgen2.token import *\nfrom typing import List\n\nTokenInfo = Union[Tuple[int, str], Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text]]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens: List[Text] = []\n    prev_row, prev_col = 1, 0\n\n    for t in iterable:\n        # If tuple is length 2, it is (type, string)\n        if len(t) == 2:\n            # Compatibility mode: just append token with a space if NAME or NUMBER,\n            # else just append token\n            tok_type, tok_val = cast(Tuple[int, Text], t)\n            if tok_type in (NAME, NUMBER):\n                tokens.append(tok_val + \" \")\n            elif tok_type in (NEWLINE, NL):\n                tokens.append(\"\\n\")\n            else:\n                tokens.append(tok_val)\n            prev_row = prev_col = 0  # Reset positions as we have no position info\n            continue\n\n        # Full token info tuple: (type, string, start, end, line)\n        tok_type, tok_val, start, end, line = cast(\n            Tuple[int, Text, Tuple[int, int], Tuple[int, int], Text], t\n        )\n        srow, scol = start\n        erow, ecol = end\n\n        # Add whitespace to advance columns and lines if needed\n        if srow > prev_row:\n            # New lines in between tokens\n            tokens.append(\"\\n\" * (srow - prev_row))\n            prev_col = 0\n        if scol > prev_col:\n            tokens.append(\" \" * (scol - prev_col))\n\n        tokens.append(tok_val)\n\n        prev_row, prev_col = erow, ecol\n\n    return \"\".join(tokens)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\nfrom typing import Iterable, Tuple, Union, Text\n\n# Import token constants\nfrom blib2to3.pgen2.token import *\n\n# Type alias for input tokens\nTokenInfo = Union[Tuple[int, str], Tuple[int, str, Tuple[int, int], Tuple[int, int], Text]]\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    tokens = []\n    prev_row, prev_col = 1, 0  # Track the position of the last token's end\n\n    for t in iterable:\n        if len(t) == 2:\n            # Compatibility mode: tuple of (tok_type, tok_val) only\n            # Append tokens separated by space for names and numbers\n            tok_type, tok_val = t\n            if tok_type in (NAME, NUMBER):\n                tok_val += \" \"\n            tokens.append(tok_val)\n            # We cannot reliably fix spacing here, so just continue\n            continue\n\n        # Full token info\n        tok_type, tok_val, (srow, scol), (erow, ecol), line = t\n\n        # Add appropriate whitespace based on start position relative to previous token end\n        # If this token is on the same line as previous token\n        if srow == prev_row:\n            space = scol - prev_col\n            if space > 0:\n                tokens.append(\" \" * space)\n        else:\n            # Different line(s) from previous token\n            # Insert the line breaks needed to reach the start row\n            for _ in range(srow - prev_row):\n                tokens.append(\"\\n\")\n            # Insert spaces to column start of this token\n            if scol > 0:\n                tokens.append(\" \" * scol)\n\n        tokens.append(tok_val)\n        prev_row, prev_col = erow, ecol\n\n        # Adjust after NEWLINE or NL tokens: newline increases prev_row, resets col to 0\n        if tok_type in (NEWLINE, NL):\n            prev_row += 1\n            prev_col = 0\n\n    return \"\".join(tokens)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n"]}
