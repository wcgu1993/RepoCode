{"task_id": 351, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The C function get_normal_name returns the input string excluding any substring starting with a colon ':'\n    # This Python imitation returns the part of orig_enc before the first colon, or the whole string if no colon.\n    idx = orig_enc.find(':')\n    if idx == -1:\n        return orig_enc\n    return orig_enc[:idx]", "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The function is expected to normalize encoding names similar to tokenizer.c\n    # For example, following transformations are common:\n    # - Lowercase the string\n    # - Remove dashes and underscores\n    # - Replace common aliases like 'utf8' to 'utf-8'\n    # Since tokenizer.c is not provided, we approximate the logic.\n\n    # Lowercase original encoding\n    enc = orig_enc.lower()\n\n    # Remove dashes and underscores\n    enc = enc.replace('-', '').replace('_', '')\n\n    # Common normalization replacements\n    if enc == 'utf8':\n        enc = 'utf8'\n    elif enc == 'utf16le':\n        enc = 'utf16le'\n    elif enc == 'utf16be':\n        enc = 'utf16be'\n    elif enc == 'utf32le':\n        enc = 'utf32le'\n    elif enc == 'utf32be':\n        enc = 'utf32be'\n    elif enc in ('latin1', 'latin', 'iso88591', 'iso8859-1'):\n        enc = 'latin1'\n    elif enc == 'ascii':\n        enc = 'ascii'\n    else:\n        # For other encodings, just return the lowercase with no dashes or underscores\n        enc = enc\n\n    return enc", "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    normal_name = orig_enc.lower()\n    for prefix in (\"utf-\", \"ansi_\", \"cp\", \"ibm/\"):\n        if normal_name.startswith(prefix):\n            normal_name = normal_name[len(prefix):]\n            break\n    # Replace non-alphanumeric characters by underscores\n    normal_name = ''.join(c if c.isalnum() else '_' for c in normal_name)\n    return normal_name", "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Strip known prefixes\n    prefixes = [\"rfc\", \"iso\", \"windows\", \"cp\", \"ibm\"]\n    enc = orig_enc.lower()\n    for prefix in prefixes:\n        if enc.startswith(prefix):\n            enc = enc[len(prefix):]\n            break\n\n    # Remove -, _, and spaces\n    enc = enc.replace(\"-\", \"\").replace(\"_\", \"\").replace(\" \", \"\")\n\n    # Map aliases to standard names\n    alias_map = {\n        \"latin1\": \"latin1\",\n        \"latin2\": \"latin2\",\n        \"koi8r\": \"koi8-r\",\n        \"koi8u\": \"koi8-u\",\n        \"utf8\": \"utf-8\",\n        \"utf16\": \"utf-16\",\n        \"utf32\": \"utf-32\",\n        \"ascii\": \"ascii\",\n        \"big5\": \"big5\",\n        \"gb2312\": \"gb2312\",\n        \"gbk\": \"gbk\",\n        \"shiftjis\": \"shift_jis\",\n        \"sjis\": \"shift_jis\",\n        \"eucjp\": \"euc-jp\",\n        \"euckr\": \"euc-kr\",\n        \"windows1252\": \"windows-1252\",\n    }\n\n    if enc in alias_map:\n        return alias_map[enc]\n    return orig_enc.lower()", "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The function converts the encoding name to a normalized form.\n    # According to typical tokenizer.c logic, this usually means:\n    # - converting to lowercase\n    # - replacing '-' with '_'\n    # - removing certain suffixes like \"-legacy\" or \"-legacy-text\"\n    \n    name = orig_enc.lower()\n    name = name.replace('-', '_')\n    if name.endswith('_legacy_text'):\n        name = name[:-12]\n    elif name.endswith('_legacy'):\n        name = name[:-7]\n    return name"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The C function get_normal_name returns the input string excluding any substring starting with a colon ':'\n    # This Python imitation returns the part of orig_enc before the first colon, or the whole string if no colon.\n    idx = orig_enc.find(':')\n    if idx == -1:\n        return orig_enc\n    return orig_enc[:idx]\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The function is expected to normalize encoding names similar to tokenizer.c\n    # For example, following transformations are common:\n    # - Lowercase the string\n    # - Remove dashes and underscores\n    # - Replace common aliases like 'utf8' to 'utf-8'\n    # Since tokenizer.c is not provided, we approximate the logic.\n\n    # Lowercase original encoding\n    enc = orig_enc.lower()\n\n    # Remove dashes and underscores\n    enc = enc.replace('-', '').replace('_', '')\n\n    # Common normalization replacements\n    if enc == 'utf8':\n        enc = 'utf8'\n    elif enc == 'utf16le':\n        enc = 'utf16le'\n    elif enc == 'utf16be':\n        enc = 'utf16be'\n    elif enc == 'utf32le':\n        enc = 'utf32le'\n    elif enc == 'utf32be':\n        enc = 'utf32be'\n    elif enc in ('latin1', 'latin', 'iso88591', 'iso8859-1'):\n        enc = 'latin1'\n    elif enc == 'ascii':\n        enc = 'ascii'\n    else:\n        # For other encodings, just return the lowercase with no dashes or underscores\n        enc = enc\n\n    return enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    normal_name = orig_enc.lower()\n    for prefix in (\"utf-\", \"ansi_\", \"cp\", \"ibm/\"):\n        if normal_name.startswith(prefix):\n            normal_name = normal_name[len(prefix):]\n            break\n    # Replace non-alphanumeric characters by underscores\n    normal_name = ''.join(c if c.isalnum() else '_' for c in normal_name)\n    return normal_name\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Strip known prefixes\n    prefixes = [\"rfc\", \"iso\", \"windows\", \"cp\", \"ibm\"]\n    enc = orig_enc.lower()\n    for prefix in prefixes:\n        if enc.startswith(prefix):\n            enc = enc[len(prefix):]\n            break\n\n    # Remove -, _, and spaces\n    enc = enc.replace(\"-\", \"\").replace(\"_\", \"\").replace(\" \", \"\")\n\n    # Map aliases to standard names\n    alias_map = {\n        \"latin1\": \"latin1\",\n        \"latin2\": \"latin2\",\n        \"koi8r\": \"koi8-r\",\n        \"koi8u\": \"koi8-u\",\n        \"utf8\": \"utf-8\",\n        \"utf16\": \"utf-16\",\n        \"utf32\": \"utf-32\",\n        \"ascii\": \"ascii\",\n        \"big5\": \"big5\",\n        \"gb2312\": \"gb2312\",\n        \"gbk\": \"gbk\",\n        \"shiftjis\": \"shift_jis\",\n        \"sjis\": \"shift_jis\",\n        \"eucjp\": \"euc-jp\",\n        \"euckr\": \"euc-kr\",\n        \"windows1252\": \"windows-1252\",\n    }\n\n    if enc in alias_map:\n        return alias_map[enc]\n    return orig_enc.lower()\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # The function converts the encoding name to a normalized form.\n    # According to typical tokenizer.c logic, this usually means:\n    # - converting to lowercase\n    # - replacing '-' with '_'\n    # - removing certain suffixes like \"-legacy\" or \"-legacy-text\"\n    \n    name = orig_enc.lower()\n    name = name.replace('-', '_')\n    if name.endswith('_legacy_text'):\n        name = name[:-12]\n    elif name.endswith('_legacy'):\n        name = name[:-7]\n    return name\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1')\ntest_0()\n\ndef test_1():\n    assert _get_normal_name(\"latin-1\") == 'iso-8859-1'\ntest_1()\n\ndef test_3():\n    assert _get_normal_name('cp850') == 'cp850'\ntest_3()\n\ndef test_6():\n    assert _get_normal_name('ISO-8859-1-BOM') == 'iso-8859-1'\ntest_6()\n\ndef test_7():\n    assert _get_normal_name(\"utf-8-bom_SIG\") == \"utf-8\"\ntest_7()\n\ndef test_8():\n    assert 'utf-8' == _get_normal_name('utf-8-SIG')\ntest_8()\n\ndef test_9():\n    assert _get_normal_name('iso-latin-1') == 'iso-8859-1'\ntest_9()\n\ndef test_11():\n    assert _get_normal_name(\"LATIN-1\") == \"iso-8859-1\"\ntest_11()\n\ndef test_12():\n    assert _get_normal_name(\"utf-8-\") == \"utf-8\"\ntest_12()\n\ndef test_13():\n    assert _get_normal_name(\"iso-8859-1-sig\") == \"iso-8859-1\"\ntest_13()\n\ndef test_14():\n    assert _get_normal_name(\"iso-latin-1\") == \"iso-8859-1\"\ntest_14()\n\ndef test_15():\n    assert _get_normal_name('ascii') == 'ascii'\ntest_15()\n\ndef test_18():\n    assert _get_normal_name(\"utf-32-le\") == \"utf-32-le\"\ntest_18()\n\ndef test_19():\n    assert _get_normal_name\ntest_19()\n\ndef test_20():\n    assert _get_normal_name('utf-8-bom') == 'utf-8'\ntest_20()\n\ndef test_22():\n    assert 'utf-8' == _get_normal_name('utf-8-FOO-BAR')\ntest_22()\n\ndef test_23():\n    assert _get_normal_name('ascii')\ntest_23()\n\ndef test_24():\n    assert _get_normal_name('utf-8-BOM') == \"utf-8\"\ntest_24()\n\ndef test_27():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8-bogus\")\ntest_27()\n\ndef test_28():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1\")\ntest_28()\n\ndef test_31():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-foo')\ntest_31()\n\ndef test_32():\n    assert _get_normal_name('cp932') == 'cp932'\ntest_32()\n\ndef test_33():\n    assert _get_normal_name(\"utf-8-VARIANT\") == \"utf-8\"\ntest_33()\n\ndef test_34():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-cpp\")\ntest_34()\n\ndef test_35():\n    assert _get_normal_name(\"latin-1-BOM123\") == \"iso-8859-1\"\ntest_35()\n\ndef test_36():\n    assert _get_normal_name('utf_8') == 'utf-8'\ntest_36()\n\ndef test_37():\n    assert _get_normal_name(\"utf-8-BOM\") == \"utf-8\"\ntest_37()\n\ndef test_38():\n    assert _get_normal_name(\"latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_38()\n\ndef test_39():\n    assert _get_normal_name(\"utf-8-BOM89\") == \"utf-8\"\ntest_39()\n\ndef test_40():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-BOM')\ntest_40()\n\ndef test_42():\n    assert _get_normal_name(\"UTF-8\") == \"utf-8\"\ntest_42()\n\ndef test_43():\n    assert _get_normal_name('latin_1_SIG') == 'iso-8859-1'\ntest_43()\n\ndef test_44():\n    assert _get_normal_name(\"LATIN-1-UNICODE-SIG\") == \"iso-8859-1\"\ntest_44()\n\ndef test_45():\n    assert _get_normal_name('latin_1') == 'iso-8859-1'\ntest_45()\n\ndef test_46():\n    assert _get_normal_name(\"iso-8859-1\") == 'iso-8859-1'\ntest_46()\n\ndef test_47():\n    assert _get_normal_name('latin-1_sig') == 'iso-8859-1'\ntest_47()\n\ndef test_48():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-SIG\")\ntest_48()\n\ndef test_49():\n    assert _get_normal_name('latin-9') == 'latin-9'\ntest_49()\n\ndef test_52():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8\")\ntest_52()\n\ndef test_53():\n    assert _get_normal_name(\"iso-latin-1-SIG\") == \"iso-8859-1\"\ntest_53()\n\ndef test_54():\n    assert 'utf-8' == _get_normal_name('utf-8-fo-foo')\ntest_54()\n\ndef test_56():\n    assert _get_normal_name(\"latin-1-bOM\") == 'iso-8859-1'\ntest_56()\n\ndef test_57():\n    assert _get_normal_name(\"iso-latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_57()\n\ndef test_59():\n    assert _get_normal_name(\"iso-latin-1\") == 'iso-8859-1'\ntest_59()\n\ndef test_61():\n    assert _get_normal_name('utf-8') == 'utf-8'\ntest_61()\n\ndef test_62():\n    assert _get_normal_name(\"latin-1-1\") == \"iso-8859-1\"\ntest_62()\n\ndef test_64():\n    assert _get_normal_name('utf-8-BOM') == 'utf-8'\ntest_64()\n\ndef test_66():\n    assert _get_normal_name(\"cp1252\") == \"cp1252\"\ntest_66()\n\ndef test_70():\n    assert _get_normal_name(\"latin-1-VARIANT\") == \"iso-8859-1\"\ntest_70()\n\ndef test_71():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1-SIG\")\ntest_71()\n\ndef test_74():\n    assert _get_normal_name(\"latin-1-BOM\") == \"iso-8859-1\"\ntest_74()\n\ndef test_75():\n    assert _get_normal_name(\"utf-8-strict89\") == \"utf-8\"\ntest_75()\n\ndef test_76():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-windows\")\ntest_76()\n\ndef test_77():\n    assert _get_normal_name(\"iso-8859-15\") == \"iso-8859-15\"\ntest_77()\n\ndef test_78():\n    assert _get_normal_name(\"utf_8\") == \"utf-8\"\ntest_78()\n\ndef test_79():\n    assert _get_normal_name(\"utf-8-bogus\") == \"utf-8\"\ntest_79()\n\ndef test_80():\n    assert 'utf-8' == _get_normal_name('utf_8')\ntest_80()\n\ndef test_82():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom_underscore\")\ntest_82()\n\ndef test_83():\n    assert _get_normal_name(\"iso-8859-1\") == \"iso-8859-1\"\ntest_83()\n\ndef test_84():\n    assert _get_normal_name('utf8') == 'utf8'\ntest_84()\n\ndef test_85():\n    assert _get_normal_name(\"uTf-16\") == \"uTf-16\"\ntest_85()\n\ndef test_86():\n    assert _get_normal_name(\"latin-1-2\") == \"iso-8859-1\"\ntest_86()\n\ndef test_87():\n    assert \"utf-8\" == _get_normal_name(\"utf_8-BAZ\")\ntest_87()\n\ndef test_88():\n    assert _get_normal_name('UTF-8-SIG') == 'utf-8'\ntest_88()\n\ndef test_91():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom\")\ntest_91()\n\ndef test_92():\n    assert _get_normal_name(\"ascii\") == \"ascii\"\ntest_92()\n\ndef test_93():\n    assert _get_normal_name(\"latin-1-bom\") == \"iso-8859-1\"\ntest_93()\n\ndef test_99():\n    assert _get_normal_name('utf_8_sig') == 'utf-8'\ntest_99()\n\ndef test_101():\n    assert \"utf-8\" == _get_normal_name(\"UTF-8\")\ntest_101()\n\ndef test_102():\n    assert _get_normal_name(\"UTF-8-SIG\") == \"utf-8\"\ntest_102()\n\ndef test_104():\n    assert _get_normal_name(\"latin-1-\") == \"iso-8859-1\"\ntest_104()\n\ndef test_105():\n    assert _get_normal_name(\"Latin-1-VARIANT\") == \"iso-8859-1\"\ntest_105()\n\ndef test_106():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1_sig')\ntest_106()\n\ndef test_107():\n    assert _get_normal_name(\"iso-8859-1\") == _get_normal_name(\"latin-1\")\ntest_107()\n\ndef test_111():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1-baz\")\ntest_111()\n\ndef test_113():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-ironpython\")\ntest_113()\n\ndef test_114():\n    assert _get_normal_name('UTF-8') == 'utf-8'\ntest_114()\n\ndef test_115():\n    assert _get_normal_name(\"iso-8859-1-\") == \"iso-8859-1\"\ntest_115()\n\ndef test_118():\n    assert _get_normal_name(\"latin-1-bogus\") == \"iso-8859-1\"\ntest_118()\n\ndef test_120():\n    assert _get_normal_name(\"UTF-8-VARIANT\") == \"utf-8\"\ntest_120()\n\ndef test_121():\n    assert _get_normal_name(\"utf-8-SIG\") == \"utf-8\"\ntest_121()\n\ndef test_122():\n    assert _get_normal_name(\"utf-8-bOM\") == 'utf-8'\ntest_122()\n\ndef test_123():\n    assert _get_normal_name(\"iso-8859-1-stuff\") == \"iso-8859-1\"\ntest_123()\n\ndef test_126():\n    assert _get_normal_name(\"LATIN-1-SIG\") == \"iso-8859-1\"\ntest_126()\n\ndef test_127():\n    assert _get_normal_name(\"ISO-8859-1\") == \"iso-8859-1\"\ntest_127()\n\ndef test_128():\n    assert _get_normal_name(\"iso-latin-1-bla-bla-bla\") == \"iso-8859-1\"\ntest_128()\n\ndef test_130():\n    assert _get_normal_name(\"iso-8859-1-SIMPLE\") == \"iso-8859-1\"\ntest_130()\n\ndef test_131():\n    assert _get_normal_name(\"utf-32-be\") == \"utf-32-be\"\ntest_131()\n\ndef test_132():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-\")\ntest_132()\n\ndef test_134():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-FOO\")\ntest_134()\n\ndef test_138():\n    assert _get_normal_name('iso-8859-1_') == 'iso-8859-1'\ntest_138()\n\ndef test_139():\n    assert _get_normal_name(\"utf_8-foo-bar\") == \"utf-8\"\ntest_139()\n\ndef test_140():\n    assert _get_normal_name(\"utf-8-sig\") != \"utf-8-sig\"\ntest_140()\n\ndef test_142():\n    assert _get_normal_name(\"us-ascii\") == \"us-ascii\"\ntest_142()\n\ndef test_143():\n    assert _get_normal_name(\"utf-8-bla-bla-bla\") == \"utf-8\"\ntest_143()\n\ndef test_144():\n    assert _get_normal_name(\"utf-8-BOM-SIG\") == \"utf-8\"\ntest_144()\n\ndef test_145():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bom_underscore\")\ntest_145()\n\ndef test_146():\n    assert _get_normal_name(\"iso-8859-1-bOM\") == 'iso-8859-1'\ntest_146()\n\ndef test_149():\n    assert _get_normal_name(\"utf-8-strict\") == \"utf-8\"\ntest_149()\n\ndef test_151():\n    assert _get_normal_name(\"ISO-LATIN-1\") == \"iso-8859-1\"\ntest_151()\n\ndef test_152():\n    assert 'utf-8' == _get_normal_name('utf-8')\ntest_152()\n\ndef test_153():\n    assert 'utf-8' == _get_normal_name('UTF-8_SIG')\ntest_153()\n\ndef test_154():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom_underscore\")\ntest_154()\n\ndef test_155():\n    assert _get_normal_name(\"utf-8-bla-latin-1-bla-utf-8\") == \"utf-8\"\ntest_155()\n\ndef test_156():\n    assert 'utf-8' == _get_normal_name('utf-8_sig')\ntest_156()\n\ndef test_158():\n    assert _get_normal_name(\"latin-1-strict\") == \"iso-8859-1\"\ntest_158()\n\ndef test_160():\n    assert _get_normal_name(\"ISO-LATIN-1-SIG\") == \"iso-8859-1\"\ntest_160()\n\ndef test_161():\n    assert 'utf-8' == _get_normal_name('UTF-8-SIG')\ntest_161()\n\ndef test_162():\n    assert 'utf-8' == _get_normal_name('UTF-8')\ntest_162()\n\ndef test_163():\n    assert _get_normal_name('iso_8859_1') == 'iso-8859-1'\ntest_163()\n\ndef test_164():\n    assert _get_normal_name(\"utf-8-SIG-BOM\") == \"utf-8\"\ntest_164()\n\ndef test_165():\n    assert _get_normal_name('latin-11') == 'latin-11'\ntest_165()\n\ndef test_166():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-java\")\ntest_166()\n\ndef test_167():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin_1\")\ntest_167()\n\ndef test_168():\n    assert _get_normal_name('iso-8859-1-sig') == 'iso-8859-1'\ntest_168()\n\ndef test_169():\n    assert _get_normal_name('iso_latin_1') == 'iso-8859-1'\ntest_169()\n\ndef test_170():\n    assert \"utf-8\"      == _get_normal_name(\"utf-8\")\ntest_170()\n\ndef test_171():\n    assert _get_normal_name(\"Latin-1\") == \"iso-8859-1\"\ntest_171()\n\ndef test_172():\n    assert _get_normal_name(\"UTF-8-bOM\") == 'utf-8'\ntest_172()\n\ndef test_173():\n    assert _get_normal_name(\"uTf-16-Sig\") == \"uTf-16-Sig\"\ntest_173()\n\ndef test_175():\n    assert _get_normal_name('latin-1-SIG') == 'iso-8859-1'\ntest_175()\n\ndef test_176():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-mac\")\ntest_176()\n\ndef test_177():\n    assert _get_normal_name(\"iso-latin-1-bOM\") == 'iso-8859-1'\ntest_177()\n\ndef test_178():\n    assert _get_normal_name(\"LATIN-1-BOM\") == \"iso-8859-1\"\ntest_178()\n\ndef test_179():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-foo\")\ntest_179()\n\ndef test_184():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1_sig')\ntest_184()\n\ndef test_187():\n    assert _get_normal_name(\"utf-16-le\") == \"utf-16-le\"\ntest_187()\n\ndef test_189():\n    assert 'utf-8' == _get_normal_name('utf-8--foo')\ntest_189()\n\ndef test_190():\n    assert _get_normal_name('latin-1_') == 'iso-8859-1'\ntest_190()\n\ndef test_191():\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla\") == \"utf-8\"\ntest_191()\n\ndef test_192():\n    assert \"utf-8\" == _get_normal_name(\"UTF_8-BAR\")\ntest_192()\n\ndef test_195():\n    assert _get_normal_name('LATIN-1') == 'iso-8859-1'\ntest_195()\n\ndef test_196():\n    assert _get_normal_name(\"latin-1-sig\") == \"iso-8859-1\"\ntest_196()\n\ndef test_197():\n    assert \"utf-8\" == _get_normal_name(\"utf-8\")\ntest_197()\n\ndef test_198():\n    assert _get_normal_name(\"utf-8-stuff\") == \"utf-8\"\ntest_198()\n\ndef test_199():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-bom')\ntest_199()\n\ndef test_201():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-bom\")\ntest_201()\n\ndef test_202():\n    assert _get_normal_name('iso-8859-1_sig') == 'iso-8859-1'\ntest_202()\n\ndef test_203():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-sig')\ntest_203()\n\ndef test_204():\n    assert _get_normal_name('latin-1-bOM') == \"iso-8859-1\"\ntest_204()\n\ndef test_206():\n    assert \"utf-8\" == _get_normal_name(\"utf_8\")\ntest_206()\n\ndef test_208():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-dos\")\ntest_208()\n\ndef test_209():\n    assert _get_normal_name(\"latin-1-SIG\") == \"iso-8859-1\"\ntest_209()\n\ndef test_212():\n    assert _get_normal_name(\"utf-8\") == 'utf-8'\ntest_212()\n\ndef test_215():\n    assert _get_normal_name(\"utf-8\") == \"utf-8\"\ntest_215()\n\ndef test_216():\n    assert _get_normal_name('utf-8-SIG') == \"utf-8\"\ntest_216()\n\ndef test_218():\n    assert _get_normal_name(\"UTF8\") == \"UTF8\"\ntest_218()\n\ndef test_221():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-sig')\ntest_221()\n\ndef test_222():\n    assert _get_normal_name('latin-1-SIG') == \"iso-8859-1\"\ntest_222()\n\ndef test_223():\n    assert 'iso-8859-1' == _get_normal_name('latin-1')\ntest_223()\n\ndef test_227():\n    assert 'iso-8859-1' == _get_normal_name('Latin-1-BAR')\ntest_227()\n\ndef test_228():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1-FOO-BAR')\ntest_228()\n\ndef test_232():\n    assert _get_normal_name('UTF-8_sig') == 'utf-8'\ntest_232()\n\ndef test_235():\n    assert _get_normal_name('utf-8-SIG') == 'utf-8'\ntest_235()\n\ndef test_236():\n    assert _get_normal_name('iso-8859-1-bom') == 'iso-8859-1'\ntest_236()\n\ndef test_237():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-SIG')\ntest_237()\n\ndef test_238():\n    assert _get_normal_name(\"utf-8-bom_unicode\") == \"utf-8\"\ntest_238()\n\ndef test_240():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1\")\ntest_240()\n\ndef test_241():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN_1-BAR\")\ntest_241()\n\ndef test_242():\n    assert _get_normal_name(\"utf-8-bom-sig\") == \"utf-8\"\ntest_242()\n\ndef test_243():\n    assert _get_normal_name('iso8859-15') == 'iso8859-15'\ntest_243()\n\ndef test_244():\n    assert _get_normal_name(\"foo\") == \"foo\"\ntest_244()\n\ndef test_245():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-foo')\ntest_245()\n\ndef test_246():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-bogus\")\ntest_246()\n\ndef test_247():\n    assert _get_normal_name('cp1252') == 'cp1252'\ntest_247()\n\ndef test_248():\n    assert _get_normal_name('UTF-8-BOM') == 'utf-8'\ntest_248()\n\ndef test_249():\n    assert _get_normal_name(\"latin-1\") == \"iso-8859-1\"\ntest_249()\n\ndef test_250():\n    assert 'utf-8' == _get_normal_name('utf-8-some-bom')\ntest_250()\n\ndef test_251():\n    assert _get_normal_name(\"UTF-8-BOM\") == \"utf-8\"\ntest_251()\n\ndef test_253():\n    assert \"iso-8859-1\" == _get_normal_name(\"LATIN-1\")\ntest_253()\n\ndef test_254():\n    assert _get_normal_name('UTF_8-sig') == 'utf-8'\ntest_254()\n\ndef test_255():\n    assert _get_normal_name(\"utf-32\") == \"utf-32\"\ntest_255()\n\ndef test_256():\n    assert _get_normal_name(\"latin-1-strict89\") == \"iso-8859-1\"\ntest_256()\n\ndef test_257():\n    assert _get_normal_name(\"uTf-8\") == \"utf-8\"\ntest_257()\n\ndef test_258():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-windows\")\ntest_258()\n\ndef test_259():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-SIG\")\ntest_259()\n\ndef test_260():\n    assert 'utf-8' == _get_normal_name('utf-8-fooooo')\ntest_260()\n\ndef test_262():\n    assert _get_normal_name(\"ISO-8859-1-SIG\") == \"iso-8859-1\"\ntest_262()\n\ndef test_263():\n    assert _get_normal_name('iso-8859-1-BOM') == 'iso-8859-1'\ntest_263()\n\ndef test_264():\n    assert _get_normal_name(\"utf-8-sig\") == \"utf-8\"\ntest_264()\n\ndef test_269():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bom\")\ntest_269()\n\ndef test_270():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1')\ntest_270()\n\ndef test_274():\n    assert _get_normal_name('iso-8859-1') == 'iso-8859-1'\ntest_274()\n\ndef test_275():\n    assert 'iso-8859-1' == _get_normal_name('iso-latin-1')\ntest_275()\n\ndef test_276():\n    assert _get_normal_name('UTF-8-sig') == 'utf-8'\ntest_276()\n\ndef test_277():\n    assert _get_normal_name('latin-1') == 'iso-8859-1'\ntest_277()\n\ndef test_279():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-SIG')\ntest_279()\n\ndef test_280():\n    assert _get_normal_name(\"utf-8-SIMPLE\") == \"utf-8\"\ntest_280()\n\ndef test_282():\n    assert 'iso-8859-1' == _get_normal_name('ISO-8859-1-SIG')\ntest_282()\n\ndef test_283():\n    assert _get_normal_name(\"UTF-8-BOM-SIG\") == \"utf-8\"\ntest_283()\n\ndef test_284():\n    assert 'iso-8859-1' == _get_normal_name('latin-1--foo')\ntest_284()\n\ndef test_285():\n    assert _get_normal_name(\"utf-8--simple\") == \"utf-8\"\ntest_285()\n\ndef test_286():\n    assert _get_normal_name(\"latin-1-bla-bla-latin-1\") == \"iso-8859-1\"\ntest_286()\n\ndef test_288():\n    assert _get_normal_name(\"iso-8859-1-SIG\") == \"iso-8859-1\"\ntest_288()\n\ndef test_289():\n    assert _get_normal_name(\"iso_8859_1\") == \"iso-8859-1\"\ntest_289()\n\ndef test_290():\n    assert _get_normal_name('utf-8-sig') == 'utf-8'\ntest_290()\n\ndef test_291():\n    assert _get_normal_name('ANSI_X3.110-1983') == 'ANSI_X3.110-1983'\ntest_291()\n\ndef test_293():\n    assert _get_normal_name(\"utf_8_sig\") == \"utf-8\"\ntest_293()\n\ndef test_294():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-csharp\")\ntest_294()\n\ndef test_296():\n    assert _get_normal_name(\"latin-1-bom89\") == \"iso-8859-1\"\ntest_296()\n\ndef test_300():\n    assert 'utf-8' == _get_normal_name('utf-8-bom')\ntest_300()\n\ndef test_301():\n    assert _get_normal_name(\"latin_1\") == \"iso-8859-1\"\ntest_301()\n\ndef test_302():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1-bogus\")\ntest_302()\n\ndef test_303():\n    assert 'utf-8' == _get_normal_name('utf-8-sig')\ntest_303()\n\ndef test_305():\n    assert _get_normal_name(\"mac-roman\") == \"mac-roman\"\ntest_305()\n\ndef test_306():\n    assert 'utf-8' == _get_normal_name('utf_8_BOM')\ntest_306()\n\ndef test_307():\n    assert _get_normal_name(\"utf-8!\") == \"utf-8!\"\ntest_307()\n\ndef test_308():\n    assert _get_normal_name(\"uTf-8-SIG\") == \"utf-8\"\ntest_308()\n\ndef test_309():\n    assert _get_normal_name(\"iso-8859-1-1\") == \"iso-8859-1\"\ntest_309()\n\ndef test_310():\n    assert 'iso-8859-1' == _get_normal_name('latin-1_sig')\ntest_310()\n\ndef test_313():\n    assert _get_normal_name(\"UTF-8\") == 'utf-8'\ntest_313()\n\ndef test_314():\n    assert 'utf-8' == _get_normal_name('utf-8-fo--foo')\ntest_314()\n\ndef test_316():\n    assert _get_normal_name(\"latin-1-SIMPLE\") == \"iso-8859-1\"\ntest_316()\n\ndef test_317():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-latin-1\")\ntest_317()\n\ndef test_318():\n    assert _get_normal_name(\"uTf-8-BOM\") == \"utf-8\"\ntest_318()\n\ndef test_319():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-sig')\ntest_319()\n\ndef test_323():\n    assert _get_normal_name('latin-1-BOM') == 'iso-8859-1'\ntest_323()\n\ndef test_327():\n    assert 'iso-8859-1' == _get_normal_name('latin-1-FOO')\ntest_327()\n\ndef test_328():\n    assert _get_normal_name(\"iso-8859-1-2\") == \"iso-8859-1\"\ntest_328()\n\ndef test_329():\n    assert _get_normal_name('latin-1-bom') == 'iso-8859-1'\ntest_329()\n\ndef test_330():\n    assert _get_normal_name(\"utf-8-bom_UNIX\") == \"utf-8\"\ntest_330()\n\ndef test_331():\n    assert _get_normal_name(\"utf-8-bom\") == \"utf-8\"\ntest_331()\n\ndef test_332():\n    assert _get_normal_name(\"utf8\") == \"utf8\"\ntest_332()\n\ndef test_333():\n    assert _get_normal_name(\"utf-16\") == \"utf-16\"\ntest_333()\n\ndef test_334():\n    assert 'utf-8' == _get_normal_name('utf-8-BOM')\ntest_334()\n\ndef test_335():\n    assert \"iso-8859-1\" == _get_normal_name(\"latin-1-\")\ntest_335()\n\ndef test_336():\n    assert \"iso-8859-1\" == _get_normal_name(\"iso-8859-1\")\ntest_336()\n\ndef test_337():\n    assert _get_normal_name(\"utf-8-bom89\") == \"utf-8\"\ntest_337()\n\ndef test_338():\n    assert _get_normal_name(\"utf-16-be\") == \"utf-16-be\"\ntest_338()\n\ndef test_340():\n    assert 'utf-8' == _get_normal_name('utf-8-foo')\ntest_340()\n\ndef test_341():\n    assert _get_normal_name('latin-1-sig') == 'iso-8859-1'\ntest_341()\n\ndef test_342():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-mac\")\ntest_342()\n\ndef test_344():\n    assert _get_normal_name(\"LATIN-1-UNICODE\") == \"iso-8859-1\"\ntest_344()\n\ndef test_346():\n    assert _get_normal_name(\"LATIN-1-UNICODE-BOM-SIG\") == \"iso-8859-1\"\ntest_346()\n\ndef test_348():\n    assert 'iso-8859-1' == _get_normal_name('iso-8859-1-some-bom')\ntest_348()\n\ndef test_350():\n    assert _get_normal_name(\"iso_latin_1\") == \"iso-8859-1\"\ntest_350()\n\ndef test_351():\n    assert _get_normal_name('latin-1') == \"iso-8859-1\"\ntest_351()\n\ndef test_352():\n    assert _get_normal_name(\"utf-8-sig\") == _get_normal_name(\"utf-8\")\ntest_352()\n\ndef test_353():\n    assert _get_normal_name('uTF-8') == 'utf-8'\ntest_353()\n\ndef test_354():\n    assert \"utf-8\" == _get_normal_name(\"utf-8-linux\")\ntest_354()\n\ndef test_2():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_2\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla-bla\") == output\ntest_2()\n\ndef test_4():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_4\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF') == output\ntest_4()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"CP1252\") == output\ntest_5()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM\") == output\ntest_10()\n\ndef test_16():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_16\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998\") == output\ntest_16()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf_8') == output\ntest_17()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE-BOM\") == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32\") == output\ntest_25()\n\ndef test_26():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_26\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993\") == output\ntest_26()\n\ndef test_29():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_29\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8_sig') == output\ntest_29()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-be-bom\") == output\ntest_30()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp1252') == output\ntest_41()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_50()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-16:2001\") == output\ntest_51()\n\ndef test_55():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_55\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-BOM123\") == output\ntest_55()\n\ndef test_58():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_58\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-le-bom\") == output\ntest_58()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-100:1993:bogus\") == output\ntest_60()\n\ndef test_63():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_63\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8-bom\") == output\ntest_63()\n\ndef test_65():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_65\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"windows-1252\") == output\ntest_65()\n\ndef test_67():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_67\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp-1252\") == output\ntest_67()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-BE\") == output\ntest_68()\n\ndef test_69():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_69\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF_8_SIG') == output\ntest_69()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  latin_1-baz\") == output\ntest_72()\n\ndef test_73():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_73\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-xxx\") == output\ntest_73()\n\ndef test_81():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_81\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin 1') == output\ntest_81()\n\ndef test_89():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_89\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-latin1\") == output\ntest_89()\n\ndef test_90():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_90\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin-1') == output\ntest_90()\n\ndef test_94():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_94\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp1252-sig\") == output\ntest_94()\n\ndef test_95():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_95\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1_stuff') == output\ntest_95()\n\ndef test_96():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_96\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-latin-1') == output\ntest_96()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    UTF8') == output\ntest_97()\n\ndef test_98():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_98\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE\") == output\ntest_98()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso8859-1\") == output\ntest_100()\n\ndef test_103():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_103\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1--foo') == output\ntest_103()\n\ndef test_108():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_108\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('8859') == output\ntest_108()\n\ndef test_109():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_109\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_109()\n\ndef test_110():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_110\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8-sig') == output\ntest_110()\n\ndef test_112():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_112\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    uTF-8') == output\ntest_112()\n\ndef test_116():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_116\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso-8859-15') == output\ntest_116()\n\ndef test_117():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_117\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    latin1') == output\ntest_117()\n\ndef test_119():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_119\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-foo') == output\ntest_119()\n\ndef test_124():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_124\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-Le\") == output\ntest_124()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-9\") == output\ntest_125()\n\ndef test_129():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_129\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_sig') == output\ntest_129()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8') == output\ntest_133()\n\ndef test_135():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_135\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bom\") == output\ntest_135()\n\ndef test_136():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_136\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-stuff') == output\ntest_136()\n\ndef test_137():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_137\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-bOM\") == output\ntest_137()\n\ndef test_141():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_141\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp-SIG') == output\ntest_141()\n\ndef test_147():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_147\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-SIG\") == output\ntest_147()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-bom\") == output\ntest_148()\n\ndef test_150():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_150\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF8\") == output\ntest_150()\n\ndef test_157():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_157\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-32-b\") == output\ntest_157()\n\ndef test_159():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_159\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla\") == output\ntest_159()\n\ndef test_174():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_174\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-ir-6\") == output\ntest_174()\n\ndef test_180():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_180\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE-BOM-SIG\") == output\ntest_180()\n\ndef test_181():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_181\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_181()\n\ndef test_182():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_182\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1-UNICODE\") == output\ntest_182()\n\ndef test_183():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_183\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-BE-BOM\") == output\ntest_183()\n\ndef test_185():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_185\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-15\") == output\ntest_185()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla-bla\") == output\ntest_186()\n\ndef test_188():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_188\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ANSI_X3.110-1983\") == output\ntest_188()\n\ndef test_193():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_193\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-16-bom\") == output\ntest_193()\n\ndef test_194():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_194\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('iso8859-1') == output\ntest_194()\n\ndef test_200():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_200\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16-LE\") == output\ntest_200()\n\ndef test_205():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_205\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_8859_15') == output\ntest_205()\n\ndef test_207():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_207\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1-sig') == output\ntest_207()\n\ndef test_210():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_210\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1_SIG') == output\ntest_210()\n\ndef test_211():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_211\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"LATIN1\") == output\ntest_211()\n\ndef test_213():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_213\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--bla-bla-bla-bla\") == output\ntest_213()\n\ndef test_214():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_214\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"iso-8859-1\") == output\ntest_214()\n\ndef test_217():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_217\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf8') == output\ntest_217()\n\ndef test_219():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_219\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('UTF8-sig') == output\ntest_219()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla-bla\") == output\ntest_224()\n\ndef test_225():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_225\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf8') == output\ntest_225()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf') == output\ntest_226()\n\ndef test_229():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_229\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bla-bla-bla-bla-bla-bla\") == output\ntest_229()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_8859-1:1998:bogus\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"8859\") == output\ntest_231()\n\ndef test_233():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_233\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-16\") == output\ntest_233()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ascii_sig\") == output\ntest_234()\n\ndef test_239():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_239\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"UTF-32-LE\") == output\ntest_239()\n\ndef test_252():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_252\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"cp819\") == output\ntest_252()\n\ndef test_261():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_261\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"  utf_8-BAZ\") == output\ntest_261()\n\ndef test_265():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_265\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-B\") == output\ntest_265()\n\ndef test_266():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_266\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1--\") == output\ntest_266()\n\ndef test_267():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_267\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf8\") == output\ntest_267()\n\ndef test_268():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_268\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ASCii\") == output\ntest_268()\n\ndef test_271():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_271\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BoM\") == output\ntest_271()\n\ndef test_272():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_272\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf-8-sig') == output\ntest_272()\n\ndef test_273():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_273\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('LATIN1') == output\ntest_273()\n\ndef test_278():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_278\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('utf_8_sig') == output\ntest_278()\n\ndef test_281():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_281\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932-SIG') == output\ntest_281()\n\ndef test_287():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_287\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin1\") == output\ntest_287()\n\ndef test_292():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_292\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-BOM\") == output\ntest_292()\n\ndef test_295():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_295\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin-1') == output\ntest_295()\n\ndef test_297():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_297\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"Latin1\") == output\ntest_297()\n\ndef test_298():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_298\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1-foo') == output\ntest_298()\n\ndef test_299():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_299\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp932_SIG') == output\ntest_299()\n\ndef test_304():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_304\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1') == output\ntest_304()\n\ndef test_311():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_311\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"utf-8-bOM\") == output\ntest_311()\n\ndef test_312():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_312\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    iso_latin_1') == output\ntest_312()\n\ndef test_315():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_315\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-BOM!\") == output\ntest_315()\n\ndef test_320():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_320\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"mac_roman\") == output\ntest_320()\n\ndef test_321():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_321\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-xx\") == output\ntest_321()\n\ndef test_322():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_322\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"us-ascii\") == output\ntest_322()\n\ndef test_324():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_324\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-8-Sig\") == output\ntest_324()\n\ndef test_325():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_325\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"uTf-16-SIG\") == output\ntest_325()\n\ndef test_326():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_326\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('    utf-8') == output\ntest_326()\n\ndef test_339():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_339\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"latin-1-bla-bla-bla-bla\") == output\ntest_339()\n\ndef test_343():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_343\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name(\"ISO_646.IRV:1991\") == output\ntest_343()\n\ndef test_345():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_345\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('cp-1252') == output\ntest_345()\n\ndef test_347():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_347\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('latin1-sig') == output\ntest_347()\n\ndef test_349():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/_get_normal_name/test_349\", \"rb\") as f:\n        output = pickle.load(f)\n    assert _get_normal_name('euc_jp') == output\ntest_349()\n\n\n"]}
{"task_id": 352, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    import codecs\n    from typing import Callable, Tuple, List\n\n    ENCODING_PATTERN = re.compile(\n        rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\"\n    )\n\n    def find_cookie(line: bytes) -> str:\n        match = ENCODING_PATTERN.match(line)\n        if match:\n            return match.group(1).decode('ascii')\n        return ''\n\n    lines = []\n    try:\n        first_line = readline()\n    except Exception:\n        first_line = b\"\"\n    if first_line is None:\n        first_line = b\"\"\n    lines.append(first_line)\n\n    encoding = None\n    bom_found = False\n\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = \"utf-8-sig\"\n        # Remove BOM from the first line for cookie check\n        line_to_check = first_line[len(codecs.BOM_UTF8):]\n    else:\n        line_to_check = first_line\n\n    cookie_encoding = find_cookie(line_to_check)\n\n    if not cookie_encoding:\n        try:\n            second_line = readline()\n        except Exception:\n            second_line = b\"\"\n        if second_line is None:\n            second_line = b\"\"\n        lines.append(second_line)\n        if second_line:\n            cookie_encoding = find_cookie(second_line)\n\n    if cookie_encoding:\n        # Validate cookie_encoding\n        try:\n            codecs.lookup(cookie_encoding)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {cookie_encoding}\")\n        if bom_found and cookie_encoding.lower() != \"utf-8\":\n            # Both BOM and cookie exist and differ\n            raise SyntaxError(\"encoding problem: utf-8 BOM and encoding cookie mismatch\")\n        encoding = cookie_encoding\n\n    if encoding is None:\n        encoding = \"utf-8\"\n\n    return encoding, lines", "def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    import codecs\n\n    cookie_re = re.compile(rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\")\n    lines = []\n\n    def read_or_empty():\n        try:\n            return readline()\n        except Exception:\n            return b''\n\n    # Read first line\n    first_line = read_or_empty()\n    lines.append(first_line)\n\n    # Check for UTF-8 BOM\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = \"utf-8-sig\"\n        # remove BOM for the purpose of detecting cookie on next line (if any)\n        line_1 = first_line[len(codecs.BOM_UTF8):]\n    else:\n        bom_found = False\n        encoding = None\n        line_1 = first_line\n\n    # Check first line for cookie\n    m = cookie_re.match(line_1)\n    if m:\n        try:\n            cookie_encoding = m.group(1).decode('ascii')\n        except UnicodeDecodeError:\n            raise SyntaxError(\"invalid coding cookie\")\n    else:\n        cookie_encoding = None\n\n    # Read second line\n    second_line = read_or_empty()\n    if second_line:\n        lines.append(second_line)\n    else:\n        second_line = b''\n\n    # Check second line for cookie (only if not found on first line)\n    if not cookie_encoding:\n        m = cookie_re.match(second_line)\n        if m:\n            try:\n                cookie_encoding = m.group(1).decode('ascii')\n            except UnicodeDecodeError:\n                raise SyntaxError(\"invalid coding cookie\")\n\n    if bom_found:\n        if cookie_encoding and cookie_encoding.lower() not in (\"utf-8\", \"utf_8\"):\n            raise SyntaxError(\"encoding problem: utf-8 BOM and coding cookie disagree\")\n        return encoding, lines\n    else:\n        if cookie_encoding:\n            try:\n                # Validate encoding name\n                codecs.lookup(cookie_encoding)\n            except LookupError:\n                raise SyntaxError(f\"unknown encoding: {cookie_encoding}\")\n            return cookie_encoding, lines\n        else:\n            return \"utf-8\", lines", "def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    from typing import Callable, Tuple, List\n    encoding = None\n    bom_found = False\n    raw = []\n    cookie_re = re.compile(br\"coding[=:]\\s*([-\\w.]+)\")\n    def get_cookie(line):\n        # Check for coding cookie in a line\n        match = cookie_re.search(line)\n        if not match:\n            return None\n        return match.group(1).decode(\"ascii\")\n\n    # Read first line\n    try:\n        first_line = readline()\n    except Exception:\n        first_line = b''\n\n    raw.append(first_line)\n\n    # Check for BOM\n    if first_line.startswith(b'\\xef\\xbb\\xbf'):\n        bom_found = True\n        encoding = 'utf-8-sig'\n        # Remove BOM so next line check is correct\n        first_line = first_line[3:]\n\n    # Check for encoding cookie in first line\n    cookie1 = get_cookie(first_line)\n\n    # Read second line\n    try:\n        second_line = readline()\n    except Exception:\n        second_line = b''\n\n    if second_line:\n        raw.append(second_line)\n        cookie2 = get_cookie(second_line)\n    else:\n        cookie2 = None\n\n    cookie = cookie1 or cookie2\n\n    if cookie:\n        try:\n            # Validate encoding name by trying to lookup codec\n            ''.encode(cookie)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {cookie}\")\n\n    if bom_found:\n        if cookie and cookie.lower() != 'utf-8':\n            raise SyntaxError(\"encoding problem: BOM and coding cookie disagree\")\n        return encoding, raw\n\n    if cookie:\n        return cookie, raw\n\n    return 'utf-8', raw", "from typing import Callable, Tuple, List\nimport re\nimport codecs\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    coding_re = re.compile(rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\")\n    lines = []\n    bom_found = False\n\n    def find_cookie(line: bytes) -> str:\n        # look for coding cookie on the line\n        m = coding_re.search(line)\n        if not m:\n            return ''\n        return m.group(1).decode('ascii')\n\n    # read at most two lines and store them in list\n    try:\n        first_line = readline()\n    except StopIteration:\n        first_line = b''\n    if not first_line:\n        return 'utf-8', []\n\n    lines.append(first_line)\n\n    # Check for BOM first\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = 'utf-8-sig'\n        # Remove the BOM from the first line for cookie detection\n        first_line_stripped = first_line[len(codecs.BOM_UTF8):]\n        cookie_encoding = find_cookie(first_line_stripped)\n    else:\n        encoding = ''\n        cookie_encoding = find_cookie(first_line)\n\n    # Read second line if needed\n    if not cookie_encoding:\n        try:\n            second_line = readline()\n        except StopIteration:\n            second_line = b''\n        if second_line:\n            lines.append(second_line)\n            cookie_encoding = find_cookie(second_line)\n\n    if not encoding:\n        # No BOM found, use cookie or default utf-8\n        if cookie_encoding:\n            encoding = cookie_encoding\n        else:\n            encoding = 'utf-8'\n    else:\n        # Encoding set to utf-8-sig due to BOM\n        if cookie_encoding:\n            # Check if cookie encoding conflicts with BOM encoding\n            norm_cookie = codecs.lookup(cookie_encoding).name\n            if norm_cookie != 'utf-8':\n                raise SyntaxError(\"encoding problem: utf-8 BOM and \" +\n                                  \"encoding cookie disagree\")\n    # Validate encoding\n    try:\n        codecs.lookup(encoding)\n    except LookupError:\n        raise SyntaxError(\"invalid encoding: {}\".format(encoding))\n\n    return encoding, lines", "def detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    from typing import Callable, Tuple, List\n\n    BOM_UTF8 = b'\\xef\\xbb\\xbf'\n    COOKIE_RE = re.compile(br\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\")\n\n    def _get_normal_name(orig_enc: str) -> str:\n        # Normalize encoding name for comparison\n        try:\n            import codecs\n            return codecs.lookup(orig_enc).name\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {orig_enc!r}\")\n\n    lines: List[bytes] = []\n    try:\n        first = readline()\n    except Exception:\n        first = b''\n    if first.startswith(BOM_UTF8):\n        # UTF-8 BOM found, encoding is 'utf-8-sig'\n        encoding = 'utf-8-sig'\n        lines.append(first)\n        try:\n            second = readline()\n        except Exception:\n            second = b''\n        if second:\n            lines.append(second)\n        # Even though PEP 263 says check for cookie on first or second line,\n        # Python itself does not check for further encodings once BOM is found.\n        # However, PEP 263 says if both BOM and cookie present but disagree, error.\n        # So we will check the second line for cookie if present.\n        if lines and len(lines) > 1:\n            m = COOKIE_RE.match(lines[1])\n            if m:\n                cookie_enc = m.group(1).decode('ascii')\n                # Normalize both\n                normalized_bom = _get_normal_name('utf-8-sig')\n                normalized_cookie = _get_normal_name(cookie_enc)\n                if normalized_bom != normalized_cookie:\n                    raise SyntaxError(\n                        f\"encoding mismatch: BOM says utf-8-sig but cookie says {cookie_enc!r}\"\n                    )\n        return encoding, lines\n\n    # No BOM, check first line for cookie\n    lines.append(first)\n    m = COOKIE_RE.match(first)\n    if m:\n        cookie_enc = m.group(1).decode('ascii')\n        try:\n            _get_normal_name(cookie_enc)\n        except SyntaxError:\n            raise SyntaxError(f\"invalid encoding cookie: {cookie_enc!r}\")\n        # Also read second line to check for second cookie (only first two lines allowed)\n        try:\n            second = readline()\n        except Exception:\n            second = b''\n        if second:\n            lines.append(second)\n            m2 = COOKIE_RE.match(second)\n            if m2:\n                cookie_enc_2 = m2.group(1).decode('ascii')\n                if cookie_enc != cookie_enc_2:\n                    raise SyntaxError(\n                        f\"encoding mismatch: first line has {cookie_enc!r}, \"\n                        f\"second line has {cookie_enc_2!r}\"\n                    )\n        return cookie_enc, lines\n\n    # No BOM, no cookie\n    try:\n        second = readline()\n    except Exception:\n        second = b''\n    if second:\n        lines.append(second)\n    return 'utf-8', lines"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    import codecs\n    from typing import Callable, Tuple, List\n\n    ENCODING_PATTERN = re.compile(\n        rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\"\n    )\n\n    def find_cookie(line: bytes) -> str:\n        match = ENCODING_PATTERN.match(line)\n        if match:\n            return match.group(1).decode('ascii')\n        return ''\n\n    lines = []\n    try:\n        first_line = readline()\n    except Exception:\n        first_line = b\"\"\n    if first_line is None:\n        first_line = b\"\"\n    lines.append(first_line)\n\n    encoding = None\n    bom_found = False\n\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = \"utf-8-sig\"\n        # Remove BOM from the first line for cookie check\n        line_to_check = first_line[len(codecs.BOM_UTF8):]\n    else:\n        line_to_check = first_line\n\n    cookie_encoding = find_cookie(line_to_check)\n\n    if not cookie_encoding:\n        try:\n            second_line = readline()\n        except Exception:\n            second_line = b\"\"\n        if second_line is None:\n            second_line = b\"\"\n        lines.append(second_line)\n        if second_line:\n            cookie_encoding = find_cookie(second_line)\n\n    if cookie_encoding:\n        # Validate cookie_encoding\n        try:\n            codecs.lookup(cookie_encoding)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {cookie_encoding}\")\n        if bom_found and cookie_encoding.lower() != \"utf-8\":\n            # Both BOM and cookie exist and differ\n            raise SyntaxError(\"encoding problem: utf-8 BOM and encoding cookie mismatch\")\n        encoding = cookie_encoding\n\n    if encoding is None:\n        encoding = \"utf-8\"\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    import codecs\n\n    cookie_re = re.compile(rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\")\n    lines = []\n\n    def read_or_empty():\n        try:\n            return readline()\n        except Exception:\n            return b''\n\n    # Read first line\n    first_line = read_or_empty()\n    lines.append(first_line)\n\n    # Check for UTF-8 BOM\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = \"utf-8-sig\"\n        # remove BOM for the purpose of detecting cookie on next line (if any)\n        line_1 = first_line[len(codecs.BOM_UTF8):]\n    else:\n        bom_found = False\n        encoding = None\n        line_1 = first_line\n\n    # Check first line for cookie\n    m = cookie_re.match(line_1)\n    if m:\n        try:\n            cookie_encoding = m.group(1).decode('ascii')\n        except UnicodeDecodeError:\n            raise SyntaxError(\"invalid coding cookie\")\n    else:\n        cookie_encoding = None\n\n    # Read second line\n    second_line = read_or_empty()\n    if second_line:\n        lines.append(second_line)\n    else:\n        second_line = b''\n\n    # Check second line for cookie (only if not found on first line)\n    if not cookie_encoding:\n        m = cookie_re.match(second_line)\n        if m:\n            try:\n                cookie_encoding = m.group(1).decode('ascii')\n            except UnicodeDecodeError:\n                raise SyntaxError(\"invalid coding cookie\")\n\n    if bom_found:\n        if cookie_encoding and cookie_encoding.lower() not in (\"utf-8\", \"utf_8\"):\n            raise SyntaxError(\"encoding problem: utf-8 BOM and coding cookie disagree\")\n        return encoding, lines\n    else:\n        if cookie_encoding:\n            try:\n                # Validate encoding name\n                codecs.lookup(cookie_encoding)\n            except LookupError:\n                raise SyntaxError(f\"unknown encoding: {cookie_encoding}\")\n            return cookie_encoding, lines\n        else:\n            return \"utf-8\", lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    from typing import Callable, Tuple, List\n    encoding = None\n    bom_found = False\n    raw = []\n    cookie_re = re.compile(br\"coding[=:]\\s*([-\\w.]+)\")\n    def get_cookie(line):\n        # Check for coding cookie in a line\n        match = cookie_re.search(line)\n        if not match:\n            return None\n        return match.group(1).decode(\"ascii\")\n\n    # Read first line\n    try:\n        first_line = readline()\n    except Exception:\n        first_line = b''\n\n    raw.append(first_line)\n\n    # Check for BOM\n    if first_line.startswith(b'\\xef\\xbb\\xbf'):\n        bom_found = True\n        encoding = 'utf-8-sig'\n        # Remove BOM so next line check is correct\n        first_line = first_line[3:]\n\n    # Check for encoding cookie in first line\n    cookie1 = get_cookie(first_line)\n\n    # Read second line\n    try:\n        second_line = readline()\n    except Exception:\n        second_line = b''\n\n    if second_line:\n        raw.append(second_line)\n        cookie2 = get_cookie(second_line)\n    else:\n        cookie2 = None\n\n    cookie = cookie1 or cookie2\n\n    if cookie:\n        try:\n            # Validate encoding name by trying to lookup codec\n            ''.encode(cookie)\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {cookie}\")\n\n    if bom_found:\n        if cookie and cookie.lower() != 'utf-8':\n            raise SyntaxError(\"encoding problem: BOM and coding cookie disagree\")\n        return encoding, raw\n\n    if cookie:\n        return cookie, raw\n\n    return 'utf-8', raw\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\nfrom typing import Callable, Tuple, List\nimport re\nimport codecs\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    coding_re = re.compile(rb\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\")\n    lines = []\n    bom_found = False\n\n    def find_cookie(line: bytes) -> str:\n        # look for coding cookie on the line\n        m = coding_re.search(line)\n        if not m:\n            return ''\n        return m.group(1).decode('ascii')\n\n    # read at most two lines and store them in list\n    try:\n        first_line = readline()\n    except StopIteration:\n        first_line = b''\n    if not first_line:\n        return 'utf-8', []\n\n    lines.append(first_line)\n\n    # Check for BOM first\n    if first_line.startswith(codecs.BOM_UTF8):\n        bom_found = True\n        encoding = 'utf-8-sig'\n        # Remove the BOM from the first line for cookie detection\n        first_line_stripped = first_line[len(codecs.BOM_UTF8):]\n        cookie_encoding = find_cookie(first_line_stripped)\n    else:\n        encoding = ''\n        cookie_encoding = find_cookie(first_line)\n\n    # Read second line if needed\n    if not cookie_encoding:\n        try:\n            second_line = readline()\n        except StopIteration:\n            second_line = b''\n        if second_line:\n            lines.append(second_line)\n            cookie_encoding = find_cookie(second_line)\n\n    if not encoding:\n        # No BOM found, use cookie or default utf-8\n        if cookie_encoding:\n            encoding = cookie_encoding\n        else:\n            encoding = 'utf-8'\n    else:\n        # Encoding set to utf-8-sig due to BOM\n        if cookie_encoding:\n            # Check if cookie encoding conflicts with BOM encoding\n            norm_cookie = codecs.lookup(cookie_encoding).name\n            if norm_cookie != 'utf-8':\n                raise SyntaxError(\"encoding problem: utf-8 BOM and \" +\n                                  \"encoding cookie disagree\")\n    # Validate encoding\n    try:\n        codecs.lookup(encoding)\n    except LookupError:\n        raise SyntaxError(\"invalid encoding: {}\".format(encoding))\n\n    return encoding, lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    import re\n    from typing import Callable, Tuple, List\n\n    BOM_UTF8 = b'\\xef\\xbb\\xbf'\n    COOKIE_RE = re.compile(br\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\")\n\n    def _get_normal_name(orig_enc: str) -> str:\n        # Normalize encoding name for comparison\n        try:\n            import codecs\n            return codecs.lookup(orig_enc).name\n        except LookupError:\n            raise SyntaxError(f\"unknown encoding: {orig_enc!r}\")\n\n    lines: List[bytes] = []\n    try:\n        first = readline()\n    except Exception:\n        first = b''\n    if first.startswith(BOM_UTF8):\n        # UTF-8 BOM found, encoding is 'utf-8-sig'\n        encoding = 'utf-8-sig'\n        lines.append(first)\n        try:\n            second = readline()\n        except Exception:\n            second = b''\n        if second:\n            lines.append(second)\n        # Even though PEP 263 says check for cookie on first or second line,\n        # Python itself does not check for further encodings once BOM is found.\n        # However, PEP 263 says if both BOM and cookie present but disagree, error.\n        # So we will check the second line for cookie if present.\n        if lines and len(lines) > 1:\n            m = COOKIE_RE.match(lines[1])\n            if m:\n                cookie_enc = m.group(1).decode('ascii')\n                # Normalize both\n                normalized_bom = _get_normal_name('utf-8-sig')\n                normalized_cookie = _get_normal_name(cookie_enc)\n                if normalized_bom != normalized_cookie:\n                    raise SyntaxError(\n                        f\"encoding mismatch: BOM says utf-8-sig but cookie says {cookie_enc!r}\"\n                    )\n        return encoding, lines\n\n    # No BOM, check first line for cookie\n    lines.append(first)\n    m = COOKIE_RE.match(first)\n    if m:\n        cookie_enc = m.group(1).decode('ascii')\n        try:\n            _get_normal_name(cookie_enc)\n        except SyntaxError:\n            raise SyntaxError(f\"invalid encoding cookie: {cookie_enc!r}\")\n        # Also read second line to check for second cookie (only first two lines allowed)\n        try:\n            second = readline()\n        except Exception:\n            second = b''\n        if second:\n            lines.append(second)\n            m2 = COOKIE_RE.match(second)\n            if m2:\n                cookie_enc_2 = m2.group(1).decode('ascii')\n                if cookie_enc != cookie_enc_2:\n                    raise SyntaxError(\n                        f\"encoding mismatch: first line has {cookie_enc!r}, \"\n                        f\"second line has {cookie_enc_2!r}\"\n                    )\n        return cookie_enc, lines\n\n    # No BOM, no cookie\n    try:\n        second = readline()\n    except Exception:\n        second = b''\n    if second:\n        lines.append(second)\n    return 'utf-8', lines\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_4():\n    assert \"utf-8\" == detect_encoding(lambda: b'')[0]\ntest_4()\n\ndef test_13():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf')[0]\ntest_13()\n\ndef test_17():\n    assert detect_encoding(iter([b\"# coding: \\xFF\\xFF\\xFF\\xFF\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: \\xFF\\xFF\\xFF\\xFF', b'foo'])\ntest_17()\n\ndef test_23():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n# coding=utf-8')[0]\ntest_23()\n\ndef test_32():\n    assert detect_encoding(lambda: b\"#coding=UTF-8\\n\") == (\"utf-8\", [b\"#coding=UTF-8\\n\"])\ntest_32()\n\ndef test_36():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\nprint\"])\ntest_36()\n\ndef test_39():\n    assert detect_encoding(lambda: b\"# coding:ascii\\n\") == (\"ascii\", [b\"# coding:ascii\\n\"])\ntest_39()\n\ndef test_52():\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\") == ('cp1252', [b\"#coding=cp1252\\n\"])\ntest_52()\n\ndef test_59():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding=utf-8')[0]\ntest_59()\n\ndef test_61():\n    assert detect_encoding(iter([b\"# coding: utf-8\", b\"foo\"]).__next__) == ('utf-8', [b'# coding: utf-8'])\ntest_61()\n\ndef test_63():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\nprint\"])\ntest_63()\n\ndef test_67():\n    assert detect_encoding(lambda: b\"# coding=ascii\\n\") == (\"ascii\", [b\"# coding=ascii\\n\"])\ntest_67()\n\ndef test_69():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\n\"])\ntest_69()\n\ndef test_74():\n    assert detect_encoding(lambda: b\"#coding: utf-8\\n\") == (\"utf-8\", [b\"#coding: utf-8\\n\"])\ntest_74()\n\ndef test_90():\n    assert detect_encoding(lambda: b\"\") == (\"utf-8\", [])\ntest_90()\n\ndef test_99():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\"])\ntest_99()\n\ndef test_102():\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == ('utf-8', [b'# -*- coding: utf-8 -*-\\n'])\ntest_102()\n\ndef test_103():\n    assert \"utf-8\" == detect_encoding(lambda: b'# coding=')[0]\ntest_103()\n\ndef test_106():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\r\\nprint\"])\ntest_106()\n\ndef test_108():\n    assert detect_encoding(lambda: b\"#coding:UTF-8\\n\") == (\"utf-8\", [b\"#coding:UTF-8\\n\"])\ntest_108()\n\ndef test_113():\n    assert detect_encoding(lambda: b\"#coding= cp949\\n\") == (\"cp949\", [b\"#coding= cp949\\n\"])\ntest_113()\n\ndef test_118():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\n\"])\ntest_118()\n\ndef test_121():\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf# coding=utf-8\\n') == ('utf-8-sig', [b'# coding=utf-8\\n'])\ntest_121()\n\ndef test_126():\n    assert detect_encoding(lambda:b'\\xe3\\x83\\x9b\\n') == ('utf-8', [b'\\xe3\\x83\\x9b\\n'])\ntest_126()\n\ndef test_128():\n    assert detect_encoding(\n        iter([b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"foo = 'bar'\"])\ntest_128()\n\ndef test_129():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_129()\n\ndef test_130():\n    assert \"utf-8\" == detect_encoding(lambda: b'a = 1')[0]\ntest_130()\n\ndef test_138():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\\r\\nprint\"])\ntest_138()\n\ndef test_154():\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"foo = 'bar'\"]).__next__\n    ) == (\"utf-8\", [b\"#!/usr/bin/python\", b\"foo = 'bar'\"])\ntest_154()\n\ndef test_156():\n    assert detect_encoding(lambda: b'# coding=utf-8\\n') == ('utf-8', [b'# coding=utf-8\\n'])\ntest_156()\n\ndef test_162():\n    assert detect_encoding(lambda: b\"#coding=euc-kr\\n\") == (\"euc-kr\", [b\"#coding=euc-kr\\n\"])\ntest_162()\n\ndef test_165():\n    assert detect_encoding((lambda: b\"# coding: ascii\\n\").__call__) == (\"ascii\", [b\"# coding: ascii\\n\"])\ntest_165()\n\ndef test_167():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# -*- coding: utf-8 -*-\\n') == ('utf-8-sig', [b'# -*- coding: utf-8 -*-\\n'])\ntest_167()\n\ndef test_169():\n    assert detect_encoding(iter([b'# coding: utf-8', b'', b'', b'', b'']).__next__) == (\"utf-8\", [b'# coding: utf-8'])\ntest_169()\n\ndef test_172():\n    assert detect_encoding(lambda:b'') == ('utf-8', [])\ntest_172()\n\ndef test_173():\n    assert detect_encoding(iter([]).__next__) == (\"utf-8\", [])\ntest_173()\n\ndef test_176():\n    assert detect_encoding(lambda: b\"#coding=cp949\\n\") == (\"cp949\", [b\"#coding=cp949\\n\"])\ntest_176()\n\ndef test_177():\n    assert detect_encoding(lambda:b'# coding=utf-8\\n\\n') == ('utf-8', [b'# coding=utf-8\\n\\n'])\ntest_177()\n\ndef test_181():\n    assert detect_encoding(lambda: b\"#coding: cp949\\n\") == (\"cp949\", [b\"#coding: cp949\\n\"])\ntest_181()\n\ndef test_189():\n    assert detect_encoding(lambda: b\"#coding=utf-8\\n\") == (\"utf-8\", [b\"#coding=utf-8\\n\"])\ntest_189()\n\ndef test_198():\n    assert detect_encoding(lambda: b\"#coding=euc_kr\\n\") == (\"euc_kr\", [b\"#coding=euc_kr\\n\"])\ntest_198()\n\ndef test_199():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8\\n') == ('utf-8-sig', [b'# coding: utf-8\\n'])\ntest_199()\n\ndef test_203():\n    assert detect_encoding(iter([b\"# coding:\", b\"foo\"]).__next__) == ('utf-8', [b'# coding:', b'foo'])\ntest_203()\n\ndef test_204():\n    assert detect_encoding(lambda: b'') == ('utf-8', [])\ntest_204()\n\ndef test_208():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\"])\ntest_208()\n\ndef test_209():\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\") == ('cp1252', [b\"# coding=cp1252\\n\"])\ntest_209()\n\ndef test_212():\n    assert detect_encoding((lambda: b\"# coding: ascii\\r\\n\\r\\nprint\").__call__) == (\"ascii\", [b\"# coding: ascii\\r\\n\\r\\nprint\"])\ntest_212()\n\ndef test_220():\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf# coding: utf-8-sig\\n') == ('utf-8-sig', [b'# coding: utf-8-sig\\n'])\ntest_220()\n\ndef test_222():\n    assert detect_encoding(lambda: b\"\") == ('utf-8', [])\ntest_222()\n\ndef test_235():\n    assert detect_encoding(lambda: b'# -*- coding: iso8859-15 -*-\\n') == ('iso8859-15', [b'# -*- coding: iso8859-15 -*-\\n'])\ntest_235()\n\ndef test_246():\n    assert detect_encoding(lambda: b\"#coding:cp949\\n\") == (\"cp949\", [b\"#coding:cp949\\n\"])\ntest_246()\n\ndef test_248():\n    assert \"utf-8-sig\" == detect_encoding(lambda: b'\\xef\\xbb\\xbf\\na = 1')[0]\ntest_248()\n\ndef test_250():\n    assert detect_encoding(lambda: b'# coding: utf-8\\n') == ('utf-8', [b'# coding: utf-8\\n'])\ntest_250()\n\ndef test_3():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_3\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding=ascii\\n\") == output\ntest_3()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"foo\", b\"bar\"]).__next__) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n') == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n# coding=utf-8\\n') == output\ntest_9()\n\ndef test_10():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_10\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n\\n') == output\ntest_10()\n\ndef test_12():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_12\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_12()\n\ndef test_21():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_21\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n') == output\ntest_21()\n\ndef test_25():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_25\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding: utf-8-sig\\n') == output\ntest_25()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n\") == output\ntest_40()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# vim: set fileencoding=utf-8 :\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_44()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\") == output\ntest_46()\n\ndef test_50():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_50\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_50()\n\ndef test_57():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_57\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_57()\n\ndef test_60():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_60\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=cp1252\\n') == output\ntest_60()\n\ndef test_66():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_66\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n') == output\ntest_66()\n\ndef test_72():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_72\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\xef\\xbb\\xbf\\n') == output\ntest_72()\n\ndef test_77():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_77\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\n\") == output\ntest_77()\n\ndef test_78():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_78\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# coding:ascii\\n\") == output\ntest_78()\n\ndef test_79():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_79\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n') == output\ntest_79()\n\ndef test_80():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_80\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf\\n') == output\ntest_80()\n\ndef test_84():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_84\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n# hello') == output\ntest_84()\n\ndef test_85():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_85\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n') == output\ntest_85()\n\ndef test_87():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_87\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding=utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_87()\n\ndef test_92():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_92\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n') == output\ntest_92()\n\ndef test_93():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_93\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: utf-8', b'# coding: utf-8', b'', b'', b'']).__next__) == output\ntest_93()\n\ndef test_97():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_97\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8\", b\"foo\"]).__next__) == output\ntest_97()\n\ndef test_100():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_100\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding((lambda: b\"\\n# coding: ascii\").__call__) == output\ntest_100()\n\ndef test_104():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_104\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# vim: set fileencoding=ascii :\\n\") == output\ntest_104()\n\ndef test_111():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_111\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'\\xef\\xbb\\xbf', b'', b'', b'', b'']).__next__) == output\ntest_111()\n\ndef test_125():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_125\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\") == output\ntest_125()\n\ndef test_127():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_127\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'def foo(): pass']).__next__) == output\ntest_127()\n\ndef test_132():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_132\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# coding: utf-8\\n') == output\ntest_132()\n\ndef test_133():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_133\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"#!/usr/bin/python\", b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_133()\n\ndef test_142():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_142\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: utf-8 -*-\\n') == output\ntest_142()\n\ndef test_146():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_146\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n# -*- coding: ascii -*-\\n\") == output\ntest_146()\n\ndef test_148():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_148\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b'# coding: latin1', b'\\xef\\xbb\\xbf', b'', b'', b'']).__next__) == output\ntest_148()\n\ndef test_155():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_155\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n') == output\ntest_155()\n\ndef test_160():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_160\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"#!/usr/bin/python\", b\"# -*- coding: utf-8 -*-\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_160()\n\ndef test_175():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_175\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252 \") == output\ntest_175()\n\ndef test_184():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_184\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda:b'\\n# coding=utf-8\\n') == output\ntest_184()\n\ndef test_186():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_186\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n\\n') == output\ntest_186()\n\ndef test_190():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_190\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=iso-8859-1') == output\ntest_190()\n\ndef test_191():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_191\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n\") == output\ntest_191()\n\ndef test_192():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_192\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\r\\n') == output\ntest_192()\n\ndef test_195():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_195\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n# coding=latin-1') == output\ntest_195()\n\ndef test_197():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_197\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# coding=latin-1\\n\\n') == output\ntest_197()\n\ndef test_201():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_201\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# -*- coding: cp1252 -*-\\n') == output\ntest_201()\n\ndef test_206():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_206\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'# coding=latin-1\\n# coding: utf-8\\n') == output\ntest_206()\n\ndef test_215():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_215\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252\\n\\n1\") == output\ntest_215()\n\ndef test_218():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_218\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n\\n') == output\ntest_218()\n\ndef test_224():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_224\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding:\\n\") == output\ntest_224()\n\ndef test_226():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_226\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"# coding=cp1252 \") == output\ntest_226()\n\ndef test_230():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_230\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#!/usr/bin/python\\n\") == output\ntest_230()\n\ndef test_231():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_231\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(iter([b\"# coding: utf-8-sig\", b\"foo\"]).__next__) == output\ntest_231()\n\ndef test_234():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_234\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"\\xef\\xbb\\xbf\\n\") == output\ntest_234()\n\ndef test_237():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_237\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\xef\\xbb\\xbf') == output\ntest_237()\n\ndef test_240():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_240\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=latin-1 :\\n\\n\\nprint(\"hello world!\")') == output\ntest_240()\n\ndef test_244():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_244\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(\n        iter([b\"# coding: utf-8\", b\"foo = 'bar'\"]).__next__\n    ) == output\ntest_244()\n\ndef test_245():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_245\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'#!/usr/bin/python\\n# vim: set fileencoding=utf-8 :\\n') == output\ntest_245()\n\ndef test_247():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_247\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b\"#coding=cp1252\\n\\n1\") == output\ntest_247()\n\ndef test_249():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_249\", \"rb\") as f:\n        output = pickle.load(f)\n    assert detect_encoding(lambda: b'\\n#!/usr/bin/python\\n' b'# vim: set fileencoding=latin-1:\\n' b'# coding=latin-1\\n\\n') == output\ntest_249()\n\n\ndef test_extra_5():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_5\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=utf-8\\n\") == output\ntest_extra_5()\n\ndef test_extra_6():\n\twith open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/detect_encoding/test_extra_6\", \"rb\") as f:\n\t\toutput = pickle.load(f)\n\t# Test with a single line that has a valid encoding cookie\n\tassert detect_encoding(lambda: b\"# coding=cp1252\\n\") == output\ntest_extra_6()"]}
{"task_id": 353, "project": "test-apps/black/src", "module": "blib2to3.pgen2.tokenize", "predictions": ["def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import io\n    from token import COMMENT, NL, NEWLINE, INDENT, DEDENT\n\n    out = io.StringIO()\n    prev_end = (1, 0)\n    indents = 0\n    last_line = ''\n    for token in iterable:\n        tok_type, tok_string, start, end, line_text = token[:5]\n        (start_line, start_col) = start\n        (end_line, end_col) = end\n\n        # Add whitespace based on position difference\n        if prev_end[0] < start_line:\n            # new lines in between\n            out.write('\\n' * (start_line - prev_end[0]))\n            out.write(' ' * start_col)\n        else:\n            # same line\n            out.write(' ' * (start_col - prev_end[1]))\n\n        out.write(tok_string)\n        prev_end = (end_line, end_col)\n\n    return out.getvalue()", "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import tokenize\n\n    result = []\n    prev_end = (1, 0)\n    linebuf = ''\n\n    for tok in iterable:\n        toknum, tokval = tok[:2]\n        start = getattr(tok, 'start', None)\n        end = getattr(tok, 'end', None)\n        if start is None or end is None:\n            # If token does not have start/end, just append value\n            result.append(tokval)\n            continue\n\n        srow, scol = start\n        erow, ecol = end\n        # Handle gaps between tokens\n        if prev_end[0] < srow:\n            # Newlines needed\n            linebuf += '\\n' * (srow - prev_end[0])\n            linebuf += ' ' * scol\n        else:\n            # Spaces needed on the same line\n            linebuf += ' ' * (scol - prev_end[1])\n\n        linebuf += tokval\n        prev_end = (erow, ecol)\n\n    return linebuf", "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).__next__\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n\n    from io import StringIO\n    from tokenize import TokenInfo\n\n    # Buffer to accumulate result\n    result = []\n    # Track previous token ending position (row, col)\n    prev_end = (0, 0)\n    # Track previous line text\n    line_buffer = \"\"\n\n    for tok in iterable:\n        # Unpack token info fields: number, string, start, end, line\n        # Since iterable elements have at least two elements, but ideally are TokenInfo objects\n        # Defensive: support tuple-like token or TokenInfo object\n        if isinstance(tok, TokenInfo):\n            toknum, tokval, startpos, endpos, line = tok\n        else:\n            # Minimal expected: first two items token number and string\n            toknum = tok[0]\n            tokval = tok[1]\n            startpos = tok[2] if len(tok) > 2 else (0, 0)\n            endpos = tok[3] if len(tok) > 3 else (0, 0)\n            line = tok[4] if len(tok) > 4 else \"\"\n\n        srow, scol = startpos\n        erow, ecol = endpos\n\n        # Handle line and column gaps by adding whitespace and newlines as needed\n        if prev_end[0] < srow:\n            # New lines between previous token end and current token start\n            # Append newlines to join lines\n            for i in range(prev_end[0], srow - 1):\n                result.append('\\n')\n            # Reset column position after new lines\n            prev_end = (srow, 0)\n\n        if prev_end[1] < scol:\n            # Spaces between previous token end column and current token start column\n            result.append(' ' * (scol - prev_end[1]))\n\n        result.append(tokval)\n        prev_end = (erow, ecol)\n\n    return ''.join(result)", "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import tokenize\n    from io import StringIO\n\n    # Build a list of tuples where each tuple is (type, string)\n    tokens = []\n    for token in iterable:\n        # token might be a TokenInfo or a tuple, so get type and string accordingly\n        if hasattr(token, 'type') and hasattr(token, 'string'):\n            typ = token.type\n            string = token.string\n        else:\n            typ = token[0]\n            string = token[1]\n        tokens.append((typ, string))\n    return tokenize.untokenize(tokens)", "def untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import io\n    from token import COMMENT, NL, NEWLINE\n\n    tokens = list(iterable)\n    out = io.StringIO()\n    prev_type = prev_text = prev_end = None\n    indents = 0\n    for index, tok in enumerate(tokens):\n        tok_type, tok_text = tok.type, tok.string\n        start_line, start_col = tok.start\n        end_line, end_col = tok.end\n\n        if prev_end is not None:\n            # calculate lines and spaces needed between tokens\n            line_diff = start_line - prev_end[0]\n            if line_diff > 0:\n                out.write(\"\\n\" * line_diff)\n                out.write(\" \" * start_col)\n            else:\n                space_diff = start_col - prev_end[1]\n                if space_diff > 0:\n                    out.write(\" \" * space_diff)\n\n        out.write(tok_text)\n        prev_type, prev_text, prev_end = tok_type, tok_text, (end_line, end_col)\n\n    return out.getvalue()"], "test": ["\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import io\n    from token import COMMENT, NL, NEWLINE, INDENT, DEDENT\n\n    out = io.StringIO()\n    prev_end = (1, 0)\n    indents = 0\n    last_line = ''\n    for token in iterable:\n        tok_type, tok_string, start, end, line_text = token[:5]\n        (start_line, start_col) = start\n        (end_line, end_col) = end\n\n        # Add whitespace based on position difference\n        if prev_end[0] < start_line:\n            # new lines in between\n            out.write('\\n' * (start_line - prev_end[0]))\n            out.write(' ' * start_col)\n        else:\n            # same line\n            out.write(' ' * (start_col - prev_end[1]))\n\n        out.write(tok_string)\n        prev_end = (end_line, end_col)\n\n    return out.getvalue()\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import tokenize\n\n    result = []\n    prev_end = (1, 0)\n    linebuf = ''\n\n    for tok in iterable:\n        toknum, tokval = tok[:2]\n        start = getattr(tok, 'start', None)\n        end = getattr(tok, 'end', None)\n        if start is None or end is None:\n            # If token does not have start/end, just append value\n            result.append(tokval)\n            continue\n\n        srow, scol = start\n        erow, ecol = end\n        # Handle gaps between tokens\n        if prev_end[0] < srow:\n            # Newlines needed\n            linebuf += '\\n' * (srow - prev_end[0])\n            linebuf += ' ' * scol\n        else:\n            # Spaces needed on the same line\n            linebuf += ' ' * (scol - prev_end[1])\n\n        linebuf += tokval\n        prev_end = (erow, ecol)\n\n    return linebuf\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).__next__\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n\n    from io import StringIO\n    from tokenize import TokenInfo\n\n    # Buffer to accumulate result\n    result = []\n    # Track previous token ending position (row, col)\n    prev_end = (0, 0)\n    # Track previous line text\n    line_buffer = \"\"\n\n    for tok in iterable:\n        # Unpack token info fields: number, string, start, end, line\n        # Since iterable elements have at least two elements, but ideally are TokenInfo objects\n        # Defensive: support tuple-like token or TokenInfo object\n        if isinstance(tok, TokenInfo):\n            toknum, tokval, startpos, endpos, line = tok\n        else:\n            # Minimal expected: first two items token number and string\n            toknum = tok[0]\n            tokval = tok[1]\n            startpos = tok[2] if len(tok) > 2 else (0, 0)\n            endpos = tok[3] if len(tok) > 3 else (0, 0)\n            line = tok[4] if len(tok) > 4 else \"\"\n\n        srow, scol = startpos\n        erow, ecol = endpos\n\n        # Handle line and column gaps by adding whitespace and newlines as needed\n        if prev_end[0] < srow:\n            # New lines between previous token end and current token start\n            # Append newlines to join lines\n            for i in range(prev_end[0], srow - 1):\n                result.append('\\n')\n            # Reset column position after new lines\n            prev_end = (srow, 0)\n\n        if prev_end[1] < scol:\n            # Spaces between previous token end column and current token start column\n            result.append(' ' * (scol - prev_end[1]))\n\n        result.append(tokval)\n        prev_end = (erow, ecol)\n\n    return ''.join(result)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tokin generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import tokenize\n    from io import StringIO\n\n    # Build a list of tuples where each tuple is (type, string)\n    tokens = []\n    for token in iterable:\n        # token might be a TokenInfo or a tuple, so get type and string accordingly\n        if hasattr(token, 'type') and hasattr(token, 'string'):\n            typ = token.type\n            string = token.string\n        else:\n            typ = token[0]\n            string = token[1]\n        tokens.append((typ, string))\n    return tokenize.untokenize(tokens)\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n", "\nimport sys\nsys.path.insert(1, \"/input/test-apps/black/src\")\nimport unittest, pytest\nimport math\nimport random\nimport re\nimport copy\nimport datetime\nimport itertools\nimport collections\nimport heapq\nimport statistics\nimport functools\nimport hashlib\nimport numpy\nimport numpy as np\nimport string\nfrom typing import *\nfrom collections import *\nimport pickle\nimport timeout_decorator\n\n\n\n\n\nfrom typing import (\n    Callable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Text,\n    Tuple,\n    Pattern,\n    Union,\n    cast,\n)\nfrom blib2to3.pgen2.token import *\nfrom blib2to3.pgen2.grammar import Grammar\n\n__author__ = \"Ka-Ping Yee <ping@lfw.org>\"\n__credits__ = \"GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro\"\n\nimport regex as re\nfrom codecs import BOM_UTF8, lookup\nfrom blib2to3.pgen2.token import *\n\nfrom blib2to3.pgen2 import token\n\n__all__ = [x for x in dir(token) if x[0] != \"_\"] + [\n    \"tokenize\",\n    \"generate_tokens\",\n    \"untokenize\",\n]\ndel token\n\n\ndef group(*choices):\n    return \"(\" + \"|\".join(choices) + \")\"\n\n\ndef any(*choices):\n    return group(*choices) + \"*\"\n\n\ndef maybe(*choices):\n    return group(*choices) + \"?\"\n\n\ndef _combinations(*l):\n    return set(x + y for x in l for y in l + (\"\",) if x.casefold() != y.casefold())\n\n\nWhitespace = r\"[ \\f\\t]*\"\nComment = r\"#[^\\r\\n]*\"\nIgnore = Whitespace + any(r\"\\\\\\r?\\n\" + Whitespace) + maybe(Comment)\nName = (  # this is invalid but it's fine because Name comes after Number in all groups\n    r\"\\w+\"\n)\n\nBinnumber = r\"0[bB]_?[01]+(?:_[01]+)*\"\nHexnumber = r\"0[xX]_?[\\da-fA-F]+(?:_[\\da-fA-F]+)*[lL]?\"\nOctnumber = r\"0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?\"\nDecnumber = group(r\"[1-9]\\d*(?:_\\d+)*[lL]?\", \"0[lL]?\")\nIntnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)\nExponent = r\"[eE][-+]?\\d+(?:_\\d+)*\"\nPointfloat = group(r\"\\d+(?:_\\d+)*\\.(?:\\d+(?:_\\d+)*)?\", r\"\\.\\d+(?:_\\d+)*\") + maybe(\n    Exponent\n)\nExpfloat = r\"\\d+(?:_\\d+)*\" + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r\"\\d+(?:_\\d+)*[jJ]\", Floatnumber + r\"[jJ]\")\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n_litprefix = r\"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?\"\nTriple = group(_litprefix + \"'''\", _litprefix + '\"\"\"')\nString = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"',\n)\n\nOperator = group(\n    r\"\\*\\*=?\",\n    r\">>=?\",\n    r\"<<=?\",\n    r\"<>\",\n    r\"!=\",\n    r\"//=?\",\n    r\"->\",\n    r\"[+\\-*/%&@|^=<>:]=?\",\n    r\"~\",\n)\n\nBracket = \"[][(){}]\"\nSpecial = group(r\"\\r?\\n\", r\"[:;.,`@]\")\nFunny = group(Operator, Bracket, Special)\n\nContStr = group(\n    _litprefix + r\"'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" + group(\"'\", r\"\\\\\\r?\\n\"),\n    _litprefix + r'\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' + group('\"', r\"\\\\\\r?\\n\"),\n)\nPseudoExtras = group(r\"\\\\\\r?\\n\", Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\npseudoprog = re.compile(PseudoToken, re.UNICODE)\nsingle3prog = re.compile(Single3)\ndouble3prog = re.compile(Double3)\n\n_strprefixes = (\n    _combinations(\"r\", \"R\", \"f\", \"F\")\n    | _combinations(\"r\", \"R\", \"b\", \"B\")\n    | {\"u\", \"U\", \"ur\", \"uR\", \"Ur\", \"UR\"}\n)\n\nendprogs = {\n    \"'\": re.compile(Single),\n    '\"': re.compile(Double),\n    \"'''\": single3prog,\n    '\"\"\"': double3prog,\n    **{f\"{prefix}'''\": single3prog for prefix in _strprefixes},\n    **{f'{prefix}\"\"\"': double3prog for prefix in _strprefixes},\n    **{prefix: None for prefix in _strprefixes},\n}\n\ntriple_quoted = (\n    {\"'''\", '\"\"\"'}\n    | {f\"{prefix}'''\" for prefix in _strprefixes}\n    | {f'{prefix}\"\"\"' for prefix in _strprefixes}\n)\nsingle_quoted = (\n    {\"'\", '\"'}\n    | {f\"{prefix}'\" for prefix in _strprefixes}\n    | {f'{prefix}\"' for prefix in _strprefixes}\n)\n\ntabsize = 8\n\n\nclass TokenError(Exception):\n    pass\n\n\nclass StopTokenizing(Exception):\n    pass\n\n\ndef printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line):  # for testing\n    (srow, scol) = xxx_todo_changeme\n    (erow, ecol) = xxx_todo_changeme1\n    print(\n        \"%d,%d-%d,%d:\\t%s\\t%s\" % (srow, scol, erow, ecol, tok_name[type], repr(token))\n    )\n\n\nCoord = Tuple[int, int]\nTokenEater = Callable[[int, Text, Coord, Coord, Text], None]\n\n\ndef tokenize(readline: Callable[[], Text], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\n\nGoodTokenInfo = Tuple[int, Text, Coord, Coord, Text]\nTokenInfo = Union[Tuple[int, str], GoodTokenInfo]\n\n\nclass Untokenizer:\n\n    tokens: List[Text]\n    prev_row: int\n    prev_col: int\n\n    def __init__(self) -> None:\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start: Coord) -> None:\n        row, col = start\n        assert row <= self.prev_row\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable: Iterable[TokenInfo]) -> Text:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(cast(Tuple[int, str], t), iterable)\n                break\n            tok_type, token, start, end, line = cast(\n                Tuple[int, Text, Coord, Coord, Text], t\n            )\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token: Tuple[int, Text], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\n\ncookie_re = re.compile(r\"^[ \\t\\f]*#.*?coding[:=][ \\t]*([-\\w.]+)\", re.ASCII)\nblank_re = re.compile(br\"^[ \\t\\f]*(?:[#\\r\\n]|$)\", re.ASCII)\n\n\ndef _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc\n\n\ndef detect_encoding(readline: Callable[[], bytes]) -> Tuple[str, List[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return bytes()\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]\n\n\ndef untokenize(iterable: Iterable[TokenInfo]) -> Text:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n    Untokenized source will match input exactly\n\n    Round-trip invariant for limited input:\n    # Output text will tokenize the back to the input\n    t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n    newcode = untokenize(t1)\n    readline = iter(newcode.splitlines(1)).next\n    t2 = [tok[:2] for tok in generate_tokens(readline)]\n    assert t1 == t2\n    \"\"\"\n    import io\n    from token import COMMENT, NL, NEWLINE\n\n    tokens = list(iterable)\n    out = io.StringIO()\n    prev_type = prev_text = prev_end = None\n    indents = 0\n    for index, tok in enumerate(tokens):\n        tok_type, tok_text = tok.type, tok.string\n        start_line, start_col = tok.start\n        end_line, end_col = tok.end\n\n        if prev_end is not None:\n            # calculate lines and spaces needed between tokens\n            line_diff = start_line - prev_end[0]\n            if line_diff > 0:\n                out.write(\"\\n\" * line_diff)\n                out.write(\" \" * start_col)\n            else:\n                space_diff = start_col - prev_end[1]\n                if space_diff > 0:\n                    out.write(\" \" * space_diff)\n\n        out.write(tok_text)\n        prev_type, prev_text, prev_end = tok_type, tok_text, (end_line, end_col)\n\n    return out.getvalue()\n\n\ndef generate_tokens(\n    readline: Callable[[], Text], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    numchars = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: Tuple[int, int]\n    endprog: Pattern[str]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum = lnum + 1\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (\n                    STRING,\n                    contstr + line[:end],\n                    strstart,\n                    (lnum, end),\n                    contline + line,\n                )\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column = column + 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos = pos + 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, pos + len(comment_token)),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0:\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)  # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    if token[-1] == \"\\n\":  # continued string\n                        strstart = (lnum, start)\n                        endprog = (\n                            endprogs[initial]\n                            or endprogs[token[1]]\n                            or endprogs[token[2]]\n                        )\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        yield (STRING, token, spos, epos, line)\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                else:\n                    if initial in \"([{\":\n                        parenlev = parenlev + 1\n                    elif initial in \")]}\":\n                        parenlev = parenlev - 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos = pos + 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n\n\n\nimport pickle\ndef test_0():\n    assert 123.e+45 == float(eval(untokenize([(NUMBER, \"123.e+45\")])))\ntest_0()\n\ndef test_2():\n    assert untokenize([(3, 'a+'), (3, 'b')]) == 'a+b'\ntest_2()\n\ndef test_12():\n    assert 0x123 == int(eval(untokenize([(NUMBER, \"0x123\")])))\ntest_12()\n\ndef test_38():\n    assert untokenize( []) == \"\"\ntest_38()\n\ndef test_39():\n    assert untokenize([(3, 'if'), (3, ' '), (10, 'x'), (3, ':'), (3, ' '), (10, 'pass')]) == 'if x: pass'\ntest_39()\n\ndef test_57():\n    assert 123.e45 == float(eval(untokenize([(NUMBER, \"123.e45\")])))\ntest_57()\n\ndef test_58():\n    assert __name__ != '__main__' or untokenize(tokenize('def foo(): pass\\n')) == 'def foo(): pass\\n'\ntest_58()\n\ndef test_59():\n    assert print( untokenize( [(1, 'import'), (1, 'sys'), (44, '\\n')] )) == None\ntest_59()\n\ndef test_65():\n    assert 123 == int(eval(untokenize([(NUMBER, \"123\")])))\ntest_65()\n\ndef test_73():\n    assert 123. == float(eval(untokenize([(NUMBER, \"123.\")])))\ntest_73()\n\ndef test_5():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_5\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' ')]) == output\ntest_5()\n\ndef test_6():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_6\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((1, \"Hello\"), (1, \",\"), (1, \"world\"))) == output\ntest_6()\n\ndef test_7():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_7\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(4, \"x\"), (10, \"=\"), (4, \"5\"), (4, \"+\"), (4, \"8\")]) == output\ntest_7()\n\ndef test_8():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_8\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_8()\n\ndef test_9():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_9\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_9()\n\ndef test_17():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_17\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, ''), (2, 'a')]) == output\ntest_17()\n\ndef test_18():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_18\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' ')]) == output\ntest_18()\n\ndef test_22():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_22\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' '), (1, ' ')]) == output\ntest_22()\n\ndef test_24():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_24\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (0, ' ')]) == output\ntest_24()\n\ndef test_27():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_27\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(\n        [(1, 'import'), (1, 'sys'), (1, '\\n'), (1, 'print'), (1, ' '), (1, 'sys'), (1, '.'),\n         (1, 'stdout'), (1, '.'), (1, 'write'), (1, '('), (3, \"'\\\\ntest\\\\n'\"), (1, ')'), (1, ';'),\n         (1, '\\n')]) == output\ntest_27()\n\ndef test_30():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_30\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (OP, '='), (NAME, 'd')]) == output\ntest_30()\n\ndef test_31():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_31\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, \"hello\"), (NAME, \"world\")]) == output\ntest_31()\n\ndef test_40():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_40\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n')]) == output\ntest_40()\n\ndef test_41():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_41\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([\n            (STRING, '\"hello\"'), \n            (COMMENT, '# single comment'), \n            (STRING, '\"world\"'), \n            (NEWLINE, '\\n'),\n            ]) == output\ntest_41()\n\ndef test_44():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_44\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(NAME, 'x'), (NAME, 'y')]) == output\ntest_44()\n\ndef test_45():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_45\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'def'), (1, 'func(x):'), (1, 'print'), (1, '('), (1, 'x'), (1, ')'), (1, ':'), (1, 'pass')]) == output\ntest_45()\n\ndef test_46():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_46\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1'), (3, '\\n')]) == output\ntest_46()\n\ndef test_51():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_51\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' ')]) == output\ntest_51()\n\ndef test_53():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_53\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, '\\n'), (1, '    '), (2, '1'), (2, '+'), (2, '1')]) == output\ntest_53()\n\ndef test_62():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_62\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize(((2, \"Hello\"), (2, \",\"), (2, \"world\"))) == output\ntest_62()\n\ndef test_68():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_68\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (0, ' ')]) == output\ntest_68()\n\ndef test_70():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_70\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(0, '\\n'), (3, '\\n'), (0, ' '), (0, ' '), (1, ' '), (1, ' ')]) == output\ntest_70()\n\ndef test_71():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_71\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize( [(1, '\\n'), (3, '\\n')]) == output\ntest_71()\n\ndef test_74():\n    with open(\"/output/test-apps+black+src/test_output/blib2to3+pgen2+tokenize/untokenize/test_74\", \"rb\") as f:\n        output = pickle.load(f)\n    assert untokenize([(1, 'a'), (1, ' '), (1, 'b')]) == output\ntest_74()\n\n\n"]}
